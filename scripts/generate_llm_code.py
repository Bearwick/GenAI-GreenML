#!/usr/bin/env python3
import os
import re
import subprocess
import sys
from typing import List, Optional


BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
REPOS_DIR = os.path.join(BASE_DIR, "repos")
BASE_NAME = "GENAIGREENML"
GEMINI_REQUESTS = 0
GEMINI_SLEEP_EVERY = 10
SCRIPTS_VENV_PY = os.path.join(BASE_DIR, "scripts", "venv", "bin", "python")

ALL_LLMS = ["gemini", "chatgpt", "codex", "claude", "groq"]
gemini_api = None
chatgpt_api = None
codex_api = None
claude_api = None
groq_api = None

DATASET_EXTS = [
    ".csv",
    ".tsv",
    ".txt",
    ".json",
    ".jsonl",
    ".xlsx",
    ".xls",
    ".parquet",
    ".feather",
    ".npy",
    ".npz",
    ".pkl",
    ".pickle",
    ".h5",
    ".hdf5",
    ".arff",
    ".sav",
    ".mat",
]


def find_files(project: str, exts: Optional[List[str]] = None) -> List[str]:
    matches = []
    for root, _, files in os.walk(project):
        if "/venv/" in root or "/.venv/" in root or "/.git/" in root or "/." in root:
            continue
        for name in files:
            if name == "requirements.txt":
                continue
            if exts:
                if not any(name.endswith(ext) for ext in exts):
                    continue
            matches.append(os.path.join(root, name))
    return matches


def pick_source_file(project: str) -> Optional[str]:
    original = os.path.join(project, "original.py")
    if os.path.isfile(original):
        return original
    py_files = [f for f in find_files(project, [".py"]) if not os.path.basename(f).startswith(BASE_NAME)]
    return py_files[0] if len(py_files) == 1 else None


def find_dataset_file(project: str, src_file: str) -> Optional[str]:
    # 1) Prefer GENAIGREENMLDATASET.*
    preferred = [
        f
        for f in find_files(project)
        if os.path.basename(f).startswith("GENAIGREENMLDATASET.")
    ]
    if preferred:
        return preferred[0]

    # 2) Try to find dataset referenced in code.
    try:
        with open(src_file, "r", encoding="utf-8", errors="ignore") as f:
            code = f.read()
    except OSError:
        code = ""

    pattern = re.compile(r"([A-Za-z0-9_./-]+(" + "|".join(re.escape(e) for e in DATASET_EXTS) + r"))")
    match = pattern.search(code)
    if match:
        ref = os.path.basename(match.group(1))
        for f in find_files(project):
            if os.path.basename(f) == ref:
                return f

    # 3) Fallback to first dataset file.
    candidates = [f for f in find_files(project, DATASET_EXTS)]
    return candidates[0] if candidates else None


def dataset_headers(path: Optional[str]) -> str:
    if not path:
        return ""
    try:
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            return f.readline().strip()
    except OSError:
        return ""


def generate_with_gemini(mode: str, src_file: str, headers: str) -> str:
    global GEMINI_REQUESTS
    GEMINI_REQUESTS += 1
    if GEMINI_REQUESTS % GEMINI_SLEEP_EVERY == 0:
        print(f"[i] Gemini rate limit pause (60s) after {GEMINI_REQUESTS} requests")
        import time

        time.sleep(60)

    try:
        with open(src_file, "r", encoding="utf-8", errors="ignore") as f:
            source_code = f.read()
        return gemini_api.generate_code(mode, source_code, headers).strip()
    except Exception:
        return ""


def generate_with_chatgpt(mode: str, src_file: str, headers: str) -> str:
    try:
        with open(src_file, "r", encoding="utf-8", errors="ignore") as f:
            source_code = f.read()
        return chatgpt_api.generate_code(mode, source_code, headers).strip()
    except Exception:
        return ""


def generate_with_codex(mode: str, src_file: str, headers: str) -> str:
    try:
        with open(src_file, "r", encoding="utf-8", errors="ignore") as f:
            source_code = f.read()
        return codex_api.generate_code(mode, source_code, headers).strip()
    except Exception:
        return ""


def generate_with_claude(mode: str, src_file: str, headers: str) -> str:
    try:
        with open(src_file, "r", encoding="utf-8", errors="ignore") as f:
            source_code = f.read()
        return claude_api.generate_code(mode, source_code, headers).strip()
    except Exception:
        return ""


def generate_with_groq(mode: str, src_file: str, headers: str) -> str:
    try:
        with open(src_file, "r", encoding="utf-8", errors="ignore") as f:
            source_code = f.read()
        return groq_api.generate_code(mode, source_code, headers).strip()
    except Exception:
        return ""


def write_output(out_file: str, llm_name: str, mode: str, code: str) -> None:
    with open(out_file, "w", encoding="utf-8") as f:
        f.write(f"# Generated by {os.path.basename(__file__)}\n")
        f.write(f"# LLM: {llm_name}\n")
        f.write(f"# Mode: {mode}\n\n")
        f.write(code)


def update_requirements(project: str) -> None:
    venv_py = os.path.join(project, "venv", "bin", "python")
    if not os.path.isfile(venv_py):
        print(f"[!] venv python not found; skipping requirements update for {os.path.basename(project)}")
        return

    subprocess.run(
        [venv_py, "-m", "pip", "show", "pipreqs"],
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )
    subprocess.run(
        [venv_py, "-m", "pip", "install", "-q", "pipreqs"],
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )
    subprocess.run(
        [venv_py, "-m", "pipreqs", project, "--force", "--encoding", "utf-8", "--ignore", os.path.join(project, "venv")],
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )


def main() -> None:
    if os.path.isfile(SCRIPTS_VENV_PY) and os.path.realpath(sys.executable) != os.path.realpath(SCRIPTS_VENV_PY):
        os.execv(SCRIPTS_VENV_PY, [SCRIPTS_VENV_PY] + sys.argv)
    elif not os.path.isfile(SCRIPTS_VENV_PY):
        print("[!] scripts venv not found. Create it with: python3 -m venv scripts/venv")

    if not os.path.isdir(REPOS_DIR):
        raise SystemExit(f"Repos directory not found: {REPOS_DIR}")

    llms = os.sys.argv[1:] or ALL_LLMS
    sys.path.insert(0, os.path.join(BASE_DIR, "scripts", "APIs"))
    global gemini_api
    global chatgpt_api
    global codex_api
    global claude_api
    global groq_api
    try:
        import gemini_api  # type: ignore
    except Exception:
        gemini_api = None
    try:
        import chatgpt_api  # type: ignore
    except Exception:
        chatgpt_api = None
    try:
        import openai_codex_api as codex_api  # type: ignore
    except Exception:
        codex_api = None
    try:
        import claude_api  # type: ignore
    except Exception:
        claude_api = None
    try:
        import groq_api  # type: ignore
    except Exception:
        groq_api = None

    if chatgpt_api is None or codex_api is None:
        print("[!] OpenAI SDK missing in scripts/venv? Install with: scripts/venv/bin/pip install openai")
    if claude_api is None:
        print("[!] Claude SDK missing in scripts/venv? Install with: scripts/venv/bin/pip install anthropic")
    if groq_api is None:
        print("[!] Groq SDK missing in scripts/venv? Install with: scripts/venv/bin/pip install openai")

    for project in sorted(os.listdir(REPOS_DIR)):
        project_path = os.path.join(REPOS_DIR, project)
        if not os.path.isdir(project_path):
            continue

        if find_files(project_path, None) and any(
            os.path.basename(f).startswith("GENAIGREENMLDATASET.") for f in find_files(project_path, None)
        ):
            print(f"[i] Skipping {project} (GENAIGREENMLDATASET present)")
            continue

        print(f"[*] Project: {project}")

        src_file = pick_source_file(project_path)
        if not src_file:
            print(f"[i] Skipping {project} (no single python file or original.py)")
            continue

        headers = dataset_headers(find_dataset_file(project_path, src_file))

        primary_llm = llms[0]
        if primary_llm == "gemini" and gemini_api is not None:
            code = generate_with_gemini("original_telemetry", src_file, headers)
            if code:
                write_output(os.path.join(project_path, f"{BASE_NAME}_original_telemetry.py"), primary_llm, "original_telemetry", code)
            else:
                print("[i] Skipping gemini (not implemented or no response)")
        elif primary_llm == "chatgpt" and chatgpt_api is not None:
            code = generate_with_chatgpt("original_telemetry", src_file, headers)
            if code:
                write_output(os.path.join(project_path, f"{BASE_NAME}_original_telemetry.py"), primary_llm, "original_telemetry", code)
            else:
                print("[i] Skipping chatgpt (not implemented or no response)")
        elif primary_llm == "codex" and codex_api is not None:
            code = generate_with_codex("original_telemetry", src_file, headers)
            if code:
                write_output(os.path.join(project_path, f"{BASE_NAME}_original_telemetry.py"), primary_llm, "original_telemetry", code)
            else:
                print("[i] Skipping codex (not implemented or no response)")
        elif primary_llm == "claude" and claude_api is not None:
            code = generate_with_claude("original_telemetry", src_file, headers)
            if code:
                write_output(os.path.join(project_path, f"{BASE_NAME}_original_telemetry.py"), primary_llm, "original_telemetry", code)
            else:
                print("[i] Skipping claude (not implemented or no response)")
        elif primary_llm == "groq" and groq_api is not None:
            code = generate_with_groq("original_telemetry", src_file, headers)
            if code:
                write_output(os.path.join(project_path, f"{BASE_NAME}_original_telemetry.py"), primary_llm, "original_telemetry", code)
            else:
                print("[i] Skipping groq (not implemented or no response)")
        else:
            print(f"[i] Skipping {primary_llm} (not implemented or no response)")

        for llm in llms:
            if llm == "gemini" and gemini_api is not None:
                code = generate_with_gemini("assisted", src_file, headers)
                if code:
                    write_output(os.path.join(project_path, f"{BASE_NAME}_assisted_{llm}.py"), llm, "assisted", code)
                else:
                    print("[i] Skipping gemini (not implemented or no response)")

                code = generate_with_gemini("autonomous", src_file, headers)
                if code:
                    write_output(os.path.join(project_path, f"{BASE_NAME}_autonomous_{llm}.py"), llm, "autonomous", code)
                else:
                    print("[i] Skipping gemini (not implemented or no response)")
            elif llm == "chatgpt" and chatgpt_api is not None:
                code = generate_with_chatgpt("assisted", src_file, headers)
                if code:
                    write_output(os.path.join(project_path, f"{BASE_NAME}_assisted_{llm}.py"), llm, "assisted", code)
                else:
                    print("[i] Skipping chatgpt (not implemented or no response)")

                code = generate_with_chatgpt("autonomous", src_file, headers)
                if code:
                    write_output(os.path.join(project_path, f"{BASE_NAME}_autonomous_{llm}.py"), llm, "autonomous", code)
                else:
                    print("[i] Skipping chatgpt (not implemented or no response)")
            elif llm == "codex" and codex_api is not None:
                code = generate_with_codex("assisted", src_file, headers)
                if code:
                    write_output(os.path.join(project_path, f"{BASE_NAME}_assisted_{llm}.py"), llm, "assisted", code)
                else:
                    print("[i] Skipping codex (not implemented or no response)")

                code = generate_with_codex("autonomous", src_file, headers)
                if code:
                    write_output(os.path.join(project_path, f"{BASE_NAME}_autonomous_{llm}.py"), llm, "autonomous", code)
                else:
                    print("[i] Skipping codex (not implemented or no response)")
            elif llm == "claude" and claude_api is not None:
                code = generate_with_claude("assisted", src_file, headers)
                if code:
                    write_output(os.path.join(project_path, f"{BASE_NAME}_assisted_{llm}.py"), llm, "assisted", code)
                else:
                    print("[i] Skipping claude (not implemented or no response)")

                code = generate_with_claude("autonomous", src_file, headers)
                if code:
                    write_output(os.path.join(project_path, f"{BASE_NAME}_autonomous_{llm}.py"), llm, "autonomous", code)
                else:
                    print("[i] Skipping claude (not implemented or no response)")
            elif llm == "groq" and groq_api is not None:
                code = generate_with_groq("assisted", src_file, headers)
                if code:
                    write_output(os.path.join(project_path, f"{BASE_NAME}_assisted_{llm}.py"), llm, "assisted", code)
                else:
                    print("[i] Skipping groq (not implemented or no response)")

                code = generate_with_groq("autonomous", src_file, headers)
                if code:
                    write_output(os.path.join(project_path, f"{BASE_NAME}_autonomous_{llm}.py"), llm, "autonomous", code)
                else:
                    print("[i] Skipping groq (not implemented or no response)")
            else:
                print(f"[i] Skipping {llm} (not implemented or no response)")

        update_requirements(project_path)

    print("[+] Done")


if __name__ == "__main__":
    main()
