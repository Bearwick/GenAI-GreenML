# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge


RANDOM_STATE = 42
DATASET_PATH = "heart.csv"


def _normalize_columns(cols):
    out = []
    for c in cols:
        c = str(c)
        c = re.sub(r"\s+", " ", c.strip())
        if c.lower().startswith("unnamed:"):
            out.append(None)
        else:
            out.append(c)
    return out


def _try_read_csv(path):
    # Try default CSV parsing first
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d):
        if d is None or d.empty:
            return True
        # If it became a single wide string column, parsing likely failed (wrong separator)
        if d.shape[1] == 1:
            col0 = d.columns[0]
            sample = d[col0].astype(str).head(5)
            if sample.str.contains(r"[;,\s]\d").any():
                return True
        return False

    if _looks_wrong(df):
        # Retry with semicolon separator and comma decimal (common European format)
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            # Final fallback: python engine with delimiter inference
            df = pd.read_csv(path, sep=None, engine="python")
    return df


def _sanitize_dataframe(df):
    df = df.copy()
    norm_cols = _normalize_columns(df.columns)
    keep_mask = [c is not None for c in norm_cols]
    df = df.loc[:, keep_mask]
    df.columns = [c for c in norm_cols if c is not None]

    # If headers are actually first data row (common in some files), try to recover:
    # Detect if all column names look numeric and data rows also numeric-like.
    def _is_numberish(s):
        try:
            float(str(s).replace(",", "."))
            return True
        except Exception:
            return False

    if df.shape[1] > 1 and all(_is_numberish(c) for c in df.columns):
        # Promote current header row into data if it seems numeric
        new_row = pd.DataFrame([list(df.columns)], columns=[f"col_{i}" for i in range(df.shape[1])])
        df2 = df.copy()
        df2.columns = [f"col_{i}" for i in range(df.shape[1])]
        df = pd.concat([new_row, df2], ignore_index=True)

    # Drop fully empty rows/cols
    df = df.dropna(axis=0, how="all").dropna(axis=1, how="all")
    return df


def _coerce_numeric_where_possible(df):
    df = df.copy()
    for c in df.columns:
        if df[c].dtype == object:
            # Try to coerce; keep original if mostly non-numeric
            coerced = pd.to_numeric(df[c].astype(str).str.replace(",", ".", regex=False), errors="coerce")
            non_na_ratio = coerced.notna().mean() if len(coerced) else 0.0
            if non_na_ratio >= 0.7:
                df[c] = coerced
    return df


def _choose_target_and_task(df):
    # Prefer an obvious target name if present
    lowered = {c.lower(): c for c in df.columns}
    for cand in ["target", "class", "label", "outcome", "y"]:
        if cand in lowered:
            ycol = lowered[cand]
            y = df[ycol]
            return ycol, y, None

    # Otherwise choose last column by default if it is usable
    last_col = df.columns[-1]
    y = df[last_col]
    # If last column is completely empty, search for a usable numeric column
    if y.isna().all():
        num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
        for c in reversed(num_cols):
            if not df[c].isna().all():
                return c, df[c], None

    return last_col, y, None


def _is_classification_target(y):
    y_non_na = y.dropna()
    if y_non_na.empty:
        return False
    # If integer-like with small number of unique values => classification
    if pd.api.types.is_bool_dtype(y_non_na):
        return True
    if pd.api.types.is_numeric_dtype(y_non_na):
        uniq = pd.unique(y_non_na)
        uniq = uniq[~pd.isna(uniq)]
        if len(uniq) <= 20:
            # check "integer-ish"
            vals = pd.Series(uniq).astype(float)
            if np.all(np.isclose(vals, np.round(vals))):
                return True
    # If object with limited unique values => classification
    if y_non_na.dtype == object:
        if y_non_na.nunique(dropna=True) <= 50:
            return True
    return False


def _bounded_regression_score(y_true, y_pred):
    # Convert regression quality into stable [0,1]: 1/(1+MSE/var)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mse = np.mean((y_true - y_pred) ** 2) if y_true.size else np.inf
    var = np.var(y_true) if y_true.size else 0.0
    denom = 1.0 + (mse / (var + 1e-12))
    score = 1.0 / denom
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    df = _try_read_csv(DATASET_PATH)
    df = _sanitize_dataframe(df)
    df = _coerce_numeric_where_possible(df)

    assert df is not None and not df.empty, "Dataset is empty after loading/sanitization."

    target_col, y_raw, _ = _choose_target_and_task(df)

    # Build X/y and drop rows with missing target
    df_model = df.copy()
    if target_col in df_model.columns:
        df_model = df_model.loc[~df_model[target_col].isna()].copy()

    assert not df_model.empty, "No rows with non-missing target."

    y = df_model[target_col]
    X = df_model.drop(columns=[target_col], errors="ignore")

    # If there are no feature columns, create a constant feature
    if X.shape[1] == 0:
        X = pd.DataFrame({"const": np.ones(len(df_model), dtype=float)})

    # Replace inf safely
    X = X.replace([np.inf, -np.inf], np.nan)

    # Determine task
    is_clf = _is_classification_target(y)

    # If classification but <2 classes, fallback to regression pipeline for robustness
    if is_clf:
        y_non_na = y.dropna()
        n_classes = y_non_na.nunique(dropna=True)
        if n_classes < 2:
            is_clf = False

    # Identify column types
    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if is_clf:
        # Logistic regression is strong + lightweight; use liblinear for small datasets/CPU
        model = LogisticRegression(
            solver="liblinear",
            max_iter=300,
            random_state=RANDOM_STATE,
        )
    else:
        # Ridge is stable, cheap, and handles collinearity; suitable fallback
        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)

    clf = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", model),
    ])

    # Prepare y for modeling
    if is_clf:
        # Ensure classification labels are not floats with NaN
        y_clean = y.copy()
        if pd.api.types.is_numeric_dtype(y_clean):
            y_clean = pd.to_numeric(y_clean, errors="coerce")
            y_clean = y_clean.astype(int)
        else:
            y_clean = y_clean.astype(str)
    else:
        y_clean = pd.to_numeric(y, errors="coerce")
        # If y becomes all-NaN, use a trivial constant y to keep pipeline runnable
        if y_clean.isna().all():
            y_clean = pd.Series(np.zeros(len(y), dtype=float), index=y.index)

    # Train/test split
    stratify = None
    if is_clf:
        # Only stratify if enough samples per class
        vc = y_clean.value_counts()
        if (vc.min() if len(vc) else 0) >= 2:
            stratify = y_clean

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_clean, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
    )

    assert len(X_train) > 0 and len(X_test) > 0, "Train/test split produced empty set."

    clf.fit(X_train, y_train)

    if is_clf:
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        y_pred = clf.predict(X_test)
        # Ensure finite
        y_test_arr = np.asarray(pd.to_numeric(y_test, errors="coerce"), dtype=float)
        y_pred_arr = np.asarray(pd.to_numeric(pd.Series(y_pred), errors="coerce"), dtype=float)
        mask = np.isfinite(y_test_arr) & np.isfinite(y_pred_arr)
        if mask.sum() == 0:
            accuracy = 0.0
        else:
            accuracy = _bounded_regression_score(y_test_arr[mask], y_pred_arr[mask])

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight, CPU-friendly models (LogisticRegression with liblinear; Ridge fallback) to minimize compute/energy.
# - Employs a single sklearn Pipeline + ColumnTransformer to avoid repeated preprocessing and ensure reproducibility.
# - Robust CSV loading retries alternate separators/decimals; normalizes headers and drops 'Unnamed' columns to prevent schema issues.
# - Coerces numeric-like object columns to numeric (errors='coerce') and handles NaN/inf via imputers to keep runtime stable.
# - Task detection is schema-agnostic; if classification is ill-posed (<2 classes), falls back to regression and maps performance to a bounded [0,1] proxy score: 1/(1+MSE/var).