# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
import numpy as np
import pandas as pd

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator


SEED = 12345


def _set_reproducible_seeds(seed: int = SEED) -> None:
    random.seed(seed)
    np.random.seed(seed)


def _read_csv_robust(path: str, headers_line: str) -> pd.DataFrame:
    expected_ncols = len(headers_line.split())
    header_names = [f"c{i}" for i in range(expected_ncols)]

    def _looks_wrong(df: pd.DataFrame) -> bool:
        if df is None or df.empty:
            return True
        if df.shape[1] != expected_ncols:
            return True
        sample = df.head(20)
        numeric_ratio = sample.apply(pd.to_numeric, errors="coerce").notna().to_numpy().mean()
        return numeric_ratio < 0.95

    try:
        df = pd.read_csv(path, header=None)
        if df.shape[1] == 1:
            df = pd.read_csv(path, header=None, delim_whitespace=True, engine="python")
        if _looks_wrong(df):
            df = pd.read_csv(path, header=None, sep=";", decimal=",", engine="python")
            if df.shape[1] == 1:
                df = pd.read_csv(path, header=None, sep=";", decimal=",", delim_whitespace=True, engine="python")
    except Exception:
        df = pd.read_csv(path, header=None, sep=";", decimal=",", engine="python")

    if df.shape[1] != expected_ncols:
        try:
            df = pd.read_csv(path, header=None, delim_whitespace=True, engine="python")
        except Exception:
            pass

    df = df.iloc[:, :expected_ncols].copy()
    df.columns = header_names
    for c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    df = df.dropna(axis=0, how="any")
    return df


def _build_spark(app_name: str = "SparkMLClassification_Heart") -> SparkSession:
    spark = (
        SparkSession.builder.appName(app_name)
        .config("spark.sql.shuffle.partitions", "8")
        .config("spark.default.parallelism", "8")
        .config("spark.ui.enabled", "false")
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("ERROR")
    return spark


def classify(dataset_path: str, dataset_headers: str) -> float:
    _set_reproducible_seeds(SEED)
    spark = _build_spark()

    try:
        pdf = _read_csv_robust(dataset_path, dataset_headers)

        feature_cols = list(pdf.columns[:12])
        thal_col = pdf.columns[12]

        pdf = pdf.iloc[:, :13].copy()
        pdf["label"] = np.where(pdf[thal_col].isin([3.0, 7.0]), 0.0, 1.0)

        sdf = spark.createDataFrame(pdf)

        assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
        assembled = assembler.transform(sdf)

        scaler = StandardScaler(inputCol="features", outputCol="Scaled_features", withMean=False, withStd=True)
        scaled = scaler.fit(assembled).transform(assembled).select("Scaled_features", "label")

        training, test = scaled.randomSplit([0.5, 0.5], seed=SEED)

        rf = RandomForestClassifier(
            labelCol="label",
            featuresCol="Scaled_features",
            numTrees=200,
            seed=SEED,
        )
        model = rf.fit(training)
        preds = model.transform(test)

        evaluator = MulticlassClassificationEvaluator(
            labelCol="label",
            predictionCol="prediction",
            metricName="accuracy",
        )
        accuracy = float(evaluator.evaluate(preds))
    finally:
        spark.stop()

    return accuracy


def main() -> None:
    dataset_path = "heart.csv"
    dataset_headers = "70.0 1.0 4.0 130.0 322.0 0.0 2.0 109.0 0.0 2.4 2.0 3.0 3.0 2"
    accuracy = classify(dataset_path, dataset_headers)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused imports and dead/commented code paths to reduce startup overhead and driver-side work.
# - Replaced per-row Python apply() labeling with vectorized NumPy (np.where + isin) to cut CPU time and memory churn.
# - Avoided unnecessary Spark actions (show/collect/plots/prints) that trigger jobs and extra cluster computation.
# - Reduced data movement by selecting only needed columns after scaling (Scaled_features, label) before split/training.
# - Set fixed random seeds (Python/NumPy/Spark split/RF) for reproducible and stable results.
# - Tuned Spark runtime overhead with fewer shuffle partitions and disabled UI to lower background work and resource usage.
# - Implemented robust CSV parsing with fallbacks (default -> whitespace -> ';' + decimal ',') while validating shape/numericity.