# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import math
import pickle
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "dict.pickle"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _looks_like_single_column(df):
    if df.shape[1] <= 1:
        return True
    # If most rows have NaN except one column, parsing likely wrong
    nan_frac = df.isna().mean(axis=0)
    if (nan_frac > 0.95).sum() >= max(1, df.shape[1] - 1):
        return True
    return False


def _load_table_from_pickle(path):
    with open(path, "rb") as f:
        obj = pickle.load(f)

    # Try to coerce into a DataFrame robustly
    if isinstance(obj, pd.DataFrame):
        return obj
    if isinstance(obj, pd.Series):
        return obj.to_frame()
    if isinstance(obj, dict):
        # Common patterns: dict of arrays, dict with 'data'/'df', dict of dicts
        for key in ["df", "data", "dataset", "frame"]:
            if key in obj:
                try:
                    return pd.DataFrame(obj[key])
                except Exception:
                    pass
        try:
            return pd.DataFrame(obj)
        except Exception:
            # maybe dict-like mapping with inconsistent lengths
            rows = []
            for k, v in obj.items():
                rows.append({"key": k, "value": v})
            return pd.DataFrame(rows)
    if isinstance(obj, (list, tuple, np.ndarray)):
        try:
            return pd.DataFrame(obj)
        except Exception:
            return pd.DataFrame({"value": list(obj)})

    # Fallback: wrap scalar/object
    return pd.DataFrame({"value": [obj]})


def _robust_load_dataset(path):
    ext = os.path.splitext(path)[1].lower()
    if ext in [".pkl", ".pickle"]:
        df = _load_table_from_pickle(path)
    else:
        # CSV fallback logic (even if not used here)
        try:
            df = pd.read_csv(path)
            if _looks_like_single_column(df):
                df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df = pd.read_csv(path, sep=";", decimal=",")

    if not isinstance(df, pd.DataFrame):
        df = pd.DataFrame(df)

    df = df.copy()
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)
    return df


def _coerce_numeric_columns(df, numeric_cols):
    df2 = df.copy()
    for c in numeric_cols:
        df2[c] = pd.to_numeric(df2[c], errors="coerce")
    return df2


def _pick_target_column(df):
    # Prefer columns that look like labels
    lowered = {c: str(c).strip().lower() for c in df.columns}
    preferred_names = ["label", "labels", "target", "y", "class", "outcome"]
    for cname in df.columns[::-1]:
        if lowered.get(cname, "") in preferred_names:
            return cname

    # If there are object columns with few unique values, likely classification label
    obj_cols = [c for c in df.columns if df[c].dtype == "object" or str(df[c].dtype).startswith("string")]
    best = None
    best_card = None
    for c in obj_cols:
        s = df[c].astype(str)
        nunq = s.nunique(dropna=True)
        if 2 <= nunq <= 50:
            if best is None or nunq < best_card:
                best = c
                best_card = nunq
    if best is not None:
        return best

    # Otherwise, choose a non-constant numeric column if any
    numeric_candidates = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() >= 3 and s.nunique(dropna=True) >= 2:
            numeric_candidates.append(c)
    if numeric_candidates:
        # Choose the last numeric non-constant column (often target is last)
        return numeric_candidates[-1]

    # As last resort, pick the last column
    return df.columns[-1] if df.shape[1] else None


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() < 2:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    ss_res = float(np.sum((yt - yp) ** 2))
    ss_tot = float(np.sum((yt - np.mean(yt)) ** 2))
    if ss_tot <= 0:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    # Map to [0,1] for stable "accuracy" proxy
    return float(max(0.0, min(1.0, (r2 + 1.0) / 2.0)))


def main():
    df = _robust_load_dataset(DATASET_PATH)
    assert df is not None and isinstance(df, pd.DataFrame)
    assert df.shape[0] > 0 and df.shape[1] > 0

    # Remove fully empty columns
    df = df.dropna(axis=1, how="all")
    assert df.shape[1] > 0

    target_col = _pick_target_column(df)
    if target_col is None or target_col not in df.columns:
        # Trivial fallback: constant accuracy proxy
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # If no features remain, fallback to trivial baseline
    if X.shape[1] == 0:
        # If classification possible, majority class accuracy; else 0.0
        y_tmp = y_raw.copy()
        if y_tmp.dtype == "object" or str(y_tmp.dtype).startswith("string"):
            y_tmp = y_tmp.astype(str)
        else:
            y_tmp = pd.to_numeric(y_tmp, errors="coerce")
        y_tmp = y_tmp.dropna()
        if y_tmp.shape[0] == 0:
            accuracy = 0.0
        else:
            vals, counts = np.unique(np.asarray(y_tmp), return_counts=True)
            accuracy = float(np.max(counts) / np.sum(counts))
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Identify numeric vs categorical features robustly
    # Coerce candidates with numeric parsing, but keep original for categoricals
    numeric_cols = []
    categorical_cols = []
    for c in X.columns:
        s_num = pd.to_numeric(X[c], errors="coerce")
        non_na = s_num.notna().sum()
        # Consider numeric if enough values convert
        if non_na >= max(3, int(0.6 * len(X))):
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)

    # Coerce numeric columns (safe)
    X = _coerce_numeric_columns(X, numeric_cols)

    # Clean infinities for numeric columns
    if numeric_cols:
        X[numeric_cols] = X[numeric_cols].replace([np.inf, -np.inf], np.nan)

    # Decide task type
    is_classification = False
    y = y_raw.copy()

    # If target is object-like => classification
    if y.dtype == "object" or str(y.dtype).startswith("string"):
        is_classification = True
        y = y.astype(str)
    else:
        # Try numeric conversion; if few unique values, treat as classification
        y_num = pd.to_numeric(y, errors="coerce")
        if y_num.notna().sum() >= 3:
            nunq = y_num.nunique(dropna=True)
            if 2 <= nunq <= 20:
                is_classification = True
                y = y_num
            else:
                is_classification = False
                y = y_num
        else:
            is_classification = True
            y = y.astype(str)

    # Drop rows with missing target
    valid_mask = pd.Series(True, index=df.index)
    if is_classification:
        valid_mask = y.notna()
    else:
        valid_mask = y.notna() & np.isfinite(np.asarray(y, dtype=float))
    X = X.loc[valid_mask]
    y = y.loc[valid_mask]
    assert X.shape[0] > 1

    # Build preprocessing
    transformers = []
    if numeric_cols:
        num_pipe = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ])
        transformers.append(("num", num_pipe, list(numeric_cols)))
    if categorical_cols:
        cat_pipe = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ])
        transformers.append(("cat", cat_pipe, list(categorical_cols)))

    preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    # Split
    if is_classification:
        # Ensure at least two classes
        y_for_split = y
        try:
            classes = pd.Series(y_for_split).dropna().unique()
        except Exception:
            classes = np.unique(np.asarray(y_for_split))
        if len(classes) < 2:
            # Fallback to regression-like constant baseline score
            accuracy = 0.0
            print(f"ACCURACY={accuracy:.6f}")
            return

        # Stratify only if feasible
        stratify = None
        try:
            vc = pd.Series(y_for_split).value_counts(dropna=True)
            if (vc.min() >= 2) and (len(vc) >= 2):
                stratify = y_for_split
        except Exception:
            stratify = None

        X_train, X_test, y_train, y_test = train_test_split(
            X, y_for_split, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
        )
        assert X_train.shape[0] > 0 and X_test.shape[0] > 0

        # Lightweight linear classifier
        clf = LogisticRegression(
            solver="lbfgs",
            max_iter=200,
            n_jobs=1,
            multi_class="auto",
        )
        model = Pipeline(steps=[("prep", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE
        )
        assert X_train.shape[0] > 0 and X_test.shape[0] > 0

        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        model = Pipeline(steps=[("prep", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used lightweight linear models (LogisticRegression / Ridge) for CPU-friendly training and inference.
# - Implemented ColumnTransformer with median/mode imputation + StandardScaler + OneHotEncoder to avoid manual, repeated preprocessing.
# - Kept feature engineering minimal and avoided heavy ensembles/deep learning for energy efficiency.
# - Added robust schema handling: column normalization, unnamed-column drop, numeric coercion, inf/NaN safety, and safe target selection fallback.
# - Regression fallback uses a bounded R2-derived score mapped to [0,1] to keep "ACCURACY" stable and comparable without extra computation.