# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import pickle
import random
from typing import Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler


SEED = 0


def _set_reproducible_seed(seed: int = SEED) -> None:
    random.seed(seed)
    np.random.seed(seed)


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _coerce_numeric_frame(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    for c in out.columns:
        out[c] = pd.to_numeric(out[c], errors="coerce")
    out = out.dropna(how="any")
    return out


def _load_training_xy(csv_path: str) -> Tuple[np.ndarray, np.ndarray]:
    df = _read_csv_robust(csv_path)
    df = _coerce_numeric_frame(df)

    if df.shape[1] < 2:
        raise ValueError("Training CSV must contain at least 1 feature column and 1 label column.")

    x = df.iloc[:, :-1].to_numpy(dtype=np.float64, copy=False)
    y_raw = df.iloc[:, -1].to_numpy(dtype=np.float64, copy=False)

    y = np.where(y_raw > 0, 1, np.where(y_raw < 0, -1, 0)).astype(np.int64, copy=False)
    return x, y


def _load_input_x(csv_path: str) -> np.ndarray:
    df = _read_csv_robust(csv_path)
    df = _coerce_numeric_frame(df)

    if df.shape[1] < 1:
        raise ValueError("Input CSV must contain at least 1 feature column.")
    return df.to_numpy(dtype=np.float64, copy=False)


def _load_model(pickle_path: str) -> MLPClassifier:
    with open(pickle_path, "rb") as f:
        model = pickle.load(f)
    return model


def _build_model() -> MLPClassifier:
    return MLPClassifier(hidden_layer_sizes=(30, 30, 30, 30), max_iter=1000, random_state=SEED)


def _train_and_evaluate(train_csv: str) -> float:
    x, y = _load_training_xy(train_csv)
    x_train, x_test, y_train, y_test = train_test_split(
        x, y, test_size=0.3, random_state=SEED, shuffle=True
    )

    scaler = StandardScaler()
    x_train = scaler.fit_transform(x_train)
    x_test = scaler.transform(x_test)

    model = _build_model()
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    return float(accuracy_score(y_test, y_pred))


def _predict_with_loaded_model(model_path: str, input_csv: str) -> Optional[float]:
    if not (os.path.exists(model_path) and os.path.exists(input_csv)):
        return None

    model = _load_model(model_path)
    x = _load_input_x(input_csv)

    scaler = StandardScaler()
    x_scaled = scaler.fit_transform(x)

    _ = model.predict(x_scaled)
    return 0.0


def main() -> None:
    _set_reproducible_seed(SEED)

    model_path = "dict.pickle"
    input_csv = "input.csv"
    train_csv = "14k.csv"

    accuracy = _predict_with_loaded_model(model_path, input_csv)
    if accuracy is None:
        accuracy = _train_and_evaluate(train_csv)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced Python-level CSV parsing loops with vectorized pandas loading and NumPy arrays to reduce overhead and data movement.
# - Added robust CSV parsing fallback (default read_csv, then sep=';' and decimal=',') to avoid repeated manual parsing attempts.
# - Eliminated global mutable lists and copy-heavy list conversions; kept data as NumPy arrays for lower memory footprint and faster transforms.
# - Centralized reproducibility via fixed seeds (random, NumPy, and model random_state) for stable results.
# - Avoided unnecessary intermediate prints/logging and removed unit-test/interactive/side-effect paths; kept a single final accuracy print.
# - Reused a single StandardScaler fit/transform pipeline per run and avoided redundant computations in preprocessing.