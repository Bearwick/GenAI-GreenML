# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")

DATA_PATH = "data/AirQualityUCI.csv"

def robust_read_csv(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        return pd.read_csv(path, sep=';', decimal=',')
    if df.shape[1] <= 1 or any(';' in str(col) for col in df.columns):
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            pass
    return df

def clean_columns(df):
    cols = []
    for c in df.columns:
        c_str = str(c)
        c_str = ' '.join(c_str.strip().split())
        cols.append(c_str)
    df.columns = cols
    drop_cols = [c for c in df.columns if c == '' or c.lower().startswith('unnamed')]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors='ignore')
    df = df.dropna(axis=1, how='all')
    return df

def make_unique(cols):
    seen = {}
    new = []
    for c in cols:
        if c not in seen:
            seen[c] = 0
            new.append(c)
        else:
            seen[c] += 1
            new.append(f"{c}_{seen[c]}")
    return new

def choose_target(df):
    if df.shape[1] == 0:
        return None
    for key in ['target', 'label', 'risk', 'class', 'y']:
        for col in df.columns:
            if key in col.lower():
                return col
    numeric_info = []
    for col in df.columns:
        s = pd.to_numeric(df[col].astype(str).str.replace(',', '.', regex=False), errors='coerce')
        if s.notna().mean() > 0.5 and s.nunique(dropna=True) > 1:
            numeric_info.append((s.var(), col))
    if numeric_info:
        numeric_info.sort(reverse=True)
        return numeric_info[0][1]
    return df.columns[0]

def add_datetime_features(df):
    df = df.copy()
    for col in list(df.columns):
        col_low = col.lower()
        if 'date' in col_low or 'time' in col_low:
            parsed = pd.to_datetime(df[col], errors='coerce', dayfirst=True, infer_datetime_format=True)
            if parsed.notna().mean() > 0.5:
                df[col + '_year'] = parsed.dt.year
                df[col + '_month'] = parsed.dt.month
                df[col + '_day'] = parsed.dt.day
                df[col + '_hour'] = parsed.dt.hour
                df[col + '_minute'] = parsed.dt.minute
                df[col + '_dayofweek'] = parsed.dt.dayofweek
                df = df.drop(columns=[col])
    return df

df = robust_read_csv(DATA_PATH)
df = clean_columns(df)
df.columns = make_unique(df.columns)

assert df.shape[0] > 0

target_col = choose_target(df)
if target_col is None:
    target_col = df.columns[0]
y_raw = df[target_col].copy() if target_col in df.columns else pd.Series(np.zeros(len(df)))
df = df.drop(columns=[target_col], errors='ignore')

df = add_datetime_features(df)

mask = y_raw.notna()
df = df.loc[mask].reset_index(drop=True)
y_raw = y_raw.loc[mask].reset_index(drop=True)

target_is_numeric = False
y_num = pd.to_numeric(y_raw.astype(str).str.replace(',', '.', regex=False), errors='coerce')
if y_num.notna().mean() > 0.5:
    y = y_num.replace([np.inf, -np.inf], np.nan)
    if (y == -200).any():
        y = y.replace(-200, np.nan)
    target_is_numeric = True
    mask2 = y.notna()
    df = df.loc[mask2].reset_index(drop=True)
    y = y.loc[mask2].reset_index(drop=True)
else:
    y_dt = pd.to_datetime(y_raw, errors='coerce', dayfirst=True, infer_datetime_format=True)
    if y_dt.notna().mean() > 0.5:
        y = y_dt.map(lambda x: x.toordinal() if pd.notna(x) else np.nan)
        target_is_numeric = True
        mask2 = y.notna()
        df = df.loc[mask2].reset_index(drop=True)
        y = y.loc[mask2].reset_index(drop=True)
    else:
        y = y_raw.astype(str)
        target_is_numeric = False

assert len(df) > 0

numeric_cols = []
categorical_cols = []
for col in df.columns:
    s = df[col]
    if pd.api.types.is_bool_dtype(s):
        df[col] = s.astype(float)
        numeric_cols.append(col)
    elif pd.api.types.is_numeric_dtype(s):
        numeric_cols.append(col)
    else:
        s_num = pd.to_numeric(s.astype(str).str.replace(',', '.', regex=False), errors='coerce')
        if s_num.notna().mean() > 0.5:
            df[col] = s_num
            numeric_cols.append(col)
        else:
            df[col] = s.astype(str)
            categorical_cols.append(col)

for col in numeric_cols:
    s = df[col].replace([np.inf, -np.inf], np.nan)
    if (s == -200).any():
        s = s.replace(-200, np.nan)
    df[col] = s

numeric_cols = [col for col in numeric_cols if df[col].notna().any()]
categorical_cols = [col for col in categorical_cols if df[col].notna().any()]

categorical_cols_filtered = []
for col in categorical_cols:
    nunique = df[col].nunique(dropna=True)
    if len(df) == 0:
        continue
    if nunique <= 50 or (nunique / len(df)) <= 0.5:
        categorical_cols_filtered.append(col)

feature_cols_final = numeric_cols + categorical_cols_filtered
if not feature_cols_final:
    df['constant'] = 1.0
    numeric_cols = ['constant']
    categorical_cols_filtered = []
    feature_cols_final = ['constant']

if len(df) < 2:
    df = pd.concat([df, df], ignore_index=True)
    y = pd.concat([y, y], ignore_index=True)

if target_is_numeric:
    nunique = y.nunique(dropna=True)
    if nunique <= 20 and (nunique / len(y)) < 0.2:
        problem_type = 'classification'
    else:
        problem_type = 'regression'
else:
    problem_type = 'classification'

X = df[feature_cols_final]

stratify = None
if problem_type == 'classification':
    try:
        vc = y.value_counts()
        if y.nunique(dropna=True) > 1 and vc.min() >= 2:
            stratify = y
    except Exception:
        stratify = None

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0

try:
    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=True)
except TypeError:
    ohe = OneHotEncoder(handle_unknown='ignore', sparse=True)

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', ohe)
])

transformers = []
if numeric_cols:
    transformers.append(('num', numeric_transformer, numeric_cols))
if categorical_cols_filtered:
    transformers.append(('cat', categorical_transformer, categorical_cols_filtered))

preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')

if problem_type == 'classification':
    n_classes = y.nunique(dropna=True)
    if n_classes < 2 or n_classes > 50:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear', random_state=42)
else:
    if y.nunique(dropna=True) < 2:
        model = DummyRegressor(strategy='mean')
    else:
        model = Ridge(alpha=1.0)

clf = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

if problem_type == 'classification':
    if len(y_test) == 0:
        accuracy = 0.0
    else:
        try:
            accuracy = accuracy_score(y_test, y_pred)
        except Exception:
            accuracy = float(np.mean(np.array(y_test) == np.array(y_pred))) if len(y_test) else 0.0
else:
    if len(y_test) >= 2 and y_test.nunique(dropna=True) > 1:
        r2 = r2_score(y_test, y_pred)
        accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))
    else:
        if len(y_test) == 0:
            accuracy = 0.0
        else:
            y_test_arr = np.array(y_test, dtype=float)
            y_pred_arr = np.array(y_pred, dtype=float)
            mae = np.mean(np.abs(y_test_arr - y_pred_arr))
            scale = np.mean(np.abs(y_test_arr)) + 1e-9
            accuracy = max(0.0, min(1.0, 1.0 - mae / scale))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Robust CSV parsing with delimiter/decimal fallback and cleaned headers to avoid schema assumptions.
# - Lightweight preprocessing: simple datetime decomposition and low-cardinality one-hot encoding to limit feature size.
# - Energy-efficient models (LogisticRegression/Ridge) with Dummy baselines for degenerate targets.
# - Regression accuracy proxy uses bounded (r2+1)/2 or MAE fallback to keep scores in [0,1].