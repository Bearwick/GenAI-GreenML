# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import pickle
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _safe_read_csv(path):
    # Fallback strategy per requirements; returns DataFrame or raises last exception.
    last_exc = None
    for kwargs in ({}, {"sep": ";", "decimal": ","}):
        try:
            df = pd.read_csv(path, **kwargs)
            if df is None or df.shape[0] == 0:
                continue
            # If parsing is "wrong" (single column with separators inside), try alternative.
            if df.shape[1] == 1:
                s = df.iloc[:, 0].astype(str)
                if s.str.contains(";").mean() > 0.3 or s.str.contains(",").mean() > 0.3:
                    last_exc = ValueError("Likely wrong CSV delimiter/decimal; retrying.")
                    continue
            return df
        except Exception as e:
            last_exc = e
            continue
    raise last_exc if last_exc is not None else RuntimeError("Failed to read CSV.")


def _load_dataset(dataset_path):
    ext = os.path.splitext(dataset_path)[1].lower()

    if ext in [".csv", ".txt"]:
        df = _safe_read_csv(dataset_path)
        df.columns = _normalize_columns(df.columns)
        df = _drop_unnamed(df)
        return df

    if ext in [".pickle", ".pkl"]:
        with open(dataset_path, "rb") as f:
            obj = pickle.load(f)

        # Handle common pickle contents robustly: DataFrame, dict -> DataFrame, list of dicts, etc.
        if isinstance(obj, pd.DataFrame):
            df = obj.copy()
            df.columns = _normalize_columns(df.columns)
            df = _drop_unnamed(df)
            return df

        if isinstance(obj, dict):
            # If dict looks like {"X":..., "y":...} try to assemble.
            keys_lower = {str(k).lower(): k for k in obj.keys()}
            if "x" in keys_lower and "y" in keys_lower:
                X = obj[keys_lower["x"]]
                y = obj[keys_lower["y"]]
                X_df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X.copy()
                y_ser = pd.Series(y) if not isinstance(y, (pd.Series, pd.DataFrame)) else (y.iloc[:, 0] if isinstance(y, pd.DataFrame) else y)
                X_df.columns = _normalize_columns(X_df.columns)
                X_df = _drop_unnamed(X_df)
                df = X_df.copy()
                df["target"] = y_ser.values
                df.columns = _normalize_columns(df.columns)
                df = _drop_unnamed(df)
                return df

            # Otherwise try direct DataFrame creation.
            try:
                df = pd.DataFrame(obj)
                if isinstance(df, pd.DataFrame) and df.shape[0] > 0 and df.shape[1] > 0:
                    df.columns = _normalize_columns(df.columns)
                    df = _drop_unnamed(df)
                    return df
            except Exception:
                pass

        # Try to interpret as array-like.
        try:
            df = pd.DataFrame(obj)
            if df.shape[0] > 0 and df.shape[1] > 0:
                df.columns = _normalize_columns(df.columns)
                df = _drop_unnamed(df)
                return df
        except Exception as e:
            raise RuntimeError(f"Unsupported pickle content type: {type(obj)}") from e

    raise RuntimeError(f"Unsupported dataset extension: {ext}")


def _choose_target(df):
    # Prefer explicit target-like names; otherwise pick last column with usable variance.
    cols = list(df.columns)

    preferred = []
    for c in cols:
        cl = str(c).strip().lower()
        if cl in {"target", "label", "class", "y"}:
            preferred.append(c)
        elif any(tok in cl for tok in ["target", "label", "class"]):
            preferred.append(c)

    # Remove duplicates while keeping order.
    seen = set()
    preferred = [c for c in preferred if not (c in seen or seen.add(c))]

    candidates = preferred + [cols[-1]] if cols else preferred

    # Choose the first candidate that exists and is non-constant after cleaning.
    for c in candidates:
        if c not in df.columns:
            continue
        ser = df[c]
        nun = ser.nunique(dropna=True)
        if nun is not None and nun >= 2:
            return c

    # Otherwise, pick a non-constant numeric-like column.
    numeric_like = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() >= max(3, int(0.05 * len(df))):
            if s.nunique(dropna=True) >= 2:
                numeric_like.append(c)
    if numeric_like:
        return numeric_like[-1]

    # Last resort: last column if exists.
    return cols[-1] if cols else None


def _make_xy(df, target_col):
    df2 = df.copy()

    # Ensure there is at least some data
    assert df2 is not None and isinstance(df2, pd.DataFrame)
    assert df2.shape[0] > 0 and df2.shape[1] > 0

    if target_col is None or target_col not in df2.columns:
        # If no target, fabricate a constant target (forces regression fallback to trivial baseline)
        df2["target"] = 0
        target_col = "target"

    y = df2[target_col]
    X = df2.drop(columns=[target_col], errors="ignore")

    # If X is empty, create a dummy feature to allow pipeline to run end-to-end.
    if X.shape[1] == 0:
        X = pd.DataFrame({"__dummy__": np.zeros(len(df2), dtype=np.float32)})

    # Normalize columns again after potential modifications
    X.columns = _normalize_columns(X.columns)
    X = _drop_unnamed(X)

    return X, y


def _infer_task_and_prepare_target(y_raw):
    # Decide classification vs regression robustly.
    y = y_raw.copy()

    # If y is a DataFrame, take first column.
    if isinstance(y, pd.DataFrame):
        y = y.iloc[:, 0]

    # If numeric-like and few unique values, treat as classification.
    y_num = pd.to_numeric(y, errors="coerce")
    num_non_na = y_num.notna().sum()
    uniq_non_na_num = y_num.nunique(dropna=True)

    if num_non_na > 0 and uniq_non_na_num is not None and uniq_non_na_num <= 20:
        # Classification with numeric-coded classes
        y_cls = y_num
        # For stability, keep only non-NaN rows later via split masking.
        return "classification", y_cls

    # If object/string with few unique, treat as classification.
    uniq = y.nunique(dropna=True)
    if uniq is not None and uniq >= 2 and uniq <= 50:
        return "classification", y.astype("object")

    # Otherwise regression
    return "regression", y_num


def _build_preprocessor(X):
    # Identify column types from actual df columns; do not assume schema.
    X2 = X.copy()

    # Coerce numeric-like columns safely; keep original for categorical detection.
    numeric_cols = []
    categorical_cols = []

    for c in X2.columns:
        s = X2[c]
        if pd.api.types.is_numeric_dtype(s):
            numeric_cols.append(c)
        else:
            # Try numeric coercion; if many values convert, treat as numeric.
            s_num = pd.to_numeric(s, errors="coerce")
            ratio = (s_num.notna().sum() / max(1, len(s_num)))
            if ratio >= 0.8:
                numeric_cols.append(c)
                X2[c] = s_num
            else:
                categorical_cols.append(c)

    # Build lightweight transformers.
    num_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False)),  # sparse-friendly and cheap
    ])

    cat_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    pre = ColumnTransformer(
        transformers=[
            ("num", num_pipe, numeric_cols),
            ("cat", cat_pipe, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )
    return pre, X2, numeric_cols, categorical_cols


def _bounded_regression_accuracy(y_true, y_pred):
    # Stable proxy in [0,1]: 1 / (1 + RMSE/scale), with robust scale from IQR or std.
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    rmse = float(np.sqrt(np.mean((yt - yp) ** 2)))
    q75, q25 = np.percentile(yt, [75, 25])
    scale = float(q75 - q25)
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = float(np.std(yt)) if np.isfinite(np.std(yt)) and np.std(yt) > 1e-12 else 1.0
    return float(1.0 / (1.0 + rmse / scale))


def main():
    dataset_path = "dict.pickle"
    df = _load_dataset(dataset_path)

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Remove completely empty rows/columns
    df = df.dropna(axis=0, how="all")
    df = df.dropna(axis=1, how="all")
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _choose_target(df)
    X, y_raw = _make_xy(df, target_col)

    task, y_prepared = _infer_task_and_prepare_target(y_raw)

    # Align X and y: remove rows where target is NaN for numeric targets.
    if task == "classification":
        # For classification, drop rows with missing labels
        y_series = y_prepared
        mask = ~pd.isna(y_series)
    else:
        y_series = pd.to_numeric(y_prepared, errors="coerce")
        mask = np.isfinite(y_series.to_numpy(dtype=float, na_value=np.nan))

    X = X.loc[mask].reset_index(drop=True)
    y_series = y_series.loc[mask].reset_index(drop=True)

    assert X.shape[0] > 1

    preprocessor, X_coerced, _, _ = _build_preprocessor(X)

    # If classification but only 1 class, fallback to regression proxy with Ridge.
    if task == "classification":
        n_classes = pd.Series(y_series).nunique(dropna=True)
        if n_classes < 2:
            task = "regression"
            y_series = pd.to_numeric(y_series, errors="coerce")
            y_series = y_series.fillna(y_series.median() if np.isfinite(y_series.median()) else 0.0)

    # Train/test split with defensive sizing.
    test_size = 0.2
    if X_coerced.shape[0] < 10:
        test_size = 0.3

    if task == "classification":
        # Attempt stratification only if enough samples per class.
        y_tmp = pd.Series(y_series)
        vc = y_tmp.value_counts(dropna=True)
        stratify = y_tmp if (vc.min() >= 2 and len(vc) >= 2) else None

        X_train, X_test, y_train, y_test = train_test_split(
            X_coerced, y_tmp, test_size=test_size, random_state=RANDOM_STATE, stratify=stratify
        )

        assert X_train.shape[0] > 0 and X_test.shape[0] > 0

        # Lightweight linear classifier; saga handles sparse one-hot efficiently.
        clf = LogisticRegression(
            max_iter=300,
            solver="saga",
            n_jobs=1,
            tol=1e-3,
        )

        model = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", clf),
        ])

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        y_tmp = pd.to_numeric(pd.Series(y_series), errors="coerce")
        y_tmp = y_tmp.replace([np.inf, -np.inf], np.nan)
        if y_tmp.notna().sum() == 0:
            y_tmp = pd.Series(np.zeros(len(X_coerced), dtype=float))
        else:
            fillv = y_tmp.median()
            if not np.isfinite(fillv):
                fillv = 0.0
            y_tmp = y_tmp.fillna(fillv)

        X_train, X_test, y_train, y_test = train_test_split(
            X_coerced, y_tmp, test_size=test_size, random_state=RANDOM_STATE
        )
        assert X_train.shape[0] > 0 and X_test.shape[0] > 0

        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)

        model = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", reg),
        ])

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_accuracy(y_test.to_numpy(dtype=float), np.asarray(y_pred, dtype=float))

    print(f”ACCURACY={accuracy:.6f}”)


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight, CPU-friendly models (LogisticRegression with sparse one-hot; Ridge for regression fallback) to minimize compute/energy.
# - Robust schema handling: normalizes headers, drops Unnamed columns, infers target defensively, and proceeds even with missing/unknown columns.
# - Efficient preprocessing via ColumnTransformer + Pipeline to avoid duplicated work and ensure reproducibility.
# - OneHotEncoder produces sparse matrices; StandardScaler(with_mean=False) avoids densifying sparse data, reducing memory and CPU cost.
# - Fallback CSV parsing strategy (default then sep=';' & decimal=',') reduces failures without expensive heuristics.
# - Regression fallback reports bounded accuracy proxy in [0,1] using 1/(1+RMSE/scale) for stable end-to-end scoring.