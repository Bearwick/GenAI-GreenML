# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import pickle
import random
import numpy as np
import pandas as pd
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

RANDOM_SEED = 42
MODEL_PATH = "dict.pickle"

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

def get_dataset_headers():
    headers = globals().get("DATASET_HEADERS")
    if headers is None:
        return None
    if isinstance(headers, str):
        parts = [h.strip() for h in headers.split(",") if h.strip()]
        return parts if parts else None
    if isinstance(headers, (list, tuple, np.ndarray, pd.Index)):
        parts = [str(h).strip() for h in headers if str(h).strip()]
        return parts if parts else None
    return None

def _needs_fallback(df):
    if df is None or df.empty:
        return False
    if df.shape[1] == 1:
        header = str(df.columns[0])
        if ";" in header:
            return True
        if df.shape[0] > 0:
            first_val = df.iloc[0, 0]
            if isinstance(first_val, str) and ";" in first_val:
                return True
    return False

def read_csv_with_fallback(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is not None:
        if _needs_fallback(df):
            try:
                df_alt = pd.read_csv(path, sep=";", decimal=",")
                if df_alt.shape[1] >= df.shape[1]:
                    df = df_alt
            except Exception:
                pass
    else:
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            return None
    return df

def clean_dataframe(df):
    if df is None:
        return None
    cols = df.columns.astype(str)
    df = df.loc[:, ~cols.str.match(r"^Unnamed")]
    df = df.dropna(axis=1, how="all")
    return df

def apply_headers(df):
    headers = get_dataset_headers()
    if headers and len(headers) == df.shape[1]:
        df.columns = headers
    return df

def detect_label_column(df):
    if df is None or df.empty:
        return None
    colnames = [str(c).strip().lower() for c in df.columns]
    for idx in range(len(colnames) - 1, -1, -1):
        name = colnames[idx]
        if name in {"label", "labels", "class", "target", "y"}:
            return df.columns[idx]
    last_col = df.columns[-1]
    series = pd.to_numeric(df[last_col], errors="coerce")
    valid = series.dropna()
    if valid.empty:
        return None
    unique_vals = valid.unique()
    if unique_vals.size <= 5 and unique_vals.size / max(len(valid), 1) <= 0.2:
        return last_col
    return None

def extract_features_labels(df):
    label_col = detect_label_column(df)
    numeric_df = df.apply(pd.to_numeric, errors="coerce")
    if label_col:
        y = numeric_df[label_col].to_numpy()
        X = numeric_df.drop(columns=[label_col]).to_numpy()
        if X.size == 0:
            return None, None
        mask = ~np.isnan(X).any(axis=1) & ~np.isnan(y)
        X = X[mask]
        y = y[mask]
        y = np.sign(y).astype(int)
        return X, y
    X = numeric_df.to_numpy()
    if X.size == 0:
        return None, None
    mask = ~np.isnan(X).any(axis=1)
    X = X[mask]
    return X, None

def load_data(path):
    df = read_csv_with_fallback(path)
    if df is None or df.empty:
        return None, None
    df = clean_dataframe(df)
    if df is None or df.empty:
        return None, None
    df = apply_headers(df)
    return extract_features_labels(df)

def get_candidate_paths():
    paths = []
    if MODEL_PATH.lower().endswith(".csv") and os.path.isfile(MODEL_PATH):
        paths.append(MODEL_PATH)
    for name in ("input.csv", "14k.csv"):
        if os.path.isfile(name) and name not in paths:
            paths.append(name)
    return paths

def load_evaluation_data():
    candidates = get_candidate_paths()
    first_data = (None, None)
    for path in candidates:
        X, y = load_data(path)
        if X is not None and X.size > 0:
            if first_data[0] is None:
                first_data = (X, y)
            if y is not None and y.size > 0:
                return X, y
    return first_data

def load_model(path):
    if not os.path.isfile(path):
        return None
    try:
        with open(path, "rb") as f:
            return pickle.load(f)
    except Exception:
        return None

def is_model_fitted(model):
    return hasattr(model, "support_") and model.support_ is not None

def train_and_evaluate(X, y):
    if X is None or y is None or len(X) == 0:
        return None, 0.0
    unique_classes = np.unique(y)
    model = svm.SVC(kernel="linear")
    if unique_classes.size < 2:
        try:
            model.fit(X, y)
            acc = accuracy_score(y, model.predict(X))
        except Exception:
            acc = 0.0
        return model, acc
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=RANDOM_SEED
        )
        model.fit(X_train, y_train)
        acc = accuracy_score(y_test, model.predict(X_test))
        return model, acc
    except Exception:
        try:
            model.fit(X, y)
            acc = accuracy_score(y, model.predict(X))
        except Exception:
            acc = 0.0
        return model, acc

def evaluate_model(model, X, y):
    if model is None or X is None or y is None or len(X) == 0:
        return 0.0, False
    try:
        preds = model.predict(X)
        return accuracy_score(y, preds), True
    except Exception:
        return 0.0, False

def main():
    model = load_model(MODEL_PATH)
    X, y = load_evaluation_data()
    accuracy = 0.0
    if X is not None and X.size > 0 and y is not None and y.size > 0:
        if model is None or not is_model_fitted(model):
            _, accuracy = train_and_evaluate(X, y)
        else:
            accuracy, ok = evaluate_model(model, X, y)
            if not ok:
                _, accuracy = train_and_evaluate(X, y)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# Used vectorized pandas/numpy operations for parsing and label mapping to avoid Python loops.
# Added CSV parsing fallback and column cleanup to prevent malformed data and reduce reprocessing.
# Removed global mutable state and model-saving side effects to lower memory and I/O overhead.
# Avoided redundant training by reusing a fitted model with fallback training only when needed.
# Fixed random seeds and deterministic splits to ensure reproducible results.