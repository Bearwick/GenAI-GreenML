# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pickle
import os

def load_data_efficiently(file_path):
    if not os.path.exists(file_path):
        return None, None
    
    try:
        df = pd.read_csv(file_path, header=None, engine='c', low_memory=True)
        if not np.issubdtype(df.iloc[0, 0], np.number):
            df = pd.read_csv(file_path, header=0, engine='c', low_memory=True)
    except (pd.errors.ParserError, ValueError):
        df = pd.read_csv(file_path, sep=';', decimal=',', header=0, engine='c', low_memory=True)

    if df.iloc[:, -1].dtype == object or df.iloc[:, -1].isnull().all():
        if df.iloc[:, -1].astype(str).str.strip().eq('').all():
            df = df.iloc[:, :-1]

    data = df.to_numpy(dtype=np.float64)
    X = data[:, :-1]
    y = data[:, -1]
    y = np.where(y > 0, 1, np.where(y < 0, -1, 0))
    return X, y

def run_pipeline():
    seed = 42
    np.random.seed(seed)
    
    data_source = '14k.csv'
    if not os.path.exists(data_source):
        data_source = 'input.csv'
    
    X, y = load_data_efficiently(data_source)
    
    if X is None:
        return

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed
    )

    model = LinearSVC(random_state=seed, tol=1e-4, max_iter=2000, dual='auto')
    model.fit(X_train, y_train)
    
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary
# 1. Replaced manual CSV parsing loop with pandas.read_csv (C engine), reducing runtime and energy for I/O.
# 2. Switched from SVC(kernel='linear') to LinearSVC, which is significantly more computationally efficient for linear problems.
# 3. Utilized NumPy vectorization (np.where) for label transformation instead of Python-level branching and list appending.
# 4. Reduced memory footprint by using NumPy arrays and avoiding redundant list-to-array conversions.
# 5. Eliminated redundant model serialization (pickle.dump/load) during the prediction phase to save disk I/O and CPU cycles.
# 6. Set fixed random seeds and enabled 'dual=auto' in LinearSVC to ensure reproducibility and optimal algorithmic selection.
# 7. Optimized data handling to remove trailing empty columns efficiently using vectorized operations.