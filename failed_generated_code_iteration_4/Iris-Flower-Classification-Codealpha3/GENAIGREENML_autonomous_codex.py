# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, r2_score

dataset_path = "Iris.csv"

def read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is None or df.shape[1] <= 1:
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            if df is None:
                raise
    return df

df = read_csv_robust(dataset_path)
df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
df = df.loc[:, [c for c in df.columns if not re.match(r'^Unnamed', c, flags=re.IGNORECASE)]]

assert df.shape[0] > 0 and df.shape[1] > 0

cols = df.columns.tolist()
lower_map = {c.lower(): c for c in cols}
target_col = None
for key in ['species', 'target', 'label', 'class']:
    if key in lower_map:
        target_col = lower_map[key]
        break
if target_col is None:
    object_cols = df.select_dtypes(include=['object']).columns.tolist()
    if object_cols:
        unique_counts = {c: df[c].nunique(dropna=True) for c in object_cols}
        candidates = [c for c in object_cols if unique_counts.get(c, 0) > 1]
        if candidates:
            target_col = min(candidates, key=lambda c: unique_counts[c])
        else:
            target_col = object_cols[0]
    else:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if numeric_cols:
            nonconst = [c for c in numeric_cols if df[c].nunique(dropna=True) > 1]
            if nonconst:
                target_col = max(nonconst, key=lambda c: df[c].var(skipna=True))
            else:
                target_col = numeric_cols[0]
        else:
            target_col = cols[-1]

feature_cols = [c for c in cols if c != target_col]
if len(feature_cols) == 0:
    df['__index__'] = np.arange(len(df))
    feature_cols = ['__index__']

y_raw = df[target_col]
if y_raw.dtype == 'object':
    y = y_raw
else:
    y = pd.to_numeric(y_raw, errors='coerce')

mask = y.notna()
df = df.loc[mask].copy()
y = y.loc[mask]

assert len(df) > 0

X = df[feature_cols].copy()
for c in X.columns:
    if X[c].dtype == 'object':
        converted = pd.to_numeric(X[c], errors='coerce')
        if converted.notna().mean() > 0.8:
            X[c] = converted
    else:
        X[c] = pd.to_numeric(X[c], errors='coerce')
X.replace([np.inf, -np.inf], np.nan, inplace=True)

numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = [c for c in X.columns if c not in numeric_features]

transformers = []
if numeric_features:
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))
    ])
    transformers.append(('cat', categorical_transformer, categorical_features))

if transformers:
    preprocessor = ColumnTransformer(transformers=transformers)
else:
    preprocessor = 'passthrough'

y_series = pd.Series(y)
if y_series.dtype == 'object':
    is_classification = True
else:
    unique_count = y_series.nunique(dropna=True)
    is_classification = unique_count <= max(20, int(len(y_series) * 0.5))

accuracy = 0.0

if len(y_series) < 2:
    accuracy = 1.0
else:
    n_classes = y_series.nunique(dropna=True) if is_classification else None
    stratify = None
    if is_classification and n_classes is not None and n_classes >= 2:
        counts = y_series.value_counts()
        if counts.min() >= 2:
            stratify = y_series
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_series, test_size=0.2, random_state=42, stratify=stratify
    )
    assert len(X_train) > 0 and len(X_test) > 0
    if is_classification:
        if n_classes is None or n_classes < 2:
            most_common = y_train.iloc[0] if hasattr(y_train, 'iloc') else y_train[0]
            y_pred = np.array([most_common] * len(y_test))
            accuracy = accuracy_score(y_test, y_pred) if len(y_test) > 0 else 1.0
        else:
            model = LogisticRegression(max_iter=200, solver='liblinear')
            clf = Pipeline(steps=[('preprocessor', preprocessor),
                                 ('model', model)])
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
    else:
        model = LinearRegression()
        reg = Pipeline(steps=[('preprocessor', preprocessor),
                             ('model', model)])
        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        accuracy = (r2 + 1.0) / 2.0
        accuracy = float(max(0.0, min(1.0, accuracy)))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# - Used lightweight logistic/linear regression with simple preprocessing to minimize CPU load.
# - Employed ColumnTransformer with minimal imputation, scaling, and one-hot encoding for reproducibility.
# - For regression fallback, mapped R2 to a bounded [0,1] accuracy proxy via (R2+1)/2.