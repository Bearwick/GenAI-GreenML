# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
import sys
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

def load_dataset(path):
    try:
        df = pd.read_csv(path)
        # Robust check for separator issues (common in European CSVs)
        if df.shape[1] <= 1:
            df = pd.read_csv(path, sep=';', decimal=',')
    except Exception:
        # If file is missing or unreadable, create a dummy structure for flow continuity
        return pd.DataFrame()
    
    # Normalize column names: strip whitespace and collapse internal spaces
    df.columns = [str(col).strip() for col in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    
    return df

def process_and_train():
    dataset_path = 'Iris.csv'
    df = load_dataset(dataset_path)
    
    if df.empty:
        # Silent failure prevention for pipeline continuity
        print("ACCURACY=0.000000")
        return

    # Identify target column: Prefer 'Species', 'class', or the last column
    cols = list(df.columns)
    target_candidates = ['Species', 'species', 'class', 'target']
    target_col = None
    
    for cand in target_candidates:
        if cand in cols:
            target_col = cand
            break
    
    if target_col is None:
        target_col = cols[-1]

    # Features: exclude Id-like columns and the target
    feature_cols = [c for c in cols if c != target_col and 'id' not in c.lower()]
    
    if not feature_cols:
        # Fallback to all columns except target if Id-filter was too aggressive
        feature_cols = [c for c in cols if c != target_col]

    # Preprocessing: Convert to numeric, handle NaNs
    X = df[feature_cols].copy()
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    
    y = df[target_col]
    
    # Handle missing values by dropping or simple imputation
    # Drop rows where target is NaN
    valid_idx = y.dropna().index
    X = X.loc[valid_idx]
    y = y.loc[valid_idx]
    
    if X.empty or len(np.unique(y)) < 2:
        # Trivial baseline if dataset is too small or single-class
        print("ACCURACY=0.000000")
        return

    # Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) > 1 else None
    )

    # Energy-Efficient Pipeline: 
    # 1. Simple Imputer (Mean) - low CPU cost
    # 2. Standard Scaler - essential for Linear Model convergence
    # 3. Logistic Regression - extremely lightweight, O(features * samples) complexity
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler()),
        ('model', LogisticRegression(
            max_iter=200, 
            solver='lbfgs', 
            multi_class='auto', 
            random_state=42,
            n_jobs=1 # Minimize parallel overhead for small datasets
        ))
    ])

    # Model Training
    pipeline.fit(X_train, y_train)

    # Evaluation
    predictions = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    process_and_train()

# OPTIMIZATION SUMMARY
# 1. Model Selection: Logistic Regression was chosen over Random Forests or SVMs because it has significantly lower 
#    computational overhead (FLOPs) during both training and inference for small, linearly separable datasets like Iris.
# 2. Scaling: StandardScaler ensures the Gradient Descent (L-BFGS) converges in fewer iterations, reducing CPU cycles.
# 3. Robust Schema Handling: Implemented flexible column detection and whitespace normalization to prevent 
#    crashes on malformed CSVs without requiring manual intervention.
# 4. CPU Efficiency: Used n_jobs=1 to avoid the overhead of spawning multiple processes for a trivial-sized task.
# 5. Memory Management: Used basic Scikit-learn Pipelines to avoid redundant data copying and ensure a streamlined 
#    transformation-to-prediction flow.
# 6. Fallback Logic: Included defensive checks for empty data or single-class targets to ensure the code 
#    exits gracefully with a valid output format.