# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import json
import csv
from typing import List, Dict, Any, Optional

import torch
from transformers import pipeline


SEED = 42
MODEL_NAME = "cross-encoder/nli-distilroberta-base"
INTENT_OPTIONS = [
    "Book Appointment",
    "Product Inquiry",
    "Pricing Negotiation",
    "Support Request",
    "Follow-Up",
]


def _set_reproducibility(seed: int = SEED) -> None:
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    try:
        torch.use_deterministic_algorithms(True)
    except Exception:
        pass
    try:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    except Exception:
        pass


class IntentDetector:
    def __init__(self, intent_options: List[str] = INTENT_OPTIONS):
        device = 0 if torch.cuda.is_available() else -1
        dtype = torch.float16 if torch.cuda.is_available() else torch.float32

        self.intent_options = intent_options
        self.intent_pipeline = pipeline(
            task="zero-shot-classification",
            model=MODEL_NAME,
            device=device,
            torch_dtype=dtype,
        )

    def classify_intent(self, dialogue: str) -> Dict[str, str]:
        classification = self.intent_pipeline(dialogue, self.intent_options)
        top_intent = classification["labels"][0]
        explanation = f"Based on the conversation, the customer is likely interested in '{top_intent.lower()}'."
        return {"predicted_intent": top_intent, "rationale": explanation}


def create_conversation(messages: List[Dict[str, Any]], max_messages: Optional[int] = None) -> str:
    if not messages:
        return ""
    if max_messages is not None and max_messages > 0:
        messages = messages[-max_messages:]

    lines = []
    append = lines.append
    for m in messages:
        sender = (m.get("sender") or "").capitalize()
        text = m.get("text") or ""
        append(f"{sender}: {text}")
    return "\n".join(lines)


def _load_conversations(input_file: str) -> List[Dict[str, Any]]:
    with open(input_file, "r", encoding="utf-8") as infile:
        data = json.load(infile)
    return data if isinstance(data, list) else []


def _write_json(path: str, records: List[Dict[str, Any]]) -> None:
    with open(path, "w", encoding="utf-8") as f:
        json.dump(records, f, ensure_ascii=False, indent=2)


def _write_csv(path: str, records: List[Dict[str, Any]]) -> None:
    fieldnames = ["conversation_id", "predicted_intent", "rationale"]
    with open(path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(records)


def _accuracy_from_inputs(records: List[Dict[str, Any]]) -> float:
    if not records:
        return 0.0

    correct = 0
    total = 0
    for r in records:
        pred = r.get("predicted_intent")
        true = r.get("true_intent")
        if true is None:
            true = r.get("intent")
        if true is None:
            true = r.get("label")
        if true is None:
            continue
        total += 1
        if pred == true:
            correct += 1

    return (correct / total) if total else 0.0


def predict_intents(input_file: str, json_output: str, csv_output: str) -> float:
    _set_reproducibility(SEED)
    conversations = _load_conversations(input_file)

    model = IntentDetector()
    output_data: List[Dict[str, Any]] = []
    append = output_data.append

    for entry in conversations:
        conv_id = entry.get("conversation_id")
        formatted_text = create_conversation(entry.get("messages") or [])
        intent_result = model.classify_intent(formatted_text)
        append(
            {
                "conversation_id": conv_id,
                "predicted_intent": intent_result["predicted_intent"],
                "rationale": intent_result["rationale"],
            }
        )

    os.makedirs(os.path.dirname(json_output) or ".", exist_ok=True)
    _write_json(json_output, output_data)
    _write_csv(csv_output, output_data)

    accuracy = _accuracy_from_inputs(output_data)
    return accuracy


def main() -> None:
    accuracy = predict_intents(
        input_file="data/input.json",
        json_output="data/output/predictions.json",
        csv_output="data/output/predictions.csv",
    )
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused preprocessing (emoji/punctuation cleaning) that was not used in the original workflow, avoiding redundant computation.
# - Avoided global model initialization; instantiate once inside the prediction workflow to prevent unnecessary imports/initialization when used as a module.
# - Enabled deterministic behavior where feasible via fixed seeds and deterministic torch settings for reproducibility.
# - Selected device and torch dtype automatically (fp16 on GPU, fp32 on CPU) to reduce energy/runtime on supported hardware while preserving outputs intent.
# - Reduced overhead in tight loops by using local variable bindings (append) and simple for-loops with minimal intermediate structures.
# - Used UTF-8 explicit encoding for I/O to avoid fallback overhead and improve robustness without additional processing.