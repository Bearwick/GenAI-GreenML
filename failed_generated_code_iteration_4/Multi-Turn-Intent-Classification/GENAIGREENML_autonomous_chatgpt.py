# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import json
import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score


RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c = str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        out.append(c)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _read_json_as_dataframe(path):
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    if isinstance(data, list):
        df = pd.DataFrame(data)
        return df

    if isinstance(data, dict):
        # Try common structures: {"data":[...]}, {"items":[...]}, or single record
        for key in ("data", "items", "records", "examples", "conversations"):
            if key in data and isinstance(data[key], list):
                return pd.DataFrame(data[key])
        return pd.DataFrame([data])

    # Fallback: wrap unknown into single-row df
    return pd.DataFrame([{"value": str(data)}])


def _robust_read_table(path):
    # CSV parsing fallback requested, but dataset path is JSON; still implement defensively if file isn't JSON.
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df2.shape[1] > df.shape[1]:
                df = df2
        return df
    except Exception:
        return None


def _load_dataset(path):
    ext = os.path.splitext(path)[1].lower()
    df = None
    if ext in (".json", ".jsonl"):
        if ext == ".jsonl":
            # JSON Lines
            rows = []
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        rows.append(json.loads(line))
                    except Exception:
                        continue
            df = pd.DataFrame(rows)
        else:
            df = _read_json_as_dataframe(path)
    else:
        df = _robust_read_table(path)
        if df is None:
            # Last-resort attempt to interpret as JSON
            df = _read_json_as_dataframe(path)

    if df is None:
        df = pd.DataFrame()

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)
    return df


def _safe_textify_cell(x):
    if x is None:
        return ""
    if isinstance(x, float) and np.isnan(x):
        return ""
    if isinstance(x, (dict, list)):
        try:
            return json.dumps(x, ensure_ascii=False, sort_keys=True)
        except Exception:
            return str(x)
    return str(x)


def _build_text_feature(df):
    # Create a single lightweight text field from likely conversation columns; fallback to all object columns.
    cols = list(df.columns)

    likely_text = []
    for c in cols:
        cl = c.lower()
        if any(k in cl for k in ("text", "message", "utterance", "conversation", "dialog", "chat", "prompt", "query")):
            likely_text.append(c)

    object_cols = [c for c in cols if df[c].dtype == "object"]
    chosen = likely_text if likely_text else object_cols

    if not chosen:
        # No object columns: fallback to stringify numeric columns as minimal text
        chosen = [c for c in cols]

    # Concatenate a limited number of columns to keep compute bounded
    chosen = chosen[:8]

    text = df[chosen].apply(lambda row: " ".join(_safe_textify_cell(v) for v in row.values), axis=1)
    return text.astype(str)


def _infer_target(df):
    # Prefer explicit label-like columns
    label_like = []
    for c in df.columns:
        cl = c.lower()
        if cl in ("label", "intent", "target", "y", "class", "category", "outcome"):
            label_like.append(c)
    for c in label_like:
        if c in df.columns:
            s = df[c]
            nunique = s.nunique(dropna=True)
            if nunique >= 2:
                return c

    # Otherwise pick a non-constant column with few unique values for classification
    best = None
    best_score = -1
    for c in df.columns:
        s = df[c]
        nunique = s.nunique(dropna=True)
        if nunique < 2:
            continue
        if s.dtype == "object":
            # Prefer moderate cardinality
            score = 10_000 - nunique
        else:
            # Numeric: prefer smaller unique count (might be class ids)
            score = 5_000 - nunique
        if score > best_score:
            best_score = score
            best = c
    return best


def _choose_task(y):
    # Return ("classification", y_encoded) or ("regression", y_float)
    y_series = pd.Series(y)

    # If object or bool -> classification
    if y_series.dtype == "object" or y_series.dtype == "bool":
        y_clean = y_series.astype(str)
        return "classification", y_clean

    # Try numeric coercion
    y_num = pd.to_numeric(y_series, errors="coerce")
    ok = y_num.notna()
    if ok.sum() < max(10, int(0.5 * len(y_num))):
        # Too many NaNs; fall back to classification on string
        y_clean = y_series.astype(str)
        return "classification", y_clean

    y_num = y_num.astype(float)

    # Decide if numeric classification is plausible
    nunique = pd.Series(y_num[ok]).nunique(dropna=True)
    if nunique <= 20:
        # likely class ids
        y_intish = np.isclose(y_num[ok] % 1.0, 0.0).mean() > 0.95
        if y_intish and nunique >= 2:
            return "classification", y_num.round().astype(int).astype(str)

    return "regression", y_num


def main():
    dataset_path = os.path.join("data", "input.json")
    df = _load_dataset(dataset_path)

    # Defensive: ensure non-empty dataframe
    if df is None or df.shape[0] == 0:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = df.copy()
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Build features
    text_series = _build_text_feature(df)
    X_text = text_series.values

    # Choose target
    target_col = _infer_target(df)
    if target_col is None or target_col not in df.columns:
        # No reliable target: run trivial baseline with proxy "accuracy"
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    y_raw = df[target_col]
    task, y = _choose_task(y_raw)

    # Drop rows with missing target for stability
    mask = pd.Series(y).notna()
    if task == "regression":
        mask = mask & np.isfinite(pd.to_numeric(pd.Series(y), errors="coerce")).values

    df2 = df.loc[mask].copy()
    if df2.shape[0] < 4:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    text_series2 = _build_text_feature(df2)
    X_text2 = text_series2.values
    y2 = pd.Series(y).loc[mask].reset_index(drop=True)
    X_text2 = pd.Series(X_text2).reset_index(drop=True).values

    # If classification but fewer than 2 classes -> regression fallback
    if task == "classification":
        n_classes = pd.Series(y2).nunique(dropna=True)
        if n_classes < 2:
            task = "regression"
            y2 = pd.to_numeric(pd.Series(y_raw).loc[mask], errors="coerce").astype(float).reset_index(drop=True)

    # Train/test split
    if task == "classification":
        stratify = y2 if pd.Series(y2).nunique(dropna=True) >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X_text2, y2, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X_text2, y2, test_size=0.2, random_state=RANDOM_STATE
        )

    assert len(X_train) > 0 and len(X_test) > 0

    # Lightweight text model
    # Use small TF-IDF to stay CPU-friendly; linear models converge fast and are strong baselines for text.
    tfidf = TfidfVectorizer(
        lowercase=True,
        strip_accents="unicode",
        ngram_range=(1, 2),
        max_features=20000,
        min_df=1,
        max_df=0.95
    )

    if task == "classification":
        model = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            C=1.0
        )
        pipe = Pipeline([("tfidf", tfidf), ("clf", model)])
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Regression: Ridge is stable and efficient for sparse TF-IDF
        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        pipe = Pipeline([("tfidf", tfidf), ("reg", model)])
        # Ensure numeric
        y_train_num = pd.to_numeric(pd.Series(y_train), errors="coerce").astype(float).values
        y_test_num = pd.to_numeric(pd.Series(y_test), errors="coerce").astype(float).values

        # Drop NaNs if any slipped through
        tr_ok = np.isfinite(y_train_num)
        te_ok = np.isfinite(y_test_num)

        if tr_ok.sum() < 2 or te_ok.sum() < 1:
            accuracy = 0.0
            print(f"ACCURACY={accuracy:.6f}")
            return

        pipe.fit(np.array(X_train)[tr_ok], y_train_num[tr_ok])
        y_pred = pipe.predict(np.array(X_test)[te_ok])

        # Convert R^2 into bounded [0,1] proxy for "accuracy"
        r2 = float(r2_score(y_test_num[te_ok], y_pred))
        accuracy = float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses TF-IDF + linear models (LogisticRegression/Ridge): strong baselines for text, minimal parameters, fast CPU training/inference.
# - Caps TF-IDF max_features to bound memory/compute; uses (1,2)-grams for modest accuracy gains without heavy modeling.
# - Avoids deep learning/transformers to minimize energy use; fully deterministic with fixed RANDOM_STATE where applicable.
# - Robust schema handling: infers target, concatenates likely text columns, safely stringifies nested JSON, drops unnamed columns.
# - Defensive preprocessing: handles missing/constant targets, coerces numerics, and falls back to bounded R2-based proxy in [0,1] for regression.