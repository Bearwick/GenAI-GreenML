# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import math
from typing import Tuple, Optional

import numpy as np
import pandas as pd


SEED = 42
K = 7
TRAIN_PATH = "MNIST_train.csv"
TEST_PATH = "MNIST_test.csv"


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 2:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        if df2.shape[1] > df.shape[1]:
            df = df2
    return df


def _resolve_schema(df: pd.DataFrame) -> Tuple[str, list]:
    cols = list(df.columns)
    label_col = "label" if "label" in df.columns else cols[0]
    feature_cols = [c for c in cols if c != label_col]
    if not feature_cols:
        raise ValueError("No feature columns found in dataset.")
    return label_col, feature_cols


def _to_numpy(df: pd.DataFrame, label_col: str, feature_cols: list) -> Tuple[np.ndarray, np.ndarray]:
    y = pd.to_numeric(df[label_col], errors="coerce").astype("Int64")
    X = df[feature_cols].apply(pd.to_numeric, errors="coerce")
    valid = y.notna() & X.notna().all(axis=1)
    yv = y[valid].astype(np.int64).to_numpy(copy=False)
    Xv = X[valid].to_numpy(dtype=np.float32, copy=False)
    return Xv, yv


def _knn_predict_single(train_X: np.ndarray, train_y: np.ndarray, x: np.ndarray, k: int) -> int:
    diff = train_X - x
    d2 = np.einsum("ij,ij->i", diff, diff, optimize=True)

    if k < d2.size:
        idx = np.argpartition(d2, k)[:k]
        d2k = d2[idx]
        yk = train_y[idx]
    else:
        d2k = d2
        yk = train_y

    inv_dist = 1.0 / (np.sqrt(d2k, dtype=np.float32) + 1e-12)

    weights = {}
    for cls, w in zip(yk.tolist(), inv_dist.tolist()):
        weights[cls] = weights.get(cls, 0.0) + w

    best_cls = None
    best_weight = -math.inf
    for cls, w in weights.items():
        if w > best_weight:
            best_weight = w
            best_cls = cls
    return int(best_cls)


def main() -> None:
    np.random.seed(SEED)

    train_df = _read_csv_robust(TRAIN_PATH)
    test_df = _read_csv_robust(TEST_PATH)

    train_label_col, train_feature_cols = _resolve_schema(train_df)
    test_label_col, test_feature_cols = _resolve_schema(test_df)

    common_features = [c for c in train_feature_cols if c in set(test_feature_cols)]
    if not common_features:
        raise ValueError("No common feature columns between train and test datasets.")

    train_X, train_y = _to_numpy(train_df, train_label_col, common_features)
    test_X, test_y = _to_numpy(test_df, test_label_col, common_features)

    preds = np.empty(test_y.shape[0], dtype=np.int64)
    for i in range(test_X.shape[0]):
        preds[i] = _knn_predict_single(train_X, train_y, test_X[i], K)

    accuracy = (preds == test_y).mean() * 100.0
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced manual line-by-line CSV parsing with pandas and a robust delimiter/decimal fallback to reduce Python-level overhead and data movement.
# - Derived label/features from df.columns (preferring 'label') and used the intersection of train/test feature columns to avoid schema assumptions.
# - Converted data once to contiguous NumPy arrays (float32 for pixels, int64 for labels) to lower memory footprint and speed arithmetic.
# - Implemented vectorized distance computation via einsum and used argpartition for top-k selection to avoid O(n log n) sorting and inner Python loops.
# - Removed redundant scans (e.g., skipping header rows) by filtering invalid rows in a single boolean mask.
# - Avoided storing per-sample verbose outputs; computed only final accuracy to minimize I/O and runtime.
# - Set a fixed seed for reproducibility (even though KNN is deterministic) and used deterministic NumPy operations where feasible.