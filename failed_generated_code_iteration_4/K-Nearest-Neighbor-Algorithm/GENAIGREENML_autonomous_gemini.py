# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

def load_data(path):
    # Robust CSV parsing with fallbacks
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    
    # Normalize column names
    df.columns = [str(c).strip() for c in df.columns]
    df.columns = [" ".join(str(c).split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def solve():
    dataset_path = 'MNIST_train.csv'
    
    try:
        df = load_data(dataset_path)
    except Exception:
        # If file is missing or unreadable, exit gracefully for the pipeline
        return

    if df.empty:
        return

    # Identify target and features
    # Standard MNIST has 'label' as the first column
    possible_targets = ['label', 'target', 'y', 'class']
    target_col = None
    
    for pt in possible_targets:
        if pt in df.columns:
            target_col = pt
            break
            
    if target_col is None:
        target_col = df.columns[0]
        
    y = df[target_col]
    X = df.drop(columns=[target_col])

    # Convert to numeric and handle missing values
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
        
    # Drop columns that are entirely NaN after coercion
    X = X.dropna(axis=1, how='all')
    
    # Defensive check: ensure we have features and samples
    if X.empty or len(y) < 2:
        return

    # Split data (80/20) - fixed seed for reproducibility
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Check for classification vs regression
    # MNIST is classification (0-9). 
    # If target is continuous-like, we use a proxy accuracy.
    is_classification = True
    if y.dtype.kind in 'fc' or len(np.unique(y)) > 100:
        is_classification = False

    if is_classification:
        # Energy-efficient Logistic Regression with lbfgs solver
        # Limited max_iter to reduce CPU cycles while maintaining decent convergence
        model = Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
            ('scaler', StandardScaler()),
            ('clf', LogisticRegression(
                solver='lbfgs', 
                multi_class='auto', 
                max_iter=100, 
                tol=0.01,
                n_jobs=1,
                random_state=42
            ))
        ])
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        # Fallback for regression tasks (e.g. if the label was parsed as continuous)
        from sklearn.linear_model import Ridge
        reg = Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler()),
            ('model', Ridge())
        ])
        reg.fit(X_train, y_train)
        score = reg.score(X_test, y_test)
        # Bounding R^2-like score to [0,1] for ACCURACY proxy
        accuracy = max(0, min(1, score))

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# Optimization Summary:
# 1. Model Choice: Used Logistic Regression (LBFGS), which is significantly more energy-efficient than Deep Learning or large Ensembles for CPU-bound tasks.
# 2. Convergence Control: Set 'tol=0.01' and 'max_iter=100' to prevent unnecessary CPU cycles on marginal improvements.
# 3. Scaling: Used StandardScaler to ensure the linear model converges faster, reducing total compute time.
# 4. Memory Management: Robust CSV parsing and column cleanup prevent overhead from redundant or malformed data.
# 5. PCA Omission: While PCA can reduce features, for 784 features, the overhead of computing SVD often exceeds the savings in the linear model fit; thus, direct fitting on scaled features was preferred for simplicity and total energy balance.
# 6. Fallback Logic: Implemented a robust check for target types to ensure the script runs end-to-end regardless of minor schema variations.