# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import json
import re
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score

def load_data(path):
    df = None
    if path.lower().endswith('.json'):
        try:
            df = pd.read_json(path)
        except Exception:
            try:
                df = pd.read_json(path, lines=True)
            except Exception:
                try:
                    with open(path, 'r') as f:
                        data = json.load(f)
                    df = pd.DataFrame(data)
                except Exception:
                    df = pd.DataFrame()
    else:
        try:
            df = pd.read_csv(path)
            if df.shape[1] == 1:
                try:
                    df_alt = pd.read_csv(path, sep=';', decimal=',')
                    if df_alt.shape[1] > 1:
                        df = df_alt
                except Exception:
                    pass
        except Exception:
            try:
                df = pd.read_csv(path, sep=';', decimal=',')
            except Exception:
                df = pd.DataFrame()
    if isinstance(df, pd.Series):
        df = df.to_frame()
    return df

def normalize_columns(df):
    if df is None:
        return pd.DataFrame()
    df = df.copy()
    new_cols = []
    for c in df.columns:
        c_str = str(c)
        c_str = re.sub(r'\s+', ' ', c_str.strip())
        new_cols.append(c_str)
    df.columns = new_cols
    df = df.loc[:, ~df.columns.str.match(r'^Unnamed')]
    return df

def choose_target(df):
    candidates = ['cuisine', 'target', 'label', 'class', 'y']
    col_map = {c.lower(): c for c in df.columns}
    for cand in candidates:
        if cand in col_map:
            return col_map[cand]
    num_cols = []
    for c in df.columns:
        col = pd.to_numeric(df[c], errors='coerce')
        if col.notna().sum() > 0 and col.nunique(dropna=True) > 1:
            num_cols.append(c)
    if num_cols:
        return num_cols[0]
    for c in df.columns:
        if df[c].nunique(dropna=True) > 1:
            return c
    return df.columns[0]

def has_list_like(series):
    if series is None:
        return False
    try:
        sample = series.dropna().head(5)
    except Exception:
        return False
    for v in sample:
        if isinstance(v, (list, tuple, set)):
            return True
    return False

def combine_text(X):
    if isinstance(X, np.ndarray):
        X = pd.DataFrame(X)
    elif not hasattr(X, 'iterrows'):
        X = pd.DataFrame(X)
    def row_to_text(row):
        parts = []
        for v in row:
            if v is None or (isinstance(v, float) and np.isnan(v)):
                continue
            if isinstance(v, (list, tuple, set)):
                parts.extend([str(i) for i in v])
            elif isinstance(v, dict):
                parts.extend([str(k) for k in v.keys()])
                parts.extend([str(val) for val in v.values()])
            else:
                parts.append(str(v))
        if not parts:
            return "empty"
        return ' '.join(parts)
    return X.apply(row_to_text, axis=1)

def to_numeric_df(X):
    X_df = pd.DataFrame(X)
    return X_df.apply(pd.to_numeric, errors='coerce')

def to_str_df(X):
    X_df = pd.DataFrame(X)
    def convert(v):
        if v is None or (isinstance(v, float) and np.isnan(v)):
            return np.nan
        if isinstance(v, (list, tuple, set)):
            return ' '.join([str(i) for i in v])
        if isinstance(v, dict):
            return ' '.join([str(k) for k in v.keys()] + [str(val) for val in v.values()])
        return str(v)
    return X_df.applymap(convert)

def normalize_label(v):
    if isinstance(v, (list, tuple, set)):
        return ' '.join([str(i) for i in v])
    if isinstance(v, dict):
        return ' '.join([str(k) for k in v.keys()] + [str(val) for val in v.values()])
    return v

path = "train.json"
df = load_data(path)
df = normalize_columns(df)
assert df is not None and df.shape[0] > 0 and df.shape[1] > 0

target_col = choose_target(df)
df = df.copy()
df = df.loc[:, ~df.columns.str.contains(r'^Unnamed', na=False)]
df = df[df[target_col].notna()]
assert df.shape[0] > 0

X = df.drop(columns=[target_col])
y = df[target_col]

if X.shape[1] == 0:
    X = pd.DataFrame({'const': np.ones(len(df))})

X = X.replace([np.inf, -np.inf], np.nan)

if y.dtype == object or str(y.dtype).startswith('category'):
    task = 'classification'
else:
    unique = y.nunique(dropna=True)
    task = 'classification' if unique <= 20 else 'regression'

if task == 'classification' and y.nunique(dropna=True) < 2:
    task = 'regression'

if task == 'classification':
    y = y.apply(normalize_label)
else:
    y = pd.to_numeric(y, errors='coerce')
    y = y.replace([np.inf, -np.inf], np.nan)
    mask = y.notna()
    X = X.loc[mask]
    y = y.loc[mask]
    if len(y) == 0:
        X = pd.DataFrame({'const': np.ones(len(df))})
        y = pd.Series(np.zeros(len(df)))

if task == 'classification':
    text_cols = [c for c in X.columns if X[c].dtype == object or has_list_like(X[c])]
    if text_cols:
        X = X[text_cols]

if X.shape[1] == 0:
    X = pd.DataFrame({'const': np.ones(len(df))})

n_samples = len(X)
assert n_samples > 0

test_size = 0.2 if n_samples >= 10 else 0.5
stratify = None
if task == 'classification' and y.nunique(dropna=True) > 1:
    counts = y.value_counts()
    if counts.min() >= 2:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=42, stratify=stratify
)
assert len(X_train) > 0 and len(X_test) > 0

trivial_classification = False
if task == 'classification' and y_train.nunique(dropna=True) < 2:
    trivial_classification = True
    majority = y_train.iloc[0] if len(y_train) > 0 else y.mode().iloc[0]
    y_pred = np.full(len(y_test), majority)
    accuracy = accuracy_score(y_test, y_pred) if len(y_test) > 0 else 0.0
else:
    if task == 'classification':
        contains_text = any([c in X.columns for c in X_train.columns if X_train[c].dtype == object or has_list_like(X_train[c])])
        if contains_text:
            model = Pipeline([
                ('combine', FunctionTransformer(combine_text, validate=False)),
                ('vect', CountVectorizer(max_features=5000, binary=True)),
                ('clf', MultinomialNB())
            ])
        else:
            num_pipeline = Pipeline([
                ('to_num', FunctionTransformer(to_numeric_df, validate=False)),
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler())
            ])
            model = Pipeline([
                ('preprocess', num_pipeline),
                ('clf', LogisticRegression(max_iter=200, solver='liblinear'))
            ])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        numeric_cols = []
        categorical_cols = []
        for c in X.columns:
            if has_list_like(X[c]):
                categorical_cols.append(c)
                continue
            col_numeric = pd.to_numeric(X[c], errors='coerce')
            if col_numeric.notna().sum() > 0 and (X[c].dtype != object or col_numeric.notna().sum() / len(X[c]) > 0.8):
                numeric_cols.append(c)
            else:
                categorical_cols.append(c)
        if len(numeric_cols) == 0 and len(categorical_cols) == 0:
            numeric_cols = ['const']
            X_train = pd.DataFrame({'const': np.ones(len(X_train))}, index=X_train.index)
            X_test = pd.DataFrame({'const': np.ones(len(X_test))}, index=X_test.index)
        transformers = []
        if numeric_cols:
            numeric_transformer = Pipeline([
                ('to_num', FunctionTransformer(to_numeric_df, validate=False)),
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler(with_mean=False))
            ])
            transformers.append(('num', numeric_transformer, numeric_cols))
        if categorical_cols:
            categorical_transformer = Pipeline([
                ('to_str', FunctionTransformer(to_str_df, validate=False)),
                ('imputer', SimpleImputer(strategy='most_frequent')),
                ('onehot', OneHotEncoder(handle_unknown='ignore'))
            ])
            transformers.append(('cat', categorical_transformer, categorical_cols))
        if transformers:
            preprocessor = ColumnTransformer(transformers=transformers)
            model = Pipeline([
                ('preprocess', preprocessor),
                ('reg', Ridge(alpha=1.0))
            ])
        else:
            model = Pipeline([
                ('to_num', FunctionTransformer(to_numeric_df, validate=False)),
                ('imputer', SimpleImputer(strategy='median')),
                ('reg', Ridge(alpha=1.0))
            ])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        if len(y_test) < 2:
            accuracy = 0.0
        else:
            score = r2_score(y_test, y_pred)
            if not np.isfinite(score):
                score = 0.0
            accuracy = max(0.0, min(1.0, (score + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Selected lightweight CountVectorizer + MultinomialNB for text inputs to keep CPU and memory use low.
# - Limited vocabulary size and used binary counts to reduce feature dimensionality and energy cost.
# - Applied simple linear models with minimal preprocessing for numeric/regression fallbacks.
# - Implemented robust schema handling and coercion to avoid expensive failures and ensure reproducibility.