# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import json
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, FunctionTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.impute import SimpleImputer


def _normalize_columns(cols):
    out = []
    for c in cols:
        if c is None:
            c = ""
        c = str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        out.append(c)
    return out


def _drop_unnamed_columns(df):
    drop_cols = [c for c in df.columns if isinstance(c, str) and c.strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _safe_read_table(path):
    # Robust parsing: try csv defaults then fallback to European separators
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError("CSV parsing produced <=1 column, retry with sep/decimal fallback.")
        return df
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
        return df


def _safe_read_json(path):
    # Try line-delimited JSON then regular JSON list/dict
    try:
        df = pd.read_json(path, lines=True)
        if df.shape[0] == 0 and df.shape[1] == 0:
            raise ValueError("Empty JSON with lines=True.")
        return df
    except Exception:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        df = pd.DataFrame(data)
        return df


def _load_dataset(path):
    lower = path.lower()
    if lower.endswith(".json"):
        df = _safe_read_json(path)
    elif lower.endswith(".csv"):
        df = _safe_read_table(path)
    else:
        # Best-effort: try CSV then JSON
        try:
            df = _safe_read_table(path)
        except Exception:
            df = _safe_read_json(path)

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed_columns(df)
    return df


def _coerce_numeric_series(s):
    s = pd.to_numeric(s, errors="coerce")
    s = s.replace([np.inf, -np.inf], np.nan)
    return s


def _choose_target_and_task(df):
    # Prefer common labels for this project, but never hard-fail; choose best available.
    cols_l = {c.lower(): c for c in df.columns}
    preferred_targets = ["cuisine", "label", "target", "y", "class"]
    for pt in preferred_targets:
        if pt in cols_l:
            tcol = cols_l[pt]
            y_raw = df[tcol]
            y_nonnull = y_raw.dropna()
            nunique = y_nonnull.nunique(dropna=True)
            if nunique >= 2:
                return tcol, "classification"

    # Otherwise, pick a non-constant object column with >=2 unique values (classification)
    obj_cols = [c for c in df.columns if df[c].dtype == "object"]
    for c in obj_cols:
        nunique = df[c].dropna().nunique(dropna=True)
        if nunique >= 2 and nunique <= max(2, int(0.5 * len(df))):
            return c, "classification"

    # Otherwise pick a numeric, non-constant column for regression
    numeric_candidates = []
    for c in df.columns:
        s = _coerce_numeric_series(df[c])
        if s.notna().sum() >= max(5, int(0.1 * len(df))):
            nunique = s.dropna().nunique()
            if nunique >= 2:
                numeric_candidates.append((c, s.notna().mean(), nunique))
    if numeric_candidates:
        numeric_candidates.sort(key=lambda x: (x[1], x[2]), reverse=True)
        return numeric_candidates[0][0], "regression"

    # Fallback: last column as target; task decided later
    tcol = df.columns[-1]
    y_raw = df[tcol]
    if y_raw.dropna().nunique(dropna=True) >= 2:
        return tcol, "classification"
    return tcol, "regression"


def _join_ingredients_cell(x):
    # Convert typical "ingredients" field (list of strings) to a single string for vectorizers
    if isinstance(x, list) or isinstance(x, tuple) or isinstance(x, set):
        parts = []
        for v in x:
            if v is None:
                continue
            parts.append(str(v))
        return " ".join(parts)
    if pd.isna(x):
        return ""
    return str(x)


def _make_text_from_row(row):
    # Compact text representation for mixed-type rows; avoids heavy feature engineering
    parts = []
    for v in row:
        if v is None or (isinstance(v, float) and np.isnan(v)):
            continue
        if isinstance(v, (list, tuple, set)):
            parts.append(_join_ingredients_cell(v))
        else:
            parts.append(str(v))
    return " ".join(parts)


def _build_features(df, target_col):
    X = df.drop(columns=[target_col], errors="ignore").copy()

    # Special-case: if "ingredients" exists, use it as primary text feature.
    cols_l = {c.lower(): c for c in X.columns}
    ing_col = cols_l.get("ingredients", None)
    if ing_col is not None:
        X_text = X[[ing_col]].copy()
        X_text[ing_col] = X_text[ing_col].map(_join_ingredients_cell)
        return X_text, "ingredients_text", ing_col

    # Otherwise: if multiple columns, compress into one text column to remain CPU-light and robust.
    if X.shape[1] == 0:
        X_text = pd.DataFrame({"__text__": [""] * len(df)})
        return X_text, "row_text", "__text__"

    # If there's exactly one column, use it (as text or structured).
    if X.shape[1] == 1:
        c = X.columns[0]
        if X[c].dtype == "object" or isinstance(X[c].iloc[0], (list, tuple, set)):
            X_text = X[[c]].copy()
            X_text[c] = X_text[c].map(_join_ingredients_cell)
            return X_text, "single_text", c
        return X, "structured", None

    # Mixed columns: build a row-wise text field (cheap, no schema assumptions)
    X_text = pd.DataFrame({"__text__": X.apply(_make_text_from_row, axis=1)})
    return X_text, "row_text", "__text__"


def _bounded_regression_accuracy(y_true, y_pred):
    # Convert regression quality to a stable [0,1] "accuracy" proxy using clipped R^2.
    r2 = r2_score(y_true, y_pred)
    if not np.isfinite(r2):
        r2 = -1.0
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    dataset_path = "train.json"
    df = _load_dataset(dataset_path)

    assert df is not None and isinstance(df, pd.DataFrame)
    df = df.copy()
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed_columns(df)
    df = df.dropna(how="all")
    assert len(df) > 0

    target_col, task = _choose_target_and_task(df)

    # Prepare y
    y_raw = df[target_col] if target_col in df.columns else pd.Series([np.nan] * len(df))
    if task == "regression":
        y = _coerce_numeric_series(y_raw)
        valid = y.notna()
    else:
        y = y_raw.astype("object")
        valid = y.notna()

    # Remove rows with missing target
    df2 = df.loc[valid].reset_index(drop=True)
    y = y.loc[valid].reset_index(drop=True)

    assert len(df2) > 1

    X, feat_mode, text_col = _build_features(df2, target_col)

    # If classification but too few classes, fallback to regression if possible
    if task == "classification":
        n_classes = y.dropna().nunique(dropna=True)
        if n_classes < 2:
            # fallback: try regression on numeric coercion
            y_num = _coerce_numeric_series(y_raw.loc[valid])
            if y_num.notna().sum() > 1 and y_num.nunique() >= 2:
                task = "regression"
                y = y_num.reset_index(drop=True)
            else:
                # trivial baseline: constant predictor, "accuracy" = 1.0 if all same
                accuracy = 1.0
                print(f"ACCURACY={accuracy:.6f}")
                return

    # Train/test split
    if task == "classification":
        stratify = y if y.nunique(dropna=True) >= 2 and (y.value_counts().min() >= 2) else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

    assert len(X_train) > 0 and len(X_test) > 0

    # Build pipeline
    if feat_mode in ("ingredients_text", "single_text", "row_text"):
        # Text vectorization + MultinomialNB is very CPU-friendly and strong baseline for bag-of-words
        # Use conservative feature cap to keep memory/CPU low
        text_pipeline = Pipeline(steps=[
            ("tfidf", TfidfVectorizer(
                lowercase=True,
                max_features=20000,
                ngram_range=(1, 1),
                min_df=2,
                token_pattern=r"(?u)\b[a-zA-Z][a-zA-Z+\-']+\b"
            ))
        ])

        preprocessor = ColumnTransformer(
            transformers=[
                ("text", text_pipeline, text_col),
            ],
            remainder="drop",
            sparse_threshold=0.3,
        )

        if task == "classification":
            model = MultinomialNB(alpha=0.5)
        else:
            # Ridge handles sparse tf-idf efficiently on CPU
            model = Ridge(alpha=1.0, random_state=42)

        clf = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ])
    else:
        # Structured data path: lightweight linear model with OHE for categoricals
        Xs = X.copy()
        Xs.columns = _normalize_columns(Xs.columns)

        # Identify columns
        num_cols, cat_cols = [], []
        for c in Xs.columns:
            if pd.api.types.is_numeric_dtype(Xs[c]):
                num_cols.append(c)
            else:
                # attempt numeric coercion; if mostly numeric keep as numeric
                s_num = _coerce_numeric_series(Xs[c])
                if s_num.notna().mean() >= 0.95 and s_num.nunique(dropna=True) >= 2:
                    Xs[c] = s_num
                    num_cols.append(c)
                else:
                    cat_cols.append(c)

        numeric_transformer = Pipeline(steps=[
            ("impute", SimpleImputer(strategy="median")),
        ])
        categorical_transformer = Pipeline(steps=[
            ("impute", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ])

        preprocessor = ColumnTransformer(
            transformers=[
                ("num", numeric_transformer, num_cols),
                ("cat", categorical_transformer, cat_cols),
            ],
            remainder="drop",
            sparse_threshold=0.3,
        )

        if task == "classification":
            # Small, CPU-friendly solver; good baseline
            model = LogisticRegression(
                max_iter=200,
                solver="liblinear",
                random_state=42
            )
        else:
            model = Ridge(alpha=1.0, random_state=42)

        clf = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ])

        X_train, X_test = Xs.loc[X_train.index], Xs.loc[X_test.index]

    # Fit + evaluate
    clf.fit(X_train, y_train)

    if task == "classification":
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        y_pred = clf.predict(X_test)
        y_test_num = _coerce_numeric_series(pd.Series(y_test))
        y_pred_num = pd.Series(y_pred)
        # Align and drop NaNs defensively
        mask = y_test_num.notna() & pd.to_numeric(y_pred_num, errors="coerce").notna()
        if mask.sum() < 2:
            accuracy = 0.0
        else:
            accuracy = _bounded_regression_accuracy(y_test_num[mask].to_numpy(), pd.to_numeric(y_pred_num[mask], errors="coerce").to_numpy())

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses a CPU-friendly baseline (MultinomialNB for text, LogisticRegression/Ridge for structured) to minimize compute and memory.
# - Robust schema handling: auto-detects target, handles missing/unknown columns, normalizes headers, and drops 'Unnamed' columns.
# - Energy-aware feature design: prefers a single text channel (ingredients or row-wise text) with capped TF-IDF features to bound CPU/RAM.
# - Reproducibility: fixed random_state in splits/models; preprocessing encapsulated in sklearn Pipeline/ColumnTransformer to avoid redundant work.
# - Defensive data hygiene: numeric coercion with errors='coerce', NaN/inf handling, non-empty asserts, safe fallbacks for degenerate targets.
# - Regression fallback reports ACCURACY as clipped (R^2+1)/2 in [0,1] to maintain a stable scalar metric without extra computation.