# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import random
import math
import pandas as pd

SEED = 42
random.seed(SEED)

DATASET_HEADERS = "town,apartments_condos_multis_per_residential_parcels_2011,assessed_home_value_changes_2009-2013,births_per_1000_residents_2010,boaters_per_10000_residents_2012,burglaries_per_10000_residents_2011,cars_motorcycles_&_trucks_average_age_2012,cars_per_1000_residents_2012,class_size_in_school_district_2011-2012,condos_as_perc_of_parcels_2012,crashes_per_1000_residents_2007-2011,culture_and_rec_spending_per_person_2012,education_spending_as_a_percent_2012,education_spending_per_resident_2012,expenditures_per_resident_2012,females_percent_in_community_2010,fire_dept_spending_as_a_percent_2012,firefighter_costs_per_resident_2012,fixed_costs_percent_2012,gun_licenses_per_1000_residents_2012,historic_places_per_10000_2013,home_schooled_per_1000_students_2011-2012,homes_built_in_39_or_before,household_member_who_is_2_races_or_more_per_1000_households_2010,households_average_size_2010,households_one-person_2010,hybrid_cars_per_1000_vehicles_2013,in_home_since_1969_or_earlier,income_average_per_resident_2010,income_change_per_resident_2007-2010,inmates_in_state_prison_per_1000_residents,liquor_licenses_per_10000_2011,median_age_2011,miles_driven_daily_per_household_05-07,minority_students_per_district_2012-2013,motorcycles_change_in_ownership_2000-2012,motorcycles_per_1000_2012,multi-generation_households_2010,police_costs_per_resident_2013,police_employees_per_10000_residents_2011,police_spending_as_a_percent_2012,population_change_1950-2010,population_change_2010-2011,presidential_fundraising_obama_vs_romney,property_crimes_per_10000_residents_2012,property_tax_change_09-13,pupils_per_cost_average_by_district_2011-2012,residential_taxes_as_percent_of_all_property_taxes_2013,saltwater_fishing_licenses_per_1000_2013,school_district_growth_09-13,single-person_households_percent_65_and_older,snowmobiles_per_10000_residents_2012,state_aid_as_a_percent_of_town_budget_2012,students_in_public_schools_2011,tax-exempt_property_2012,taxable_property_by_percent_2012,teacher_salaries_by_average_2011,teachers_percent_under_40_years_old_2011-2012,trucks_per_1000_residents_2012,violent_crimes_per_10000_residents_2012,voters_as_a_percent_of_population_2012,voters_change_in_registrations_between_1982-2012,voters_democrats_as_a_percent_2012,2020_votes,2020_biden_margin,population,vax_level"
EXPECTED_HEADERS = [h.strip() for h in DATASET_HEADERS.split(",")]

def _is_parsing_ok(df, expected_headers):
    if df is None or df.empty or df.shape[1] <= 1:
        return False
    cols = [str(c).strip() for c in df.columns]
    exp_set = set(expected_headers)
    match = sum(1 for c in cols if c in exp_set)
    if match < max(1, len(cols) // 2):
        return False
    id_col = expected_headers[0] if expected_headers else None
    class_col = expected_headers[-1] if expected_headers else None
    for c in cols:
        if c == id_col or c == class_col:
            continue
        if df[c].dtype == object:
            sample = df[c].dropna()
            if not sample.empty:
                for s in sample.astype(str).head(5):
                    if "," in s and s.replace(",", "").replace(".", "").replace("-", "").isdigit():
                        return False
    return True

def read_data(csv_path, expected_headers):
    df = pd.read_csv(csv_path)
    if not _is_parsing_ok(df, expected_headers):
        df = pd.read_csv(csv_path, sep=";", decimal=",")
    df.columns = [str(c).strip() for c in df.columns]
    records = df.to_dict(orient="records")
    examples = []
    for rec in records:
        ex = {}
        for k, v in rec.items():
            if pd.isna(v):
                ex[k] = None
            elif isinstance(v, str):
                vs = v.strip()
                if vs == "":
                    ex[k] = None
                else:
                    try:
                        ex[k] = float(vs)
                    except ValueError:
                        ex[k] = vs
            else:
                try:
                    ex[k] = float(v)
                except (TypeError, ValueError):
                    ex[k] = v
        examples.append(ex)
    return examples, list(df.columns)

def train_test_split(examples, test_perc, seed):
    rng = random.Random(seed)
    shuffled = examples[:]
    rng.shuffle(shuffled)
    test_size = int(round(test_perc * len(examples)))
    return shuffled[test_size:], shuffled[:test_size]

class TreeNodeInterface:
    __slots__ = ()
    def classify(self, example):
        raise NotImplementedError

class DecisionNode(TreeNodeInterface):
    __slots__ = ("test_attr_name", "test_attr_threshold", "child_lt", "child_ge", "child_miss")
    def __init__(self, test_attr_name, test_attr_threshold, child_lt, child_ge, child_miss):
        self.test_attr_name = test_attr_name
        self.test_attr_threshold = test_attr_threshold
        self.child_ge = child_ge
        self.child_lt = child_lt
        self.child_miss = child_miss
    def classify(self, example):
        test_val = example[self.test_attr_name]
        if test_val is None:
            return self.child_miss.classify(example)
        if test_val < self.test_attr_threshold:
            return self.child_lt.classify(example)
        return self.child_ge.classify(example)

class LeafNode(TreeNodeInterface):
    __slots__ = ("pred_class", "pred_class_count", "total_count", "prob")
    def __init__(self, pred_class, pred_class_count, total_count):
        self.pred_class = pred_class
        self.pred_class_count = pred_class_count
        self.total_count = total_count
        self.prob = pred_class_count / total_count if total_count else 0.0
    def classify(self, example):
        return self.pred_class, self.prob

class DecisionTree:
    __slots__ = ("id_name", "class_name", "min_leaf_count", "root")
    def __init__(self, examples, id_name, class_name, min_leaf_count=1):
        self.id_name = id_name
        self.class_name = class_name
        self.min_leaf_count = min_leaf_count
        self.root = self.learn_tree(examples)
    def learn_tree(self, examples):
        attribute_list = [a for a in examples[0] if a != self.id_name and a != self.class_name]
        return attributeSplit(attribute_list, examples, self.min_leaf_count, self.class_name)
    def classify(self, example):
        return self.root.classify(example)

def attributeSplit(attribute_set, examples, min_leaf_count, class_name):
    attribute_name, threshold, examples_lt, examples_ge = getBestAttributeAndSplit(attribute_set, examples, class_name)
    if len(examples_ge) <= min_leaf_count or len(examples_lt) <= min_leaf_count:
        predictiveClass, predictiveClassCount = getPredictiveClass(examples, class_name)
        return LeafNode(predictiveClass, predictiveClassCount, len(examples))
    attribute_set.remove(attribute_name)
    child_lt = attributeSplit(attribute_set, examples_lt, min_leaf_count, class_name)
    child_ge = attributeSplit(attribute_set, examples_ge, min_leaf_count, class_name)
    child_miss = child_lt if len(examples_lt) >= len(examples_ge) else child_ge
    return DecisionNode(attribute_name, threshold, child_lt, child_ge, child_miss)

def getBestAttributeAndSplit(attribute_set, examples, class_label):
    max_name = ""
    max_info = 0.0
    max_threshold = None
    max_lt = []
    max_ge = []
    for attribute in attribute_set:
        info_gain, threshold, lt, ge = getInfoGain(attribute, examples, class_label)
        if info_gain > max_info:
            max_name = attribute
            max_info = info_gain
            max_threshold = threshold
            max_lt = lt
            max_ge = ge
    return max_name, max_threshold, max_lt, max_ge

def getInfoGain(attribute, examples, class_label):
    max_info_gain = 0.0
    threshold = 0.0
    lt_split = []
    ge_split = []
    min_val, max_val, step = getRange(attribute, examples)
    if step <= 0:
        return max_info_gain, threshold, lt_split, ge_split
    base_entropy = entropy(examples, class_label)
    total = len(examples)
    cur = min_val + step
    while cur < max_val:
        lt, ge = splitExamplesOnAttribute(attribute, examples, cur)
        if total:
            pc1 = len(lt) / total
            pc2 = len(ge) / total
            info_gain = base_entropy - (pc1 * entropy(lt, class_label) + pc2 * entropy(ge, class_label))
        else:
            info_gain = 0.0
        if info_gain > max_info_gain:
            max_info_gain = info_gain
            threshold = cur
            lt_split = lt
            ge_split = ge
        cur += step
    return max_info_gain, threshold, lt_split, ge_split

def getRange(attribute, examples):
    min_val = 1000000.0
    max_val = -1000000.0
    for ex in examples:
        val = ex[attribute]
        if val is None:
            continue
        v = float(val)
        if v < min_val:
            min_val = v
        if v > max_val:
            max_val = v
    step = (max_val - min_val) / 15.0
    return min_val, max_val, step

def splitExamplesOnAttribute(attribute, examples, threshold):
    lt = []
    ge = []
    for example in examples:
        val = example[attribute]
        if val is None:
            continue
        if val >= threshold:
            ge.append(example)
        else:
            lt.append(example)
    return lt, ge

def entropy(examples, class_label):
    total = len(examples)
    if total == 0:
        return 0.0
    counts = {}
    for example in examples:
        lbl = example[class_label]
        counts[lbl] = counts.get(lbl, 0) + 1
    ent = 0.0
    for count in counts.values():
        p = count / total
        if p:
            ent -= p * math.log(p, 2)
    return ent

def getPredictiveClass(examples, class_label):
    classDict = {}
    max_name = ""
    max_count = 0
    for example in examples:
        class_name = example[class_label]
        if class_name not in classDict:
            classDict[class_name] = 0
        else:
            classDict[class_name] += 1
        if classDict[class_name] > max_count:
            max_name = class_name
            max_count = classDict[class_name]
    return max_name, max_count

def evaluate_model(model, test_examples):
    correct = 0
    total = len(test_examples)
    for example in test_examples:
        pred, _ = model.classify(example)
        if pred == example[model.class_name]:
            correct += 1
    return correct / total if total else 0.0

def find_column(columns, target_names):
    lower_map = {c.lower(): c for c in columns}
    for name in target_names:
        if name.lower() in lower_map:
            return lower_map[name.lower()]
    return None

if __name__ == "__main__":
    path_to_csv = "town_vax_data.csv"
    examples, columns = read_data(path_to_csv, EXPECTED_HEADERS)
    id_attr_name = find_column(columns, [EXPECTED_HEADERS[0], "town"]) or columns[0]
    class_attr_name = find_column(columns, [EXPECTED_HEADERS[-1], "vax_level"]) or columns[-1]
    train_examples, test_examples = train_test_split(examples, 0.25, SEED)
    tree = DecisionTree(train_examples, id_attr_name, class_attr_name, 10)
    accuracy = evaluate_model(tree, test_examples)
    print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Added robust pandas parsing with delimiter/decimal fallback and lightweight schema checks to avoid misreads.
# - Used deterministic seeding and in-place shuffling to reduce overhead and ensure reproducible splits.
# - Reduced redundant entropy work by caching base entropy per attribute and counting labels directly.
# - Applied __slots__ in node classes to cut memory usage during tree construction.
# - Removed per-instance output and unused visualization logic to eliminate unnecessary I/O and computation.