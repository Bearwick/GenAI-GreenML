# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import random
import math

random.seed(42)

def read_data(csv_path):
    try:
        df = pd.read_csv(csv_path)
        if df.shape[1] <= 1:
            raise ValueError
    except:
        df = pd.read_csv(csv_path, sep=';', decimal=',')
    
    headers = df.columns.tolist()
    data = df.where(pd.notnull(df), None).values.tolist()
    return data, headers

def train_test_split(examples, test_perc):
    test_size = round(test_perc * len(examples))
    shuffled = random.sample(examples, len(examples))
    return shuffled[test_size:], shuffled[:test_size]

class TreeNodeInterface:
    def classify(self, example):
        raise NotImplementedError

class DecisionNode(TreeNodeInterface):
    def __init__(self, test_attr_idx, test_attr_threshold, child_lt, child_ge, child_miss):
        self.test_attr_idx = test_attr_idx
        self.test_attr_threshold = test_attr_threshold
        self.child_ge = child_ge
        self.child_lt = child_lt
        self.child_miss = child_miss

    def classify(self, example):
        test_val = example[self.test_attr_idx]
        if test_val is None:
            return self.child_miss.classify(example)
        return self.child_lt.classify(example) if test_val < self.test_attr_threshold else self.child_ge.classify(example)

class LeafNode(TreeNodeInterface):
    def __init__(self, pred_class, pred_class_count, total_count):
        self.pred_class = pred_class
        self.prob = pred_class_count / total_count if total_count > 0 else 0

    def classify(self, example):
        return self.pred_class, self.prob

def get_entropy(examples, class_idx):
    if not examples:
        return 0
    counts = {}
    for ex in examples:
        lbl = ex[class_idx]
        counts[lbl] = counts.get(lbl, 0) + 1
    
    n = len(examples)
    ent = 0
    for c in counts.values():
        p = c / n
        if p > 0:
            ent -= p * math.log2(p)
    return ent

def get_predictive_class(examples, class_idx):
    counts = {}
    m_lbl, m_cnt = "", 0
    for ex in examples:
        lbl = ex[class_idx]
        c = counts.get(lbl, 0)
        if c > m_cnt:
            m_cnt = c
            m_lbl = lbl
        counts[lbl] = c + 1
    return m_lbl, m_cnt

def get_range(attr_idx, examples):
    mn, mx = 1000000.0, -1000000.0
    found = False
    for ex in examples:
        v = ex[attr_idx]
        if v is not None:
            fv = float(v)
            if fv < mn: mn = fv
            if fv > mx: mx = fv
            found = True
    if not found:
        return 0, 0, 0
    return mn, mx, (mx - mn) / 15

def get_best_split(attr_set, examples, class_idx, attr_to_idx):
    best = {"name": "", "ig": 0.0, "threshold": None, "lt": [], "ge": []}
    parent_ent = get_entropy(examples, class_idx)
    n_total = len(examples)

    for attr_name in attr_set:
        idx = attr_to_idx[attr_name]
        mn, mx, step = get_range(idx, examples)
        curr_t = mn + step
        while curr_t < mx:
            lt, ge = [], []
            for ex in examples:
                v = ex[idx]
                if v is not None:
                    if v >= curr_t: ge.append(ex)
                    else: lt.append(ex)
            
            if lt and ge:
                ig = parent_ent - ((len(lt)/n_total * get_entropy(lt, class_idx)) + (len(ge)/n_total * get_entropy(ge, class_idx)))
                if ig > best["ig"]:
                    best = {"name": attr_name, "ig": ig, "threshold": curr_t, "lt": lt, "ge": ge}
            curr_t += step
    return best

def attribute_split(attr_set, examples, min_leaf_count, class_idx, attr_to_idx):
    best = get_best_split(attr_set, examples, class_idx, attr_to_idx)
    
    if not best["name"] or len(best["lt"]) <= min_leaf_count or len(best["ge"]) <= min_leaf_count:
        p_cls, p_cnt = get_predictive_class(examples, class_idx)
        return LeafNode(p_cls, p_cnt, len(examples))

    attr_name = best["name"]
    attr_set.remove(attr_name)
    
    child_lt = attribute_split(attr_set, best["lt"], min_leaf_count, class_idx, attr_to_idx)
    child_ge = attribute_split(attr_set, best["ge"], min_leaf_count, class_idx, attr_to_idx)
    child_miss = child_lt if len(best["lt"]) >= len(best["ge"]) else child_ge
    
    return DecisionNode(attr_to_idx[attr_name], best["threshold"], child_lt, child_ge, child_miss)

class DecisionTree:
    def __init__(self, examples, id_name, class_name, headers, min_leaf_count=1):
        self.class_idx = headers.index(class_name)
        attr_to_idx = {h: i for i, h in enumerate(headers)}
        attr_set = {h for h in headers if h != id_name and h != class_name}
        self.root = attribute_split(attr_set, examples, min_leaf_count, self.class_idx, attr_to_idx)

    def classify(self, example):
        return self.root.classify(example)

if __name__ == '__main__':
    csv_path = 'town_vax_data.csv'
    data, headers = read_data(csv_path)
    train_ex, test_ex = train_test_split(data, 0.25)
    
    tree = DecisionTree(train_ex, 'town', 'vax_level', headers, 10)
    
    correct = 0
    c_idx = headers.index('vax_level')
    for ex in test_ex:
        pred, _ = tree.root.classify(ex)
        if pred == ex[c_idx]:
            correct += 1
    
    accuracy = correct / len(test_ex) if test_ex else 0
    print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Replaced dictionary-based example storage with list-of-lists and index-based lookups to significantly reduce memory overhead and hashing time.
# 2. Implemented robust CSV parsing with pandas to handle varying separators and speed up initial data loading.
# 3. Optimized entropy calculation by using frequency counters instead of building lists of full objects for each class.
# 4. Reduced redundant computation by pre-calculating parent entropy at each split node and avoiding multiple type conversions of the same data.
# 5. Minimized object creation by passing indices and sets by reference, following the original logic's attribute exhaustion behavior.
# 6. Set a fixed random seed to ensure reproducible results across runs without adding computational variance.
# 7. Removed all non-essential logging, string formatting, and visualizations to focus resources solely on the core ML task.