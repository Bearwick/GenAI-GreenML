# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

def load_data(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError
    except:
        df = pd.read_csv(path, sep=';', decimal=',')
    
    # Normalize column names
    df.columns = [str(col).strip().replace('\n', ' ') for col in df.columns]
    df.columns = [" ".join(col.split()) for col in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def solve():
    dataset_path = 'town_vax_data.csv'
    try:
        df = load_data(dataset_path)
    except Exception:
        # Fallback for environment issues
        print("ACCURACY=0.000000")
        return

    if df.empty:
        print("ACCURACY=0.000000")
        return

    # Identify Target
    target_col = 'vax_level'
    if target_col not in df.columns:
        # Heuristic: pick the last column if vax_level is missing
        target_col = df.columns[-1]

    # Data Cleaning: Handle non-numeric columns and target
    y = df[target_col].astype(str)
    X = df.drop(columns=[target_col])
    
    # Drop high-cardinality ID-like columns (like town names)
    if 'town' in X.columns:
        X = X.drop(columns=['town'])

    # Identify column types
    numeric_features = []
    categorical_features = []
    
    for col in X.columns:
        # Attempt to coerce to numeric to see if it's possible
        converted = pd.to_numeric(X[col], errors='coerce')
        if converted.notna().sum() > (0.5 * len(X)):
            X[col] = converted
            numeric_features.append(col)
        else:
            categorical_features.append(col)

    # Preprocessing Pipeline
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ],
        remainder='drop'
    )

    # Model Selection: Decision Tree (Lightweight, interpretable, CPU-friendly)
    # Aligns with provided project context on Decision Trees
    clf = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', DecisionTreeClassifier(
            max_depth=5, 
            min_samples_leaf=5, 
            random_state=42
        ))
    ])

    # Defensive check for target classes
    if y.nunique() < 2:
        # Trivial baseline if only one class exists
        print("ACCURACY=1.000000")
        return

    # Train/Test Split
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y if y.nunique() > 1 else None
        )
    except:
        # Fallback if stratify fails
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

    # Fit and Predict
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    
    # Evaluation
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# OPTIMIZATION SUMMARY
# 1. Model Choice: DecisionTreeClassifier was selected as it is computationally efficient on CPUs, 
#    handles non-linear relationships without complex kernels, and aligns with the project context.
# 2. Energy Efficiency: By limiting max_depth=5, we prevent unnecessary computations/overfitting 
#    and ensure the model remains small and fast to evaluate.
# 3. Robustness: Implemented a two-stage CSV parser (comma vs semicolon/decimal) to handle common 
#    European/US data format discrepancies.
# 4. Feature Engineering: Minimal approach using StandardScaler and OneHotEncoder within a 
#    Pipeline to reduce memory overhead and ensure data consistency.
# 5. Data Integrity: Automatic coercion of numeric types handles cases where numeric data is 
#    misclassified as objects due to stray strings or formatting issues.
# 6. Fallbacks: The script includes guards against empty datasets, missing targets, or 
#    single-class distributions to ensure it always runs end-to-end.