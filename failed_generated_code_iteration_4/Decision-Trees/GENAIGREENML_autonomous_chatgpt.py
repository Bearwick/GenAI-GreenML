# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "town_vax_data.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _try_read_csv(path):
    # Robust CSV parsing fallback: try default, else try semicolon+comma decimal.
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(dfx):
        if dfx is None or dfx.empty:
            return True
        # If only 1 column, likely separator issue.
        if dfx.shape[1] <= 1:
            return True
        return False

    if _looks_wrong(df):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if not _looks_wrong(df2):
                df = df2
        except Exception:
            pass

    if df is None:
        raise RuntimeError("Failed to read CSV with available parsers.")
    return df


def _safe_numeric_series(s):
    s2 = pd.to_numeric(s, errors="coerce")
    s2 = s2.replace([np.inf, -np.inf], np.nan)
    return s2


def _pick_target(df):
    # Prefer explicit target if present, else choose best-effort numeric non-constant, else any non-constant.
    cols_lower = {c.lower(): c for c in df.columns}
    for name in ["vax_level", "target", "label", "y"]:
        if name in cols_lower:
            return cols_lower[name]

    # Prefer last column if seems label-like (low cardinality or object)
    if df.shape[1] >= 2:
        last_col = df.columns[-1]
        s = df[last_col]
        nun = s.nunique(dropna=True)
        if (s.dtype == "object" and nun >= 2) or (nun >= 2 and nun <= max(20, int(0.1 * len(df)))):
            return last_col

    numeric_candidates = []
    for c in df.columns:
        s_num = _safe_numeric_series(df[c])
        nun = s_num.nunique(dropna=True)
        if nun >= 2:
            numeric_candidates.append((c, nun, s_num.notna().mean()))
    if numeric_candidates:
        # Choose with highest non-null ratio then moderate cardinality preference
        numeric_candidates.sort(key=lambda x: (x[2], -min(x[1], 1000)), reverse=True)
        return numeric_candidates[0][0]

    # Fallback: any non-constant column
    for c in df.columns:
        if df[c].nunique(dropna=True) >= 2:
            return c

    # If everything constant, just return last column; downstream will handle.
    return df.columns[-1]


def _infer_task(y_raw):
    # Return ("classification" or "regression"), and y_processed
    y = y_raw.copy()

    # If object: classification
    if y.dtype == "object":
        y2 = y.astype(str).fillna("NA")
        if y2.nunique(dropna=True) >= 2:
            return "classification", y2
        return "regression", _safe_numeric_series(y)

    # If numeric-like:
    y_num = _safe_numeric_series(y)
    nun = y_num.nunique(dropna=True)
    if nun <= 1:
        return "regression", y_num

    # Heuristic: small integer-ish unique counts => classification
    if nun <= 20:
        # Check if values are close to integers
        vals = y_num.dropna().values
        if vals.size > 0:
            if np.mean(np.isclose(vals, np.round(vals), atol=1e-8)) > 0.98:
                return "classification", y_num.round().astype("Int64").astype(str).fillna("NA")
        # Also if looks like bounded class codes
        if y_num.dropna().min() >= 0 and y_num.dropna().max() <= 100 and nun <= 10:
            return "classification", y_num.astype(str).fillna("NA")

    return "regression", y_num


def _regression_accuracy_proxy(y_true, y_pred):
    # Stable bounded proxy in [0,1]: 1 / (1 + NRMSE), with NRMSE normalized by y-range.
    yt = np.asarray(y_true, dtype=float)
    yp = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(yt) & np.isfinite(yp)
    if mask.sum() == 0:
        return 0.0
    yt = yt[mask]
    yp = yp[mask]
    mse = np.mean((yt - yp) ** 2)
    rmse = float(np.sqrt(mse))
    y_range = float(np.nanmax(yt) - np.nanmin(yt))
    if not np.isfinite(y_range) or y_range <= 1e-12:
        denom = float(np.nanstd(yt))
        if not np.isfinite(denom) or denom <= 1e-12:
            denom = 1.0
    else:
        denom = y_range
    nrmse = rmse / denom
    acc = 1.0 / (1.0 + nrmse)
    if not np.isfinite(acc):
        return 0.0
    return float(np.clip(acc, 0.0, 1.0))


def main():
    if not os.path.exists(DATASET_PATH):
        raise FileNotFoundError(f"Dataset not found: {DATASET_PATH}")

    df = _try_read_csv(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Ensure non-empty
    assert df.shape[0] > 0 and df.shape[1] > 0, "Empty dataset after loading/cleaning."

    target_col = _pick_target(df)
    if target_col not in df.columns:
        # Should not happen, but be defensive: fall back to last column
        target_col = df.columns[-1]

    y_raw = df[target_col]
    X = df.drop(columns=[target_col])

    # If X ends up empty, build a trivial feature from index to keep pipeline valid
    if X.shape[1] == 0:
        X = pd.DataFrame({"__idx__": np.arange(len(df))})

    task, y = _infer_task(y_raw)

    # Prepare feature types
    X = X.copy()
    X.columns = _normalize_columns(X.columns)
    X = _drop_unnamed(X)

    num_features = []
    cat_features = []
    for c in X.columns:
        # Treat as numeric if coercion yields enough non-nan values
        s_num = _safe_numeric_series(X[c])
        non_na_ratio = s_num.notna().mean()
        if non_na_ratio >= 0.8:
            num_features.append(c)
            X[c] = s_num
        else:
            # Keep as categorical (string); also keep original values
            cat_features.append(c)
            X[c] = X[c].astype(str).replace({"nan": np.nan})

    # If no numeric and no categorical (unlikely), create trivial numeric feature
    if len(num_features) == 0 and len(cat_features) == 0:
        X["__idx__"] = np.arange(len(X))
        num_features = ["__idx__"]

    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, num_features),
            ("cat", categorical_transformer, cat_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Split with defensiveness
    test_size = 0.2 if len(df) >= 10 else 0.3
    if task == "classification":
        # If too many rare classes, stratify can fail; only stratify when safe.
        y_for_split = y
        can_stratify = y_for_split.nunique(dropna=True) >= 2
        if can_stratify:
            vc = y_for_split.value_counts(dropna=True)
            can_stratify = bool((vc.min() >= 2) and (len(y_for_split) >= 10))
        stratify = y_for_split if can_stratify else None

        X_train, X_test, y_train, y_test = train_test_split(
            X, y_for_split, test_size=test_size, random_state=RANDOM_STATE, stratify=stratify
        )
    else:
        y_num = _safe_numeric_series(y)
        # Remove rows with missing y for regression to avoid propagating NaNs into fitting
        mask = y_num.notna()
        X2 = X.loc[mask].reset_index(drop=True)
        y2 = y_num.loc[mask].reset_index(drop=True)
        assert len(y2) > 1, "Not enough valid target values for regression."
        X_train, X_test, y_train, y_test = train_test_split(
            X2, y2, test_size=test_size, random_state=RANDOM_STATE
        )

    assert len(X_train) > 0 and len(X_test) > 0, "Empty train/test split."

    if task == "classification":
        # If after processing y has <2 classes, fallback to regression proxy on numeric-coerced y.
        if pd.Series(y_train).nunique(dropna=True) < 2 or pd.Series(y_test).nunique(dropna=True) < 1:
            y_num_all = _safe_numeric_series(y_raw)
            mask = y_num_all.notna()
            X2 = X.loc[mask].reset_index(drop=True)
            y2 = y_num_all.loc[mask].reset_index(drop=True)
            if len(y2) <= 1:
                accuracy = 0.0
                print(f"ACCURACY={accuracy:.6f}")
                return
            X_train, X_test, y_train, y_test = train_test_split(
                X2, y2, test_size=test_size, random_state=RANDOM_STATE
            )
            model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
            clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
            clf.fit(X_train, y_train)
            preds = clf.predict(X_test)
            accuracy = _regression_accuracy_proxy(y_test, preds)
            print(f"ACCURACY={accuracy:.6f}")
            return

        # Lightweight linear classifier; saga handles sparse one-hot efficiently.
        model = LogisticRegression(
            max_iter=300,
            solver="saga",
            penalty="l2",
            C=1.0,
            n_jobs=1,
            random_state=RANDOM_STATE,
        )
        clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        preds = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, preds))
    else:
        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        reg = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        reg.fit(X_train, y_train)
        preds = reg.predict(X_test)
        accuracy = _regression_accuracy_proxy(y_test, preds)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses CPU-friendly linear models (LogisticRegression/Ridge) to minimize compute and memory vs. ensembles/deep nets.
# - ColumnTransformer+Pipeline ensures one-pass, reproducible preprocessing and avoids duplicated transformations.
# - OneHotEncoder(handle_unknown="ignore") keeps categorical handling simple and sparse for efficiency.
# - Numeric coercion with errors='coerce' plus median imputation prevents costly failures and stabilizes training.
# - Robust CSV parsing fallback (default then sep=';' and decimal=',') reduces manual intervention and reruns.
# - Defensive target selection avoids hard schema assumptions; falls back to a valid task path if target unusable.
# - Regression fallback "accuracy" is a bounded proxy: ACCURACY = 1/(1+NRMSE), keeping output in [0,1] and stable.