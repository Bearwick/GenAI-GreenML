# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

def load_data(path):
    # Robust CSV parsing
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError
    except:
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except:
            return pd.DataFrame()
    
    # Normalize column names
    df.columns = [str(c).strip().replace('\n', ' ') for c in df.columns]
    df.columns = [" ".join(c.split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def solve():
    data_path = 'voting_data.csv'
    if not os.path.exists(data_path):
        # Fallback for environment issues: create dummy if file missing to ensure run
        print("ACCURACY=0.000000")
        return

    df = load_data(data_path)
    if df.empty:
        print("ACCURACY=0.000000")
        return

    # Identify columns dynamically
    cols = df.columns.tolist()
    text_col = None
    label_col = None

    # Priority 1: Match suggested headers
    for c in cols:
        if c.lower() == 'text': text_col = c
        if c.lower() == 'label': label_col = c

    # Priority 2: Inference based on content/type
    if not text_col:
        obj_cols = df.select_dtypes(include=['object']).columns
        text_col = obj_cols[0] if len(obj_cols) > 0 else cols[0]
    
    if not label_col:
        remaining = [c for c in cols if c != text_col]
        label_col = remaining[-1] if remaining else cols[-1]

    # Preprocessing
    df = df.dropna(subset=[text_col, label_col])
    X = df[text_col].astype(str)
    y = df[label_col].astype(str)

    if len(X) < 10: # Minimum sample check
        print("ACCURACY=0.000000")
        return

    # Ensure at least 2 classes for classification
    if y.nunique() < 2:
        print("ACCURACY=1.000000") # Trivial case
        return

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=y if y.nunique() > 1 else None
    )

    # Green pipeline: TF-IDF + Logistic Regression (Liblinear)
    # Energy efficient: Linear model, no heavy embeddings, limited features
    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(
            max_features=2000, 
            stop_words='english', 
            ngram_range=(1, 1),
            sublinear_tf=True
        )),
        ('clf', LogisticRegression(
            solver='liblinear', 
            penalty='l2', 
            C=1.0, 
            random_state=42,
            max_iter=100
        ))
    ])

    # Train
    pipeline.fit(X_train, y_train)

    # Evaluate
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# Optimization Summary:
# 1. Model Choice: Used Logistic Regression with the 'liblinear' solver. It is computationally 
#    lightweight, converges quickly on small-to-medium datasets, and runs efficiently on CPU.
# 2. Feature Engineering: Employed TfidfVectorizer with a capped feature limit (2000). This prevents 
#    memory bloat and reduces the dimensionality of the feature space, saving energy during training.
# 3. Text Processing: Used 'sublinear_tf' to scale term frequencies logarithmically, providing 
#    robustness with minimal extra CPU cycles.
# 4. Pipeline Efficiency: Used sklearn.pipeline.Pipeline to minimize data movement and ensure 
#    transformations are applied only once per fold/split.
# 5. Robustness: Implemented multi-stage CSV parsing and dynamic column detection to prevent 
#    hard failures and manual intervention requirements.
# 6. Hardware: Designed specifically for single-core CPU execution, avoiding high-wattage GPU/TPU dependencies.