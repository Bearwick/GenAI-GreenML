# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier

def read_data(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
        return df
    if df.shape[1] == 1:
        try:
            df2 = pd.read_csv(path, sep=';', decimal=',')
            if df2.shape[1] > 1:
                df = df2
        except Exception:
            pass
    return df

def normalize_name(x):
    x = str(x)
    x = x.strip()
    x = " ".join(x.split())
    return x

path = "EireJet (1).csv"
df = read_data(path)
df.columns = [normalize_name(c) for c in df.columns]
df = df.loc[:, [c for c in df.columns if not str(c).lower().startswith('unnamed')]]

dataset_headers = ["Gender", "Frequent Flyer", "Age", "Type of Travel", "Class", "Flight Distance", "Inflight wifi service", "Departure/Arrival time convenient", "Ease of Online booking", "Gate location", "Food and drink", "Online boarding", "Seat comfort", "Inflight entertainment", "On-board service", "Leg room service", "Baggage handling", "Checkin service", "Inflight service", "Cleanliness", "Departure Delay in Minutes", "Arrival Delay in Minutes", "satisfaction"]

target_candidates = [h for h in dataset_headers if "satisfaction" in h.lower()]
target_norms = [normalize_name(h).lower() for h in target_candidates]

target_col = None
for c in df.columns:
    if normalize_name(c).lower() in target_norms:
        target_col = c
        break
if target_col is None:
    for h in dataset_headers[::-1]:
        h_norm = normalize_name(h).lower()
        for c in df.columns:
            if normalize_name(c).lower() == h_norm:
                target_col = c
                break
        if target_col is not None:
            break
if target_col is None:
    numeric_candidates = []
    for c in df.columns:
        coerced = pd.to_numeric(df[c], errors='coerce')
        if coerced.notna().sum() > 0:
            numeric_candidates.append((c, coerced.nunique(dropna=True)))
    non_constant = [c for c, n in numeric_candidates if n > 1]
    if non_constant:
        target_col = non_constant[0]
    elif len(df.columns) > 0:
        target_col = df.columns[-1]

assert target_col is not None and target_col in df.columns

df = df.replace([np.inf, -np.inf], np.nan)
df = df.dropna(axis=0, subset=[target_col])
assert len(df) > 0

y = df[target_col]
X = df.drop(columns=[target_col])
if X.shape[1] == 0:
    X = pd.DataFrame({"bias": np.ones(len(y))})

y_coerced = pd.to_numeric(y, errors='coerce')
y_numeric_ratio = y_coerced.notna().mean() if len(y) > 0 else 0
nunique = y.nunique(dropna=True)

if y.dtype == 'object' or str(y.dtype).startswith('category'):
    if y_numeric_ratio > 0.9 and y_coerced.nunique(dropna=True) > 20:
        problem_type = "regression"
    else:
        problem_type = "classification"
else:
    if nunique <= 20:
        problem_type = "classification"
    else:
        problem_type = "regression"

if problem_type == "classification" and nunique < 2:
    problem_type = "regression"

numeric_features = []
categorical_features = []
for c in X.columns:
    s = X[c]
    coerced = pd.to_numeric(s, errors='coerce')
    numeric_ratio = coerced.notna().mean() if len(coerced) > 0 else 0
    if s.dtype != 'object' or numeric_ratio > 0.8:
        X[c] = coerced
        numeric_features.append(c)
    else:
        categorical_features.append(c)

if len(numeric_features) == 0 and len(categorical_features) == 0:
    X = pd.DataFrame({"bias": np.ones(len(y))})
    numeric_features = ["bias"]
    categorical_features = []

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler(with_mean=False))
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
])

preprocess = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='drop'
)

accuracy = None

if problem_type == "classification":
    le = LabelEncoder()
    y_enc = le.fit_transform(y.astype(str))
    if len(np.unique(y_enc)) < 2:
        problem_type = "regression"
    else:
        min_count = np.min(np.bincount(y_enc))
        stratify = y_enc if min_count >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_enc, test_size=0.2, random_state=42, stratify=stratify
        )
        assert len(X_train) > 0 and len(X_test) > 0
        if len(np.unique(y_train)) < 2:
            model = DummyClassifier(strategy='most_frequent')
        else:
            model = LogisticRegression(max_iter=200, solver='liblinear')
        clf = Pipeline(steps=[('preprocess', preprocess), ('model', model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

if problem_type == "regression":
    y_num = pd.to_numeric(y, errors='coerce')
    if y_num.isna().any():
        fill_value = y_num.median() if y_num.notna().sum() > 0 else 0.0
        y_num = y_num.fillna(fill_value)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_num, test_size=0.2, random_state=42
    )
    assert len(X_train) > 0 and len(X_test) > 0
    model = Ridge(alpha=1.0)
    clf = Pipeline(steps=[('preprocess', preprocess), ('model', model)])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    r2 = r2_score(y_test, y_pred) if len(np.unique(y_test)) > 1 else 0.0
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

if accuracy is None:
    accuracy = 0.0

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# - Lightweight linear models (logistic regression or ridge) keep CPU usage and memory low.
# - ColumnTransformer with simple imputation and one-hot encoding ensures reproducible, minimal preprocessing.
# - Regression fallback uses a bounded R2-to-accuracy proxy for stable [0,1] scoring when classification is unsuitable.