# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier

def load_robust_csv(path):
    try:
        df = pd.read_csv(path, header=None, engine='c', low_memory=False)
        if df.shape[1] < 5:
            raise ValueError
    except:
        df = pd.read_csv(path, header=None, sep=';', decimal=',', engine='c', low_memory=False)
    return df

def process_nids_data():
    train_path = "data/raw/Train.txt"
    test_path = "data/raw/Test.txt"
    
    train_df = load_robust_csv(train_path)
    test_df = load_robust_csv(test_path)

    label_idx = 41
    y_train = (train_df.iloc[:, label_idx] != 'normal').astype(np.int8)
    y_test = (test_df.iloc[:, label_idx] != 'normal').astype(np.int8)

    X_train = train_df.drop(columns=[label_idx, 42] if train_df.shape[1] > 42 else [label_idx])
    X_test = test_df.drop(columns=[label_idx, 42] if test_df.shape[1] > 42 else [label_idx])
    
    del train_df
    del test_df

    cat_cols = X_train.select_dtypes(include=['object']).columns
    X_train = pd.get_dummies(X_train, columns=cat_cols, dtype=np.int8)
    X_test = pd.get_dummies(X_test, columns=cat_cols, dtype=np.int8)
    
    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train).astype(np.float32)
    X_test = scaler.transform(X_test).astype(np.float32)

    pca = PCA(n_components=0.95, random_state=42)
    X_train = pca.fit_transform(X_train)
    X_test = pca.transform(X_test)

    model = XGBClassifier(
        n_estimators=200,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        eval_metric="logloss",
        random_state=42,
        tree_method='hist',
        n_jobs=-1
    )
    
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    process_nids_data()

# Optimization Summary
# 1. Memory Reduction: Converted numerical features to float32 and encoded labels/dummies as int8 to minimize RAM usage.
# 2. Computational Efficiency: Used XGBoost's 'hist' tree_method which is significantly faster and more energy-efficient for large datasets.
# 3. Redundant Data Handling: Removed unnecessary columns (difficulty score) and deleted large raw dataframes immediately after feature extraction.
# 4. Streamlined Pipeline: Replaced multi-step custom preprocessing imports with a consolidated, efficient local processing logic.
# 5. Robust I/O: Implemented a C-engine based CSV loader with fallback logic to handle different delimiters without overhead.
# 6. Resource Management: Removed plotting and model serialization to eliminate disk I/O and secondary library overhead.
# 7. Optimized Dimensionality Reduction: Maintained PCA for variance preservation while ensuring the output uses memory-efficient datatypes for the classifier.