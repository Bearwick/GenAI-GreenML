# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")

DATASET_PATH = "diabetes.csv"
EXPECTED_HEADERS = "Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age,Outcome"

def normalize_name(name):
    name = str(name).strip()
    name = re.sub(r"\s+", " ", name)
    return name

def robust_read_csv(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    def looks_bad(d):
        if d is None:
            return True
        if d.shape[1] <= 1:
            return True
        cols_joined = " ".join([str(c) for c in d.columns])
        if ";" in cols_joined and "," not in cols_joined:
            return True
        return False
    if looks_bad(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df = pd.read_csv(path, sep=";", decimal=",", engine="python")
    return df

df = robust_read_csv(DATASET_PATH)
assert df is not None and df.shape[0] > 0

df = df.copy()
df.columns = [normalize_name(c) for c in df.columns]
df = df.loc[:, ~df.columns.str.contains(r"^Unnamed", case=False, regex=True)]

expected_headers = [normalize_name(h) for h in EXPECTED_HEADERS.split(",") if h.strip()]
col_map = {normalize_name(c).lower(): c for c in df.columns}
expected_mapped = [col_map.get(normalize_name(h).lower()) for h in expected_headers if normalize_name(h).lower() in col_map]

target_col = None
for cand in ["Outcome"]:
    key = normalize_name(cand).lower()
    if key in col_map:
        target_col = col_map[key]
        break
if target_col is None:
    numeric_candidates = []
    for col in df.columns:
        series = pd.to_numeric(df[col], errors="coerce")
        if series.notna().sum() > 0 and series.nunique(dropna=True) > 1:
            numeric_candidates.append(col)
    if numeric_candidates:
        target_col = numeric_candidates[-1]
    else:
        target_col = df.columns[-1]

if expected_mapped and len(expected_mapped) >= 2:
    feature_cols = [c for c in expected_mapped if c != target_col]
    if not feature_cols:
        feature_cols = [c for c in df.columns if c != target_col]
else:
    feature_cols = [c for c in df.columns if c != target_col]

feature_cols = [c for c in feature_cols if c in df.columns]
feature_cols = [c for c in feature_cols if not df[c].isna().all()]
filtered_features = [c for c in feature_cols if df[c].nunique(dropna=True) > 1]
if filtered_features:
    feature_cols = filtered_features
if not feature_cols:
    df["_constant"] = 1.0
    feature_cols = ["_constant"]

y_raw = df[target_col]
y_num = pd.to_numeric(y_raw, errors="coerce")
if y_num.notna().sum() / max(len(y_raw), 1) >= 0.5:
    y = y_num.replace([np.inf, -np.inf], np.nan)
else:
    y = y_raw.astype(str)
    y = y.where(y_raw.notna(), np.nan)

mask = ~pd.isna(y)
X = df[feature_cols].loc[mask].copy()
y = y.loc[mask]

assert len(X) > 0 and len(y) > 0

numeric_cols = []
categorical_cols = []
for col in feature_cols:
    if col not in X.columns:
        continue
    series = X[col]
    if series.isna().all():
        continue
    num_series = pd.to_numeric(series, errors="coerce")
    if num_series.notna().sum() / max(len(series), 1) >= 0.5:
        if num_series.notna().sum() == 0:
            continue
        num_series = num_series.replace([np.inf, -np.inf], np.nan)
        X[col] = num_series
        numeric_cols.append(col)
    else:
        cat_series = series.astype(str)
        cat_series = cat_series.where(series.notna(), np.nan)
        X[col] = cat_series
        categorical_cols.append(col)

selected_cols = numeric_cols + categorical_cols
if not selected_cols:
    X = pd.DataFrame({"_constant": np.ones(len(y))}, index=y.index)
    numeric_cols = ["_constant"]
    categorical_cols = []
else:
    X = X[selected_cols]

n_samples = len(y)
unique_classes = y.nunique(dropna=True)
classification_condition = False
if unique_classes >= 2:
    if y.dtype == object:
        classification_condition = True
    else:
        classification_condition = unique_classes <= max(20, int(0.1 * n_samples))

if unique_classes < 2:
    task = "classification"
    model = DummyClassifier(strategy="most_frequent")
elif classification_condition:
    task = "classification"
    model = LogisticRegression(max_iter=200, solver="liblinear")
else:
    task = "regression"
    model = Ridge(alpha=1.0)

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False))
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

transformers = []
if numeric_cols:
    transformers.append(("num", numeric_transformer, numeric_cols))
if categorical_cols:
    transformers.append(("cat", categorical_transformer, categorical_cols))

preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)
pipeline = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

test_size = 0.2 if len(X) >= 5 else 0.5
stratify = y if task == "classification" and y.nunique(dropna=True) >= 2 else None
try:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )
except Exception:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )

assert len(X_train) > 0 and len(X_test) > 0

try:
    pipeline.fit(X_train, y_train)
    preds = pipeline.predict(X_test)
except Exception:
    if task == "classification":
        fallback_model = DummyClassifier(strategy="most_frequent")
    else:
        fallback_model = DummyRegressor(strategy="mean")
    pipeline = Pipeline(steps=[("preprocess", preprocessor), ("model", fallback_model)])
    pipeline.fit(X_train, y_train)
    preds = pipeline.predict(X_test)

if task == "classification":
    accuracy = accuracy_score(y_test, preds)
else:
    r2 = r2_score(y_test, preds)
    if np.isnan(r2):
        r2 = 0.0
    accuracy = (r2 + 1.0) / 2.0
    if accuracy < 0.0:
        accuracy = 0.0
    if accuracy > 1.0:
        accuracy = 1.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Lightweight linear and dummy models were used to keep CPU and energy use minimal.
# - Preprocessing is streamlined with imputation and one-hot encoding via a single Pipeline/ColumnTransformer.
# - Feature type detection is conservative to avoid unnecessary conversions and computation.
# - Regression fallback uses a bounded accuracy proxy (r2+1)/2 with clamping to [0,1].
# - StandardScaler with_mean=False maintains sparse compatibility and reduces overhead.