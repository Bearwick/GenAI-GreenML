# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

def load_data(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    
    df.columns = [" ".join(str(c).strip().split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def run_pipeline():
    path = "diabetes.csv"
    df = load_data(path)
    
    if df.empty:
        return

    target_candidate = 'Outcome'
    if target_candidate not in df.columns:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            target_col = numeric_cols[-1]
        else:
            target_col = df.columns[-1]
    else:
        target_col = target_candidate

    X = df.drop(columns=[target_col])
    y = df[target_col]

    for col in X.columns:
        X.loc[:, col] = pd.to_numeric(X[col], errors='coerce')
    y = pd.to_numeric(y, errors='coerce')

    mask = y.notna()
    X = X[mask]
    y = y[mask]

    if len(X) < 2:
        return

    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
    
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', Pipeline([
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler())
            ]), numeric_features)
        ], remainder='drop')

    unique_targets = np.unique(y[~np.isnan(y)])
    is_classification = len(unique_targets) < 10 and np.all(np.mod(unique_targets, 1) == 0)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    if is_classification and len(np.unique(y_train)) > 1:
        clf = LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')
        pipeline = Pipeline([('prep', preprocessor), ('model', clf)])
        pipeline.fit(X_train, y_train)
        preds = pipeline.predict(X_test)
        score = accuracy_score(y_test, preds)
    else:
        reg = Ridge(random_state=42)
        pipeline = Pipeline([('prep', preprocessor), ('model', reg)])
        pipeline.fit(X_train, y_train)
        preds = pipeline.predict(X_test)
        r2 = r2_score(y_test, preds)
        score = max(0, min(1, r2))

    print(f"ACCURACY={score:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary:
# 1. Model Choice: Logistic Regression with 'liblinear' solver was selected for its low computational 
#    footprint and high efficiency on small to medium-sized tabular datasets like Pima Diabetes.
# 2. Preprocessing: Scikit-learn Pipeline and ColumnTransformer are used to minimize data movement 
#    and ensure memory-efficient transformations. 
# 3. Numeric Coercion: Defensive programming ensures all features are coerced to numeric types 
#    pre-emptively, preventing runtime overhead from dynamic type checking.
# 4. CPU-Friendly: Avoided large tree ensembles or deep learning to minimize CO2 footprint 
#    and energy consumption during training and inference.
# 5. Robustness: Implemented multi-stage CSV parsing and automated target detection to ensure 
#    execution without manual configuration.
# 6. Proxy Metric: In cases where the target appears continuous, R-squared (clamped to [0,1]) 
#    serves as a lightweight performance proxy.