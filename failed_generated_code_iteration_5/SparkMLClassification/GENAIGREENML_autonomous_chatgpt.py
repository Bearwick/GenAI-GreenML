# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score

warnings.filterwarnings("ignore")


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        normed.append(c2)
    return normed


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _try_read_csv(path):
    # First attempt: default
    df1 = None
    try:
        df1 = pd.read_csv(path)
    except Exception:
        df1 = None

    def _bad_parse(df):
        if df is None:
            return True
        if df.shape[1] <= 1:
            return True
        if df.shape[0] < 2:
            return True
        return False

    if not _bad_parse(df1):
        return df1

    # Fallback: semicolon separator + comma decimal
    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        if not _bad_parse(df2):
            return df2
    except Exception:
        pass

    # Last resort: try python engine with sep inference (still lightweight)
    try:
        df3 = pd.read_csv(path, sep=None, engine="python")
        return df3
    except Exception as e:
        raise RuntimeError(f"Failed to read CSV: {e}")


def _safe_accuracy_from_r2_like(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    if y_true.size == 0:
        return 0.0
    denom = np.sum((y_true - np.mean(y_true)) ** 2)
    if denom <= 1e-12:
        return 1.0 if np.allclose(y_true, y_pred) else 0.0
    r2 = 1.0 - (np.sum((y_true - y_pred) ** 2) / denom)
    acc = 0.5 + 0.5 * float(np.clip(r2, -1.0, 1.0))
    return float(np.clip(acc, 0.0, 1.0))


def _pick_target_and_features(df, expected_headers=None):
    cols = list(df.columns)

    # Prefer expected target if present
    target = None
    if expected_headers:
        expected_headers_norm = _normalize_columns(expected_headers)
        # map normalized -> original
        norm_map = {re.sub(r"\s+", " ", str(c).strip()): c for c in cols}
        for cand in ["Outcome", "Target", "label", "Label", "y"]:
            if cand in expected_headers_norm and cand in norm_map:
                target = norm_map[cand]
                break
        if target is None and "Outcome" in norm_map:
            target = norm_map["Outcome"]

    # Else try common names
    if target is None:
        for cand in ["Outcome", "target", "Target", "label", "Label", "class", "Class", "y"]:
            if cand in df.columns:
                target = cand
                break

    # Else choose a numeric non-constant column as target
    if target is None:
        numeric_candidates = []
        for c in cols:
            s = pd.to_numeric(df[c], errors="coerce")
            nun = int(s.nunique(dropna=True))
            if nun >= 2:
                numeric_candidates.append((nun, c))
        numeric_candidates.sort(reverse=True)
        target = numeric_candidates[0][1] if numeric_candidates else cols[-1]

    features = [c for c in cols if c != target]
    if not features:
        # fallback: if only one column, create empty feature set and handle later
        features = []
    return target, features


def main():
    dataset_path = "diabetes.csv"
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"Dataset not found at path: {dataset_path}")

    df = _try_read_csv(dataset_path)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    assert df.shape[0] > 0 and df.shape[1] > 0, "Empty dataset after loading."

    expected_headers = [
        "Pregnancies",
        "Glucose",
        "BloodPressure",
        "SkinThickness",
        "Insulin",
        "BMI",
        "DiabetesPedigreeFunction",
        "Age",
        "Outcome",
    ]

    target_col, feature_cols = _pick_target_and_features(df, expected_headers=expected_headers)

    # Coerce numeric-like columns carefully; keep objects for OHE
    for c in df.columns:
        if c == target_col:
            continue
        if df[c].dtype == "object":
            # attempt numeric conversion; only adopt if it doesn't destroy too much signal
            s_num = pd.to_numeric(df[c], errors="coerce")
            if s_num.notna().mean() >= 0.8:
                df[c] = s_num

    # Prepare y
    y_raw = df[target_col]
    # Try numeric target first for robust modeling
    y_num = pd.to_numeric(y_raw, errors="coerce")

    # Decide task type
    is_classification = False
    y_for_model = None

    if y_raw.dtype == "object":
        # object target -> classification if >=2 unique
        nun = int(y_raw.nunique(dropna=True))
        if nun >= 2:
            is_classification = True
            y_for_model = y_raw.astype(str).fillna("MISSING")
    else:
        # numeric target: classification if looks like binary / small integer classes
        nun = int(y_num.nunique(dropna=True))
        if nun >= 2 and nun <= 20:
            # check if integer-like
            finite_vals = y_num[np.isfinite(y_num)]
            if finite_vals.size > 0:
                int_like = np.allclose(finite_vals, np.round(finite_vals))
                if int_like and nun <= 10:
                    is_classification = True
                    y_for_model = y_num
                else:
                    y_for_model = y_num
            else:
                y_for_model = y_num
        else:
            y_for_model = y_num

    # Build X
    X = df[feature_cols].copy() if feature_cols else pd.DataFrame(index=df.index)

    # Drop rows with missing target
    mask_y = pd.Series(True, index=df.index)
    if is_classification:
        mask_y = y_for_model.notna()
    else:
        mask_y = pd.to_numeric(y_for_model, errors="coerce").notna()
    X = X.loc[mask_y]
    y_for_model = y_for_model.loc[mask_y]

    assert X.shape[0] > 0, "No samples after dropping missing targets."

    # Identify columns for preprocessing
    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Train/test split
    # If classification but only one class, fallback to regression path
    if is_classification:
        y_class = y_for_model
        # Handle potential float labels (e.g., 0.0/1.0) safely
        y_unique = pd.Series(y_class).dropna().unique()
        if len(y_unique) < 2:
            is_classification = False
            y_for_model = pd.to_numeric(y_for_model, errors="coerce")

    if is_classification:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_for_model, test_size=0.2, random_state=42, stratify=y_for_model
        )
        assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Empty split."

        model = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            C=1.0,
        )

        clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Regression fallback
        y_reg = pd.to_numeric(y_for_model, errors="coerce")
        mask = y_reg.notna()
        X = X.loc[mask]
        y_reg = y_reg.loc[mask]
        assert X.shape[0] > 1, "Not enough samples for regression."

        X_train, X_test, y_train, y_test = train_test_split(
            X, y_reg, test_size=0.2, random_state=42
        )
        assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Empty split."

        model = Ridge(alpha=1.0, random_state=42)
        reg = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        accuracy = _safe_accuracy_from_r2_like(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression / Ridge) for strong CPU efficiency and low energy use.
# - Employs a single sklearn Pipeline + ColumnTransformer to avoid repeated preprocessing and ensure reproducibility.
# - Robust CSV loading with fallback delimiter/decimal settings to reduce manual intervention and wasted reruns.
# - Defensive schema handling: normalizes column names, drops "Unnamed:*", auto-selects target/features if mismatch occurs.
# - Safe numeric coercion + imputation; OneHotEncoder is sparse to minimize memory/compute for categorical expansion.
# - Regression fallback returns a bounded [0,1] proxy accuracy from clipped R^2: acc = 0.5 + 0.5*clip(R2,[-1,1]).