# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import pickle
import random
from typing import Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

SEED = 42


def _set_reproducible(seed: int = SEED) -> None:
    os.environ.setdefault("PYTHONHASHSEED", str(seed))
    random.seed(seed)
    np.random.seed(seed)


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _coerce_numeric_df(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    for c in out.columns:
        if out[c].dtype == object:
            out[c] = pd.to_numeric(out[c], errors="coerce")
    return out


def _infer_features_and_labels(df: pd.DataFrame) -> Tuple[np.ndarray, Optional[np.ndarray]]:
    df = _coerce_numeric_df(df)
    df = df.dropna(axis=0, how="all")
    df = df.dropna(axis=1, how="all")
    if df.shape[1] == 0:
        raise ValueError("Empty dataset after parsing.")

    values = df.to_numpy(dtype=np.float64, copy=False)
    if values.shape[1] >= 2:
        X = values[:, :-1]
        y_raw = values[:, -1]
        y = np.where(y_raw > 0, 1, np.where(y_raw < 0, -1, 0)).astype(np.int64, copy=False)
        return X, y
    return values, None


def _load_model(model_path: str) -> object:
    with open(model_path, "rb") as f:
        return pickle.load(f)


def _evaluate_loaded_model_accuracy(model_path: str) -> float:
    model = _load_model(model_path)

    X = None
    y = None
    if hasattr(model, "X_") and hasattr(model, "y_"):
        X = getattr(model, "X_")
        y = getattr(model, "y_")
        try:
            X = np.asarray(X, dtype=np.float64)
            y = np.asarray(y)
        except Exception:
            X = None
            y = None

    if X is None or y is None or X.ndim != 2 or len(y) != X.shape[0] or X.shape[0] < 2:
        return float("nan")

    preds = model.predict(X)
    return float(accuracy_score(y, preds))


def _train_and_evaluate_accuracy(train_csv: str = "14k.csv") -> float:
    df = _read_csv_robust(train_csv)
    X, y = _infer_features_and_labels(df)
    if y is None:
        return float("nan")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=SEED, shuffle=True, stratify=None
    )

    clf = MLPClassifier(hidden_layer_sizes=(30, 30, 30, 30), max_iter=1000, random_state=SEED)

    pipe = Pipeline(
        steps=[
            ("scaler", StandardScaler(copy=False)),
            ("mlp", clf),
        ]
    )

    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    return float(accuracy_score(y_test, y_pred))


def main() -> None:
    _set_reproducible(SEED)

    accuracy = float("nan")
    if os.path.exists("dict.pickle"):
        try:
            accuracy = _evaluate_loaded_model_accuracy("dict.pickle")
        except Exception:
            accuracy = float("nan")

    if not np.isfinite(accuracy):
        try:
            accuracy = _train_and_evaluate_accuracy("14k.csv")
        except Exception:
            accuracy = float("nan")

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced manual CSV parsing loops with vectorized pandas parsing to reduce Python-level overhead and redundant conversions.
# - Added robust CSV read fallback (default, then sep=';' and decimal=',') to prevent mis-parsing and avoid costly downstream fixes.
# - Converted object columns to numeric once and dropped fully-empty rows/columns to minimize unnecessary computation and memory.
# - Used NumPy arrays end-to-end for features/labels to reduce data movement and intermediate Python lists.
# - Combined scaling + model into a single sklearn Pipeline to avoid repeated transforms and keep preprocessing tightly coupled.
# - Disabled StandardScaler copies (copy=False) to reduce memory footprint during normalization.
# - Set fixed seeds (PYTHONHASHSEED/random/numpy and model random_state) for reproducible, stable results.
# - Removed interactive behavior, plots, logging, and file-writing side effects; only prints final accuracy in the required format.