# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import os
import json
import csv
import torch
from transformers import pipeline

def load_data(path):
    try:
        with open(path, 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return []

def run_pipeline():
    input_path = "data/input.json"
    output_json = "data/output/predictions.json"
    output_csv = "data/output/predictions.csv"
    os.makedirs("data/output", exist_ok=True)

    data = load_data(input_path)
    if not data:
        print(f"ACCURACY={0.0:.6f}")
        return

    intents = [
        "Book Appointment",
        "Product Inquiry",
        "Pricing Negotiation",
        "Support Request",
        "Follow-Up"
    ]

    texts, ids, gold_labels = [], [], []
    for entry in data:
        ids.append(entry.get('conversation_id', 'unknown'))
        messages = entry.get('messages', [])
        conversation_str = "\n".join(f"{m.get('sender', '').capitalize()}: {m.get('text', '')}" for m in messages)
        texts.append(conversation_str)
        gold_labels.append(entry.get('intent') or entry.get('label'))

    device = 0 if torch.cuda.is_available() else -1
    classifier = pipeline(
        "zero-shot-classification",
        model="cross-encoder/nli-distilroberta-base",
        device=device
    )

    results = classifier(texts, intents, truncation=True, batch_size=8)

    predictions = []
    correct_count = 0
    total_labeled = 0

    for i, res in enumerate(results):
        top_intent = res['labels'][0]
        rationale = f"Based on the conversation, the customer is likely interested in '{top_intent.lower()}'."
        
        predictions.append({
            "conversation_id": ids[i],
            "predicted_intent": top_intent,
            "rationale": rationale
        })

        if gold_labels[i]:
            total_labeled += 1
            if top_intent == gold_labels[i]:
                correct_count += 1

    with open(output_json, 'w') as f:
        json.dump(predictions, f, indent=2)

    with open(output_csv, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=["conversation_id", "predicted_intent", "rationale"])
        writer.writeheader()
        writer.writerows(predictions)

    accuracy = correct_count / total_labeled if total_labeled > 0 else 0.0
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary
# 1. Implemented batch inference in the transformers pipeline to reduce overhead and improve throughput.
# 2. Enabled hardware acceleration (GPU) detection to reduce execution time when a compatible device is available.
# 3. Removed the unused 'emoji' library dependency and the associated 'clean_and_lowercase' function to reduce memory footprint and startup time.
# 4. Used list comprehensions and generator expressions for string formatting and data preparation to minimize loop overhead.
# 5. Applied 'truncation=True' to the classifier to handle long conversations efficiently and avoid processing redundant tokens.
# 6. Optimized I/O by batch-writing results to JSON and CSV after processing all entries rather than writing per row.
# 7. Reduced memory footprint by avoiding redundant intermediate list structures for message processing.