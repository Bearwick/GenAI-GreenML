# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import json
import csv
import random
import logging
from typing import List, Optional
from transformers import pipeline

os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
logging.getLogger("transformers").setLevel(logging.ERROR)

SEED = 42

def set_seed(seed: int = SEED):
    random.seed(seed)
    try:
        import numpy as np
        np.random.seed(seed)
    except Exception:
        pass
    try:
        import torch
        torch.manual_seed(seed)
        torch.set_grad_enabled(False)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    except Exception:
        pass

class IntentDetector:
    def __init__(self, intent_options: List[str]):
        self.intent_options = intent_options
        self.intent_lower = {intent: intent.lower() for intent in intent_options}
        self.intent_pipeline = pipeline(
            task="zero-shot-classification",
            model="cross-encoder/nli-distilroberta-base",
        )

    def classify_batch(self, dialogues: List[str], batch_size: int):
        return self.intent_pipeline(dialogues, self.intent_options, batch_size=batch_size)

    def rationale(self, intent: str) -> str:
        return f"Based on the conversation, the customer is likely interested in '{self.intent_lower.get(intent, intent.lower())}'."

def create_conversation(messages: List[dict], max_messages: Optional[int] = None) -> str:
    if max_messages is not None:
        messages = messages[-max_messages:]
    lines = []
    for m in messages:
        sender = m.get("sender", "")
        if sender is None:
            sender = ""
        if not isinstance(sender, str):
            sender = str(sender)
        lines.append(f"{sender.capitalize()}: {m.get('text', '')}")
    return "\n".join(lines)

LABEL_CANDIDATES = (
    "intent",
    "label",
    "true_intent",
    "target",
    "ground_truth",
    "gold_intent",
    "expected_intent",
    "class",
    "category",
)

def detect_label_key(conversations: List[dict]) -> Optional[str]:
    for key in LABEL_CANDIDATES:
        for entry in conversations:
            if key in entry:
                return key
    return None

def load_conversations(path: str):
    ext = os.path.splitext(path)[1].lower()
    if ext == ".json":
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    if ext == ".csv":
        try:
            import pandas as pd
            df = pd.read_csv(path)
            if df.shape[1] == 1:
                df = pd.read_csv(path, sep=";", decimal=",")
            return df.to_dict(orient="records")
        except Exception:
            with open(path, newline="", encoding="utf-8") as f:
                return list(csv.DictReader(f))
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def normalize_conversations(data):
    if isinstance(data, list):
        return data
    if isinstance(data, dict):
        for v in data.values():
            if isinstance(v, list):
                return v
    return []

def process_batch(model: IntentDetector, texts: List[str], ids: List, labels: Optional[List], label_key: Optional[str], output_data: List[dict], correct: int, total: int):
    results = model.classify_batch(texts, len(texts))
    if isinstance(results, dict):
        results = [results]
    append = output_data.append
    rationale_fn = model.rationale
    if label_key:
        for i, res in enumerate(results):
            intent = res["labels"][0]
            append({"conversation_id": ids[i], "predicted_intent": intent, "rationale": rationale_fn(intent)})
            true_label = labels[i]
            if true_label is not None:
                total += 1
                if str(true_label).strip().lower() == intent.lower():
                    correct += 1
    else:
        for i, res in enumerate(results):
            intent = res["labels"][0]
            append({"conversation_id": ids[i], "predicted_intent": intent, "rationale": rationale_fn(intent)})
    return correct, total

def predict_intents(input_file: str, json_output: str, csv_output: str, batch_size: int = 8) -> float:
    conversations = normalize_conversations(load_conversations(input_file))
    label_key = detect_label_key(conversations)
    output_data = []
    correct = 0
    total = 0
    batch_size = max(1, int(batch_size)) if batch_size else 1
    batch_texts = []
    batch_ids = []
    batch_labels = [] if label_key else None

    for entry in conversations:
        batch_ids.append(entry.get("conversation_id"))
        batch_texts.append(create_conversation(entry.get("messages", [])))
        if label_key:
            batch_labels.append(entry.get(label_key))
        if len(batch_texts) == batch_size:
            correct, total = process_batch(intent_model, batch_texts, batch_ids, batch_labels, label_key, output_data, correct, total)
            batch_texts.clear()
            batch_ids.clear()
            if batch_labels is not None:
                batch_labels.clear()

    if batch_texts:
        correct, total = process_batch(intent_model, batch_texts, batch_ids, batch_labels, label_key, output_data, correct, total)

    json_dir = os.path.dirname(json_output)
    if json_dir:
        os.makedirs(json_dir, exist_ok=True)
    csv_dir = os.path.dirname(csv_output)
    if csv_dir:
        os.makedirs(csv_dir, exist_ok=True)

    with open(json_output, "w", encoding="utf-8") as json_file:
        json.dump(output_data, json_file, indent=2)

    with open(csv_output, "w", newline="", encoding="utf-8") as csv_file:
        fieldnames = ["conversation_id", "predicted_intent", "rationale"]
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(output_data)

    return correct / total if total else 0.0

set_seed()
intent_model = IntentDetector([
    "Book Appointment",
    "Product Inquiry",
    "Pricing Negotiation",
    "Support Request",
    "Follow-Up",
])

def main():
    output_dir = "data/output"
    os.makedirs(output_dir, exist_ok=True)
    accuracy = predict_intents(
        input_file="data/input.json",
        json_output=os.path.join(output_dir, "predictions.json"),
        csv_output=os.path.join(output_dir, "predictions.csv"),
    )
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Batched inference reduces per-item pipeline overhead and tokenizer invocations.
# - Precomputed lowercase intent strings avoid repeated string transformations during rationale generation.
# - Streaming batch assembly limits intermediate memory use while preserving output order.
# - Deterministic seeding and disabled gradients improve reproducibility without extra computation.
# - Lazy CSV loading avoids importing heavy dependencies unless required.