# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder

def load_data(path):
    """Robustly load the JSON dataset and normalize columns."""
    try:
        # Standard pandas JSON read for the 'What's Cooking' format
        df = pd.read_json(path)
    except Exception:
        # Trivial fallback if JSON parsing fails
        return pd.DataFrame()

    if df.empty:
        return df

    # Normalize column names: strip whitespace, lowercase, and remove Unnamed
    df.columns = [str(col).strip() for col in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def solve():
    dataset_path = 'train.json'
    
    if not os.path.exists(dataset_path):
        # Fallback for missing file to ensure script doesn't crash during evaluation
        print("ACCURACY=0.000000")
        return

    df = load_data(dataset_path)
    
    if df.empty:
        print("ACCURACY=0.000000")
        return

    # Identify features and target based on expected schema or content
    # For What's Cooking: 'ingredients' is usually a list, 'cuisine' is the label
    target_col = None
    feature_col = None

    # Priority 1: Direct Match
    potential_targets = ['cuisine', 'type', 'category', 'label']
    potential_features = ['ingredients', 'recipe', 'text']

    for pt in potential_targets:
        if pt in df.columns:
            target_col = pt
            break
    
    for pf in potential_features:
        if pf in df.columns:
            feature_col = pf
            break

    # Priority 2: Heuristic-based fallback
    if not feature_col:
        for col in df.columns:
            if isinstance(df[col].iloc[0], list):
                feature_col = col
                break
    
    if not target_col:
        for col in df.columns:
            if col != feature_col and df[col].nunique() > 1:
                target_col = col
                break

    # Strict check to ensure we can proceed
    if not target_col or not feature_col:
        print("ACCURACY=0.000000")
        return

    # Preprocessing: Convert lists of ingredients to space-separated strings
    def process_ingredients(x):
        if isinstance(x, list):
            return " ".join([str(i).lower().replace(" ", "_") for i in x])
        return str(x).lower()

    df['processed_features'] = df[feature_col].apply(process_ingredients)
    
    # Handle Target: Encode labels
    le = LabelEncoder()
    y = le.fit_transform(df[target_col].astype(str))
    X = df['processed_features']

    # Guard against single-class datasets
    if len(np.unique(y)) < 2:
        print("ACCURACY=1.000000")
        return

    # Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Lightweight Green Pipeline:
    # 1. TfidfVectorizer: max_features limits memory/computation. 
    #    Binary=True can be faster/simpler for ingredient presence.
    # 2. LogisticRegression: liblinear is efficient for sparse text classification on CPU.
    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(
            max_features=3000, 
            binary=True, 
            ngram_range=(1, 1),
            token_pattern=r'\b\w\w+\b'
        )),
        ('clf', LogisticRegression(
            solver='liblinear', 
            multi_class='ovr', 
            C=1.0, 
            random_state=42,
            max_iter=100
        ))
    ])

    # Model Training
    pipeline.fit(X_train, y_train)

    # Evaluation
    predictions = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# Optimization Summary:
# 1. Used Logistic Regression with the 'liblinear' solver, which is computationally 
#    lightweight and highly efficient for high-dimensional sparse text data.
# 2. Implemented TfidfVectorizer with a capped 'max_features' (3000) to minimize 
#    the memory footprint and prevent the model size from scaling unnecessarily.
# 3. Utilized binary=True in vectorization to treat ingredient presence as a 
#    binary feature, reducing floating point operations compared to full TF-IDF.
# 4. Built a robust schema-derivation logic to handle variations in JSON input 
#    without manual configuration, ensuring the pipeline is end-to-end reproducible.
# 5. Pre-processed ingredient strings by replacing internal spaces with underscores 
#    to treat multi-word ingredients (e.g., 'olive oil') as single tokens, 
#    improving accuracy with minimal extra CPU cost.
# 6. Optimized for CPU by avoiding ensembles (Random Forest) or deep learning 
#    components, significantly reducing the energy required for training and inference.
# 7. Implemented stratify in train_test_split to ensure stable evaluation 
#    representative of the class distribution with a single training run.

[DOCUMENT_END]