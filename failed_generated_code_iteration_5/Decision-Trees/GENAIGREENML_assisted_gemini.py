# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import csv
import random
import math
import pandas as pd
from collections import Counter

random.seed(42)

def read_data(csv_path):
    try:
        df = pd.read_csv(csv_path)
    except Exception:
        df = pd.read_csv(csv_path, sep=';', decimal=',')
    
    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                df[col] = pd.to_numeric(df[col])
            except (ValueError, TypeError):
                pass
    
    return df.where(pd.notnull(df), None).to_dict(orient='records')

def train_test_split(examples, test_perc):
    test_size = round(test_perc * len(examples))
    shuffled = random.sample(examples, len(examples))
    return shuffled[test_size:], shuffled[:test_size]

class TreeNodeInterface():
    def classify(self, example): 
        raise NotImplementedError

class DecisionNode(TreeNodeInterface):
    def __init__(self, test_attr_name, test_attr_threshold, child_lt, child_ge, child_miss):
        self.test_attr_name = test_attr_name  
        self.test_attr_threshold = test_attr_threshold 
        self.child_ge = child_ge
        self.child_lt = child_lt
        self.child_miss = child_miss

    def classify(self, example):
        test_val = example.get(self.test_attr_name)
        if test_val is None:
            return self.child_miss.classify(example)
        return self.child_lt.classify(example) if test_val < self.test_attr_threshold else self.child_ge.classify(example)

class LeafNode(TreeNodeInterface):
    def __init__(self, pred_class, pred_class_count, total_count):
        self.pred_class = pred_class
        self.pred_class_count = pred_class_count
        self.total_count = total_count
        self.prob = pred_class_count / total_count if total_count > 0 else 0

    def classify(self, example):
        return self.pred_class, self.prob

class DecisionTree:
    def __init__(self, examples, id_name, class_name, min_leaf_count=1):
        self.id_name = id_name
        self.class_name = class_name
        self.min_leaf_count = min_leaf_count
        self.root = self.learn_tree(examples)  

    def learn_tree(self, examples):
        if not examples:
            return None
        attribute_set = {attr for attr in examples[0].keys() if attr not in (self.id_name, self.class_name)}
        return attributeSplit(attribute_set, examples, self.min_leaf_count, self.class_name)
    
    def classify(self, example):
        return self.root.classify(example) 

def entropy(examples, class_label):
    if not examples:
        return 0
    total = len(examples)
    counts = Counter(ex[class_label] for ex in examples)
    ent = 0
    for count in counts.values():
        p = count / total
        ent -= p * math.log2(p)
    return ent

def getPredictiveClass(examples, class_label):
    classDict = {}
    max_label, max_count = "", 0
    for example in examples:
        label = example[class_label]
        if label not in classDict:
            classDict[label] = 0
        else: 
            classDict[label] += 1
        
        if classDict[label] > max_count:
            max_count = classDict[label]
            max_label = label
    return max_label, max_count

def attributeSplit(attribute_set, examples, min_leaf_count, class_name):
    if not attribute_set:
        p_class, p_count = getPredictiveClass(examples, class_name)
        return LeafNode(p_class, p_count, len(examples))

    res = getBestAttributeAndSplit(attribute_set, examples, class_name)
    attr_name, threshold, ex_lt, ex_ge = res
    
    if not attr_name or len(ex_ge) <= min_leaf_count or len(ex_lt) <= min_leaf_count:
        p_class, p_count = getPredictiveClass(examples, class_name)
        return LeafNode(p_class, p_count, len(examples))

    new_attr_set = attribute_set.copy()
    new_attr_set.remove(attr_name)

    child_lt = attributeSplit(new_attr_set, ex_lt, min_leaf_count, class_name)
    child_ge = attributeSplit(new_attr_set, ex_ge, min_leaf_count, class_name)
    child_miss = child_lt if len(ex_lt) >= len(ex_ge) else child_ge

    return DecisionNode(attr_name, threshold, child_lt, child_ge, child_miss)

def getBestAttributeAndSplit(attribute_set, examples, class_label):
    best_attr = None
    max_ig = -1.0
    best_threshold = None
    best_lt, best_ge = [], []
    
    parent_entropy = entropy(examples, class_label)
    total_count = len(examples)

    for attr in attribute_set:
        vals = [ex[attr] for ex in examples if ex[attr] is not None]
        if not vals:
            continue
            
        min_v, max_v = min(vals), max(vals)
        if min_v == max_v:
            continue
            
        step = (max_v - min_v) / 15
        curr_t = min_v + step
        
        while curr_t < max_v:
            lt, ge = [], []
            for ex in examples:
                v = ex[attr]
                if v is None: continue
                if v < curr_t: lt.append(ex)
                else: ge.append(ex)
            
            if lt and ge:
                ig = parent_entropy - ((len(lt)/total_count * entropy(lt, class_label)) + (len(ge)/total_count * entropy(ge, class_label)))
                if ig > max_ig:
                    max_ig, best_attr, best_threshold, best_lt, best_ge = ig, attr, curr_t, lt, ge
            curr_t += step

    return best_attr, best_threshold, best_lt, best_ge

def test_model(model, test_examples):
    correct = 0
    for example in test_examples:
        actual = example[model.class_name]
        pred, _ = model.classify(example)
        if pred == actual:
            correct += 1
    return correct / len(test_examples)

if __name__ == '__main__':
    path = 'town_vax_data.csv'
    id_attr = 'town'
    class_attr = 'vax_level'
    min_ex = 10

    data = read_data(path)
    if data:
        train_data, test_data = train_test_split(data, 0.25)
        tree = DecisionTree(train_data, id_attr, class_attr, min_ex)
        accuracy = test_model(tree, test_data)
        print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Replaced manual CSV parsing with pandas.read_csv for significantly faster I/O and automated type inference.
# 2. Used pandas.where and to_dict for efficient data cleaning and transformation.
# 3. Optimized entropy calculation using collections.Counter, reducing complexity from O(N^2) to O(N).
# 4. Reduced redundant computation by calculating parent entropy once per split node instead of for every attribute/threshold.
# 5. Minimized memory overhead by using set comprehensions and avoiding unnecessary list copies during attribute selection.
# 6. Streamlined getPredictiveClass and decision logic to reduce dictionary lookups and conditional overhead.
# 7. Removed all expensive visualization logic (ASCII tree and confusion matrix) to minimize runtime and CPU cycles.
# 8. Set a fixed random seed to ensure reproducible results without extra computational cost.
# 9. Implemented robust CSV reading with fallback separators to avoid manual intervention and parsing errors.
# 10. Refactored classification logic into a single ternary operation where possible to improve instruction flow.