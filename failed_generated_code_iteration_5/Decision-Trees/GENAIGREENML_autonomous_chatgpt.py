# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "town_vax_data.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        # keep original punctuation (hyphens/underscores) as they are meaningful here
        normed.append(c2)
    return normed


def _robust_read_csv(path):
    # Try default parsing first, then fallback to European-style separators/decimals.
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(dfx):
        if dfx is None:
            return True
        if dfx.shape[0] == 0:
            return True
        if dfx.shape[1] <= 1:
            return True
        # If a single column contains lots of semicolons, likely wrong separator.
        first_col = dfx.columns[0]
        sample = dfx[first_col].astype(str).head(50)
        semi_rate = sample.str.contains(";").mean() if len(sample) else 0.0
        return semi_rate > 0.2

    if _looks_wrong(df):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if not _looks_wrong(df2):
                df = df2
        except Exception:
            pass

    if df is None:
        raise FileNotFoundError(f"Could not read dataset at path: {path}")
    return df


def _drop_unnamed(df):
    cols = [c for c in df.columns if not re.match(r"^Unnamed:\s*\d+\s*$", str(c))]
    return df[cols]


def _coerce_numeric_inplace(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _choose_target(df):
    # Prefer 'vax_level' if present; else try common candidates; else fallback to a suitable numeric column.
    cols_lower = {c.lower(): c for c in df.columns}
    for cand in ["vax_level", "target", "label", "y"]:
        if cand in cols_lower:
            return cols_lower[cand]

    # Use last column if it's categorical-looking and has >1 unique values.
    last_col = df.columns[-1]
    nun = df[last_col].nunique(dropna=True)
    if nun >= 2 and nun <= max(20, int(0.2 * max(1, len(df)))):
        return last_col

    # Otherwise choose a non-constant numeric column with minimal missingness.
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    best = None
    best_score = -1
    for c in numeric_cols:
        s = df[c]
        if s.nunique(dropna=True) < 2:
            continue
        missing = s.isna().mean()
        score = (1.0 - missing) * 1.0
        if score > best_score:
            best_score = score
            best = c

    if best is not None:
        return best

    # If no numeric columns, take the last column even if messy (we can still classify if >=2 classes).
    return last_col


def _is_classification_target(y):
    if y.dtype == "O" or str(y.dtype).startswith("category") or str(y.dtype).startswith("bool"):
        return True
    # Numeric target: treat as classification if it has few unique values relative to n
    nun = y.nunique(dropna=True)
    n = len(y)
    if nun < 2:
        return False
    if nun <= 20:
        return True
    if nun / max(1, n) <= 0.05:
        return True
    return False


def _bounded_regression_score(y_true, y_pred):
    # Convert regression goodness into a stable [0,1] proxy: 1 / (1 + NRMSE)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mse = np.mean((y_true - y_pred) ** 2) if len(y_true) else np.inf
    rmse = float(np.sqrt(mse))
    denom = float(np.nanstd(y_true)) if np.isfinite(np.nanstd(y_true)) and np.nanstd(y_true) > 0 else float(np.nanmean(np.abs(y_true)) + 1e-12)
    if not np.isfinite(denom) or denom <= 0:
        denom = 1.0
    nrmse = rmse / (denom + 1e-12)
    score = 1.0 / (1.0 + nrmse)
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    df = _robust_read_csv(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Basic empty check early
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _choose_target(df)

    # Separate X/y safely
    if target_col not in df.columns:
        target_col = df.columns[-1]
    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # Identify likely numeric columns by attempting coercion on non-object columns + numeric-looking objects
    # Keep this lightweight by sampling a few values.
    object_cols = X.select_dtypes(include=["object"]).columns.tolist()
    non_object_cols = [c for c in X.columns if c not in object_cols]

    # Coerce non-object columns to numeric where possible (safe no-op if already numeric)
    X = _coerce_numeric_inplace(X, non_object_cols)

    # For object columns, attempt numeric coercion if most sampled values look numeric
    for c in object_cols:
        ser = X[c].astype(str).replace({"nan": np.nan, "NaN": np.nan, "None": np.nan})
        sample = ser.dropna().head(50)
        if len(sample) == 0:
            continue
        coerced = pd.to_numeric(sample, errors="coerce")
        numeric_rate = float(coerced.notna().mean()) if len(coerced) else 0.0
        if numeric_rate >= 0.8:
            X[c] = pd.to_numeric(ser, errors="coerce")

    # Refresh column types
    num_features = X.select_dtypes(include=[np.number]).columns.tolist()
    cat_features = [c for c in X.columns if c not in num_features]

    # Clean y: if numeric-like objects, coerce; else keep as categorical
    y = y_raw.copy()
    if y.dtype == "O":
        y_num_try = pd.to_numeric(y, errors="coerce")
        # If many values become numeric, use numeric representation
        if y_num_try.notna().mean() >= 0.9 and y_num_try.nunique(dropna=True) >= 2:
            y = y_num_try

    # Drop rows where y is missing
    valid_mask = pd.Series(True, index=df.index)
    valid_mask &= ~pd.isna(y)
    X = X.loc[valid_mask].reset_index(drop=True)
    y = y.loc[valid_mask].reset_index(drop=True)

    # Replace inf with nan in numeric columns
    if len(num_features) > 0:
        X[num_features] = X[num_features].replace([np.inf, -np.inf], np.nan)

    assert len(X) > 0

    is_clf = _is_classification_target(y)

    # Handle degenerate classification labels
    if is_clf:
        # Ensure y is string labels to stabilize One-vs-Rest for multinomial etc.
        y_clf = y.astype(str)
        if y_clf.nunique(dropna=True) < 2:
            is_clf = False
        else:
            y = y_clf

    # Preprocess with ColumnTransformer for reproducibility
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, num_features),
            ("cat", categorical_transformer, cat_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Split
    if is_clf:
        # stratify if feasible
        stratify = y if y.nunique() >= 2 and y.value_counts().min() >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
        )
        assert len(X_train) > 0 and len(X_test) > 0

        # Lightweight linear classifier; saga handles sparse + multinomial, but lbfgs can be cheaper
        # Choose lbfgs for dense, saga for sparse; we can't know output sparsity pre-fit, use saga robustly with modest iters.
        model = LogisticRegression(
            max_iter=300,
            solver="saga",
            n_jobs=1,
            penalty="l2",
            C=1.0,
            multi_class="auto",
            random_state=RANDOM_STATE,
        )

        clf = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ])

        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Regression fallback
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE
        )
        assert len(X_train) > 0 and len(X_test) > 0

        # Ensure numeric y for regression
        y_train = pd.to_numeric(y_train, errors="coerce")
        y_test = pd.to_numeric(y_test, errors="coerce")
        # Drop any newly-missing y rows post-coercion
        tr_mask = ~pd.isna(y_train)
        te_mask = ~pd.isna(y_test)
        X_train = X_train.loc[tr_mask].reset_index(drop=True)
        y_train = y_train.loc[tr_mask].reset_index(drop=True)
        X_test = X_test.loc[te_mask].reset_index(drop=True)
        y_test = y_test.loc[te_mask].reset_index(drop=True)
        assert len(X_train) > 0 and len(X_test) > 0

        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)

        reg = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ])

        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        accuracy = _bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) for CPU-friendly training and inference.
# - ColumnTransformer+Pipeline ensures single-pass, reproducible preprocessing (impute+scale numeric; impute+one-hot categorical).
# - Robust CSV loader retries with ';' separator and ',' decimal to avoid costly manual fixes and prevent misparsed wide single-column files.
# - Defensive schema handling: target is inferred if missing/unknown; numeric coercion is limited and sampled for object columns to reduce overhead.
# - Avoids heavy feature engineering and large ensembles to minimize compute/energy.
# - Regression fallback uses a bounded [0,1] proxy accuracy = 1/(1+NRMSE) to stay stable across scales while still printing ACCURACY=.