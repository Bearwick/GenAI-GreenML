# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.base import BaseEstimator, ClassifierMixin


DATASET_PATH = "MNIST_train.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        normed.append(c2)
    return normed


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if isinstance(c, str) and c.strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _read_csv_robust(path):
    df = None
    last_err = None
    for kwargs in ({}, {"sep": ";", "decimal": ","}):
        try:
            df_try = pd.read_csv(path, **kwargs)
            if df_try is None or df_try.shape[0] == 0:
                continue
            df_try.columns = _normalize_columns(df_try.columns)
            df_try = _drop_unnamed(df_try)
            # Parsing sanity: need at least 2 columns and more than 0 rows
            if df_try.shape[1] >= 2 and df_try.shape[0] > 0:
                df = df_try
                break
        except Exception as e:
            last_err = e
            continue
    if df is None:
        raise RuntimeError(f"Failed to read CSV: {path}. Last error: {last_err}")
    return df


def _coerce_numeric_inplace(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")


def _pick_target_and_features(df):
    cols = list(df.columns)

    # Prefer explicit label-like targets if present
    lowered = {c: str(c).strip().lower() for c in cols}
    preferred_targets = ["label", "target", "y", "class"]
    target = None
    for pt in preferred_targets:
        for c in cols:
            if lowered[c] == pt:
                target = c
                break
        if target is not None:
            break

    # Fallback: choose a non-constant numeric column as target
    if target is None:
        numeric_candidates = []
        for c in cols:
            s = pd.to_numeric(df[c], errors="coerce")
            if s.notna().sum() >= max(10, int(0.1 * len(df))):
                nun = s.nunique(dropna=True)
                if nun >= 2:
                    numeric_candidates.append((c, nun))
        if numeric_candidates:
            numeric_candidates.sort(key=lambda x: x[1], reverse=True)
            target = numeric_candidates[0][0]

    # Final fallback: first column
    if target is None:
        target = cols[0]

    features = [c for c in cols if c != target]
    if not features:
        # If only one column, create a dummy feature from index
        df = df.copy()
        df["_idx_"] = np.arange(len(df), dtype=np.int64)
        features = ["_idx_"]
    return df, target, features


class ConstantClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, constant=None):
        self.constant = constant

    def fit(self, X, y):
        y_arr = np.asarray(y)
        if y_arr.size == 0:
            self.constant_ = 0
        else:
            vals, counts = np.unique(y_arr, return_counts=True)
            self.constant_ = vals[np.argmax(counts)]
        return self

    def predict(self, X):
        n = X.shape[0] if hasattr(X, "shape") else len(X)
        return np.full(n, self.constant_, dtype=object)


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    y_mean = float(np.mean(y_true))
    ss_tot = float(np.sum((y_true - y_mean) ** 2))
    if ss_tot <= 0.0:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    if not np.isfinite(r2):
        return 0.0
    return float(np.clip(r2, 0.0, 1.0))


def main():
    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Remove duplicate columns if any
    if df.columns.duplicated().any():
        df = df.loc[:, ~df.columns.duplicated()].copy()

    df, target_col, feature_cols = _pick_target_and_features(df)

    # Prepare y and basic cleaning
    y_raw = df[target_col].copy()
    X = df[feature_cols].copy()

    # Identify column types robustly
    # Attempt numeric coercion for columns that look numeric-heavy
    num_cols = []
    cat_cols = []
    for c in X.columns:
        s = X[c]
        if pd.api.types.is_numeric_dtype(s):
            num_cols.append(c)
        else:
            s_num = pd.to_numeric(s, errors="coerce")
            # If most values are numeric after coercion, treat as numeric
            if s_num.notna().mean() >= 0.9:
                X[c] = s_num
                num_cols.append(c)
            else:
                cat_cols.append(c)

    # Coerce target to numeric if possible (MNIST labels are numeric)
    y_num = pd.to_numeric(y_raw, errors="coerce")
    y_num_notna_ratio = float(y_num.notna().mean()) if len(y_num) else 0.0
    if y_num_notna_ratio >= 0.95:
        y = y_num
    else:
        y = y_raw.astype(str)

    # Replace inf with nan for safety
    if num_cols:
        X[num_cols] = X[num_cols].replace([np.inf, -np.inf], np.nan)

    # Drop rows with missing target
    target_mask = pd.notna(y)
    X = X.loc[target_mask].copy()
    y = y.loc[target_mask].copy()

    assert X.shape[0] > 0 and X.shape[1] > 0

    # Decide task: classification if target is discrete with smallish unique count
    # MNIST label => classification
    task = "classification"
    if pd.api.types.is_numeric_dtype(y):
        y_unique = y.nunique(dropna=True)
        if y_unique < 2:
            task = "degenerate"
        elif y_unique > max(50, int(0.2 * len(y))):
            task = "regression"
        else:
            task = "classification"
    else:
        y_unique = y.nunique(dropna=True)
        if y_unique < 2:
            task = "degenerate"
        else:
            task = "classification"

    # Build preprocessing
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False)),
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, num_cols),
            ("cat", categorical_transformer, cat_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Split
    stratify = None
    if task == "classification":
        # If too many classes or very small classes, stratify can fail; try safely
        try:
            if y.nunique() >= 2 and len(y) >= 20:
                stratify = y
        except Exception:
            stratify = None

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
    )

    assert len(X_train) > 0 and len(X_test) > 0

    if task == "classification":
        # If training split ended up single-class, fallback to constant classifier
        if pd.Series(y_train).nunique(dropna=True) < 2:
            clf = Pipeline(steps=[
                ("preprocess", preprocessor),
                ("model", ConstantClassifier()),
            ])
        else:
            # Lightweight multiclass baseline; saga handles sparse high-dim efficiently on CPU
            clf = Pipeline(steps=[
                ("preprocess", preprocessor),
                ("model", LogisticRegression(
                    solver="saga",
                    penalty="l2",
                    C=1.0,
                    max_iter=200,
                    tol=1e-3,
                    n_jobs=1,
                    random_state=RANDOM_STATE,
                    multi_class="auto",
                )),
            ])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    elif task == "regression":
        reg = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", Ridge(alpha=1.0, random_state=RANDOM_STATE)),
        ])
        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        accuracy = _bounded_regression_score(y_test, y_pred)
    else:
        # Degenerate target: score equals majority-class accuracy on test
        const = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", ConstantClassifier()),
        ])
        const.fit(X_train, y_train)
        y_pred = const.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses LogisticRegression (saga, L2) as a compact CPU-friendly baseline for MNIST-like high-dimensional sparse data; avoids deep learning/ensembles.
# - ColumnTransformer + Pipeline ensures reproducible preprocessing with no repeated transforms and enables sparse flow for efficiency.
# - StandardScaler(with_mean=False) keeps sparse matrices sparse (lower memory/compute) and improves linear model conditioning.
# - Robust CSV parsing fallback (default then sep=';' decimal=',') and defensive schema handling (normalize names, drop Unnamed, pick target safely).
# - Safe coercion of numerics, inf->nan handling, and simple imputers avoid expensive feature engineering while keeping end-to-end execution stable.
# - Regression fallback uses Ridge (closed-form-ish iterative but lightweight) and reports a bounded R2-like score clipped to [0,1] as ACCURACY proxy.