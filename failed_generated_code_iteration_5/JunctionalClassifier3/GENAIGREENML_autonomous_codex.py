# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import pickle
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")

DATASET_PATH = "dict.pickle"

def load_dataset(path):
    ext = os.path.splitext(path)[1].lower()
    if ext in [".csv", ".txt"]:
        try:
            df = pd.read_csv(path)
            if df.shape[1] == 1:
                col0 = df.columns[0]
                if isinstance(col0, str) and ";" in col0:
                    raise ValueError("bad parse")
            return df, None
        except Exception:
            df = pd.read_csv(path, sep=";", decimal=",")
            return df, None
    obj = None
    try:
        obj = pd.read_pickle(path)
    except Exception:
        try:
            with open(path, "rb") as f:
                obj = pickle.load(f)
        except Exception:
            obj = None
    if obj is None:
        try:
            df = pd.read_csv(path)
            if df.shape[1] == 1:
                col0 = df.columns[0]
                if isinstance(col0, str) and ";" in col0:
                    raise ValueError("bad parse")
            return df, None
        except Exception:
            df = pd.read_csv(path, sep=";", decimal=",")
            return df, None
    y_external = None
    if isinstance(obj, pd.DataFrame):
        df = obj
    elif isinstance(obj, dict):
        if "data" in obj or "X" in obj:
            X = obj["data"] if "data" in obj else obj["X"]
            df = pd.DataFrame(X)
            y_external = obj.get("target", obj.get("y"))
            if "feature_names" in obj and len(obj["feature_names"]) == df.shape[1]:
                df.columns = list(obj["feature_names"])
        else:
            df = pd.DataFrame(obj)
    elif isinstance(obj, tuple) and len(obj) == 2:
        df = pd.DataFrame(obj[0])
        y_external = obj[1]
    elif isinstance(obj, (list, np.ndarray)):
        df = pd.DataFrame(obj)
    else:
        df = pd.DataFrame(obj)
    return df, y_external

def normalize_columns(df):
    cols = []
    for c in df.columns:
        c_str = str(c)
        c_str = " ".join(c_str.strip().split())
        cols.append(c_str)
    df.columns = cols
    drop_cols = [c for c in df.columns if str(c).lower().startswith("unnamed")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df

df, y_external = load_dataset(DATASET_PATH)
df = normalize_columns(df)

target_from_external = None
if y_external is not None:
    try:
        y_ext = np.ravel(y_external)
        if len(y_ext) == len(df):
            target_name = "target"
            if target_name in df.columns:
                target_name = "target_external"
            df[target_name] = y_ext
            target_from_external = target_name
    except Exception:
        target_from_external = None

target_col = None
if target_from_external is not None and target_from_external in df.columns:
    target_col = target_from_external

if target_col is None:
    target_name_candidates = {"target", "label", "class", "y", "output", "result"}
    for col in df.columns:
        if str(col).lower() in target_name_candidates:
            target_col = col
            break

numeric_df = df.apply(pd.to_numeric, errors="coerce")
numeric_cols = [col for col in df.columns if numeric_df[col].notna().sum() > 0]

inferred_task = None
if target_col is None:
    n_samples = len(df)
    unique_counts = {col: numeric_df[col].nunique(dropna=True) for col in numeric_cols}
    non_constant = [c for c in numeric_cols if unique_counts.get(c, 0) > 1]
    if non_constant:
        class_threshold = max(2, min(20, int(0.1 * n_samples)))
        class_candidates = [c for c in non_constant if unique_counts.get(c, 0) <= class_threshold]
        if class_candidates:
            target_col = sorted(class_candidates, key=lambda c: unique_counts[c])[0]
            inferred_task = "classification"
        else:
            target_col = sorted(non_constant, key=lambda c: unique_counts.get(c, 0), reverse=True)[0]
            inferred_task = None
    elif numeric_cols:
        target_col = numeric_cols[0]
        inferred_task = None
    else:
        target_col = df.columns[0]
        inferred_task = "classification"

feature_cols = [c for c in df.columns if c != target_col]
if not feature_cols:
    df["_index_feature"] = np.arange(len(df))
    feature_cols = ["_index_feature"]

n_samples = len(df)
if inferred_task is not None:
    task = inferred_task
else:
    y_series = df[target_col]
    unique_count = y_series.nunique(dropna=True)
    if y_series.dtype == object or str(y_series.dtype).startswith("category") or y_series.dtype == bool:
        task = "classification"
    else:
        class_threshold = max(2, min(20, int(0.1 * n_samples)))
        if unique_count <= class_threshold and unique_count >= 2:
            task = "classification"
        else:
            task = "regression"

y_series = df[target_col]
if task == "classification":
    mask = y_series.notna()
    if mask.sum() == 0:
        y_series = pd.Series(np.zeros(len(df)), index=df.index)
        mask = y_series.notna()
    df = df[mask]
    y_series = y_series[mask]
    X = df[feature_cols].copy()
    y_encoded, uniques = pd.factorize(y_series, sort=True)
    n_classes = len(uniques)
    if n_classes < 2:
        task = "regression"
        y_numeric = pd.to_numeric(y_series, errors="coerce")
        if y_numeric.notna().sum() == 0:
            y_numeric = pd.Series(np.zeros(len(df)), index=df.index)
        else:
            y_numeric = y_numeric.fillna(y_numeric.median())
        y = y_numeric.values
    else:
        y = y_encoded
else:
    y_numeric = pd.to_numeric(y_series, errors="coerce")
    mask = y_numeric.notna() & np.isfinite(y_numeric)
    if mask.sum() == 0:
        y_numeric = pd.Series(np.zeros(len(df)), index=df.index)
        mask = y_numeric.notna()
    df = df[mask]
    y_numeric = y_numeric[mask]
    X = df[feature_cols].copy()
    y = y_numeric.values

assert len(df) > 0

numeric_features = []
categorical_features = []
drop_cols = []
for col in feature_cols:
    coerced = pd.to_numeric(X[col], errors="coerce")
    if coerced.notna().sum() > 0:
        X[col] = coerced
        numeric_features.append(col)
    else:
        if X[col].notna().sum() > 0:
            categorical_features.append(col)
        else:
            drop_cols.append(col)

if drop_cols:
    X = X.drop(columns=drop_cols)
    feature_cols = [c for c in feature_cols if c not in drop_cols]

if numeric_features:
    X[numeric_features] = X[numeric_features].replace([np.inf, -np.inf], np.nan)

if not numeric_features and not categorical_features:
    df["_index_feature"] = np.arange(len(df))
    X = df[["_index_feature"]].copy()
    numeric_features = ["_index_feature"]
    categorical_features = []

n_samples = len(X)
if n_samples < 2:
    X = pd.concat([X, X.iloc[[0]]], ignore_index=True)
    y = np.concatenate([np.array(y), np.array(y)[:1]])
    n_samples = len(X)

test_size = 0.2 if n_samples >= 5 else 0.5
if task == "classification" and len(np.unique(y)) > 1:
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )

assert len(X_train) > 0 and len(X_test) > 0

numeric_transformer = Pipeline(
    steps=[("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler(with_mean=False))]
)
categorical_transformer = Pipeline(
    steps=[("imputer", SimpleImputer(strategy="most_frequent")), ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=True))]
)

transformers = []
if numeric_features:
    transformers.append(("num", numeric_transformer, numeric_features))
if categorical_features:
    transformers.append(("cat", categorical_transformer, categorical_features))
if not transformers:
    transformers.append(("all", "passthrough", feature_cols))

preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

if task == "classification":
    model = LogisticRegression(max_iter=200, solver="liblinear")
else:
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

if task == "classification":
    accuracy = accuracy_score(y_test, y_pred)
else:
    try:
        r2 = r2_score(y_test, y_pred)
    except Exception:
        r2 = 0.0
    if not np.isfinite(r2):
        r2 = 0.0
    accuracy = float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) to minimize CPU and energy usage.
# - Applied minimal preprocessing (imputation, scaling, one-hot) with ColumnTransformer for reproducibility.
# - Implemented robust schema inference and fallback handling to keep the pipeline reliable with unknown inputs.
# - Regression accuracy proxy is clipped (R2+1)/2 to provide a stable [0,1] score without extra computation.