# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import pickle
import warnings
from typing import Tuple, Optional, Any, Dict

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor


warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)


DATASET_PATH = "dict.pickle"
RANDOM_STATE = 42


def _normalize_columns(cols) -> list:
    normed = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        normed.append(c2)
    return normed


def _drop_unnamed(df: pd.DataFrame) -> pd.DataFrame:
    drop_cols = [c for c in df.columns if isinstance(c, str) and c.strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _safe_to_dataframe(obj: Any) -> pd.DataFrame:
    if isinstance(obj, pd.DataFrame):
        return obj.copy()
    if isinstance(obj, pd.Series):
        return obj.to_frame().copy()
    if isinstance(obj, dict):
        # Try common patterns: dict of arrays/lists, dict containing 'data'/'df'
        for key in ("df", "data", "dataset", "frame"):
            if key in obj and isinstance(obj[key], (pd.DataFrame, pd.Series)):
                return _safe_to_dataframe(obj[key])
        try:
            return pd.DataFrame(obj).copy()
        except Exception:
            pass
    if isinstance(obj, (list, tuple, np.ndarray)):
        try:
            return pd.DataFrame(obj).copy()
        except Exception:
            pass
    # Fallback: create a 1-row DF from repr
    return pd.DataFrame({"value": [repr(obj)]})


def _load_dataset(path: str) -> pd.DataFrame:
    # Primary path: pickle load
    with open(path, "rb") as f:
        obj = pickle.load(f)
    df = _safe_to_dataframe(obj)

    # Normalize columns and drop unnamed
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # If DF has only one column with dict-like strings, attempt to expand (safe no-op if fails)
    if df.shape[1] == 1:
        col = df.columns[0]
        if df[col].dtype == object:
            # Try to parse dictionaries if present
            try:
                # If all entries are dicts already
                if all(isinstance(x, dict) for x in df[col].dropna().head(20).tolist()):
                    expanded = pd.json_normalize(df[col])
                    expanded.columns = _normalize_columns(expanded.columns)
                    expanded = _drop_unnamed(expanded)
                    if expanded.shape[1] >= 1:
                        df = expanded
            except Exception:
                pass

    return df


def _infer_target(df: pd.DataFrame) -> Tuple[str, bool]:
    """
    Returns (target_col, is_classification).
    Heuristic: prefer columns with few unique values (likely labels), especially named like 'label', 'class', 'target'.
    Otherwise choose a non-constant numeric column as regression target.
    """
    if df.shape[1] == 1:
        return df.columns[0], True  # degenerate; will be handled later

    cols = list(df.columns)
    low_cols = [c.lower() for c in cols]
    preferred_names = ("label", "class", "target", "y", "output")
    for pname in preferred_names:
        for c, lc in zip(cols, low_cols):
            if lc == pname or lc.endswith(f" {pname}") or lc.startswith(f"{pname} "):
                # Determine if classification by unique count
                nun = df[c].nunique(dropna=True)
                is_cls = (nun >= 2 and nun <= max(20, int(0.05 * max(1, len(df)))))
                # Even if not small, still attempt classification if non-numeric
                if not pd.api.types.is_numeric_dtype(df[c]):
                    is_cls = True
                return c, is_cls

    # Compute candidates for classification: low cardinality columns
    cls_candidates = []
    for c in cols:
        s = df[c]
        nun = s.nunique(dropna=True)
        if nun >= 2 and nun <= 20:
            cls_candidates.append((c, nun, s.isna().mean()))
    # Prefer lowest missing rate, then few unique values
    if cls_candidates:
        cls_candidates.sort(key=lambda x: (x[2], x[1]))
        return cls_candidates[0][0], True

    # Fallback regression target: first non-constant numeric column with decent variance
    num_cols = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        nun = s.nunique(dropna=True)
        if nun >= 2:
            num_cols.append((c, s.isna().mean(), nun))
    if num_cols:
        num_cols.sort(key=lambda x: (x[1], -x[2]))
        return num_cols[0][0], False

    # Ultimate fallback: last column as target (may be constant; handled later)
    return cols[-1], True


def _coerce_numeric_frame(df: pd.DataFrame, numeric_cols: list) -> pd.DataFrame:
    out = df.copy()
    for c in numeric_cols:
        out[c] = pd.to_numeric(out[c], errors="coerce")
        out[c] = out[c].replace([np.inf, -np.inf], np.nan)
    return out


def _bounded_regression_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    # Stable proxy in [0,1]: 1 / (1 + normalized MAE)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = np.mean(np.abs(y_true - y_pred))
    scale = np.nanstd(y_true)
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = np.nanmean(np.abs(y_true))
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = 1.0
    score = 1.0 / (1.0 + (mae / scale))
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    df = _load_dataset(DATASET_PATH)

    # Defensive: ensure non-empty
    df = df.copy()
    if df.shape[0] == 0:
        accuracy = 0.0
        print(f”ACCURACY={accuracy:.6f}”)
        return

    # Normalize columns again after possible expansion
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Remove fully empty columns
    df = df.dropna(axis=1, how="all")

    if df.shape[1] == 0:
        accuracy = 0.0
        print(f”ACCURACY={accuracy:.6f}”)
        return

    target_col, prefer_classification = _infer_target(df)

    if target_col not in df.columns:
        target_col = df.columns[-1]

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # If no features, use dummy baseline with empty feature matrix
    if X.shape[1] == 0:
        # Try classification if possible
        y_temp = y_raw.copy()
        # Decide classification based on unique count
        nun = y_temp.nunique(dropna=True)
        is_classification = (prefer_classification and nun >= 2 and nun <= max(50, int(0.1 * max(1, len(y_temp)))))
        if is_classification:
            y = y_temp.astype("category")
            if y.nunique(dropna=True) < 2:
                accuracy = 0.0
            else:
                y = y.cat.codes.to_numpy()
                X_dummy = np.zeros((len(y), 1), dtype=np.float32)
                X_train, X_test, y_train, y_test = train_test_split(
                    X_dummy, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y if len(np.unique(y)) > 1 else None
                )
                clf = DummyClassifier(strategy="most_frequent")
                clf.fit(X_train, y_train)
                y_pred = clf.predict(X_test)
                accuracy = float(accuracy_score(y_test, y_pred))
        else:
            y = pd.to_numeric(y_temp, errors="coerce").replace([np.inf, -np.inf], np.nan).to_numpy()
            mask = np.isfinite(y)
            if mask.sum() < 3:
                accuracy = 0.0
            else:
                y = y[mask]
                X_dummy = np.zeros((len(y), 1), dtype=np.float32)
                X_train, X_test, y_train, y_test = train_test_split(
                    X_dummy, y, test_size=0.2, random_state=RANDOM_STATE
                )
                reg = DummyRegressor(strategy="mean")
                reg.fit(X_train, y_train)
                y_pred = reg.predict(X_test)
                accuracy = _bounded_regression_accuracy(y_test, y_pred)

        print(f”ACCURACY={accuracy:.6f}”)
        return

    # Identify column types
    # Start by attempting numeric coercion evaluation without modifying original yet
    numeric_cols = []
    categorical_cols = []
    for c in X.columns:
        s = X[c]
        if pd.api.types.is_numeric_dtype(s):
            numeric_cols.append(c)
        else:
            # Check if it is numeric-looking
            s_num = pd.to_numeric(s, errors="coerce")
            # If most values parse as numeric, treat as numeric
            frac_num = np.isfinite(s_num.to_numpy(dtype=float, copy=False)).mean() if len(s_num) else 0.0
            if frac_num >= 0.9:
                numeric_cols.append(c)
            else:
                categorical_cols.append(c)

    X = _coerce_numeric_frame(X, numeric_cols)

    # Decide task type using y
    y_series = y_raw.copy()

    # If object, attempt to map to classification
    if prefer_classification or not pd.api.types.is_numeric_dtype(y_series):
        # classification attempt
        y_cat = y_series.astype("category")
        n_classes = y_cat.nunique(dropna=True)
        is_classification = n_classes >= 2 and n_classes <= max(50, int(0.2 * max(1, len(y_cat))))
        if is_classification:
            y = y_cat.cat.codes.to_numpy()
        else:
            # fallback to regression
            is_classification = False
            y = pd.to_numeric(y_series, errors="coerce").replace([np.inf, -np.inf], np.nan).to_numpy(dtype=float, copy=False)
    else:
        # numeric target: decide based on unique count
        y_num = pd.to_numeric(y_series, errors="coerce").replace([np.inf, -np.inf], np.nan)
        nun = y_num.nunique(dropna=True)
        is_classification = prefer_classification and (nun >= 2 and nun <= 20)
        if is_classification:
            y = y_num.astype("category").cat.codes.to_numpy()
        else:
            y = y_num.to_numpy(dtype=float, copy=False)

    # Build preprocessing
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, list(numeric_cols)),
            ("cat", categorical_transformer, list(categorical_cols)),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Remove rows with invalid y for regression/classification
    if is_classification:
        y_arr = np.asarray(y)
        valid = np.isfinite(y_arr)
        X2 = X.loc[valid].copy()
        y2 = y_arr[valid]
        # ensure at least 2 classes
        if len(X2) < 3 or len(np.unique(y2)) < 2:
            # trivial baseline
            accuracy = 0.0
            print(f”ACCURACY={accuracy:.6f}”)
            return

        strat = y2 if len(np.unique(y2)) > 1 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X2, y2, test_size=0.2, random_state=RANDOM_STATE, stratify=strat
        )

        if len(X_train) == 0 or len(X_test) == 0:
            accuracy = 0.0
            print(f”ACCURACY={accuracy:.6f}”)
            return

        # Lightweight classifier
        clf = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            random_state=RANDOM_STATE,
        )

        model = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", clf),
        ])

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))

    else:
        y_arr = np.asarray(y, dtype=float)
        valid = np.isfinite(y_arr)
        X2 = X.loc[valid].copy()
        y2 = y_arr[valid]
        if len(X2) < 3 or np.nanstd(y2) <= 1e-12:
            # trivial baseline
            accuracy = 0.0
            print(f”ACCURACY={accuracy:.6f}”)
            return

        X_train, X_test, y_train, y_test = train_test_split(
            X2, y2, test_size=0.2, random_state=RANDOM_STATE
        )

        if len(X_train) == 0 or len(X_test) == 0:
            accuracy = 0.0
            print(f”ACCURACY={accuracy:.6f}”)
            return

        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)

        model = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", reg),
        ])

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_accuracy(y_test, y_pred)

    print(f”ACCURACY={accuracy:.6f}”)


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used lightweight, CPU-friendly models (LogisticRegression with liblinear; Ridge for regression fallback) to minimize energy/time.
# - Robust schema handling: infer target from available columns, normalize headers, drop 'Unnamed' columns, and fall back safely.
# - ColumnTransformer + Pipeline ensures one-pass preprocessing and reproducibility; avoids manual repeated transformations.
# - Numeric coercion with errors='coerce' + inf->NaN + SimpleImputer prevents failures on messy real-world data.
# - OneHotEncoder(handle_unknown='ignore') keeps categorical handling simple and efficient; sparse matrices reduce memory use.
# - Fixed random_state and simple train/test split for determinism and minimal compute.
# - Regression fallback reports a bounded accuracy proxy in [0,1]: 1/(1+normalized MAE) for stability across scales.