# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score

warnings.filterwarnings("ignore")


DATASET_PATH = "zoo.csv"
DATASET_HEADERS = [
    "animal_name", "hair", "feathers", "eggs", "milk", "airborne", "aquatic",
    "predator", "toothed", "backbone", "breathes", "venomous", "fins", "legs",
    "tail", "domestic", "catsize", "class_type"
]


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c).strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _read_csv_robust(path):
    # Try default CSV parsing first
    try:
        df0 = pd.read_csv(path)
    except Exception:
        df0 = None

    def _looks_wrong(df):
        if df is None or df.shape[0] == 0 or df.shape[1] <= 1:
            return True
        # If we got one giant column containing commas/semicolons, parsing is likely wrong
        if df.shape[1] == 1:
            sample = df.iloc[:5, 0].astype(str).str.cat(sep=" ")
            if ("," in sample) or (";" in sample):
                return True
        return False

    if _looks_wrong(df0):
        try:
            df1 = pd.read_csv(path, sep=";", decimal=",")
            return df1
        except Exception:
            if df0 is not None:
                return df0
            raise
    return df0


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _choose_target(df):
    # Prefer known target if present
    cols_lower = {c.lower(): c for c in df.columns}
    if "class_type" in cols_lower:
        return cols_lower["class_type"]
    if "class" in cols_lower:
        return cols_lower["class"]

    # Otherwise pick a non-constant numeric-like column with smallest unique count > 1 (classification-friendly)
    candidate_scores = []
    for c in df.columns:
        s = df[c]
        if s.dtype == "object":
            # Try coercion to numeric to see if it's numeric-like
            sc = pd.to_numeric(s, errors="coerce")
            non_na = sc.dropna()
            if non_na.shape[0] == 0:
                continue
            nunique = non_na.nunique(dropna=True)
            if nunique > 1:
                candidate_scores.append((nunique, c, "numeric"))
        else:
            nunique = s.nunique(dropna=True)
            if nunique > 1:
                candidate_scores.append((nunique, c, "numeric"))

    if candidate_scores:
        candidate_scores.sort(key=lambda x: (x[0], x[1]))
        return candidate_scores[0][1]

    # Fallback: last column
    return df.columns[-1]


def _safe_accuracy_from_regression(y_true, y_pred):
    # Bounded R^2-like score in [0,1] as an "accuracy" proxy:
    # acc = max(0, 1 - SSE/SST), clamped to [0,1]
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    sse = np.sum((yt - yp) ** 2)
    sst = np.sum((yt - np.mean(yt)) ** 2)
    if not np.isfinite(sst) or sst <= 0:
        return 0.0
    score = 1.0 - (sse / sst)
    if not np.isfinite(score):
        return 0.0
    return float(np.clip(score, 0.0, 1.0))


# Load
if not os.path.exists(DATASET_PATH):
    raise FileNotFoundError(f"Dataset not found at path: {DATASET_PATH}")

df = _read_csv_robust(DATASET_PATH)
df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)

# If headers appear missing/wrong and the file has no header row, try re-reading with provided headers
if df.shape[1] == 1 and len(DATASET_HEADERS) > 1:
    # might be a single column due to bad parsing; attempt alternate read already done
    pass

# If columns don't match but count matches, assign provided headers as a gentle fallback
if len(df.columns) == len(DATASET_HEADERS):
    # Only overwrite if current columns look generic or include "Unnamed"
    generic_like = sum([("unnamed" in c.lower()) or (c.lower().startswith("x")) for c in df.columns]) > 0
    if generic_like:
        df.columns = DATASET_HEADERS

# Basic sanity
assert df.shape[0] > 0 and df.shape[1] > 0

# Normalize column names again after any changes
df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)

target_col = _choose_target(df)

# Separate X/y; do not assume animal_name always exists, but drop obvious identifier-like columns
X = df.drop(columns=[target_col], errors="ignore").copy()
y = df[target_col].copy()

# Drop rows with missing target
y_na = y.isna()
if y_na.any():
    X = X.loc[~y_na].copy()
    y = y.loc[~y_na].copy()

assert X.shape[0] > 0 and y.shape[0] > 0

# Identify categorical vs numeric features robustly
# Attempt numeric coercion for object columns; if most values become numeric, treat as numeric
numeric_features = []
categorical_features = []

for c in X.columns:
    s = X[c]
    if pd.api.types.is_numeric_dtype(s):
        numeric_features.append(c)
    else:
        sc = pd.to_numeric(s, errors="coerce")
        non_na = sc.notna().mean() if len(sc) else 0.0
        if non_na >= 0.9:
            X[c] = sc
            numeric_features.append(c)
        else:
            categorical_features.append(c)

# Coerce numeric feature columns safely and replace inf with nan
for c in numeric_features:
    X[c] = pd.to_numeric(X[c], errors="coerce")
X = X.replace([np.inf, -np.inf], np.nan)

# Determine task type: classification preferred if y has few discrete classes and at least 2 classes
task = "classification"
y_proc = y.copy()

# If y is object, keep as categories for classification
if pd.api.types.is_numeric_dtype(y_proc):
    y_num = pd.to_numeric(y_proc, errors="coerce")
    y_num = y_num.replace([np.inf, -np.inf], np.nan)
    # If too many unique values relative to size, treat as regression
    nunique = y_num.nunique(dropna=True)
    n = len(y_num.dropna())
    if nunique < 2:
        task = "regression"
        y_proc = y_num
    else:
        # Heuristic: if many unique values, likely regression
        if n > 0 and (nunique / max(n, 1)) > 0.3 and nunique > 15:
            task = "regression"
            y_proc = y_num
        else:
            # classification, but keep as int if possible
            y_proc = y_num.fillna(y_num.mode(dropna=True).iloc[0] if y_num.dropna().shape[0] else 0)
            # if still non-integer-ish, keep as numeric labels anyway
else:
    # object target: classification; impute missing labels if any
    if y_proc.isna().any():
        y_proc = y_proc.fillna(y_proc.mode(dropna=True).iloc[0] if y_proc.dropna().shape[0] else "unknown")

# If classification but <2 classes, fallback to regression (or trivial)
if task == "classification":
    if pd.Series(y_proc).nunique(dropna=True) < 2:
        task = "regression"
        y_proc = pd.to_numeric(y, errors="coerce").replace([np.inf, -np.inf], np.nan)

# Build preprocessors
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=True, with_std=True)),
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Split
if task == "classification":
    # Stratify only if feasible
    stratify = y_proc if pd.Series(y_proc).nunique(dropna=True) > 1 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_proc, test_size=0.2, random_state=42, stratify=stratify
    )
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_proc, test_size=0.2, random_state=42
    )

assert X_train.shape[0] > 0 and X_test.shape[0] > 0

# Model selection (lightweight)
if task == "classification":
    # Small, CPU-friendly linear classifier; use liblinear for small datasets
    clf = LogisticRegression(
        solver="liblinear",
        max_iter=300,
        random_state=42,
    )
    model = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", clf),
    ])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Lightweight ridge regression; stable on small datasets and handles collinearity
    reg = Ridge(alpha=1.0, random_state=42)
    model = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", reg),
    ])
    # Drop rows with missing y in regression
    y_train_num = pd.to_numeric(pd.Series(y_train), errors="coerce").replace([np.inf, -np.inf], np.nan)
    y_test_num = pd.to_numeric(pd.Series(y_test), errors="coerce").replace([np.inf, -np.inf], np.nan)

    train_mask = y_train_num.notna().to_numpy()
    test_mask = y_test_num.notna().to_numpy()

    if train_mask.sum() == 0 or test_mask.sum() == 0:
        # Trivial baseline if target is unusable
        accuracy = 0.0
    else:
        model.fit(X_train.iloc[train_mask], y_train_num.iloc[train_mask])
        y_pred = model.predict(X_test.iloc[test_mask])
        accuracy = _safe_accuracy_from_regression(y_test_num.iloc[test_mask].to_numpy(), y_pred)

print(f”ACCURACY={accuracy:.6f}”)

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) to minimize CPU time and energy vs. ensembles/deep nets.
# - ColumnTransformer + Pipeline ensures single-pass, reproducible preprocessing without redundant transforms.
# - Sparse one-hot encoding for categoricals reduces memory/compute; standardization only for numeric columns.
# - Robust CSV parsing fallback (default, then sep=';' & decimal=',') avoids repeated manual fixes.
# - Defensive schema handling: normalizes column names, drops Unnamed columns, chooses target safely, coerces numerics, and handles NaN/inf.
# - If regression fallback occurs, reports a bounded R^2-like proxy in [0,1] as ACCURACY for stability and comparability.