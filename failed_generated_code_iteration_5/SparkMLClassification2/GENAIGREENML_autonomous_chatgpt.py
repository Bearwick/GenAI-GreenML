# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


RANDOM_STATE = 42


def _clean_column_names(cols):
    cleaned = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        cleaned.append(c2)
    return cleaned


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _read_csv_robust(path):
    # Try default parsing first
    try:
        df1 = pd.read_csv(path)
    except Exception:
        df1 = None

    # Fallback to ; separator and , decimal
    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
    except Exception:
        df2 = None

    def _score_df(df):
        if df is None or not isinstance(df, pd.DataFrame):
            return -1
        if df.shape[0] == 0 or df.shape[1] == 0:
            return -1
        # Prefer more columns and non-null density
        non_null_ratio = float(df.notna().mean().mean())
        return df.shape[1] * 10 + non_null_ratio

    if _score_df(df2) > _score_df(df1):
        return df2
    return df1


def _try_parse_all_numeric(df):
    df_num = df.copy()
    for c in df_num.columns:
        if df_num[c].dtype == object:
            df_num[c] = pd.to_numeric(df_num[c], errors="coerce")
    return df_num


def _choose_target(df):
    # Prefer a non-constant numeric column, using the last such column as a common pattern in ML CSVs.
    df_num = _try_parse_all_numeric(df)
    numeric_cols = [c for c in df_num.columns if pd.api.types.is_numeric_dtype(df_num[c])]

    candidates = []
    for c in numeric_cols:
        s = df_num[c].replace([np.inf, -np.inf], np.nan).dropna()
        if s.shape[0] == 0:
            continue
        nunique = int(s.nunique())
        if nunique >= 2:
            candidates.append((c, nunique, float(s.var()) if s.shape[0] > 1 else 0.0))

    if not candidates:
        return None

    # Heuristic: choose the last column among candidates (common: label at end)
    return candidates[-1][0]


def _is_classification_target(y):
    # Decide classification if y is integer-like with small number of unique values
    y_series = pd.Series(y).replace([np.inf, -np.inf], np.nan).dropna()
    if y_series.shape[0] == 0:
        return False
    uniq = y_series.unique()
    if len(uniq) < 2:
        return False

    # If all values are close to integers and number of classes is reasonable, treat as classification
    uniq_sample = uniq[: min(len(uniq), 50)]
    close_to_int = np.all(np.isclose(uniq_sample, np.round(uniq_sample), atol=1e-8))
    if close_to_int and 2 <= len(uniq) <= 20:
        return True

    # Also treat binary-ish float targets as classification
    if len(uniq) == 2:
        return True

    return False


def _bounded_regression_score(y_true, y_pred):
    # Stable "accuracy proxy" in [0,1], robust to scale:
    # score = 1 / (1 + MAE / (IQR + eps))
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = float(np.nanmean(np.abs(y_true - y_pred)))
    q75, q25 = np.nanpercentile(y_true, [75, 25])
    iqr = float(q75 - q25)
    eps = 1e-12
    scale = iqr if iqr > eps else (float(np.nanstd(y_true)) if float(np.nanstd(y_true)) > eps else 1.0)
    score = 1.0 / (1.0 + mae / (scale + eps))
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    path = "heart.csv"
    if not os.path.exists(path):
        # Try relative to script directory
        script_dir = os.path.dirname(os.path.abspath(__file__))
        alt_path = os.path.join(script_dir, path)
        path = alt_path

    df = _read_csv_robust(path)
    if df is None:
        raise RuntimeError("Failed to read dataset.")

    df.columns = _clean_column_names(df.columns)
    df = _drop_unnamed(df)

    # If the dataset accidentally loads as a single column, try whitespace delimiter as a last resort.
    if df.shape[1] == 1:
        try:
            df_ws = pd.read_csv(path, delim_whitespace=True, header=None)
            if isinstance(df_ws, pd.DataFrame) and df_ws.shape[1] > 1:
                df = df_ws
                df.columns = [f"col_{i}" for i in range(df.shape[1])]
        except Exception:
            pass

    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _choose_target(df)
    if target_col is None:
        # Trivial baseline: no valid target; set accuracy to 0.0 but keep end-to-end execution.
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    y_raw = df[target_col]
    X = df.drop(columns=[target_col])

    # Coerce numeric columns safely; keep objects as objects for OHE.
    for c in X.columns:
        if pd.api.types.is_numeric_dtype(X[c]):
            X[c] = pd.to_numeric(X[c], errors="coerce")

    # Prepare y numeric
    y_num = pd.to_numeric(y_raw, errors="coerce").replace([np.inf, -np.inf], np.nan)
    mask = y_num.notna()
    X = X.loc[mask].copy()
    y_num = y_num.loc[mask].copy()

    assert X.shape[0] > 0

    is_clf = _is_classification_target(y_num)

    # Identify columns for preprocessing
    num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    cat_cols = [c for c in X.columns if c not in num_cols]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, num_cols),
            ("cat", categorical_transformer, cat_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Split (stratify only if classification with enough samples per class)
    stratify = None
    if is_clf:
        y_int = np.round(y_num).astype(int)
        vc = pd.Series(y_int).value_counts()
        if vc.shape[0] >= 2 and int(vc.min()) >= 2 and X.shape[0] >= 10:
            stratify = y_int
        y_for_split = y_int
    else:
        y_for_split = y_num

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_for_split, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
    )

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    if is_clf:
        # Lightweight, CPU-friendly classifier with limited iterations
        model = LogisticRegression(
            max_iter=300,
            solver="lbfgs",
            n_jobs=1,
        )
        clf = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Lightweight regression fallback; convert to bounded [0,1] proxy score
        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        reg = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])
        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        accuracy = _bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses simple, CPU-efficient models (LogisticRegression or Ridge) to minimize compute and energy vs. heavy ensembles/deep nets.
# - Reproducible train/test split with fixed random_state; sklearn Pipeline+ColumnTransformer prevents redundant preprocessing work.
# - Robust CSV loading with fallback separators/decimals and whitespace-delimited rescue for malformed single-column reads.
# - Defensive schema handling: cleans column names, drops 'Unnamed' columns, auto-selects a non-constant numeric target when headers are unreliable.
# - Numeric coercion with errors='coerce' and safe NaN/inf handling; SimpleImputer avoids expensive per-row logic.
# - OneHotEncoder with sparse output keeps memory/CPU lower for categorical features.
# - Regression fallback uses a bounded [0,1] accuracy proxy: 1/(1+MAE/scale) with scale=IQR (or std), ensuring stable output when classification is not viable.