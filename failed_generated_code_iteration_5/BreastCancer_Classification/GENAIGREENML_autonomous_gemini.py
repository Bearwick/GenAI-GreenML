# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

def load_data(path):
    # Robust CSV parsing as per requirement 3
    try:
        df = pd.read_csv(path)
        # Check if header is actually data (first row numeric test)
        first_row_is_numeric = True
        for val in df.iloc[0, :3].values:
            try:
                float(val)
            except:
                first_row_is_numeric = False
                break
        if first_row_is_numeric:
            df = pd.read_csv(path, header=None)
    except:
        df = pd.read_csv(path, sep=';', decimal=',')
    
    # Strip and normalize column names as per requirement 4
    df.columns = [str(col).strip() for col in df.columns]
    df.columns = [" ".join(col.split()) for col in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def solve():
    dataset_path = 'cancer_data.csv'
    try:
        df = load_data(dataset_path)
    except Exception:
        # If load fails, we cannot proceed, but try to return a dummy to satisfy format
        print(f"ACCURACY={0.000000:.6f}")
        return

    if df.empty:
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Coerce all columns to numeric where possible
    for col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    # Drop columns that are entirely NaN
    df = df.dropna(axis=1, how='all')

    # Identify target column (Requirement 5 & 8)
    # Context suggests labels are 1 and 2 or 0 and 1. 
    # Logic: find column with minimum unique values that is not constant.
    target_col = None
    unique_counts = df.nunique()
    
    # Priority 1: A column with exactly {1, 2} or {0, 1}
    for col in df.columns:
        vals = set(df[col].dropna().unique())
        if vals == {1.0, 2.0} or vals == {0.0, 1.0} or vals == {1, 2} or vals == {0, 1}:
            target_col = col
            break
            
    # Priority 2: Column with 2 unique values
    if target_col is None:
        binary_cols = unique_counts[unique_counts == 2].index.tolist()
        if binary_cols:
            target_col = binary_cols[-1] # Usually labels are at the end
            
    # Priority 3: Non-constant column with fewest uniques
    if target_col is None:
        non_constant = unique_counts[unique_counts > 1]
        if not non_constant.empty:
            target_col = non_constant.idxmin()
        else:
            target_col = df.columns[-1]

    # Feature selection
    y = df[target_col]
    X = df.drop(columns=[target_col])
    
    # Ensure numeric only for features
    X = X.select_dtypes(include=[np.number])
    
    # Safe handling for NaN/inf in target
    valid_idx = y.notna()
    X = X[valid_idx]
    y = y[valid_idx]

    if X.empty or y.nunique() < 2:
        # Fallback to a baseline accuracy if classification is impossible
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if y.nunique() >= 2 else None
    )

    # Pipeline: Impute -> Scale -> Lightweight Model (Logistic Regression)
    # Logistic Regression is energy efficient and CPU-friendly (Requirement 1 & 3)
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(max_iter=1000, solver='lbfgs', penalty='l2'))
    ])

    # Train
    pipeline.fit(X_train, y_train)

    # Evaluate
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# Optimization Summary:
# 1. Model Selection: Logistic Regression was chosen as it is computationally efficient, 
#    has low memory overhead, and performs well on tabular datasets like UCI Breast Cancer.
# 2. Energy Efficiency: By avoiding deep learning and large ensembles (e.g., Random Forest/XGBoost), 
#    we reduce CPU cycles and energy consumption. Logistic Regression is O(n_samples * n_features).
# 3. Preprocessing: Scikit-learn Pipeline ensures no data leakage and efficient processing. 
#    StandardScaler is used to improve convergence speed for the lbfgs solver.
# 4. Robustness: Implemented a schema-agnostic target detection logic that prioritizes 
#    binary columns or columns matching the 1/2 or 0/1 labels mentioned in the context.
# 5. Fault Tolerance: Includes logic to handle missing headers, different delimiters, 
#    and non-numeric data via coercion and imputation.
# 6. Scaling: CPU-only execution is guaranteed by using standard linear algebra backends 
#    provided by numpy/scipy through sklearn.