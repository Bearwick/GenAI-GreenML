# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
import pickle
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, r2_score

def load_robustly(path):
    # Robust loading for pickle or fallback to CSV as per requirements
    if not os.path.exists(path):
        return pd.DataFrame()
    
    try:
        with open(path, 'rb') as f:
            data = pickle.load(f)
        if isinstance(data, pd.DataFrame):
            df = data
        elif isinstance(data, dict):
            # Check if dict contains dataframes
            for k in data:
                if isinstance(data[k], pd.DataFrame):
                    df = data[k]
                    break
            else:
                df = pd.DataFrame(data)
        else:
            df = pd.DataFrame(data)
    except Exception:
        # Robust CSV fallback
        try:
            df = pd.read_csv(path)
        except Exception:
            try:
                df = pd.read_csv(path, sep=';', decimal=',')
            except Exception:
                return pd.DataFrame()
    return df

def preprocess_and_train():
    df = load_robustly('dict.pickle')
    
    if df.empty:
        print("ACCURACY=0.000000")
        return

    # Normalize column names: strip whitespace, collapse internal spaces, drop Unnamed
    df.columns = [" ".join(str(c).strip().split()) for c in df.columns]
    df = df.drop(columns=[c for c in df.columns if 'Unnamed' in c])

    if df.empty:
        print("ACCURACY=0.000000")
        return

    # Coerce numeric columns where possible
    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                # Attempt conversion if it looks numeric but is object
                df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '.'), errors='ignore')
            except:
                pass

    # Identify target based on context (values -1, 0, 1) or heuristics
    target_col = None
    # 1. Search for column with junction classification values
    for col in df.columns:
        unique_vals = set(df[col].dropna().unique())
        if unique_vals.issubset({-1, 0, 1, -1.0, 0.0, 1.0}) and len(unique_vals) >= 2:
            target_col = col
            break
    
    # 2. Heuristic search
    if not target_col:
        keywords = ['class', 'target', 'label', 'junction', 'type']
        for col in df.columns:
            if any(kw in col.lower() for kw in keywords):
                target_col = col
                break
                
    # 3. Final fallback
    if not target_col:
        target_col = df.columns[-1]

    # Clean target: drop NaNs in Y
    df = df.dropna(subset=[target_col])
    y = df[target_col]
    X = df.drop(columns=[target_col])

    if X.empty or len(y) < 10:
        print("ACCURACY=0.000000")
        return

    # Categorize features
    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

    # Determine task type
    is_classification = True
    if y.dtype == 'float64' and len(y.unique()) > 20:
        is_classification = False
    elif not np.issubdtype(y.dtype, np.number):
        is_classification = True

    # Build Pipeline
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ]
    )

    if is_classification:
        model = LogisticRegression(max_iter=1000, multi_class='auto', solver='lbfgs', C=1.0)
    else:
        # For regression fallback, we use a simple Ridge-like model or linear regression
        from sklearn.linear_model import Ridge
        model = Ridge()

    clf = Pipeline(steps=[('preprocessor', preprocessor),
                          ('classifier', model)])

    # Split
    try:
        # Try stratified split for classification
        stratify = y if is_classification and (y.value_counts() > 1).all() else None
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=stratify)
    except:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train
    clf.fit(X_train, y_train)
    
    # Predict and Score
    preds = clf.predict(X_test)
    
    if is_classification:
        accuracy = accuracy_score(y_test, preds)
    else:
        # Convert R2 to a 0-1 bounded accuracy proxy
        r2 = r2_score(y_test, preds)
        accuracy = max(0, min(1, r2))

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    preprocess_and_train()

# OPTIMIZATION SUMMARY
# 1. Model Choice: Logistic Regression was selected as the primary classifier. It is energy-efficient, 
#    computationally lightweight, and ideal for small-to-medium datasets on CPU.
# 2. Preprocessing: Used a Scikit-Learn Pipeline with ColumnTransformer to ensure 
#    reproducible and efficient data flow without redundant transformations.
# 3. Robustness: Implemented a multi-stage data loading strategy (pickle -> csv -> csv with delimiters) 
#    and automated schema inference to handle potential variability in the provided pickle file.
# 4. Feature Engineering: Minimal feature engineering (standardization and one-hot encoding) 
#    prevents overfitting and keeps the compute footprint small.
# 5. Energy Efficiency: By avoiding deep learning and large ensembles (like Random Forest/XGBoost), 
#    the solution minimizes CPU cycles and memory usage during both training and inference.
# 6. Target Identification: Heuristic logic ensures the code identifies the junction classification 
#    target (-1, 0, 1) described in the project context even if column names change.
# 7. Regression Fallback: Included a bounded R^2 score as an accuracy proxy for regression scenarios 
#    to maintain the required output format.