# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")

import os
import pickle
import re
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, r2_score
from sklearn.linear_model import LogisticRegression, Ridge

DATASET_PATH = "dict.pickle"

def read_csv_fallback(path):
    try:
        return pd.read_csv(path)
    except Exception:
        try:
            return pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            return pd.DataFrame()

def convert_to_dataframe(obj):
    if isinstance(obj, pd.DataFrame):
        return obj.copy()
    if isinstance(obj, dict):
        if "data" in obj and isinstance(obj["data"], (list, np.ndarray, pd.DataFrame)):
            X = obj["data"]
            if isinstance(X, pd.DataFrame):
                df = X.copy()
            else:
                cols = None
                for key in ["feature_names", "features", "columns"]:
                    if key in obj and isinstance(obj[key], (list, np.ndarray)):
                        cols = list(obj[key])
                        break
                df = pd.DataFrame(X, columns=cols)
            for key in ["target", "label", "labels", "y", "class"]:
                if key in obj:
                    df = df.copy()
                    df["target"] = obj[key]
                    break
            return df
        if "X" in obj and "y" in obj:
            df = pd.DataFrame(obj["X"])
            df["target"] = obj["y"]
            return df
        try:
            return pd.DataFrame(obj)
        except Exception:
            pass
        for key, val in obj.items():
            if isinstance(val, pd.DataFrame):
                df = val.copy()
                for tkey in ["target", "label", "labels", "y", "class"]:
                    if tkey in obj and tkey != key:
                        df["target"] = obj[tkey]
                        break
                return df
    if isinstance(obj, (list, tuple)):
        if len(obj) == 2 and not isinstance(obj[0], dict):
            X, y = obj
            df = pd.DataFrame(X)
            df["target"] = y
            return df
        try:
            return pd.DataFrame(obj)
        except Exception:
            pass
    try:
        return pd.DataFrame(obj)
    except Exception:
        return pd.DataFrame()

def load_dataset(path):
    obj = None
    if os.path.exists(path):
        if path.lower().endswith((".pickle", ".pkl", ".pckl")):
            try:
                with open(path, "rb") as f:
                    obj = pickle.load(f)
            except Exception:
                obj = None
        if obj is None:
            return read_csv_fallback(path)
        return convert_to_dataframe(obj)
    return read_csv_fallback(path)

def normalize_columns(df):
    new_cols = []
    for c in df.columns:
        if isinstance(c, tuple):
            c = " ".join([str(x) for x in c])
        else:
            c = str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        new_cols.append(c)
    df.columns = new_cols
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed", c, flags=re.I)]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df

def choose_target(df):
    priority = ["target", "label", "class", "y", "output"]
    cols = list(df.columns)
    for name in priority:
        for col in cols:
            cl = col.lower()
            if cl == name or cl.endswith("_" + name) or cl.endswith(" " + name) or cl.endswith(name):
                return col
    return cols[-1] if cols else None

def find_non_constant_col(df, exclude=None, numeric_only=False):
    if exclude is None:
        exclude = []
    cols = [c for c in df.columns if c not in exclude]
    for col in cols:
        series = df[col]
        if numeric_only:
            series_num = pd.to_numeric(series, errors="coerce")
            if series_num.notna().sum() == 0:
                continue
            if series_num.nunique(dropna=True) > 1:
                return col
        else:
            if series.dropna().nunique() > 1:
                return col
    return None

def make_ohe():
    try:
        return OneHotEncoder(handle_unknown="ignore", sparse=False)
    except TypeError:
        return OneHotEncoder(handle_unknown="ignore", sparse_output=False)

df = load_dataset(DATASET_PATH)
if not isinstance(df, pd.DataFrame):
    df = pd.DataFrame(df)
df = normalize_columns(df)
df = df.dropna(axis=1, how="all")
df.replace([np.inf, -np.inf], np.nan, inplace=True)
assert df.shape[0] > 0 and df.shape[1] > 0

target_col = choose_target(df)
if target_col is None or df[target_col].dropna().nunique() <= 1:
    alt = find_non_constant_col(df, exclude=[target_col] if target_col else [], numeric_only=True)
    if alt is None:
        alt = find_non_constant_col(df, exclude=[target_col] if target_col else [], numeric_only=False)
    if alt is not None:
        target_col = alt
if target_col is None:
    df["target"] = np.arange(len(df)) % 2
    target_col = "target"

y_raw = df[target_col]
X = df.drop(columns=[target_col])
if X.shape[1] == 0:
    X = pd.DataFrame({"dummy": np.zeros(len(df))})

y_numeric = pd.to_numeric(y_raw, errors="coerce")
numeric_ratio = y_numeric.notna().sum() / max(len(y_numeric), 1)
if numeric_ratio > 0.9:
    nunique = y_numeric.nunique(dropna=True)
    if nunique > 20 and nunique / max(len(y_numeric.dropna()), 1) >= 0.05:
        task = "regression"
    else:
        task = "classification"
else:
    task = "classification"

if task == "classification":
    y = y_raw
else:
    y = y_numeric

mask = y.notna()
X = X.loc[mask].copy()
y = y.loc[mask].copy()

X = X.dropna(axis=1, how="all")
if X.shape[1] == 0:
    X = pd.DataFrame({"dummy": np.zeros(len(y))})

assert len(X) > 0

if len(X) < 2:
    X = pd.concat([X, X], ignore_index=True)
    y = pd.concat([y, y], ignore_index=True)

num_cols = []
cat_cols = []
for col in X.columns:
    ser = X[col]
    ser_num = pd.to_numeric(ser, errors="coerce")
    non_na = ser.notna().sum()
    if ser_num.notna().sum() > 0 and (non_na == 0 or ser_num.notna().sum() / max(non_na, 1) >= 0.5):
        X[col] = ser_num
        num_cols.append(col)
    else:
        X[col] = ser.astype("object")
        cat_cols.append(col)

if not num_cols and not cat_cols:
    X["dummy"] = 0.0
    num_cols = ["dummy"]

num_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False))
])

cat_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", make_ohe())
])

transformers = []
if num_cols:
    transformers.append(("num", num_transformer, num_cols))
if cat_cols:
    transformers.append(("cat", cat_transformer, cat_cols))

preprocessor = ColumnTransformer(transformers=transformers, remainder="drop")

test_size = 0.2
if len(X) < 5:
    test_size = 0.5

stratify = None
if task == "classification" and y.nunique() >= 2:
    counts = y.value_counts()
    if counts.min() >= 2:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=42, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0

accuracy = 0.0
if task == "classification":
    if y.nunique() < 2:
        if len(y_test) > 0:
            constant = y_train.iloc[0] if len(y_train) > 0 else y.iloc[0]
            y_pred = np.full(shape=len(y_test), fill_value=constant)
            accuracy = accuracy_score(y_test, y_pred)
        else:
            accuracy = 0.0
    else:
        model = LogisticRegression(max_iter=200, solver="liblinear")
        clf = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred) if len(y_test) > 0 else 0.0
else:
    model = Ridge(alpha=1.0)
    reg = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])
    reg.fit(X_train, y_train)
    y_pred = reg.predict(X_test)
    if len(y_test) >= 2:
        r2 = r2_score(y_test, y_pred)
    else:
        r2 = 0.0
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) and simple preprocessing for CPU efficiency.
# - Implemented robust schema handling with numeric coercion, safe imputing, and minimal feature engineering.
# - For regression fallback, mapped R2 to a bounded [0,1] accuracy proxy to maintain a consistent output metric.