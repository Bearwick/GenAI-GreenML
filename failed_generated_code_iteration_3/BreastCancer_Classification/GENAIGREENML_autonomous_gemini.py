# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

def load_and_preprocess():
    path = 'cancer_data.csv'
    try:
        # Initial attempt with default settings
        df = pd.read_csv(path)
    except Exception:
        # Robust fallback for different delimiters/locales
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            # Final fallback: empty dataframe to trigger safety checks
            df = pd.DataFrame()

    if df.empty:
        return None, None

    # Clean column names
    df.columns = [str(col).strip() for col in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

    # Coerce numeric types and handle missing data
    for col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Drop rows where target or key features might be missing
    df = df.dropna(axis=0, how='all')
    if df.empty:
        return None, None

    # Identify Target: Heuristic-based selection
    # 1. Look for columns with 1 and 2 (as specified in context)
    # 2. Look for columns with 0 and 1
    # 3. Look for column with minimum unique values (classification target)
    target_col = None
    potential_targets = []
    
    for col in df.columns:
        unique_vals = df[col].dropna().unique()
        if len(unique_vals) == 2:
            # Prioritize columns with 1 and 2 or 0 and 1
            if set(unique_vals).issubset({0, 1, 1.0, 2.0, 2, 0.0}):
                target_col = col
                break
            potential_targets.append(col)
    
    if not target_col and potential_targets:
        target_col = potential_targets[0]
    elif not target_col:
        # Fallback to the last column if no binary target found
        target_col = df.columns[-1]

    # Fill NaNs for features (median) and drop rows with NaN in target
    df = df.dropna(subset=[target_col])
    
    y = df[target_col]
    X = df.drop(columns=[target_col])

    # Final check on sample size
    if len(X) < 5 or len(np.unique(y)) < 2:
        # If classification is impossible, return for safe baseline print
        return None, None

    return X, y

def run_pipeline():
    X, y = load_and_preprocess()
    
    if X is None or y is None:
        # Safe fallback for empty or malformed data to ensure script completes
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Train-Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Energy-Efficient Pipeline:
    # 1. Simple Median Imputer (Robust to outliers, low CPU)
    # 2. Standard Scaler (Ensures fast convergence for Logistic Regression)
    # 3. Logistic Regression (Low memory, fast inference, CPU friendly)
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(
            max_iter=1000, 
            solver='lbfgs', 
            tol=1e-4, 
            random_state=42
        ))
    ])

    # Fit Model
    pipeline.fit(X_train, y_train)

    # Evaluate
    predictions = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# OPTIMIZATION SUMMARY
# 1. Model Choice: Logistic Regression was selected as it is computationally lightweight (O(n) training complexity) 
#    and provides high efficiency on CPU compared to ensemble methods or deep learning.
# 2. Data Loading: Implemented a robust multi-pass CSV parser to handle potential delimiter variations without manual intervention.
# 3. Preprocessing: Used a Scikit-Learn Pipeline to prevent data leakage and ensure reproducible scaling.
# 4. Energy Efficiency: Avoided hyperparameter grid searches and complex feature engineering to minimize CPU cycles.
# 5. Robustness: Added heuristics to automatically detect the classification target (1/2 or 0/1) based on the project context.
# 6. Safety: Included defensive checks to ensure the script runs end-to-end even if the CSV is malformed or contains NaNs.
# 7. Scaling: StandardScaler ensures the gradient-based solver (lbfgs) converges faster, reducing training time and energy use.