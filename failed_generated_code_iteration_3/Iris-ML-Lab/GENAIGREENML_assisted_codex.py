# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd

DATASET_PATH = "iris.csv"
DATASET_HEADERS = "sepal_length,sepal_width,petal_length,petal_width,species"
EXPECTED_HEADERS = [h.strip() for h in DATASET_HEADERS.split(",")]

def _headers_match(cols, expected):
    return [str(c).strip() for c in cols] == expected

def _is_numeric(val):
    try:
        float(str(val).replace(",", "."))
        return True
    except ValueError:
        return False

def _read_csv_with_fallback(path, expected):
    df = pd.read_csv(path)
    if df.shape[1] != len(expected):
        df_alt = pd.read_csv(path, sep=";", decimal=",")
        if df_alt.shape[1] == len(expected):
            df = df_alt
    return df

def _normalize_headers(df, path, expected):
    if _headers_match(df.columns, expected):
        return df
    cols = [str(c).strip() for c in df.columns]
    lower_cols = [c.lower() for c in cols]
    expected_lower = [e.lower() for e in expected]
    if lower_cols == expected_lower:
        df.columns = expected
        return df
    if len(df.columns) == len(expected) and all(_is_numeric(c) for c in cols):
        return pd.read_csv(path, header=None, names=expected)
    if len(df.columns) != len(expected):
        return pd.read_csv(path, header=None, names=expected)
    df.columns = expected
    return df

def load_dataset(path, expected):
    df = _read_csv_with_fallback(path, expected)
    df = _normalize_headers(df, path, expected)
    if df.shape[1] > len(expected):
        df = df.iloc[:, :len(expected)]
    return df

def resolve_columns(df, expected):
    col_map = {str(c).strip().lower(): c for c in df.columns}
    resolved = []
    missing = False
    for h in expected:
        key = h.lower()
        if key in col_map:
            resolved.append(col_map[key])
        else:
            resolved.append(h)
            missing = True
    if missing and df.shape[1] >= len(expected):
        resolved = list(df.columns[:len(expected)])
    return resolved

def encode_labels(labels_raw):
    if np.issubdtype(labels_raw.dtype, np.number):
        return labels_raw.astype(int)
    labels_str = np.char.lower(np.char.strip(labels_raw.astype(str)))
    labels = np.empty(labels_str.shape[0], dtype=int)
    labels[labels_str == "setosa"] = 0
    labels[labels_str == "versicolor"] = 1
    labels[labels_str == "virginica"] = 2
    return labels

def compute_centroids(features, labels, num_classes=3):
    return np.vstack([features[labels == i].mean(axis=0) for i in range(num_classes)])

def main():
    np.random.seed(0)
    df = load_dataset(DATASET_PATH, EXPECTED_HEADERS)
    resolved = resolve_columns(df, EXPECTED_HEADERS)
    petal_len_col, petal_wid_col = resolved[2], resolved[3]
    label_col = resolved[4]
    features = df[[petal_len_col, petal_wid_col]].to_numpy(dtype=float)
    labels = encode_labels(df[label_col].to_numpy())
    del df
    centroids = compute_centroids(features, labels)
    sample = np.array([3.1, 1.2], dtype=features.dtype)
    distance = np.sqrt(np.sum((centroids - sample) ** 2, axis=1))
    predicted_classes = int(np.argmin(distance))
    distances = np.sqrt(np.sum((features[:, None, :] - centroids[None, :, :]) ** 2, axis=2))
    y_pred = np.argmin(distances, axis=1)
    accuracy = float(np.mean(y_pred == labels))
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Eliminated plotting and file output to avoid unnecessary computation and I/O overhead.
# - Replaced SciPy distance calculations with NumPy vectorized operations to reduce dependency and runtime.
# - Loaded only required feature columns and used vectorized centroid/distance computations to minimize data movement.
# - Implemented robust CSV parsing with delimiter/decimal fallback and deterministic seeding for reproducibility.