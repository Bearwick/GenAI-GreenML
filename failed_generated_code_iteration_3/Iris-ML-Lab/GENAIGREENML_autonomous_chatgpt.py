# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import NearestCentroid
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler


warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

DATASET_PATH = "iris.csv"
DATASET_HEADERS = ["sepal_length", "sepal_width", "petal_length", "petal_width", "species"]


def _normalize_columns(cols):
    norm = []
    for c in cols:
        c2 = str(c).strip()
        c2 = re.sub(r"\s+", " ", c2)
        norm.append(c2)
    return norm


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _looks_misparsed(df):
    if df is None or df.empty:
        return True
    if df.shape[1] == 1:
        sample = df.iloc[:5, 0].astype(str).str.cat(sep=" ")
        if ";" in sample or "," in sample:
            return True
    if df.shape[1] > 20:
        return True
    return False


def load_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    if df is None or _looks_misparsed(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df = None

    if df is None:
        raise FileNotFoundError(f"Could not load dataset at path: {path}")

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # If headers are missing/wrong but match expected count, assign expected headers
    if df.shape[1] == len(DATASET_HEADERS):
        normalized_expected = _normalize_columns(DATASET_HEADERS)
        normalized_current = [c.lower() for c in df.columns]
        if not all(ec.lower() in normalized_current for ec in normalized_expected):
            df.columns = normalized_expected

    return df


def choose_target_and_features(df):
    cols = list(df.columns)
    cols_lower = [c.lower() for c in cols]

    # Prefer known target name if available
    target = None
    for cand in ["species", "target", "label", "class", "y"]:
        if cand in cols_lower:
            target = cols[cols_lower.index(cand)]
            break

    if target is None:
        # Prefer a non-constant object column for classification
        obj_cols = [c for c in cols if df[c].dtype == "object"]
        obj_cols = [c for c in obj_cols if df[c].nunique(dropna=True) >= 2]
        if obj_cols:
            target = obj_cols[0]
        else:
            # Otherwise choose a non-constant numeric-like column
            numeric_candidates = []
            for c in cols:
                s = pd.to_numeric(df[c], errors="coerce")
                if s.notna().sum() >= max(5, int(0.1 * len(df))) and s.nunique(dropna=True) >= 2:
                    numeric_candidates.append(c)
            target = numeric_candidates[0] if numeric_candidates else cols[-1]

    feature_cols = [c for c in cols if c != target]
    if not feature_cols:
        feature_cols = [c for c in cols if c != target]
    return target, feature_cols


def safe_accuracy_proxy_regression(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    sse = float(np.sum((y_true - y_pred) ** 2))
    sst = float(np.sum((y_true - float(np.mean(y_true))) ** 2))
    if sst <= 1e-12:
        return 0.0
    r2 = 1.0 - (sse / sst)
    r2 = max(-1.0, min(1.0, r2))
    return float(max(0.0, min(1.0, (r2 + 1.0) / 2.0)))


def main():
    df = load_csv_robust(DATASET_PATH)

    # Basic cleanup: drop fully empty rows/cols
    df = df.dropna(axis=0, how="all").dropna(axis=1, how="all")
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    assert df is not None and not df.empty, "Dataset is empty after loading/cleanup."

    target_col, feature_cols = choose_target_and_features(df)

    # Keep only available columns
    feature_cols = [c for c in feature_cols if c in df.columns]
    if target_col not in df.columns:
        # Fallback: last column as target
        target_col = df.columns[-1]
        feature_cols = [c for c in df.columns if c != target_col]

    # Prepare X/y
    X = df[feature_cols].copy() if feature_cols else pd.DataFrame(index=df.index)
    y = df[target_col].copy()

    # Determine task type: classification if few unique labels or non-numeric
    y_numeric = pd.to_numeric(y, errors="coerce")
    y_is_numeric = y_numeric.notna().mean() >= 0.95
    n_unique = y.nunique(dropna=True)

    is_classification = (not y_is_numeric) or (n_unique <= 20 and n_unique >= 2)

    # Identify column types for preprocessing (defensive coercion for numeric candidates)
    numeric_cols = []
    categorical_cols = []
    for c in X.columns:
        s_num = pd.to_numeric(X[c], errors="coerce")
        if s_num.notna().sum() >= max(5, int(0.2 * len(X))):
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if is_classification and n_unique >= 2:
        # NearestCentroid is lightweight, CPU-friendly, and strong baseline for Iris-like numeric data
        model = NearestCentroid(metric="euclidean", shrink_threshold=None)
        pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

        # Stratify only if feasible
        stratify = y if y.nunique(dropna=True) >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify
        )

        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Regression fallback: simple baseline via LinearRegression with same preprocessor
        from sklearn.linear_model import LinearRegression

        y_reg = pd.to_numeric(y, errors="coerce")
        valid = y_reg.notna()
        X_reg = X.loc[valid].copy()
        y_reg = y_reg.loc[valid].astype(float)

        if len(X_reg) < 5 or y_reg.nunique(dropna=True) < 2:
            # Trivial constant predictor accuracy proxy
            accuracy = 0.0
        else:
            model = LinearRegression(n_jobs=1)
            pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
            X_train, X_test, y_train, y_test = train_test_split(
                X_reg, y_reg, test_size=0.2, random_state=42
            )
            assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."
            pipe.fit(X_train, y_train)
            y_pred = pipe.predict(X_test)
            accuracy = safe_accuracy_proxy_regression(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Robust CSV loading retries with alternative delimiter/decimal to avoid costly manual fixes and ensure end-to-end run.
# - Column name normalization + dropping "Unnamed:" columns prevents wasted compute on empty/index artifacts.
# - Lightweight preprocessing with ColumnTransformer: median imputation + standard scaling for numeric, one-hot for categoricals.
# - Energy-efficient model choice: NearestCentroid for classification (O(n_features * n_samples) fit; fast inference) suits Iris.
# - Defensive task selection: falls back to regression only if classification target is invalid; regression uses LinearRegression (CPU-friendly).
# - Minimal feature engineering and fixed random_state for reproducibility; avoids heavy ensembles/deep learning to reduce CPU time/energy.
# - Regression "accuracy" proxy maps bounded R2 in [-1,1] to [0,1] to keep stable scoring while printing ACCURACY.