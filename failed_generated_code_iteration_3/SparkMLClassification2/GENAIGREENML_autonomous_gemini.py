# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score

def load_data(path):
    # Robust loading logic
    try:
        # Try standard comma separator
        df = pd.read_csv(path)
        # If df has only 1 column, it might be space or semicolon separated
        if df.shape[1] <= 1:
            df = pd.read_csv(path, sep=r'\s+|;', engine='python')
    except Exception:
        # Fallback to empty dataframe to allow script to handle error gracefully
        return pd.DataFrame()
    
    # Normalize column names
    df.columns = [str(c).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    
    # Check if first row was treated as header but contains data
    # Based on DATASET_HEADERS: 70.0 1.0 4.0... (all numeric)
    first_row_is_numeric = True
    for col in df.columns:
        try:
            float(col)
        except ValueError:
            first_row_is_numeric = False
            break
    
    if first_row_is_numeric:
        # Reload without header
        try:
            df = pd.read_csv(path, sep=r'\s+|;', engine='python', header=None)
        except:
            pass
            
    return df

def run_pipeline(data_path):
    df = load_data(data_path)
    
    if df.empty:
        print("ACCURACY=0.000000")
        return

    # Clean non-numeric garbage and handle missing values
    for col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    df = df.dropna(how='all')
    if df.empty:
        print("ACCURACY=0.000000")
        return

    # Identify features and target
    # Target is assumed to be the last column
    target_col = df.columns[-1]
    X = df.drop(columns=[target_col])
    y = df[target_col]

    # Handle NaNs in target if any
    mask = y.notna()
    X = X[mask]
    y = y[mask]

    # Defensive check for target classes
    unique_classes = np.unique(y)
    if len(unique_classes) < 2:
        # If classification is impossible, return trivial accuracy or fallback
        print("ACCURACY=1.000000" if len(unique_classes) == 1 else "ACCURACY=0.000000")
        return

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if len(unique_classes) > 1 else None
    )

    # Lightweight Pipeline: Impute -> Scale -> Logistic Regression
    # Logistic Regression is energy-efficient and sufficient for heart disease datasets
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(max_iter=1000, solver='lbfgs', penalty='l2'))
    ])

    # Fit and Predict
    try:
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
    except Exception:
        accuracy = 0.0

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline("heart.csv")

# Optimization Summary:
# 1. Model Selection: Used Logistic Regression (L2 penalty) which is extremely lightweight and computationally efficient compared to ensembles or deep learning.
# 2. Pipeline Efficiency: Utilized sklearn.pipeline.Pipeline to minimize redundant transformations and ensure CPU-friendly execution.
# 3. Preprocessing: Implemented median imputation and standard scaling, which are O(n) operations and consume minimal memory/energy.
# 4. Robust Loading: Added logic to detect space/semicolon delimiters and handle cases where headers are missing (common in UCI-style heart datasets).
# 5. Resource Management: Avoided large libraries (like XGBoost or PyTorch) to keep the memory footprint small and reduce CPU cycles during initialization.
# 6. Numeric Safety: Used pd.to_numeric with errors='coerce' to ensure data integrity before modeling without manual data cleaning.