# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
import numpy as np
import pandas as pd

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator


SEED = 12345
DATASET_PATH = "heart.csv"
DATASET_HEADERS = "70.0 1.0 4.0 130.0 322.0 0.0 2.0 109.0 0.0 2.4 2.0 3.0 3.0 2"


def _set_reproducible_seeds(seed: int) -> None:
    random.seed(seed)
    np.random.seed(seed)


def _read_csv_with_fallback(path: str, headers_str: str) -> pd.DataFrame:
    expected_ncols = len(headers_str.split())

    def _try_read(**kwargs) -> pd.DataFrame:
        df_local = pd.read_csv(path, **kwargs)
        if df_local.shape[1] != expected_ncols:
            return pd.DataFrame()
        return df_local

    df = _try_read(header=None)
    if df.empty:
        df = _try_read(header=None, sep=";", decimal=",")

    if df.empty:
        df = pd.read_csv(path, header=None, delim_whitespace=True)

    if df.shape[1] != expected_ncols:
        raise ValueError(
            f"Unexpected number of columns. Got {df.shape[1]}, expected {expected_ncols}."
        )
    return df


def _build_dataframe(spark: SparkSession, df_raw: pd.DataFrame) -> "pyspark.sql.dataframe.DataFrame":
    base = df_raw.iloc[:, :13].copy()

    thal = pd.to_numeric(base.iloc[:, 12], errors="coerce")
    label = (~thal.isin([3, 7])).astype("int32")

    base.columns = [f"c{i}" for i in range(base.shape[1])]
    base["label"] = label

    for c in base.columns[:-1]:
        base[c] = pd.to_numeric(base[c], errors="coerce")

    return spark.createDataFrame(base)


def _train_evaluate(df_spark) -> float:
    feature_cols = [f"c{i}" for i in range(12)]

    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    assembled = assembler.transform(df_spark).select("features", "label")

    scaler = StandardScaler(inputCol="features", outputCol="Scaled_features", withMean=False, withStd=True)
    scaled = scaler.fit(assembled).transform(assembled).select("Scaled_features", "label")

    training, test = scaled.randomSplit([0.5, 0.5], seed=SEED)

    rf = RandomForestClassifier(
        labelCol="label",
        featuresCol="Scaled_features",
        numTrees=200,
        seed=SEED,
    )
    model = rf.fit(training)
    pred_test = model.transform(test)

    evaluator = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="rawPrediction", metricName="areaUnderROC")
    return float(evaluator.evaluate(pred_test))


def main() -> None:
    _set_reproducible_seeds(SEED)

    spark = (
        SparkSession.builder.appName("HeartClassificationGreen")
        .config("spark.sql.shuffle.partitions", "8")
        .config("spark.default.parallelism", "8")
        .getOrCreate()
    )

    try:
        df_raw = _read_csv_with_fallback(DATASET_PATH, DATASET_HEADERS)
        df_spark = _build_dataframe(spark, df_raw)
        accuracy = _train_evaluate(df_spark)
        print(f"ACCURACY={accuracy:.6f}")
    finally:
        spark.stop()


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed all show()/plot/extra prints to avoid triggering expensive Spark actions and UI/driver overhead.
# - Replaced per-row Python apply() with vectorized pandas isin() to compute the label faster and with less CPU.
# - Avoided storing/transforming unused columns by selecting only needed columns after each Spark transformation.
# - Used deterministic seeds for Python, NumPy, Spark split, and RandomForest to ensure reproducible results.
# - Reduced Spark shuffle/parallelism defaults to limit unnecessary task overhead on small datasets.
# - Implemented robust CSV parsing with fallbacks to avoid repeated manual fixes and minimize failed parse attempts.