# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def load_data(path):
    # Robust CSV loading
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError("Possible delimiter issue")
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    
    # Normalize column names
    df.columns = [str(c).strip().replace('  ', ' ') for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def solve():
    # 1. Load data
    dataset_path = 'bodyPerformance.csv'
    try:
        df = load_data(dataset_path)
    except Exception:
        # Fallback for environment issues - create dummy to ensure end-to-end runs
        print("ACCURACY=0.000000")
        return

    if df.empty:
        print("ACCURACY=0.000000")
        return

    # 2. Identify Target
    # Based on headers 'Blass' is the intended target. Fallback to last column.
    target_col = 'Blass' if 'Blass' in df.columns else df.columns[-1]
    
    # 3. Clean and Separate Features/Target
    # Remove rows where target is NaN
    df = df.dropna(subset=[target_col])
    
    X = df.drop(columns=[target_col])
    y = df[target_col]

    # 4. Feature Engineering & Preprocessing
    # Identify numeric and categorical columns
    numeric_features = []
    categorical_features = []
    
    for col in X.columns:
        # Force numeric conversion where possible
        converted = pd.to_numeric(X[col], errors='coerce')
        if converted.notna().sum() > (0.5 * len(X)):
            X.loc[:, col] = converted
            numeric_features.append(col)
        else:
            categorical_features.append(col)

    # Handle missing values for numeric columns
    for col in numeric_features:
        X.loc[:, col] = X[col].fillna(X[col].mean() if not X[col].isna().all() else 0)
    
    # Handle missing values for categorical
    for col in categorical_features:
        X.loc[:, col] = X[col].fillna('Unknown')

    # Check if we have data left
    if len(X) == 0:
        print("ACCURACY=0.000000")
        return

    # 5. Label Encoding for target if it's classification
    is_regression = False
    if y.dtype == 'object' or len(np.unique(y)) < 20:
        le = LabelEncoder()
        y = le.fit_transform(y.astype(str))
        num_classes = len(le.classes_)
    else:
        # Check if it's actually numeric for regression
        y_numeric = pd.to_numeric(y, errors='coerce')
        if y_numeric.notna().sum() > 0:
            y = y_numeric.fillna(y_numeric.mean())
            is_regression = True
        else:
            # Fallback to classification if numeric conversion fails
            le = LabelEncoder()
            y = le.fit_transform(y.astype(str))
            num_classes = len(le.classes_)

    # 6. Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 7. Pipeline construction
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numeric_features),
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
        ])

    if not is_regression:
        # Lightweight Logistic Regression
        model = Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('classifier', LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto', n_jobs=1))
        ])
    else:
        # Linear Regression as energy-efficient proxy
        from sklearn.linear_model import Ridge
        model = Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('regressor', Ridge())
        ])

    # 8. Train
    model.fit(X_train, y_train)

    # 9. Evaluate
    y_pred = model.predict(X_test)
    
    if not is_regression:
        accuracy = accuracy_score(y_test, y_pred)
    else:
        # For regression, we use a bounded R2-like score as an accuracy proxy
        from sklearn.metrics import r2_score
        r2 = r2_score(y_test, y_pred)
        accuracy = max(0, min(1, r2)) # Bound between 0 and 1

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# Optimization Summary
# 1. Model Choice: Logistic Regression (Classification) or Ridge (Regression) was chosen as they are mathematically 
#    lightweight and have low energy consumption compared to ensembles or deep learning.
# 2. Preprocessing: Scikit-learn Pipelines were used to ensure an efficient, single-pass transformation 
#    of data, minimizing memory overhead and redundant computations.
# 3. CPU-Efficiency: No specialized hardware or heavy libraries (XGBoost/LightGBM/PyTorch) used; 
#    utilized lbfgs solver which is highly efficient for small to medium numeric datasets.
# 4. Robustness: The solution includes multi-stage CSV parsing, column normalization, and type coercion 
#    to prevent runtime crashes on malformed data while maintaining a tiny carbon footprint.
# 5. Regression Proxy: In the event of regression, R^2 is used and clipped to [0,1] to meet the ACCURACY format 
#    requirement while representing model performance.