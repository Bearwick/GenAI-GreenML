# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
        return df
    if df.shape[1] == 1:
        try:
            df2 = pd.read_csv(path, sep=';', decimal=',')
            if df2.shape[1] > 1:
                df = df2
        except Exception:
            pass
    return df

path = "Iris.csv"
df = read_csv_robust(path)

if df is None:
    df = pd.DataFrame()

df.columns = [" ".join(str(c).strip().split()) for c in df.columns]
df = df.loc[:, ~df.columns.str.startswith("Unnamed")]

if df.shape[1] == 0:
    df = pd.DataFrame({"feature": [0]})

candidate_names = ["target", "label", "class", "species", "outcome", "y"]
lower_cols = {c.lower(): c for c in df.columns}
target_col = None
for name in candidate_names:
    if name in lower_cols:
        target_col = lower_cols[name]
        break

if target_col is None:
    numeric_candidates = []
    for col in df.columns:
        col_numeric = pd.to_numeric(df[col], errors='coerce')
        nunique = col_numeric.nunique(dropna=True)
        if nunique > 1:
            numeric_candidates.append((nunique, col))
    if numeric_candidates:
        target_col = sorted(numeric_candidates, key=lambda x: x[0])[0][1]
    else:
        target_col = df.columns[-1]

if target_col not in df.columns:
    target_col = df.columns[-1]

features = [c for c in df.columns if c != target_col]
if len(features) == 0:
    df = df.copy()
    df["constant"] = 1.0
    features = ["constant"]

y = df[target_col]
if y.dtype == object:
    mask = y.notna() & (y.astype(str).str.strip() != "")
else:
    mask = y.notna()
df = df.loc[mask].copy()
y = df[target_col]

assert len(df) > 0

features = [c for c in df.columns if c != target_col]
if len(features) == 0:
    df = df.copy()
    df["constant"] = 1.0
    features = ["constant"]

X = df[features].copy()
X = X.replace([np.inf, -np.inf], np.nan)

numeric_cols = []
categorical_cols = []
for col in X.columns:
    if pd.api.types.is_numeric_dtype(X[col]):
        numeric_cols.append(col)
    else:
        converted = pd.to_numeric(X[col], errors='coerce')
        non_na = converted.notna().sum()
        if non_na >= max(1, int(0.8 * len(converted))):
            X[col] = converted
            numeric_cols.append(col)
        else:
            categorical_cols.append(col)

if len(numeric_cols) + len(categorical_cols) == 0:
    X = pd.DataFrame({"constant": np.ones(len(df))})
    numeric_cols = ["constant"]
    categorical_cols = []

def build_preprocessor(num_cols, cat_cols):
    transformers = []
    if len(num_cols) > 0:
        transformers.append(("num", Pipeline([("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler())]), num_cols))
    if len(cat_cols) > 0:
        transformers.append(("cat", Pipeline([("imputer", SimpleImputer(strategy="most_frequent")), ("onehot", OneHotEncoder(handle_unknown="ignore"))]), cat_cols))
    if len(transformers) == 0:
        transformers.append(("num", Pipeline([("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler())]), num_cols))
    return ColumnTransformer(transformers=transformers, remainder="drop")

is_numeric_target = pd.api.types.is_numeric_dtype(y)
if not is_numeric_target:
    classification = True
else:
    n_unique = y.nunique(dropna=True)
    classification = n_unique <= 20

if classification:
    n_classes = y.nunique(dropna=True)
    preprocessor = build_preprocessor(numeric_cols, categorical_cols)
    if n_classes < 2:
        model = DummyClassifier(strategy="most_frequent")
    else:
        model = LogisticRegression(max_iter=200, solver="liblinear")
    clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
    n_samples = len(df)
    if n_samples >= 2:
        test_size = 0.2 if n_samples >= 5 else 0.5
        stratify = y if n_classes >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=stratify)
    else:
        X_train, X_test, y_train, y_test = X, X, y, y
    assert len(X_train) > 0 and len(X_test) > 0
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
else:
    y_numeric = pd.to_numeric(y, errors='coerce')
    mask = y_numeric.notna()
    X = X.loc[mask]
    y_numeric = y_numeric.loc[mask]
    assert len(X) > 0
    preprocessor = build_preprocessor(numeric_cols, categorical_cols)
    if len(y_numeric) < 2:
        model = DummyRegressor(strategy="mean")
    else:
        model = Ridge(alpha=1.0, solver="sparse_cg")
    reg = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
    n_samples = len(X)
    if n_samples >= 2:
        test_size = 0.2 if n_samples >= 5 else 0.5
        X_train, X_test, y_train, y_test = train_test_split(X, y_numeric, test_size=test_size, random_state=42)
    else:
        X_train, X_test, y_train, y_test = X, X, y_numeric, y_numeric
    assert len(X_train) > 0 and len(X_test) > 0
    reg.fit(X_train, y_train)
    y_pred = reg.predict(X_test)
    if len(np.unique(y_test)) < 2:
        r2 = 0.0
    else:
        r2 = r2_score(y_test, y_pred)
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models and minimal preprocessing to stay CPU- and energy-efficient.
# - Applied ColumnTransformer with imputation, scaling, and one-hot encoding for reproducible feature handling.
# - Implemented robust CSV parsing and adaptive target selection for schema-agnostic execution.
# - Regression uses a bounded (r2+1)/2 proxy to keep ACCURACY within [0,1].