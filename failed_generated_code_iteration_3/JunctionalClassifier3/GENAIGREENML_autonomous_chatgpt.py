# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import math
import pickle
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor


RANDOM_STATE = 42
DATASET_PATH = "dict.pickle"


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        if c2.lower().startswith("unnamed:"):
            normed.append(None)
        else:
            normed.append(c2)
    return normed


def _safe_read_csv(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d):
        if d is None or d.empty:
            return True
        if d.shape[1] <= 1:
            return True
        col0 = str(d.columns[0])
        if ";" in col0:
            return True
        return False

    if _looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            pass
    return df


def _load_dataset(path):
    ext = os.path.splitext(path)[1].lower()
    if ext in [".pkl", ".pickle"]:
        with open(path, "rb") as f:
            obj = pickle.load(f)

        if isinstance(obj, pd.DataFrame):
            df = obj.copy()
            return df

        if isinstance(obj, dict):
            # Try common patterns: {"X":..., "y":...} or dict of arrays to DataFrame
            keys = list(obj.keys())
            lower_map = {str(k).lower(): k for k in keys}

            if "df" in lower_map and isinstance(obj[lower_map["df"]], pd.DataFrame):
                return obj[lower_map["df"]].copy()

            x_key = None
            y_key = None
            for cand in ["x", "data", "features", "inputs"]:
                if cand in lower_map:
                    x_key = lower_map[cand]
                    break
            for cand in ["y", "target", "label", "labels", "output"]:
                if cand in lower_map:
                    y_key = lower_map[cand]
                    break

            if x_key is not None and y_key is not None:
                X = obj[x_key]
                y = obj[y_key]
                if isinstance(X, pd.DataFrame):
                    df = X.copy()
                else:
                    X_arr = np.asarray(X)
                    if X_arr.ndim == 1:
                        X_arr = X_arr.reshape(-1, 1)
                    df = pd.DataFrame(X_arr, columns=[f"f{i}" for i in range(X_arr.shape[1])])
                y_arr = np.asarray(y).reshape(-1)
                df["target"] = y_arr
                return df

            # Fallback: treat dict as column->values mapping
            try:
                df = pd.DataFrame(obj)
                return df
            except Exception:
                pass

        if isinstance(obj, (list, tuple, np.ndarray)):
            arr = np.asarray(obj)
            if arr.ndim == 1:
                arr = arr.reshape(-1, 1)
            df = pd.DataFrame(arr, columns=[f"f{i}" for i in range(arr.shape[1])])
            return df

        raise ValueError("Unsupported pickle content.")
    elif ext in [".csv", ".txt"]:
        df = _safe_read_csv(path)
        if df is None:
            raise ValueError("Failed to read CSV.")
        return df
    else:
        # Try csv first, then pickle
        df = _safe_read_csv(path)
        if isinstance(df, pd.DataFrame) and not df.empty:
            return df
        with open(path, "rb") as f:
            obj = pickle.load(f)
        if isinstance(obj, pd.DataFrame):
            return obj.copy()
        raise ValueError("Unsupported dataset format.")


def _coerce_numeric_df(df, cols):
    out = df.copy()
    for c in cols:
        out[c] = pd.to_numeric(out[c], errors="coerce")
    return out


def _pick_target(df):
    cols = list(df.columns)

    # Prefer explicit names if present
    lower = {c.lower(): c for c in cols}
    for cand in ["target", "label", "class", "y", "output"]:
        if cand in lower:
            return lower[cand]

    # Prefer last column if it looks like target (low-ish unique count relative to n)
    n = len(df)
    if n > 0:
        last = cols[-1]
        s = df[last]
        nunq = s.nunique(dropna=True)
        if nunq >= 2 and nunq <= max(20, int(0.2 * n)):
            return last

    # Prefer any non-constant numeric column with fewer unique values (classification-like)
    numeric_cols = list(df.select_dtypes(include=[np.number]).columns)
    best = None
    best_score = None
    for c in numeric_cols:
        s = df[c]
        nunq = s.nunique(dropna=True)
        if nunq < 2:
            continue
        # score: prefer fewer unique values, but not too few for tiny datasets
        score = (nunq, -s.notna().mean())
        if best is None or score < best_score:
            best = c
            best_score = score
    if best is not None and (df[best].nunique(dropna=True) <= max(20, int(0.2 * n))):
        return best

    # Otherwise choose any non-constant numeric column
    for c in numeric_cols:
        if df[c].nunique(dropna=True) >= 2:
            return c

    # Otherwise choose any non-constant column
    for c in cols:
        if df[c].nunique(dropna=True) >= 2:
            return c

    return cols[-1] if cols else None


def _is_classification_target(y):
    # Decide with robust heuristics; return (is_classification, y_processed, classes)
    y_series = pd.Series(y)
    y_nonnull = y_series.dropna()
    if y_nonnull.empty:
        return False, y_series, None

    # If object/bool/category -> classification
    if y_nonnull.dtype == bool or y_nonnull.dtype == object:
        return True, y_series.astype(str), None

    # Try numeric
    y_num = pd.to_numeric(y_series, errors="coerce")
    y_num_nonnull = y_num.dropna()
    if y_num_nonnull.empty:
        return True, y_series.astype(str), None

    nunq = y_num_nonnull.nunique()
    n = len(y_num_nonnull)
    if nunq < 2:
        return False, y_num, None

    # If values are all integers and unique count is limited -> classification
    is_intlike = np.all(np.isclose(y_num_nonnull.values, np.round(y_num_nonnull.values)))
    if is_intlike and nunq <= max(20, int(0.2 * n)):
        return True, y_num.astype(int), None

    # Otherwise regression
    return False, y_num, None


def _bounded_regression_score(y_true, y_pred):
    # Convert to stable [0,1] score using normalized MAE: 1 - MAE/(MAD+eps), clipped.
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if not np.any(mask):
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    mae = np.mean(np.abs(yt - yp))
    mad = np.mean(np.abs(yt - np.mean(yt)))
    denom = mad + 1e-12
    score = 1.0 - (mae / denom)
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


df = _load_dataset(DATASET_PATH)
assert isinstance(df, pd.DataFrame)

# Normalize columns and drop unnamed
df = df.copy()
normed = _normalize_columns(df.columns)
keep_cols = [c for c in normed if c is not None]
df.columns = [c for c in normed if c is not None] + [f"_dropped_{i}" for i, c in enumerate(normed) if c is None]
drop_cols = [c for c in df.columns if c.startswith("_dropped_")]
if drop_cols:
    df = df.drop(columns=drop_cols, errors="ignore")

# Basic cleanup: drop all-empty rows/cols
df = df.dropna(axis=0, how="all").dropna(axis=1, how="all")
assert not df.empty

target_col = _pick_target(df)
if target_col is None or target_col not in df.columns:
    # Fallback: create a synthetic target (constant) to keep pipeline running
    df["target"] = 0
    target_col = "target"

y_raw = df[target_col]
X = df.drop(columns=[target_col], errors="ignore")

# If no features left, create a single constant feature
if X.shape[1] == 0:
    X = pd.DataFrame({"bias": np.ones(len(df), dtype=float)})

# Coerce numeric columns safely; do not compute stats on object columns directly later
num_cols = list(X.select_dtypes(include=[np.number]).columns)
X = _coerce_numeric_df(X, num_cols)
X = X.replace([np.inf, -np.inf], np.nan)

cat_cols = [c for c in X.columns if c not in num_cols]

is_clf, y_proc, _ = _is_classification_target(y_raw)

# Ensure split is possible
test_size = 0.2
if len(df) < 5:
    test_size = 0.4
if len(df) < 3:
    test_size = 0.5

stratify = None
if is_clf:
    y_tmp = pd.Series(y_proc)
    if y_tmp.dropna().nunique() >= 2:
        # Only stratify if each class has at least 2 samples to avoid split errors
        vc = y_tmp.value_counts(dropna=True)
        if (vc.min() >= 2) and (len(y_tmp) >= 5):
            stratify = y_proc

X_train, X_test, y_train, y_test = train_test_split(
    X, y_proc, test_size=test_size, random_state=RANDOM_STATE, stratify=stratify
)
assert len(X_train) > 0 and len(X_test) > 0

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=True, with_std=True)),
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, num_cols),
        ("cat", categorical_transformer, cat_cols),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

accuracy = 0.0

if is_clf:
    y_train_s = pd.Series(y_train).dropna()
    n_classes = y_train_s.nunique()
    if n_classes < 2:
        # Fallback to trivial classifier
        model = DummyClassifier(strategy="most_frequent")
        clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Logistic regression is CPU-friendly and strong baseline for tabular data
        model = LogisticRegression(
            solver="lbfgs",
            max_iter=200,
            n_jobs=1,
        )
        clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])
        try:
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
        except Exception:
            # Robust fallback if data causes solver issues
            model2 = DummyClassifier(strategy="most_frequent")
            clf2 = Pipeline(steps=[("preprocess", preprocess), ("model", model2)])
            clf2.fit(X_train, y_train)
            y_pred = clf2.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Regression path with bounded proxy score in [0,1]
    y_train_num = pd.to_numeric(pd.Series(y_train), errors="coerce")
    y_test_num = pd.to_numeric(pd.Series(y_test), errors="coerce")

    if y_train_num.dropna().nunique() < 2:
        reg = DummyRegressor(strategy="mean")
        pipe = Pipeline(steps=[("preprocess", preprocess), ("model", reg)])
        pipe.fit(X_train, y_train_num)
        y_pred = pipe.predict(X_test)
        accuracy = _bounded_regression_score(y_test_num, y_pred)
    else:
        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        pipe = Pipeline(steps=[("preprocess", preprocess), ("model", reg)])
        try:
            pipe.fit(X_train, y_train_num)
            y_pred = pipe.predict(X_test)
            accuracy = _bounded_regression_score(y_test_num, y_pred)
        except Exception:
            reg2 = DummyRegressor(strategy="mean")
            pipe2 = Pipeline(steps=[("preprocess", preprocess), ("model", reg2)])
            pipe2.fit(X_train, y_train_num)
            y_pred = pipe2.predict(X_test)
            accuracy = _bounded_regression_score(y_test_num, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Uses lightweight baselines (LogisticRegression/Ridge or Dummy*) to minimize CPU time/energy; avoids heavy ensembles/deep learning.
# - ColumnTransformer+Pipeline ensures single-pass preprocessing, reproducibility, and prevents redundant transformations.
# - OneHotEncoder(handle_unknown="ignore") avoids failures on unseen categories without costly feature hashing/embeddings.
# - StandardScaler only for numeric features to help linear models converge in fewer iterations (lower compute).
# - Robust schema handling: normalizes column names, drops unnamed columns, infers target safely, and falls back to trivial models if needed.
# - Regression fallback prints a bounded [0,1] "accuracy" proxy: 1 - MAE/(MAD+eps), clipped, to remain stable across scales.