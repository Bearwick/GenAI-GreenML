# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import glob
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    out = []
    for c in cols:
        c = str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        out.append(c)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c).strip())]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _try_read_csv(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d):
        if d is None or d.empty:
            return True
        if d.shape[1] <= 1:
            return True
        return False

    if _looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            pass
    return df


def _find_dataset_path():
    # Prefer common iris filenames; otherwise pick first reasonable CSV in current tree.
    candidates = []
    for name in ["iris.csv", "Iris.csv", "IRIS.csv", "data.csv", "dataset.csv", "train.csv"]:
        if os.path.exists(name):
            candidates.append(name)

    if not candidates:
        # Search shallow first for energy efficiency
        for pat in ["*.csv", "*/*.csv", "*/*/*.csv"]:
            candidates.extend(glob.glob(pat))

    # Filter out non-dataset CSVs (common in repos)
    def bad_name(p):
        bn = os.path.basename(p).lower()
        return any(x in bn for x in ["submission", "sample", "answer", "solution", "requirements", "readme"])

    candidates = [p for p in candidates if not bad_name(p)]
    candidates = sorted(set(candidates), key=lambda p: (len(p.split(os.sep)), len(p)))

    return candidates[0] if candidates else None


def _choose_target(df):
    cols = list(df.columns)
    if not cols:
        return None

    lower_map = {c: str(c).strip().lower() for c in cols}

    # Prefer typical target names for iris and generic ML datasets
    preferred = [
        "species", "class", "target", "label", "y",
        "outcome", "category", "categorical", "type"
    ]
    for want in preferred:
        for c in cols:
            if lower_map[c] == want:
                return c

    # If there is an object column with limited unique values, prefer it as classification target
    obj_cols = [c for c in cols if df[c].dtype == "object" or pd.api.types.is_string_dtype(df[c].dtype)]
    best_obj = None
    best_score = -1
    for c in obj_cols:
        nun = df[c].nunique(dropna=True)
        if nun >= 2:
            score = -abs(nun - 3)  # iris typically 3 classes; prefer small number of classes
            if score > best_score:
                best_score = score
                best_obj = c
    if best_obj is not None:
        return best_obj

    # Otherwise choose a numeric column that's non-constant as regression target
    numeric_cols = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() >= max(3, int(0.5 * len(df))):
            numeric_cols.append((c, s))
    for c, s in numeric_cols:
        if s.nunique(dropna=True) >= 2:
            return c

    # Fallback: last column
    return cols[-1]


def _safe_accuracy_proxy_r2(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    ss_res = float(np.sum((yt - yp) ** 2))
    ss_tot = float(np.sum((yt - float(np.mean(yt))) ** 2))
    if ss_tot <= 0.0:
        return 0.0
    r2 = 1.0 - ss_res / ss_tot
    acc = max(0.0, min(1.0, (r2 + 1.0) / 2.0))
    return acc


def main():
    dataset_path = _find_dataset_path()

    if dataset_path is None or (not os.path.exists(dataset_path)):
        # Trivial fallback (no dataset found)
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _try_read_csv(dataset_path)
    if df is None:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Strip whitespace in object columns cheaply
    for c in df.columns:
        if df[c].dtype == "object" or pd.api.types.is_string_dtype(df[c].dtype):
            df[c] = df[c].astype("string").str.strip()

    # Drop fully empty columns/rows
    df = df.dropna(axis=1, how="all")
    df = df.dropna(axis=0, how="all")

    assert not df.empty, "Dataset is empty after basic cleaning."

    target_col = _choose_target(df)
    if target_col is None or target_col not in df.columns:
        target_col = df.columns[-1]

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # If no features remain, create a constant feature
    if X.shape[1] == 0:
        X = pd.DataFrame({"_const": np.ones(len(df), dtype=float)})

    # Identify column types
    numeric_features = []
    categorical_features = []
    for c in X.columns:
        s_num = pd.to_numeric(X[c], errors="coerce")
        # Treat as numeric if majority can be coerced
        if s_num.notna().sum() >= max(3, int(0.7 * len(X))):
            X[c] = s_num
            numeric_features.append(c)
        else:
            categorical_features.append(c)

    # Decide task type
    is_classification = False
    y_for_model = y_raw

    if (y_raw.dtype == "object") or pd.api.types.is_string_dtype(y_raw.dtype) or pd.api.types.is_categorical_dtype(y_raw.dtype):
        is_classification = True
    else:
        y_num = pd.to_numeric(y_raw, errors="coerce")
        # If it looks like discrete labels with few unique values, classify
        nun = y_num.nunique(dropna=True)
        if nun >= 2 and nun <= min(20, max(2, int(0.05 * len(df)) + 2)):
            is_classification = True
            y_for_model = y_num
        else:
            y_for_model = y_num

    # Build preprocessors
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True))
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3
    )

    # Prepare y and split
    if is_classification:
        y = y_for_model.astype("string").fillna("NA")
        # ensure at least 2 classes
        if y.nunique(dropna=False) < 2:
            is_classification = False
            y = pd.to_numeric(y_raw, errors="coerce")
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
    if not is_classification:
        y = pd.to_numeric(y_raw, errors="coerce")
        # Drop rows with missing target for regression
        mask = y.notna()
        X = X.loc[mask].reset_index(drop=True)
        y = y.loc[mask].reset_index(drop=True)
        assert len(y) > 2, "Not enough samples after dropping NaN targets."
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

    assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

    if is_classification:
        model = LogisticRegression(
            solver="lbfgs",
            max_iter=200,
            n_jobs=1
        )
        clf = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", model)
        ])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        model = Ridge(alpha=1.0, random_state=42)
        reg = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", model)
        ])
        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        accuracy = float(_safe_accuracy_proxy_r2(y_test, y_pred))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight, CPU-friendly models: LogisticRegression for classification; Ridge for regression fallback.
# - Minimal feature engineering with a single ColumnTransformer: median imputation + standard scaling for numeric, most-frequent + one-hot for categoricals.
# - Robust schema handling: normalizes headers, drops 'Unnamed' columns, infers target defensively, coerces numeric data with errors='coerce'.
# - Energy-aware choices: avoids heavy ensembles/deep learning, uses sparse one-hot encoding, limits max_iter, sets n_jobs=1 to prevent excess CPU parallelism.
# - Regression fallback uses a bounded "accuracy proxy" = (R^2 + 1)/2 clipped to [0,1] to satisfy ACCURACY output requirement.