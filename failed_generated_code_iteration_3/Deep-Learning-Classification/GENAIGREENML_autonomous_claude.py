# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

DATASET_PATH = "heart_failure.csv"

# Robust CSV loading
try:
    df = pd.read_csv(DATASET_PATH)
    if df.shape[1] < 3:
        df = pd.read_csv(DATASET_PATH, sep=';', decimal=',')
except Exception:
    df = pd.read_csv(DATASET_PATH, sep=';', decimal=',')

# Strip/normalize column names
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)

# Drop unnamed columns
unnamed_cols = [c for c in df.columns if c.lower().startswith('unnamed')]
if unnamed_cols:
    df = df.drop(columns=unnamed_cols)

# Identify target column
# Prefer 'DEATH_EVENT' (numeric) or 'death_event' (categorical)
target_col = None
candidate_targets = ['DEATH_EVENT', 'death_event']
for ct in candidate_targets:
    if ct in df.columns:
        target_col = ct
        break

if target_col is None:
    # Fallback: pick last column
    target_col = df.columns[-1]

# If there are duplicate target columns (DEATH_EVENT and death_event), drop the redundant one
# Use DEATH_EVENT as primary target since it's numeric
if 'DEATH_EVENT' in df.columns and 'death_event' in df.columns:
    target_col = 'DEATH_EVENT'
    df = df.drop(columns=['death_event'])

# Encode target if it's not numeric
y = df[target_col].copy()
if y.dtype == object:
    mapping = {}
    for i, val in enumerate(sorted(y.unique())):
        mapping[val] = i
    y = y.map(mapping)
    if y.isna().any():
        y = pd.to_numeric(y, errors='coerce')

y = y.astype(float)
df[target_col] = y

# Drop rows with NaN target
valid_mask = y.notna()
df = df[valid_mask].reset_index(drop=True)
y = df[target_col]

# Feature columns
feature_cols = [c for c in df.columns if c != target_col]

assert len(feature_cols) > 0, "No feature columns found"
assert len(df) > 0, "Dataset is empty after preprocessing"

# Identify numeric and categorical columns
numeric_cols = []
categorical_cols = []

for col in feature_cols:
    col_data = df[col]
    if col_data.dtype == object:
        # Check if it can be converted to numeric
        converted = pd.to_numeric(col_data, errors='coerce')
        if converted.notna().sum() / len(converted) > 0.8:
            df[col] = converted
            numeric_cols.append(col)
        else:
            categorical_cols.append(col)
    else:
        df[col] = pd.to_numeric(df[col], errors='coerce')
        numeric_cols.append(col)

# Handle NaN/inf in numeric columns
for col in numeric_cols:
    df[col] = df[col].replace([np.inf, -np.inf], np.nan)
    if df[col].isna().any():
        median_val = df[col].median()
        df[col] = df[col].fillna(median_val)

# Handle NaN in categorical columns
for col in categorical_cols:
    df[col] = df[col].fillna('missing')

X = df[list(feature_cols)]
y = df[target_col].astype(int)

# Check classification viability
n_classes = y.