# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings
from typing import Tuple, Optional

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge


warnings.filterwarnings("ignore")


DATASET_PATH = "town_vax_data.csv"
DATASET_HEADERS = [
    "town",
    "apartments_condos_multis_per_residential_parcels_2011",
    "assessed_home_value_changes_2009-2013",
    "births_per_1000_residents_2010",
    "boaters_per_10000_residents_2012",
    "burglaries_per_10000_residents_2011",
    "cars_motorcycles_&_trucks_average_age_2012",
    "cars_per_1000_residents_2012",
    "class_size_in_school_district_2011-2012",
    "condos_as_perc_of_parcels_2012",
    "crashes_per_1000_residents_2007-2011",
    "culture_and_rec_spending_per_person_2012",
    "education_spending_as_a_percent_2012",
    "education_spending_per_resident_2012",
    "expenditures_per_resident_2012",
    "females_percent_in_community_2010",
    "fire_dept_spending_as_a_percent_2012",
    "firefighter_costs_per_resident_2012",
    "fixed_costs_percent_2012",
    "gun_licenses_per_1000_residents_2012",
    "historic_places_per_10000_2013",
    "home_schooled_per_1000_students_2011-2012",
    "homes_built_in_39_or_before",
    "household_member_who_is_2_races_or_more_per_1000_households_2010",
    "households_average_size_2010",
    "households_one-person_2010",
    "hybrid_cars_per_1000_vehicles_2013",
    "in_home_since_1969_or_earlier",
    "income_average_per_resident_2010",
    "income_change_per_resident_2007-2010",
    "inmates_in_state_prison_per_1000_residents",
    "liquor_licenses_per_10000_2011",
    "median_age_2011",
    "miles_driven_daily_per_household_05-07",
    "minority_students_per_district_2012-2013",
    "motorcycles_change_in_ownership_2000-2012",
    "motorcycles_per_1000_2012",
    "multi-generation_households_2010",
    "police_costs_per_resident_2013",
    "police_employees_per_10000_residents_2011",
    "police_spending_as_a_percent_2012",
    "population_change_1950-2010",
    "population_change_2010-2011",
    "presidential_fundraising_obama_vs_romney",
    "property_crimes_per_10000_residents_2012",
    "property_tax_change_09-13",
    "pupils_per_cost_average_by_district_2011-2012",
    "residential_taxes_as_percent_of_all_property_taxes_2013",
    "saltwater_fishing_licenses_per_1000_2013",
    "school_district_growth_09-13",
    "single-person_households_percent_65_and_older",
    "snowmobiles_per_10000_residents_2012",
    "state_aid_as_a_percent_of_town_budget_2012",
    "students_in_public_schools_2011",
    "tax-exempt_property_2012",
    "taxable_property_by_percent_2012",
    "teacher_salaries_by_average_2011",
    "teachers_percent_under_40_years_old_2011-2012",
    "trucks_per_1000_residents_2012",
    "violent_crimes_per_10000_residents_2012",
    "voters_as_a_percent_of_population_2012",
    "voters_change_in_registrations_between_1982-2012",
    "voters_democrats_as_a_percent_2012",
    "2020_votes",
    "2020_biden_margin",
    "population",
    "vax_level",
]


def _normalize_columns(cols) -> list:
    normed = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        if re.match(r"^Unnamed:\s*\d+$", c2):
            normed.append(None)
        else:
            normed.append(c2)
    return normed


def _read_csv_robust(path: str) -> pd.DataFrame:
    # Try default parsing first (fast path), then a common European CSV fallback.
    df1 = pd.read_csv(path)
    if df1.shape[1] <= 1:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        if df2.shape[1] > df1.shape[1]:
            return df2
    return df1


def _looks_like_number_series(s: pd.Series) -> bool:
    if s.dtype.kind in "iufc":
        return True
    if s.dtype == "O":
        ss = s.dropna().astype(str).head(50)
        if ss.empty:
            return False
        # Heuristic: if many values can be parsed as float after cleaning
        ok = 0
        for v in ss:
            v2 = v.replace(",", ".")
            v2 = re.sub(r"[^\d\.\-eE+]", "", v2)
            try:
                float(v2)
                ok += 1
            except Exception:
                pass
        return ok / max(1, len(ss)) >= 0.8
    return False


def _coerce_numeric_columns(df: pd.DataFrame, cols: list) -> None:
    for c in cols:
        if c not in df.columns:
            continue
        if df[c].dtype.kind in "iufc":
            continue
        if df[c].dtype == "O":
            cleaned = (
                df[c]
                .astype(str)
                .str.replace(",", ".", regex=False)
                .str.replace(r"[^\d\.\-eE+]", "", regex=True)
            )
            df[c] = pd.to_numeric(cleaned, errors="coerce")


def _choose_target(df: pd.DataFrame) -> Tuple[str, str]:
    # Returns (target_col, task) where task in {"classification","regression"}
    preferred = ["vax_level", "vax", "target", "label", "y"]
    for p in preferred:
        for c in df.columns:
            if c.lower() == p:
                tgt = c
                break
        else:
            continue
        break
    else:
        tgt = None

    if tgt is None:
        # Choose a non-constant numeric column, preferring last columns (often targets)
        numeric_candidates = []
        for c in df.columns:
            if c == "town":
                continue
            if _looks_like_number_series(df[c]):
                numeric_candidates.append(c)
        if not numeric_candidates:
            # Fallback: any non-constant column
            for c in df.columns[::-1]:
                if df[c].nunique(dropna=True) > 1:
                    return c, "classification"
            return df.columns[-1], "classification"
        # Prefer the last numeric column with variability
        for c in numeric_candidates[::-1]:
            if df[c].nunique(dropna=True) > 1:
                tgt = c
                break
        if tgt is None:
            tgt = numeric_candidates[-1]

    # Determine task type
    y = df[tgt]
    if y.dtype.kind in "iufc":
        # If looks categorical (few unique), treat as classification
        nunq = y.nunique(dropna=True)
        if nunq >= 2 and nunq <= max(20, int(0.05 * len(df))):
            return tgt, "classification"
        return tgt, "regression"
    else:
        nunq = y.nunique(dropna=True)
        if nunq >= 2:
            return tgt, "classification"
        # Single class -> regression fallback
        return tgt, "regression"


def _bounded_regression_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    # Stable [0,1] proxy: 1 / (1 + normalized MAE); robust to scale by dividing by IQR (or std fallback).
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = np.mean(np.abs(y_true - y_pred))
    q75, q25 = np.nanpercentile(y_true, [75, 25])
    iqr = float(q75 - q25)
    if not np.isfinite(iqr) or iqr <= 1e-12:
        iqr = float(np.nanstd(y_true))
    if not np.isfinite(iqr) or iqr <= 1e-12:
        iqr = 1.0
    nmae = mae / iqr
    score = 1.0 / (1.0 + nmae)
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    if not os.path.exists(DATASET_PATH):
        raise FileNotFoundError(DATASET_PATH)

    df = _read_csv_robust(DATASET_PATH)

    # Normalize/clean columns
    new_cols = _normalize_columns(df.columns)
    keep_mask = [c is not None for c in new_cols]
    df = df.loc[:, keep_mask].copy()
    df.columns = [c for c in new_cols if c is not None]

    # If file came without header and got integer columns, try applying provided headers if compatible
    if all(re.match(r"^\d+$", str(c)) for c in df.columns) and len(df.columns) == len(DATASET_HEADERS):
        df.columns = DATASET_HEADERS

    # Basic cleanup of obvious empty rows
    df = df.dropna(how="all")
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col, task = _choose_target(df)

    # Build feature set (everything except target)
    feature_cols = [c for c in df.columns if c != target_col]
    if not feature_cols:
        # Minimal fallback to allow end-to-end run
        df["_constant_feat"] = 1.0
        feature_cols = ["_constant_feat"]

    # Identify numeric vs categorical features robustly
    numeric_features = []
    categorical_features = []
    for c in feature_cols:
        if _looks_like_number_series(df[c]):
            numeric_features.append(c)
        else:
            categorical_features.append(c)

    # Coerce numeric feature columns (and numeric target if regression) safely
    _coerce_numeric_columns(df, numeric_features)
    if task == "regression" and target_col in df.columns:
        _coerce_numeric_columns(df, [target_col])

    # Replace inf with NaN to avoid downstream issues
    df = df.replace([np.inf, -np.inf], np.nan)

    # Prepare X/y
    X = df[feature_cols].copy()
    y = df[target_col].copy()

    # Handle classification target
    if task == "classification":
        # If numeric but meant to be class labels, keep as-is; else ensure string labels
        if y.dtype.kind not in "iufc":
            y = y.astype(str)
        # Drop rows with missing target
        mask = ~pd.isna(y)
        X = X.loc[mask].copy()
        y = y.loc[mask].copy()
        # If still degenerate, fallback to regression
        if y.nunique(dropna=True) < 2:
            task = "regression"

    if task == "regression":
        # Ensure numeric target; drop missing target rows
        y = pd.to_numeric(y, errors="coerce")
        mask = ~pd.isna(y)
        X = X.loc[mask].copy()
        y = y.loc[mask].copy()

    assert len(X) > 1 and len(y) > 1

    # Train/test split
    stratify = y if task == "classification" and getattr(y, "nunique", lambda **_: 0)() >= 2 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=stratify
    )
    assert len(X_train) > 0 and len(X_test) > 0

    # Preprocessing: small, CPU-friendly, sparse one-hot for categoricals
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if task == "classification":
        # Logistic regression is efficient for small/medium tabular data; saga handles sparse OHE well.
        clf = LogisticRegression(
            max_iter=300,
            solver="saga",
            penalty="l2",
            C=1.0,
            n_jobs=1,
            random_state=42,
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Ridge is a lightweight linear baseline; handles multicollinearity and is stable.
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_score(np.asarray(y_test, dtype=float), np.asarray(y_pred, dtype=float))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used lightweight linear models (LogisticRegression / Ridge) to minimize CPU time and memory vs. trees/ensembles/deep nets.
# - Employed a single sklearn Pipeline + ColumnTransformer for reproducible, one-pass preprocessing (less redundant work).
# - OneHotEncoder with sparse output keeps memory low for categorical features; StandardScaler improves linear convergence.
# - Robust schema handling: normalizes headers, drops 'Unnamed' columns, infers target if missing, coerces numeric safely.
# - Regression fallback uses a bounded [0,1] accuracy proxy: ACCURACY = 1 / (1 + normalized_MAE) with IQR scaling for stability.