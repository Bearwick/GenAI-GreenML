# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np

def load_data(csv_path):
    try:
        df = pd.read_csv(csv_path)
        if df.shape[1] <= 1:
            raise ValueError
    except:
        df = pd.read_csv(csv_path, sep=';', decimal=',')
    return df

def get_entropy(labels):
    if len(labels) == 0:
        return 0
    _, counts = np.unique(labels, return_counts=True)
    probs = counts / len(labels)
    return -np.sum(probs * np.log2(probs))

class LeafNode:
    def __init__(self, labels):
        val, counts = np.unique(labels, return_counts=True)
        idx = np.argmax(counts)
        self.pred_class = val[idx]
        self.prob = counts[idx] / len(labels)
        self.pred_class_count = counts[idx]
        self.total_count = len(labels)

    def classify(self, example):
        return self.pred_class, self.prob

class DecisionNode:
    def __init__(self, attr, threshold, child_lt, child_ge, child_miss):
        self.attr = attr
        self.threshold = threshold
        self.child_lt = child_lt
        self.child_ge = child_ge
        self.child_miss = child_miss

    def classify(self, example):
        val = example.get(self.attr)
        if val is None or (isinstance(val, float) and np.isnan(val)):
            return self.child_miss.classify(example)
        return self.child_lt.classify(example) if val < self.threshold else self.child_ge.classify(example)

def build_tree(data, target_col, attributes, min_leaf_count):
    labels = data[target_col].values
    if len(attributes) == 0:
        return LeafNode(labels)

    best_ig = -1
    best_attr = None
    best_threshold = None
    best_splits = None

    parent_entropy = get_entropy(labels)
    n_total = len(labels)

    for attr in attributes:
        vals = data[attr].values
        valid_mask = ~np.isnan(vals)
        if not np.any(valid_mask):
            continue
        
        v_min, v_max = np.min(vals[valid_mask]), np.max(vals[valid_mask])
        if v_min == v_max:
            continue
            
        step = (v_max - v_min) / 15
        for i in range(1, 15):
            threshold = v_min + i * step
            lt_mask = (vals < threshold) & valid_mask
            ge_mask = (~lt_mask) & valid_mask
            
            if np.sum(lt_mask) == 0 or np.sum(ge_mask) == 0:
                continue
                
            ig = parent_entropy - (
                (np.sum(lt_mask) / n_total) * get_entropy(labels[lt_mask]) +
                (np.sum(ge_mask) / n_total) * get_entropy(labels[ge_mask])
            )
            
            if ig > best_ig:
                best_ig, best_attr, best_threshold = ig, attr, threshold
                best_splits = (lt_mask, ge_mask)

    if best_attr is None or np.sum(best_splits[0]) <= min_leaf_count or np.sum(best_splits[1]) <= min_leaf_count:
        return LeafNode(labels)

    remaining_attrs = [a for a in attributes if a != best_attr]
    lt_data = data[best_splits[0]]
    ge_data = data[best_splits[1]]
    
    child_lt = build_tree(lt_data, target_col, remaining_attrs, min_leaf_count)
    child_ge = build_tree(ge_data, target_col, remaining_attrs, min_leaf_count)
    child_miss = child_lt if len(lt_data) >= len(ge_data) else child_ge

    return DecisionNode(best_attr, best_threshold, child_lt, child_ge, child_miss)

def main():
    np.random.seed(42)
    path = 'town_vax_data.csv'
    id_col = 'town'
    target_col = 'vax_level'
    min_leaf = 10

    df = load_data(path)
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    split_idx = int(len(df) * 0.75)
    train_df = df.iloc[:split_idx]
    test_df = df.iloc[split_idx:]

    features = [c for c in df.columns if c not in [id_col, target_col]]
    
    root = build_tree(train_df, target_col, features, min_leaf)

    correct = 0
    test_dicts = test_df.to_dict('records')
    for row in test_dicts:
        pred, _ = root.classify(row)
        if pred == row[target_col]:
            correct += 1

    accuracy = correct / len(test_df)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# 1. Replaced manual CSV parsing and list-of-dictionaries with Pandas and NumPy for significant memory and speed gains.
# 2. Vectorized entropy and Information Gain calculations using NumPy to eliminate nested Python loops.
# 3. Optimized the threshold search loop by pre-calculating masks and using vectorized boolean indexing.
# 4. Reduced redundant computation by calculating parent entropy once per node split attempt.
# 5. Minimized data movement by using Pandas indexing instead of deep-copying sub-lists.
# 6. Improved memory footprint by using local NumPy arrays for calculations while preserving the recursive tree structure.
# 7. Implemented robust CSV loading with automatic separator detection to ensure portability and stability.
# 8. Set fixed random seeds for reproducibility across different environments.
# 9. Simplified the data splitting process using efficient dataframe sampling.
# 10. Removed all visualization, logging, and overhead objects to minimize energy consumption during execution.
# 11. Preserved original heuristic (15-step linear threshold search) while making it computationally efficient.
# 12. Handled missing values (NaN) efficiently using NumPy masks instead of manual None checks in loops.