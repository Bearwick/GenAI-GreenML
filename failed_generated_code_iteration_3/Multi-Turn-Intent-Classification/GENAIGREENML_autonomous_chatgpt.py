# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import json
import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score
from sklearn.base import BaseEstimator, TransformerMixin


RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _read_csv_with_fallback(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(dfx):
        if dfx is None:
            return True
        if not isinstance(dfx, pd.DataFrame) or dfx.shape[0] == 0 or dfx.shape[1] == 0:
            return True
        # Heuristic: single wide column often indicates wrong separator
        if dfx.shape[1] == 1:
            col0 = str(dfx.columns[0])
            sample = ""
            try:
                sample = str(dfx.iloc[0, 0])
            except Exception:
                sample = ""
            if (";" in col0) or (";" in sample):
                return True
        return False

    if looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            pass

    if df is None:
        raise ValueError("Could not read CSV.")
    return df


def _load_dataset(path):
    ext = os.path.splitext(path)[1].lower()
    if ext in [".csv", ".tsv"]:
        df = _read_csv_with_fallback(path)
        df.columns = _normalize_columns(df.columns)
        df = _drop_unnamed(df)
        return df

    if ext in [".json", ".jsonl"]:
        # Try JSON lines first (memory-light); fallback to normal json.
        df = None
        try:
            df = pd.read_json(path, lines=True)
        except Exception:
            try:
                with open(path, "r", encoding="utf-8") as f:
                    obj = json.load(f)
                # Common shapes: list[dict], dict with key holding list[dict], dict of lists
                if isinstance(obj, list):
                    df = pd.DataFrame(obj)
                elif isinstance(obj, dict):
                    # Prefer first list-like value that can form rows
                    list_like_keys = [k for k, v in obj.items() if isinstance(v, list)]
                    if list_like_keys:
                        df = pd.DataFrame(obj[list_like_keys[0]])
                    else:
                        df = pd.DataFrame([obj])
                else:
                    df = pd.DataFrame([{"data": str(obj)}])
            except Exception:
                df = pd.DataFrame()
        if df is None:
            df = pd.DataFrame()
        if df.shape[1] == 0:
            # Last-resort: keep raw file as single text feature
            try:
                with open(path, "r", encoding="utf-8") as f:
                    raw = f.read()
                df = pd.DataFrame({"raw_text": [raw]})
            except Exception:
                df = pd.DataFrame({"raw_text": [""]})
        df.columns = _normalize_columns(df.columns)
        df = _drop_unnamed(df)
        return df

    # Fallback: try CSV then JSON
    try:
        df = _read_csv_with_fallback(path)
        df.columns = _normalize_columns(df.columns)
        df = _drop_unnamed(df)
        return df
    except Exception:
        df = pd.read_json(path)
        df.columns = _normalize_columns(df.columns)
        df = _drop_unnamed(df)
        return df


def _is_text_like_series(s: pd.Series) -> bool:
    if s.dtype == "object":
        # consider text if average length is moderately large or many unique strings
        sample = s.dropna().astype(str).head(200)
        if len(sample) == 0:
            return False
        avg_len = sample.map(len).mean()
        uniq_ratio = sample.nunique(dropna=True) / max(1, len(sample))
        return (avg_len >= 15) or (uniq_ratio > 0.5)
    return False


def _choose_target(df: pd.DataFrame):
    cols = list(df.columns)
    if len(cols) == 0:
        return None

    # Prefer explicit common target names
    preferred = ["target", "label", "intent", "class", "y"]
    lower_map = {c.lower(): c for c in cols}
    for p in preferred:
        if p in lower_map:
            return lower_map[p]

    # Prefer low-cardinality categorical/object column (likely label)
    best_col = None
    best_score = -1
    for c in cols:
        s = df[c]
        if s.isna().all():
            continue
        nun = s.nunique(dropna=True)
        if nun < 2:
            continue
        if s.dtype == "object" or str(s.dtype).startswith("string"):
            # Score: fewer unique classes preferred, but not too many
            score = 0
            if nun <= 50:
                score += 3
            if nun <= 20:
                score += 2
            if nun <= 10:
                score += 1
            # Penalize very long text columns as target
            avg_len = s.dropna().astype(str).head(200).map(len).mean() if len(s.dropna()) else 0
            if avg_len <= 50:
                score += 1
            if score > best_score:
                best_score = score
                best_col = c

    if best_col is not None:
        return best_col

    # Otherwise choose a non-constant numeric column
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() == 0:
            continue
        if s.nunique(dropna=True) >= 2:
            return c

    # Last resort: first column
    return cols[0]


class JsonListToText(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def _to_text(self, v):
        if v is None or (isinstance(v, float) and not np.isfinite(v)):
            return ""
        # If it's list/dict, serialize compactly; else string
        if isinstance(v, (list, dict)):
            try:
                return json.dumps(v, ensure_ascii=False, separators=(",", ":"))
            except Exception:
                return str(v)
        return str(v)

    def transform(self, X):
        # X may be a DataFrame/Series/ndarray
        if isinstance(X, pd.DataFrame):
            # If multiple columns passed, concatenate
            parts = []
            for c in X.columns:
                parts.append(X[c].apply(self._to_text))
            if len(parts) == 0:
                return np.array([""], dtype=object)
            s = parts[0]
            for p in parts[1:]:
                s = s.astype(str) + " " + p.astype(str)
            return s.values.astype(object)
        if isinstance(X, pd.Series):
            return X.apply(self._to_text).values.astype(object)
        X = np.asarray(X)
        if X.ndim == 2 and X.shape[1] >= 1:
            # concatenate columns
            out = []
            for row in X:
                out.append(" ".join(self._to_text(v) for v in row))
            return np.array(out, dtype=object)
        if X.ndim == 1:
            return np.array([self._to_text(v) for v in X], dtype=object)
        return np.array([""] * len(X), dtype=object)


def _build_pipeline(df, target_col):
    feature_cols = [c for c in df.columns if c != target_col]

    # Identify column types robustly
    text_cols = []
    cat_cols = []
    num_cols = []

    for c in feature_cols:
        s = df[c]
        # Coerce numerics only for detection (do not overwrite df)
        s_num = pd.to_numeric(s, errors="coerce")
        numeric_ratio = float(s_num.notna().mean()) if len(s_num) else 0.0

        if numeric_ratio >= 0.9:
            num_cols.append(c)
        else:
            if _is_text_like_series(s):
                text_cols.append(c)
            else:
                cat_cols.append(c)

    transformers = []

    if len(text_cols) > 0:
        text_pipe = Pipeline(steps=[
            ("to_text", JsonListToText()),
            ("tfidf", TfidfVectorizer(
                lowercase=True,
                max_features=20000,
                ngram_range=(1, 2),
                min_df=2
            )),
        ])
        transformers.append(("text", text_pipe, text_cols))

    if len(cat_cols) > 0:
        cat_pipe = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ])
        transformers.append(("cat", cat_pipe, cat_cols))

    if len(num_cols) > 0:
        num_pipe = Pipeline(steps=[
            ("to_num", FunctionTransformerSafeToNumeric()),
            ("imputer", SimpleImputer(strategy="median")),
        ])
        transformers.append(("num", num_pipe, num_cols))

    if len(transformers) == 0:
        # If no usable features, create a constant feature through a text pipeline on target-less raw serialization
        fallback_col = df.columns[0]
        text_pipe = Pipeline(steps=[
            ("to_text", JsonListToText()),
            ("tfidf", TfidfVectorizer(lowercase=True, max_features=5000)),
        ])
        transformers.append(("text", text_pipe, [fallback_col]))

    pre = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    return pre


class FunctionTransformerSafeToNumeric(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        if isinstance(X, pd.DataFrame):
            Xc = X.copy()
            for c in Xc.columns:
                Xc[c] = pd.to_numeric(Xc[c], errors="coerce")
            Xc = Xc.replace([np.inf, -np.inf], np.nan)
            return Xc.values
        Xn = np.asarray(X)
        # convert each column
        if Xn.ndim == 1:
            s = pd.to_numeric(pd.Series(Xn), errors="coerce").replace([np.inf, -np.inf], np.nan)
            return s.values.reshape(-1, 1)
        out = np.empty_like(Xn, dtype=float)
        for j in range(Xn.shape[1]):
            s = pd.to_numeric(pd.Series(Xn[:, j]), errors="coerce").replace([np.inf, -np.inf], np.nan)
            out[:, j] = s.values
        return out


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    y_true = np.where(np.isfinite(y_true), y_true, np.nan)
    y_pred = np.where(np.isfinite(y_pred), y_pred, np.nan)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    denom = np.sum((y_true - np.mean(y_true)) ** 2)
    if denom <= 1e-12:
        return 0.0
    r2 = 1.0 - (np.sum((y_true - y_pred) ** 2) / denom)
    # bound to [0,1] for a stable "accuracy" proxy
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    path = "data/input.json"
    df = _load_dataset(path)

    assert isinstance(df, pd.DataFrame)
    df = df.copy()
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Drop completely empty columns
    df = df.dropna(axis=1, how="all")
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _choose_target(df)
    if target_col is None or target_col not in df.columns:
        target_col = df.columns[-1]

    y_raw = df[target_col]

    # Determine task: classification if low-cardinality non-numeric or integer-like with few classes
    y_num = pd.to_numeric(y_raw, errors="coerce")
    y_num_ratio = float(y_num.notna().mean()) if len(y_num) else 0.0

    is_classification = False
    if y_raw.dtype == "object" or str(y_raw.dtype).startswith("string"):
        nun = y_raw.nunique(dropna=True)
        if nun >= 2 and nun <= 50:
            is_classification = True
    else:
        # if mostly numeric but few unique values, treat as classification
        nun = y_num.nunique(dropna=True)
        if y_num_ratio >= 0.9 and nun >= 2 and nun <= 20:
            is_classification = True

    X = df.drop(columns=[target_col], errors="ignore")
    assert X.shape[0] > 0

    # Ensure we have at least one feature column; otherwise keep a dummy feature
    if X.shape[1] == 0:
        X = pd.DataFrame({"__dummy__": [""] * len(df)})

    pre = _build_pipeline(pd.concat([X, y_raw], axis=1), target_col=target_col)

    if is_classification:
        y = y_raw.astype(str).fillna("NA")
        if y.nunique(dropna=True) < 2:
            # fallback to regression-like trivial score
            is_classification = False
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y if y.nunique() > 1 else None
            )
            assert len(X_train) > 0 and len(X_test) > 0

            clf = LogisticRegression(
                max_iter=200,
                solver="liblinear",
                n_jobs=1
            )

            pipe = Pipeline(steps=[("pre", pre), ("model", clf)])
            pipe.fit(X_train, y_train)
            y_pred = pipe.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

    # Regression path (also used as safe fallback)
    y = pd.to_numeric(y_raw, errors="coerce").replace([np.inf, -np.inf], np.nan)
    # If too many NaNs in target, use ordinal encoding of string target as numeric fallback
    if y.notna().sum() < max(2, int(0.2 * len(y))):
        y = y_raw.astype(str).fillna("NA")
        # map to stable integers
        codes, _ = pd.factorize(y, sort=True)
        y = pd.Series(codes, index=df.index).astype(float)

    # Drop rows with missing target
    keep = y.notna()
    X2 = X.loc[keep].copy()
    y2 = y.loc[keep].astype(float).copy()
    assert len(X2) > 1

    X_train, X_test, y_train, y_test = train_test_split(
        X2, y2, test_size=0.2, random_state=RANDOM_STATE
    )
    assert len(X_train) > 0 and len(X_test) > 0

    reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
    pipe = Pipeline(steps=[("pre", pre), ("model", reg)])
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    accuracy = _bounded_regression_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) for strong CPU baselines; avoids deep learning and large ensembles.
# - ColumnTransformer+Pipeline ensures single-pass, reproducible preprocessing without redundant work.
# - TF-IDF with capped max_features and min_df reduces memory/CPU while handling text robustly.
# - Robust schema handling: infers target/features from available columns; normalizes headers; drops Unnamed columns; safe numeric coercion.
# - JSON-safe text conversion (lists/dicts -> compact JSON string) avoids failures on nested conversation structures.
# - Regression fallback uses bounded (R2+1)/2 score in [0,1] as an "accuracy" proxy to keep evaluation stable and comparable.