# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import json
import csv
import random
from transformers import pipeline

SEED = 42
random.seed(SEED)
try:
    import numpy as np
    np.random.seed(SEED)
except Exception:
    np = None
try:
    import torch
    torch.manual_seed(SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except Exception:
    torch = None


class IntentDetector:
    def __init__(self):
        self.intent_options = [
            "Book Appointment",
            "Product Inquiry",
            "Pricing Negotiation",
            "Support Request",
            "Follow-Up"
        ]
        self.intent_pipeline = pipeline(
            task="zero-shot-classification",
            model="cross-encoder/nli-distilroberta-base"
        )

    def classify_intent(self, dialogue):
        classification = self.intent_pipeline(dialogue, self.intent_options)
        top_intent = classification["labels"][0]
        explanation = f"Based on the conversation, the customer is likely interested in '{top_intent.lower()}'."
        return {"predicted_intent": top_intent, "rationale": explanation}

    def classify_batch(self, dialogues, batch_size=8):
        return self.intent_pipeline(dialogues, self.intent_options, batch_size=batch_size)


intent_model = IntentDetector()
RATIONALE_TEMPLATE = "Based on the conversation, the customer is likely interested in '{}'."


def create_conversation(messages, max_messages=None):
    if not isinstance(messages, list):
        return ""
    if max_messages is not None:
        messages = messages[-max_messages:]
    return "\n".join(
        f"{m.get('sender', '').capitalize()}: {m.get('text', '')}" for m in messages
    )


def load_conversations(path):
    with open(path, "r", encoding="utf-8") as infile:
        data = json.load(infile)
    if isinstance(data, list):
        return data
    if isinstance(data, dict):
        for key in ("conversations", "data", "items"):
            value = data.get(key)
            if isinstance(value, list):
                return value
    return []


def detect_label_key(conversations, lower_intents):
    candidate_keys = ("intent", "label", "true_intent", "ground_truth", "expected_intent", "category")
    for key in candidate_keys:
        for entry in conversations:
            value = entry.get(key)
            if isinstance(value, str) and value.strip().casefold() in lower_intents:
                return key
    for entry in conversations:
        for key, value in entry.items():
            if key in ("conversation_id", "messages"):
                continue
            if isinstance(value, str) and value.strip().casefold() in lower_intents:
                return key
    return None


def predict_intents(input_file, json_output, csv_output, batch_size=8):
    conversations = load_conversations(input_file)
    lower_intents = {intent.casefold() for intent in intent_model.intent_options}
    label_key = detect_label_key(conversations, lower_intents) if conversations else None
    output_data = []
    correct = 0
    total = 0
    bs = max(1, int(batch_size))

    batch_entries = []
    batch_texts = []

    for entry in conversations:
        batch_entries.append(entry)
        batch_texts.append(create_conversation(entry.get("messages", [])))
        if len(batch_texts) >= bs:
            results = intent_model.classify_batch(batch_texts, batch_size=bs)
            for ent, res in zip(batch_entries, results):
                top_intent = res["labels"][0]
                top_intent_lower = top_intent.lower()
                output_data.append({
                    "conversation_id": ent.get("conversation_id"),
                    "predicted_intent": top_intent,
                    "rationale": RATIONALE_TEMPLATE.format(top_intent_lower)
                })
                if label_key:
                    label = ent.get(label_key)
                    if isinstance(label, str):
                        total += 1
                        if top_intent_lower == label.strip().lower():
                            correct += 1
            batch_entries.clear()
            batch_texts.clear()

    if batch_texts:
        results = intent_model.classify_batch(batch_texts, batch_size=bs)
        for ent, res in zip(batch_entries, results):
            top_intent = res["labels"][0]
            top_intent_lower = top_intent.lower()
            output_data.append({
                "conversation_id": ent.get("conversation_id"),
                "predicted_intent": top_intent,
                "rationale": RATIONALE_TEMPLATE.format(top_intent_lower)
            })
            if label_key:
                label = ent.get(label_key)
                if isinstance(label, str):
                    total += 1
                    if top_intent_lower == label.strip().lower():
                        correct += 1

    with open(json_output, "w", encoding="utf-8") as json_file:
        json.dump(output_data, json_file, indent=2)

    with open(csv_output, "w", newline="", encoding="utf-8") as csv_file:
        fieldnames = ["conversation_id", "predicted_intent", "rationale"]
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(output_data)

    accuracy = correct / total if total else 0.0
    print(f"ACCURACY={accuracy:.6f}")


os.makedirs("data/output", exist_ok=True)
if __name__ == "__main__":
    predict_intents(
        input_file="data/input.json",
        json_output="data/output/predictions.json",
        csv_output="data/output/predictions.csv"
    )

# Optimization Summary
# Removed unused preprocessing and imports to reduce overhead.
# Added deterministic seeding for reproducible inference.
# Batched inference to minimize repeated pipeline invocations.
# Reused string templates and lowered intents to reduce redundant computation.
# Streamlined data handling to avoid unnecessary intermediate structures.