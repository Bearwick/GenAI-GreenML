# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import os
import json
import csv
import torch
from transformers import pipeline

def run_pipeline():
    input_path = "data/input.json"
    json_output = "data/output/predictions.json"
    csv_output = "data/output/predictions.csv"
    os.makedirs("data/output", exist_ok=True)

    intent_options = [
        "Book Appointment",
        "Product Inquiry",
        "Pricing Negotiation",
        "Support Request",
        "Follow-Up"
    ]

    device = 0 if torch.cuda.is_available() else (-1 if not torch.backends.mps.is_available() else "mps")
    
    classifier = pipeline(
        task="zero-shot-classification",
        model="cross-encoder/nli-distilroberta-base",
        device=device
    )

    try:
        with open(input_path, 'r') as f:
            conversations = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return

    dialogues = []
    conv_ids = []
    for entry in conversations:
        conv_ids.append(entry.get('conversation_id'))
        messages = entry.get('messages', [])
        dialogues.append("\n".join(f"{m.get('sender', '').capitalize()}: {m.get('text', '')}" for m in messages))

    if not dialogues:
        print(f"ACCURACY={0.0:.6f}")
        return

    results = classifier(dialogues, intent_options, batch_size=8)
    
    output_data = []
    correct_predictions = 0
    total_with_labels = 0

    for i, res in enumerate(results):
        top_intent = res["labels"][0]
        record = {
            "conversation_id": conv_ids[i],
            "predicted_intent": top_intent,
            "rationale": f"Based on the conversation, the customer is likely interested in '{top_intent.lower()}'."
        }
        output_data.append(record)
        
        actual_intent = conversations[i].get("intent") or conversations[i].get("label")
        if actual_intent:
            total_with_labels += 1
            if top_intent.lower() == actual_intent.lower():
                correct_predictions += 1

    with open(json_output, 'w') as f:
        json.dump(output_data, f, indent=2)

    with open(csv_output, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=["conversation_id", "predicted_intent", "rationale"])
        writer.writeheader()
        writer.writerows(output_data)

    accuracy = correct_predictions / total_with_labels if total_with_labels > 0 else 0.0
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary
# 1. Implemented batch processing in the zero-shot pipeline (batch_size=8) to improve throughput and reduce energy per sample.
# 2. Added automatic hardware acceleration detection (CUDA/MPS) to leverage efficient compute units.
# 3. Removed unused 'emoji' and 're' library dependencies and associated unused preprocessing functions to reduce memory footprint.
# 4. Refactored conversation formatting to use generator expressions, minimizing intermediate list allocations.
# 5. Optimized data flow by separating conversation ID extraction from text formatting for more efficient batching.
# 6. Reduced redundant dictionary lookups and string operations within the main loop.
# 7. Consolidated file writing operations to minimize I/O overhead.
# 8. Optimized the pipeline initialization by moving it outside the loop structure.