# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import math
import pickle
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


RANDOM_STATE = 42


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        normed.append(c2)
    return normed


def _drop_unnamed_columns(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c).strip())]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _try_read_csv(path):
    # Robust CSV fallback: default, then sep=';' with decimal=','
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1 and df.shape[0] > 0:
            raise ValueError("CSV likely parsed with wrong delimiter")
        return df
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
        return df


def _load_dataset(path):
    ext = os.path.splitext(path)[1].lower()
    if ext in [".csv", ".txt"]:
        df = _try_read_csv(path)
        return df
    if ext in [".pkl", ".pickle"]:
        with open(path, "rb") as f:
            obj = pickle.load(f)
        if isinstance(obj, pd.DataFrame):
            return obj
        if isinstance(obj, dict):
            # Prefer a DataFrame-like key; else build DataFrame directly.
            for k in ["df", "data", "dataset", "train", "X", "frame"]:
                if k in obj and isinstance(obj[k], pd.DataFrame):
                    return obj[k]
            # If dict of arrays/lists -> DataFrame
            try:
                return pd.DataFrame(obj)
            except Exception:
                pass
        if isinstance(obj, (list, tuple)):
            try:
                return pd.DataFrame(obj)
            except Exception:
                pass
        raise ValueError("Unsupported pickle content for dataset")
    if ext in [".parquet"]:
        return pd.read_parquet(path)
    if ext in [".xlsx", ".xls"]:
        return pd.read_excel(path)
    # Last-resort attempt via pandas
    try:
        return pd.read_csv(path)
    except Exception:
        with open(path, "rb") as f:
            obj = pickle.load(f)
        if isinstance(obj, pd.DataFrame):
            return obj
        return pd.DataFrame(obj)


def _coerce_numeric_columns(df, numeric_cols):
    df2 = df.copy()
    for c in numeric_cols:
        df2[c] = pd.to_numeric(df2[c], errors="coerce")
    return df2


def _select_target_and_features(df):
    # Normalize columns and drop obvious junk columns
    df = df.copy()
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed_columns(df)

    # Remove completely empty columns
    non_all_nan = [c for c in df.columns if not df[c].isna().all()]
    df = df[non_all_nan] if non_all_nan else df

    # Identify candidate numeric columns (after coercion attempt)
    numeric_candidates = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() > 0:
            numeric_candidates.append(c)

    df_num = _coerce_numeric_columns(df, numeric_candidates)

    # Pick a reasonable target: prefer a numeric column with low unique count (>1), else any non-constant numeric
    target = None
    if numeric_candidates:
        best = []
        for c in numeric_candidates:
            s = df_num[c]
            s_non = s.dropna()
            if len(s_non) == 0:
                continue
            nun = int(s_non.nunique(dropna=True))
            if nun <= 1:
                continue
            # Heuristic: if looks like classification labels: small cardinality
            best.append((nun, -float(s_non.notna().mean()), c))
        # Prefer smallest nunique > 1 (classification-like), tie-breaker by fewer missing values
        if best:
            best.sort(key=lambda x: (x[0], x[1]))
            target = best[0][2]

    # If no numeric target found, try an object column with limited classes
    if target is None:
        obj_cols = [c for c in df.columns if c not in numeric_candidates]
        best_obj = []
        for c in obj_cols:
            s = df[c]
            s_non = s.dropna()
            if len(s_non) == 0:
                continue
            nun = int(s_non.nunique(dropna=True))
            if nun <= 1:
                continue
            best_obj.append((nun, -float(s_non.notna().mean()), c))
        if best_obj:
            best_obj.sort(key=lambda x: (x[0], x[1]))
            target = best_obj[0][2]

    # If still None, fallback to first column
    if target is None and len(df.columns) > 0:
        target = df.columns[-1]

    # Features: all columns except target
    feature_cols = [c for c in df.columns if c != target]

    # If no features, create a constant feature
    if not feature_cols:
        df = df.copy()
        df["__constant__"] = 1.0
        feature_cols = ["__constant__"]

    return df, target, feature_cols, numeric_candidates


def _is_classification_target(y):
    # Determine if classification is appropriate: low cardinality and not too many unique values
    y_series = pd.Series(y)
    y_non = y_series.dropna()
    if len(y_non) == 0:
        return False
    nun = int(y_non.nunique(dropna=True))
    # If y is numeric but with few unique values, it's classification-like
    if nun < 2:
        return False
    if nun <= max(20, int(0.05 * len(y_non))):
        return True
    # If y is object/categorical, likely classification
    if y_non.dtype == object or str(y_non.dtype).startswith("category"):
        return True
    return False


def _bounded_regression_score(y_true, y_pred):
    # Stable accuracy proxy in [0,1]: 1/(1+RMSE/scale), scale from IQR or std, fallback 1
    yt = np.asarray(y_true, dtype=float)
    yp = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(yt) & np.isfinite(yp)
    yt = yt[mask]
    yp = yp[mask]
    if yt.size == 0:
        return 0.0
    rmse = float(np.sqrt(np.mean((yt - yp) ** 2)))
    q75, q25 = np.nanpercentile(yt, [75, 25])
    scale = float(q75 - q25)
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = float(np.nanstd(yt))
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = 1.0
    score = 1.0 / (1.0 + (rmse / scale))
    return float(np.clip(score, 0.0, 1.0))


def main():
    dataset_path = "dict.pickle"
    df = _load_dataset(dataset_path)

    # Ensure DataFrame
    if not isinstance(df, pd.DataFrame):
        df = pd.DataFrame(df)

    # Basic cleanup
    df = df.copy()
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed_columns(df)

    # Drop fully empty rows
    df = df.dropna(how="all")
    assert df.shape[0] > 0 and df.shape[1] > 0

    df, target_col, feature_cols, numeric_candidates = _select_target_and_features(df)

    # Split X/y
    y = df[target_col]
    X = df[feature_cols]

    # Identify column types from X (coerce numeric where possible)
    numeric_features = []
    categorical_features = []
    X_work = X.copy()

    # Determine numeric by coercion success rate
    for c in X_work.columns:
        s_num = pd.to_numeric(X_work[c], errors="coerce")
        # Numeric if at least some values convert and original isn't obviously high-cardinality text-only
        if s_num.notna().sum() > 0 and (s_num.notna().sum() >= max(3, int(0.2 * len(s_num)))):
            numeric_features.append(c)
            X_work[c] = s_num
        else:
            categorical_features.append(c)

    # Replace inf with nan for numeric columns
    for c in numeric_features:
        col = X_work[c].to_numpy(dtype=float, copy=False)
        if np.isinf(col).any():
            X_work[c] = np.where(np.isfinite(col), col, np.nan)

    # Classification vs regression decision
    classification = _is_classification_target(y)

    # Prepare y for modeling
    if classification:
        y_model = y.astype("object")
    else:
        y_model = pd.to_numeric(y, errors="coerce")

    # Filter rows with missing y for train/test
    valid_y_mask = y_model.notna()
    X_work = X_work.loc[valid_y_mask].reset_index(drop=True)
    y_model = y_model.loc[valid_y_mask].reset_index(drop=True)

    assert len(y_model) > 1 and X_work.shape[0] == len(y_model)

    # Train/test split
    stratify = y_model if classification and y_model.nunique(dropna=True) >= 2 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X_work, y_model, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    # Preprocess pipeline
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Model choice: lightweight linear models
    if classification and y_train.nunique(dropna=True) >= 2:
        clf = LogisticRegression(
            max_iter=200,
            solver="lbfgs",
            n_jobs=1,
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_score(y_test.to_numpy(dtype=float), np.asarray(y_pred, dtype=float))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses CPU-friendly, energy-efficient linear models (LogisticRegression / Ridge) instead of heavy ensembles or deep nets.
# - Employs a single sklearn Pipeline + ColumnTransformer to avoid repeated preprocessing and ensure reproducibility.
# - Robust schema handling: normalizes headers, drops 'Unnamed' columns, auto-selects target/features when schema is unknown.
# - Defensive numeric coercion with errors='coerce' and inf->nan handling prevents costly failures on messy input.
# - Minimal feature engineering: median imputation + standard scaling for numeric; most_frequent + one-hot for categoricals.
# - Falls back to regression when classification is ill-posed (<2 classes); reports a bounded [0,1] accuracy proxy: 1/(1+RMSE/scale).