# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

DATASET_PATH = "model.pkl"
RANDOM_STATE = 42
DATASET_HEADERS = None

def _read_csv_with_fallback(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is None or df.shape[1] < 2:
        try:
            df_alt = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df_alt = None
        if df_alt is not None and df_alt.shape[1] >= 2:
            df = df_alt
    return df

def load_data(path):
    df = _read_csv_with_fallback(path)
    if df is None or df.shape[1] < 2:
        try:
            df = pd.read_pickle(path)
        except Exception as exc:
            raise ValueError("Unable to load dataset") from exc
        if not isinstance(df, pd.DataFrame) or df.shape[1] < 2:
            raise ValueError("Dataset must contain at least two columns")
    headers = globals().get("DATASET_HEADERS")
    if headers and len(headers) == len(df.columns):
        df.columns = headers
    X = df.iloc[:, :-1].to_numpy(copy=False)
    y = df.iloc[:, -1].to_numpy(copy=False)
    return X, y

def preprocess_data(X):
    scaler = StandardScaler(copy=False)
    return scaler.fit_transform(X)

def train_model(X_train, y_train):
    model = RandomForestClassifier(
        n_estimators=100,
        class_weight="balanced",
        random_state=RANDOM_STATE,
        n_jobs=1
    )
    model.fit(X_train, y_train)
    return model

def main():
    np.random.seed(RANDOM_STATE)
    X, y = load_data(DATASET_PATH)
    X = preprocess_data(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE
    )
    model = train_model(X_train, y_train)
    accuracy = model.score(X_test, y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# Removed plotting, model persistence, cross-validation, and verbose reporting to eliminate unnecessary computation and I/O.
# Reused arrays and enabled in-place scaling to reduce memory allocations during preprocessing.
# Used model.score to compute accuracy directly, avoiding storage of full prediction arrays.