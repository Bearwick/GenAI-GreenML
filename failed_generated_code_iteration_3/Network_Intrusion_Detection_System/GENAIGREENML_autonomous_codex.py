# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import re
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")

data_path = "data/raw/Train.txt"
dataset_headers_str = "0,tcp,ftp_data,SF,491,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0.00,0.00,0.00,0.00,1.00,0.00,0.00,150,25,0.17,0.03,0.17,0.00,0.00,0.00,0.05,0.00,normal,20"

def read_csv_fallback(path, header="infer"):
    try:
        df = pd.read_csv(path, header=header)
    except Exception:
        df = pd.read_csv(path, header=header, sep=";", decimal=",")
    if df.shape[1] <= 1:
        try:
            df = pd.read_csv(path, header=header, sep=";", decimal=",")
        except Exception:
            pass
    return df

def looks_like_data_header(colnames):
    if colnames is None or len(colnames) == 0:
        return False
    col_str = [str(c) for c in colnames]
    numeric_like = pd.to_numeric(pd.Series(col_str), errors="coerce")
    numeric_ratio = numeric_like.notna().mean()
    digit_ratio = np.mean([bool(re.search(r"\d", s)) for s in col_str])
    token_set = {"tcp", "udp", "icmp", "sf", "rej", "s0", "normal", "attack", "ftp_data", "http", "other"}
    token_ratio = np.mean([s.strip().lower() in token_set for s in col_str])
    return (numeric_ratio > 0.4 and len(col_str) > 5) or token_ratio > 0.2 or digit_ratio > 0.6

df = read_csv_fallback(data_path)
if looks_like_data_header(df.columns):
    df = read_csv_fallback(data_path, header=None)

cols = [str(c) for c in df.columns]
cols = [re.sub(r"\s+", " ", c).strip() for c in cols]
keep_idx = [i for i, c in enumerate(cols) if not re.match(r"^Unnamed:?\s*\d*$", c)]
df = df.iloc[:, keep_idx]
cols = [cols[i] for i in keep_idx]
new_cols = []
counts = {}
for i, c in enumerate(cols):
    if c == "" or c is None:
        c = f"col_{i}"
    if c in counts:
        counts[c] += 1
        new_cols.append(f"{c}_{counts[c]}")
    else:
        counts[c] = 0
        new_cols.append(c)
df.columns = new_cols

headers_list = [h.strip() for h in dataset_headers_str.split(",")] if dataset_headers_str else None
def headers_look_like_names(headers):
    if not headers:
        return False
    alpha_ratio = np.mean([bool(re.search(r"[A-Za-z]", h)) for h in headers])
    numeric_ratio = pd.to_numeric(pd.Series(headers), errors="coerce").notna().mean()
    return alpha_ratio > 0.5 and numeric_ratio < 0.5

if headers_list and headers_look_like_names(headers_list) and len(headers_list) == df.shape[1]:
    if all(re.fullmatch(r"\d+", c) or c.startswith("col_") for c in df.columns):
        df.columns = [re.sub(r"\s+", " ", h).strip() for h in headers_list]

def select_target(df):
    patterns = ["label", "attack", "target", "class", "outcome", "y"]
    for pat in patterns:
        for col in df.columns:
            if pat in col.lower():
                return col
    obj_cols = [col for col in df.columns if df[col].dtype == object]
    if obj_cols:
        idx_map = {col: i for i, col in enumerate(df.columns)}
        last_idx = len(df.columns) - 1
        near_end = [col for col in obj_cols if idx_map[col] >= max(0, last_idx - 2)]
        if near_end:
            for col in reversed(near_end):
                if df[col].nunique(dropna=True) > 1:
                    return col
            return near_end[-1]
    numeric_candidates = []
    for col in df.columns:
        ser = pd.to_numeric(df[col], errors="coerce")
        if ser.notna().sum() > 0:
            numeric_candidates.append((col, ser.nunique(dropna=True)))
    if numeric_candidates:
        numeric_candidates.sort(key=lambda x: x[1])
        return numeric_candidates[-1][0]
    if obj_cols:
        for col in reversed(obj_cols):
            if df[col].nunique(dropna=True) > 1:
                return col
        return obj_cols[-1]
    return df.columns[-1]

target_col = select_target(df)

if target_col in df.columns:
    y_raw = df[target_col]
    X = df.drop(columns=[target_col]).copy()
else:
    y_raw = pd.Series(np.zeros(len(df)))
    X = df.copy()

if X.shape[1] == 0:
    X = pd.DataFrame({"const": np.ones(len(df))})

task = "regression"
if y_raw.dtype == object:
    task = "classification"
else:
    y_num = pd.to_numeric(y_raw, errors="coerce")
    unique_vals = y_num.nunique(dropna=True)
    threshold = min(20, max(2, int(0.05 * len(y_num)))) if len(y_num) > 0 else 0
    if unique_vals <= threshold:
        task = "classification"
    else:
        task = "regression"

if task == "classification":
    y = y_raw.replace([np.inf, -np.inf], np.nan)
    if y.dtype != object:
        y = pd.to_numeric(y, errors="coerce")
    mask = ~y.isna()
    X = X.loc[mask]
    y = y.loc[mask]
    y = y.astype(str)
else:
    y = pd.to_numeric(y_raw, errors="coerce")
    y = y.replace([np.inf, -np.inf], np.nan)
    mask = ~y.isna()
    X = X.loc[mask]
    y = y.loc[mask]

assert len(X) > 0

numeric_cols = []
categorical_cols = []
for col in X.columns:
    ser = pd.to_numeric(X[col], errors="coerce")
    non_na_ratio = ser.notna().mean()
    if non_na_ratio >= 0.8:
        numeric_cols.append(col)
        X[col] = ser
    else:
        categorical_cols.append(col)
        X[col] = X[col].astype(str)
        X[col] = X[col].replace({"nan": np.nan, "None": np.nan, "NaN": np.nan})

if numeric_cols:
    X[numeric_cols] = X[numeric_cols].replace([np.inf, -np.inf], np.nan)

for col in numeric_cols.copy():
    if X[col].isna().all():
        numeric_cols.remove(col)
        X = X.drop(columns=[col])
for col in categorical_cols.copy():
    if X[col].isna().all():
        categorical_cols.remove(col)
        X = X.drop(columns=[col])

if X.shape[1] == 0:
    X = pd.DataFrame({"const": np.ones(len(y))})
    numeric_cols = ["const"]
    categorical_cols = []

transformers = []
if numeric_cols:
    num_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler(with_mean=False))])
    transformers.append(("num", num_transformer, numeric_cols))
if categorical_cols:
    cat_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="most_frequent")), ("onehot", OneHotEncoder(handle_unknown="ignore"))])
    transformers.append(("cat", cat_transformer, categorical_cols))
preprocess = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3) if transformers else "passthrough"

if task == "classification":
    n_classes = y.nunique()
    if n_classes < 2:
        model = DummyClassifier(strategy="most_frequent")
    else:
        model = LogisticRegression(max_iter=200, solver="liblinear", multi_class="ovr")
else:
    model = Ridge(alpha=1.0, random_state=42)

clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])

if len(X) < 2:
    X_train, X_test, y_train, y_test = X, X, y, y
else:
    test_size = 0.2 if len(X) >= 5 else 0.5
    stratify = None
    if task == "classification" and y.nunique() >= 2:
        vc = y.value_counts()
        if vc.min() >= 2 and int(len(y) * test_size) >= y.nunique():
            stratify = y
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=stratify)

assert len(X_train) > 0 and len(X_test) > 0

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if task == "classification":
    accuracy = accuracy_score(y_test, y_pred)
else:
    if len(y_test) < 2:
        r2 = 0.0
    else:
        try:
            r2 = r2_score(y_test, y_pred)
        except Exception:
            r2 = 0.0
    if np.isnan(r2):
        r2 = 0.0
    accuracy = float(np.clip(r2, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) with Dummy fallback to minimize CPU and energy usage.
# - Applied ColumnTransformer with simple imputation and sparse OneHotEncoder for efficient mixed-type preprocessing.
# - StandardScaler(with_mean=False) preserves sparsity to reduce memory and computation.
# - Robust schema inference and a clipped R2 proxy for regression ensure stable end-to-end execution.