# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.exceptions import ConvergenceWarning
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if isinstance(c, str) and c.strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _robust_read_csv(path):
    df = None
    # Attempt 1: default parsing
    try:
        df = pd.read_csv(path, low_memory=False)
    except Exception:
        df = None

    def _looks_wrong(_df):
        if _df is None:
            return True
        if _df.shape[0] == 0 or _df.shape[1] <= 1:
            return True
        # If there is a single column that contains many separators, parsing likely failed
        if _df.shape[1] == 1:
            first_col = _df.columns[0]
            sample = _df[first_col].astype(str).head(10).tolist()
            if any((";" in s) or ("," in s) for s in sample):
                return True
        return False

    if _looks_wrong(df):
        # Attempt 2: semicolon separator and comma decimal
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",", low_memory=False)
            if not _looks_wrong(df2):
                df = df2
        except Exception:
            pass

    if df is None:
        raise FileNotFoundError(f"Could not read dataset at: {path}")

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)
    return df


def _select_target(df, preferred="label"):
    cols_lower = {c.lower(): c for c in df.columns if isinstance(c, str)}
    if preferred.lower() in cols_lower:
        return cols_lower[preferred.lower()]

    # fallback: pick a non-constant column with low-ish unique count (classification-friendly)
    best = None
    best_score = -1
    for c in df.columns:
        if c is None:
            continue
        if df[c].nunique(dropna=True) <= 1:
            continue
        # prefer non-feature identifiers: avoid "id" if possible
        name = str(c).lower()
        if name == "id":
            continue
        # heuristic: fewer unique values suggests label-like
        nun = df[c].nunique(dropna=True)
        score = 0
        if nun <= 50:
            score += 3
        if df[c].dtype == "O":
            score += 2
        else:
            score += 1
        # penalize very high cardinality
        if nun > max(100, int(0.5 * len(df))):
            score -= 3
        if score > best_score:
            best_score = score
            best = c

    if best is not None:
        return best

    # fallback: last column
    return df.columns[-1]


def _safe_to_numeric(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _accuracy_proxy_regression(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    var = np.var(y_true)
    mse = np.mean((y_true - y_pred) ** 2)
    if not np.isfinite(var) or var <= 1e-12:
        return 0.0
    r2 = 1.0 - (mse / var)
    # Map to [0,1] for stable "accuracy" proxy
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    warnings.filterwarnings("ignore", category=ConvergenceWarning)

    dataset_path = os.path.join("datasets", "train.csv")
    df = _robust_read_csv(dataset_path)

    # Normalize again defensively (in case upstream operations changed)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Ensure non-empty
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _select_target(df, preferred="label")

    # Define feature columns robustly
    feature_cols = [c for c in df.columns if c != target_col]
    # Avoid using an explicit ID column if present; keep it only if no other features exist
    feature_cols_wo_id = [c for c in feature_cols if str(c).lower() != "id"]
    if len(feature_cols_wo_id) > 0:
        feature_cols = feature_cols_wo_id

    # If still empty, fall back to using all except target (including id)
    if len(feature_cols) == 0:
        feature_cols = [c for c in df.columns if c != target_col]

    X = df[feature_cols].copy()
    y = df[target_col].copy()

    # Identify column types
    numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_cols = [c for c in X.columns if c not in numeric_cols]

    # Coerce numerics safely (including numeric-looking objects if any)
    # Add any object columns with high numeric convertibility to numeric set
    obj_cols = [c for c in categorical_cols if pd.api.types.is_object_dtype(X[c])]
    numeric_like = []
    for c in obj_cols:
        s = pd.to_numeric(X[c], errors="coerce")
        # if a decent fraction converts, treat as numeric
        if np.isfinite(s).mean() >= 0.8:
            X[c] = s
            numeric_like.append(c)
    if numeric_like:
        numeric_cols = list(dict.fromkeys(numeric_cols + numeric_like))
        categorical_cols = [c for c in X.columns if c not in numeric_cols]

    # Replace inf with nan for safety
    for c in numeric_cols:
        X[c] = pd.to_numeric(X[c], errors="coerce")
        X[c] = X[c].replace([np.inf, -np.inf], np.nan)

    # Decide task: classification if y has >=2 classes and not too many unique relative to n
    y_is_numeric = pd.api.types.is_numeric_dtype(y)
    y_for_check = y.copy()
    if y_is_numeric:
        y_for_check = pd.to_numeric(y_for_check, errors="coerce").replace([np.inf, -np.inf], np.nan)
    nunique_y = y_for_check.nunique(dropna=True)

    is_classification = False
    if nunique_y >= 2:
        if not y_is_numeric:
            is_classification = True
        else:
            # numeric label can still be classification if integer-like and low cardinality
            y_num = pd.to_numeric(y_for_check, errors="coerce")
            frac_intlike = np.isfinite(y_num).mean()
            if frac_intlike > 0:
                intlike = np.isfinite(y_num) & (np.abs(y_num - np.round(y_num)) < 1e-9)
                if intlike.mean() >= 0.95 and nunique_y <= 50:
                    is_classification = True

    # Preprocess y
    if is_classification:
        # Keep y as-is; drop rows with missing y
        valid = y.notna()
        X = X.loc[valid].copy()
        y = y.loc[valid].copy()
        assert len(y) > 0
    else:
        y = pd.to_numeric(y, errors="coerce").replace([np.inf, -np.inf], np.nan)
        valid = y.notna()
        X = X.loc[valid].copy()
        y = y.loc[valid].copy()
        assert len(y) > 0

    # If after filtering only one class, force regression fallback on numeric coding of y
    if is_classification and y.nunique(dropna=True) < 2:
        is_classification = False
        y = pd.to_numeric(y, errors="coerce").replace([np.inf, -np.inf], np.nan)
        valid = y.notna()
        X = X.loc[valid].copy()
        y = y.loc[valid].copy()
        assert len(y) > 0

    # Build preprocessing pipeline
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Split
    if is_classification:
        # For stratify need at least 2 samples per class; otherwise disable stratify
        stratify = y
        try:
            class_counts = y.value_counts(dropna=False)
            if (class_counts.min() < 2) or (len(class_counts) < 2):
                stratify = None
        except Exception:
            stratify = None

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    if is_classification:
        # Lightweight, strong baseline for many numeric features
        clf = LogisticRegression(
            max_iter=200,
            solver="lbfgs",
            n_jobs=1,
            multi_class="auto",
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _accuracy_proxy_regression(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses robust CSV parsing with a low-overhead fallback (sep=';' and decimal=',') to avoid manual fixes.
# - Employs lightweight, CPU-friendly models: LogisticRegression for classification, Ridge for regression fallback.
# - Keeps preprocessing minimal and reproducible via ColumnTransformer+Pipeline (impute -> scale numeric, impute -> one-hot categorical).
# - Avoids heavy ensembles/deep learning; limits iterations and parallelism (n_jobs=1) for predictable CPU/energy use.
# - Adds defensive schema handling: normalized headers, drops 'Unnamed' columns, auto-selects target if 'label' missing, skips 'id' when possible.
# - Regression fallback reports a bounded "accuracy" proxy in [0,1] using a clipped R2 mapping: accuracy=(R2+1)/2.