# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import r2_score, mean_squared_error, accuracy_score

SEED = 42
DATASET_PATH = "data/AirQualityUCI.csv"
DATASET_HEADERS = "Date,Time,CO(GT),PT08.S1(CO),NMHC(GT),C6H6(GT),PT08.S2(NMHC),NOx(GT),PT08.S3(NOx),NO2(GT),PT08.S4(NO2),PT08.S5(O3),T,RH,AH,,"

def normalize_column(name):
    return "".join(str(name).replace("\ufeff", "").strip().lower().split())

def parse_headers(header_str):
    return [h.strip() for h in header_str.split(",") if h.strip()]

def read_air_quality_csv(path, desired_cols):
    df = pd.read_csv(path, low_memory=False)
    cols_str = df.columns.astype(str)
    if df.shape[1] == 1 or cols_str.str.contains(";").any():
        desired_norm = {normalize_column(c) for c in desired_cols}
        df = pd.read_csv(
            path,
            sep=";",
            decimal=",",
            usecols=lambda c: normalize_column(c) in desired_norm,
            low_memory=False
        )
    df.columns = [str(c).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.astype(str).str.startswith("Unnamed")]
    return df

def resolve_column(columns, target):
    norm_target = normalize_column(target)
    for c in columns:
        if normalize_column(c) == norm_target:
            return c
    for c in columns:
        if norm_target in normalize_column(c):
            return c
    return None

def load_and_preprocess(path):
    headers = parse_headers(DATASET_HEADERS)
    header_norm = {normalize_column(h) for h in headers}
    base_features = ["CO(GT)", "NOx(GT)", "NO2(GT)", "C6H6(GT)", "T", "RH"]
    desired_features = [f for f in base_features if normalize_column(f) in header_norm]
    desired_cols = ["Date", "Time"] + desired_features
    df = read_air_quality_csv(path, desired_cols)
    cols = df.columns.tolist()
    date_col = resolve_column(cols, "Date") or cols[0]
    time_col = resolve_column(cols, "Time") or (cols[1] if len(cols) > 1 else cols[0])
    feature_cols = [resolve_column(cols, f) for f in desired_features]
    feature_cols = [c for c in feature_cols if c is not None]
    seen = set()
    feature_cols = [c for c in feature_cols if c not in seen and not seen.add(c)]
    df = df[[date_col, time_col] + feature_cols]
    dt = pd.to_datetime(
        df[date_col].astype(str) + " " + df[time_col].astype(str),
        format="%d/%m/%Y %H.%M.%S",
        errors="coerce"
    )
    df = df.drop(columns=[date_col, time_col])
    df["Datetime"] = dt
    df = df.dropna(subset=["Datetime"])
    df = df.set_index("Datetime")
    df[feature_cols] = df[feature_cols].apply(pd.to_numeric, errors="coerce")
    df[feature_cols] = df[feature_cols].replace(-200, np.nan)
    daily = df.resample("D").mean()
    daily = daily.dropna(subset=feature_cols)
    hospital_visits = daily[feature_cols].sum(axis=1)
    daily["Hospital_Visits"] = hospital_visits
    median_visits = hospital_visits.median()
    daily["Risk_Label"] = (hospital_visits > median_visits).astype(int)
    return daily, feature_cols

def train_and_evaluate(data, feature_cols, seed):
    X = data[feature_cols].to_numpy()
    y_reg = data["Hospital_Visits"].to_numpy()
    y_clf = data["Risk_Label"].to_numpy()
    X_train, X_test, y_reg_train, y_reg_test, y_clf_train, y_clf_test = train_test_split(
        X, y_reg, y_clf, test_size=0.2, random_state=seed
    )
    reg_model = RandomForestRegressor(random_state=seed)
    clf_model = RandomForestClassifier(random_state=seed)
    reg_model.fit(X_train, y_reg_train)
    clf_model.fit(X_train, y_clf_train)
    y_reg_pred = reg_model.predict(X_test)
    y_clf_pred = clf_model.predict(X_test)
    _ = r2_score(y_reg_test, y_reg_pred)
    _ = mean_squared_error(y_reg_test, y_reg_pred, squared=False)
    accuracy = accuracy_score(y_clf_test, y_clf_pred)
    return accuracy

def main():
    np.random.seed(SEED)
    data, feature_cols = load_and_preprocess(DATASET_PATH)
    accuracy = train_and_evaluate(data, feature_cols, SEED)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# Filtered columns during fallback CSV parsing and dropped unnamed columns to reduce I/O and memory.
# Normalized and mapped column names once to avoid repeated string processing.
# Subset to required columns early and used vectorized conversions/resampling to minimize data movement.
# Reused a single deterministic train/test split for both tasks to avoid redundant computation.