# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import pickle
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score


SEED = 42
MODEL_PATH = "dict.pickle"
INPUT_CSV = "input.csv"
TRAIN_CSV = "14k.csv"


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _coerce_numeric_frame(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    for c in out.columns:
        out[c] = pd.to_numeric(out[c], errors="coerce")
    return out


def load_model(model_path: str = MODEL_PATH):
    with open(model_path, "rb") as f:
        return pickle.load(f)


def _extract_features_from_input(df: pd.DataFrame) -> np.ndarray:
    df = _coerce_numeric_frame(df)
    df = df.dropna(axis=1, how="all")
    df = df.dropna(axis=0, how="all")
    x = df.to_numpy(dtype=np.float64, copy=False)
    if x.ndim == 1:
        x = x.reshape(-1, 1)
    return x


def _extract_supervised_xy(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray]:
    df = _coerce_numeric_frame(df)
    df = df.dropna(axis=1, how="all")
    df = df.dropna(axis=0, how="all")

    if df.shape[1] < 2:
        raise ValueError("Training CSV must contain at least 2 columns (features + label).")

    data = df.to_numpy(dtype=np.float64, copy=False)
    x = data[:, :-1]
    y_raw = data[:, -1]

    y = np.where(y_raw > 0, 1, np.where(y_raw < 0, -1, 0)).astype(np.int64, copy=False)
    return x, y


def evaluate_loaded_model(model, train_csv: str = TRAIN_CSV) -> float:
    df = _read_csv_robust(train_csv)
    x, y = _extract_supervised_xy(df)

    n = x.shape[0]
    if n == 0:
        return 0.0

    rng = np.random.RandomState(SEED)
    idx = rng.permutation(n)
    test_size = int(round(0.3 * n))
    test_size = min(max(test_size, 1), n - 1) if n > 1 else 1

    test_idx = idx[:test_size]
    train_idx = idx[test_size:]

    x_train = x[train_idx]
    y_train = y[train_idx]
    x_test = x[test_idx]
    y_test = y[test_idx]

    if hasattr(model, "fit"):
        model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    return float(accuracy_score(y_test, y_pred))


def main() -> None:
    model = load_model(MODEL_PATH)

    if os.path.exists(INPUT_CSV):
        df_in = _read_csv_robust(INPUT_CSV)
        x_in = _extract_features_from_input(df_in)
        _ = model.predict(x_in)

    accuracy = evaluate_loaded_model(model, TRAIN_CSV)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced row-by-row csv module parsing with vectorized pandas loading to reduce Python-loop overhead and data movement.
# - Implemented robust CSV parsing fallback (default read_csv, then sep=';' and decimal=',') to avoid costly manual cleaning retries.
# - Eliminated global mutable lists and repeated conversions; operate directly on NumPy arrays for lower memory churn.
# - Used copy=False where safe when converting DataFrames to NumPy to reduce unnecessary allocations.
# - Consolidated numeric coercion and empty-row/column dropping into shared helpers to minimize redundant preprocessing.
# - Added deterministic split via a fixed seed and a single permutation to ensure reproducible accuracy while avoiding repeated shuffles.
# - Avoided saving artifacts and removed all non-required prints; only final accuracy line is emitted.