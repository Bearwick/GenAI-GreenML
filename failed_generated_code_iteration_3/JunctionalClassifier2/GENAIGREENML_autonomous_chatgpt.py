# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import math
import pickle
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


RANDOM_STATE = 42


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        normed.append(c2)
    return normed


def _drop_unnamed(df):
    to_drop = []
    for c in df.columns:
        if isinstance(c, str) and re.match(r"^Unnamed:\s*\d+$", c.strip()):
            to_drop.append(c)
    if to_drop:
        df = df.drop(columns=to_drop, errors="ignore")
    return df


def _robust_read_csv(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(d):
        if d is None or not isinstance(d, pd.DataFrame) or d.shape[0] == 0:
            return True
        if d.shape[1] <= 1:
            return True
        if all(isinstance(c, str) and (";" in c) for c in d.columns):
            return True
        return False

    if looks_wrong(df):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if not looks_wrong(df2):
                df = df2
        except Exception:
            pass

    if df is None:
        raise RuntimeError("Could not read CSV.")
    return df


def _load_dataset_any(path):
    ext = os.path.splitext(path)[1].lower()

    if ext in [".csv", ".txt"]:
        df = _robust_read_csv(path)
        df.columns = _normalize_columns(df.columns)
        df = _drop_unnamed(df)
        return df, None

    if ext in [".pickle", ".pkl"]:
        with open(path, "rb") as f:
            obj = pickle.load(f)

        if isinstance(obj, pd.DataFrame):
            df = obj.copy()
            df.columns = _normalize_columns(df.columns)
            df = _drop_unnamed(df)
            return df, None

        if isinstance(obj, dict):
            dataset_headers = None
            for k in ["DATASET_HEADERS", "dataset_headers", "headers", "COLUMNS", "columns"]:
                if k in obj and isinstance(obj[k], (list, tuple)) and len(obj[k]) > 0:
                    dataset_headers = list(obj[k])
                    break

            for k in ["df", "dataframe", "data", "X", "features"]:
                if k in obj and isinstance(obj[k], pd.DataFrame):
                    df = obj[k].copy()
                    df.columns = _normalize_columns(df.columns)
                    df = _drop_unnamed(df)
                    return df, dataset_headers

            if "X" in obj and "y" in obj:
                X = obj["X"]
                y = obj["y"]
                if isinstance(X, pd.DataFrame):
                    df = X.copy()
                    df.columns = _normalize_columns(df.columns)
                    df = _drop_unnamed(df)
                else:
                    X_arr = np.asarray(X)
                    if X_arr.ndim == 1:
                        X_arr = X_arr.reshape(-1, 1)
                    cols = dataset_headers if (dataset_headers and len(dataset_headers) == X_arr.shape[1]) else [f"f{i}" for i in range(X_arr.shape[1])]
                    df = pd.DataFrame(X_arr, columns=_normalize_columns(cols))
                df["target"] = np.asarray(y)
                df.columns = _normalize_columns(df.columns)
                df = _drop_unnamed(df)
                return df, dataset_headers

            if "data" in obj and "target" in obj:
                X = obj["data"]
                y = obj["target"]
                X_arr = np.asarray(X)
                if X_arr.ndim == 1:
                    X_arr = X_arr.reshape(-1, 1)
                cols = dataset_headers if (dataset_headers and len(dataset_headers) == X_arr.shape[1]) else [f"f{i}" for i in range(X_arr.shape[1])]
                df = pd.DataFrame(X_arr, columns=_normalize_columns(cols))
                df["target"] = np.asarray(y)
                df.columns = _normalize_columns(df.columns)
                df = _drop_unnamed(df)
                return df, dataset_headers

            if "arr" in obj:
                arr = np.asarray(obj["arr"])
                if arr.ndim == 1:
                    arr = arr.reshape(-1, 1)
                df = pd.DataFrame(arr, columns=[f"f{i}" for i in range(arr.shape[1])])
                df.columns = _normalize_columns(df.columns)
                df = _drop_unnamed(df)
                return df, dataset_headers

            if len(obj) > 0:
                try:
                    df = pd.DataFrame(obj)
                    df.columns = _normalize_columns(df.columns)
                    df = _drop_unnamed(df)
                    return df, dataset_headers
                except Exception:
                    pass

        if isinstance(obj, (list, tuple, np.ndarray)):
            arr = np.asarray(obj)
            if arr.ndim == 1:
                arr = arr.reshape(-1, 1)
            df = pd.DataFrame(arr, columns=[f"f{i}" for i in range(arr.shape[1])])
            df.columns = _normalize_columns(df.columns)
            df = _drop_unnamed(df)
            return df, None

        raise RuntimeError("Unsupported pickle contents.")

    if ext in [".parquet"]:
        df = pd.read_parquet(path)
        df.columns = _normalize_columns(df.columns)
        df = _drop_unnamed(df)
        return df, None

    raise RuntimeError("Unsupported file extension.")


def _coerce_numeric_inplace(df):
    for c in df.columns:
        if df[c].dtype == "object":
            s = df[c]
            sample = s.dropna().astype(str).head(30)
            if len(sample) == 0:
                continue
            convertible = 0
            for v in sample:
                v2 = v.strip()
                v2 = v2.replace(",", ".")
                v2 = re.sub(r"[^\d\.\-\+eE]", "", v2)
                try:
                    float(v2)
                    convertible += 1
                except Exception:
                    pass
            if convertible >= max(3, int(0.6 * len(sample))):
                df[c] = pd.to_numeric(
                    s.astype(str)
                    .str.strip()
                    .str.replace(",", ".", regex=False)
                    .str.replace(r"[^\d\.\-\+eE]", "", regex=True),
                    errors="coerce",
                )


def _pick_target_column(df, dataset_headers=None):
    cols = list(df.columns)

    preferred_names = [
        "target", "label", "class", "y", "out", "output", "prediction", "junction", "state", "type",
    ]
    lowered = {c: str(c).lower() for c in cols}
    for pname in preferred_names:
        for c in cols:
            if lowered[c] == pname or lowered[c].endswith(" " + pname) or lowered[c].endswith("_" + pname):
                if df[c].notna().sum() > 0:
                    return c

    if dataset_headers and isinstance(dataset_headers, (list, tuple)) and len(dataset_headers) > 0:
        for c in cols:
            if str(c) in set(_normalize_columns(dataset_headers)):
                continue

    non_constant_candidates = []
    for c in cols:
        s = df[c]
        if s.notna().sum() == 0:
            continue
        nun = s.nunique(dropna=True)
        if nun >= 2:
            non_constant_candidates.append((c, nun))

    if not non_constant_candidates:
        return cols[-1] if cols else None

    numeric_cols = []
    for c, nun in non_constant_candidates:
        if pd.api.types.is_numeric_dtype(df[c]):
            numeric_cols.append((c, nun))
    if numeric_cols:
        numeric_cols.sort(key=lambda x: x[1])
        return numeric_cols[0][0]

    non_constant_candidates.sort(key=lambda x: x[1])
    return non_constant_candidates[0][0]


def _make_accuracy_proxy_regression(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    y_mean = float(np.mean(y_true)) if y_true.size else 0.0
    ss_tot = float(np.sum((y_true - y_mean) ** 2))
    if ss_tot <= 0.0:
        mae = float(np.mean(np.abs(y_true - y_pred))) if y_true.size else 0.0
        return float(1.0 / (1.0 + mae))
    r2 = 1.0 - (ss_res / ss_tot)
    if not np.isfinite(r2):
        r2 = -1.0
    return float(max(0.0, min(1.0, (r2 + 1.0) / 2.0)))


def main():
    path = "dict.pickle"
    df, dataset_headers = _load_dataset_any(path)

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    _coerce_numeric_inplace(df)

    assert isinstance(df, pd.DataFrame) and df.shape[0] > 0 and df.shape[1] > 0

    target_col = _pick_target_column(df, dataset_headers=dataset_headers)
    if target_col is None or target_col not in df.columns:
        target_col = df.columns[-1]

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    if X.shape[1] == 0:
        X = pd.DataFrame({"bias": np.ones(len(df), dtype=float)})

    for c in X.columns:
        if pd.api.types.is_numeric_dtype(X[c]):
            X[c] = pd.to_numeric(X[c], errors="coerce")
            X[c] = X[c].replace([np.inf, -np.inf], np.nan)

    y = y_raw.copy()
    if pd.api.types.is_numeric_dtype(y):
        y = pd.to_numeric(y, errors="coerce")
        y = y.replace([np.inf, -np.inf], np.nan)

    valid_mask = pd.Series(True, index=df.index)
    valid_mask &= pd.Series(pd.notna(y), index=df.index)
    if valid_mask.sum() < 3:
        y = y_raw.astype(str)
        valid_mask = pd.Series(pd.notna(y), index=df.index)

    X = X.loc[valid_mask].reset_index(drop=True)
    y = y.loc[valid_mask].reset_index(drop=True)

    assert X.shape[0] == y.shape[0] and X.shape[0] > 1

    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler(with_mean=False)),
            ]), numeric_features),
            ("cat", Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
            ]), categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
        n_jobs=None,
    )

    is_classification = False
    y_for_model = y

    if pd.api.types.is_numeric_dtype(y_for_model):
        y_non_na = y_for_model.dropna()
        unique_vals = pd.unique(y_non_na)
        if len(unique_vals) >= 2:
            if len(unique_vals) <= min(50, max(2, int(0.2 * len(y_for_model)))):
                if np.all(np.isclose(unique_vals, np.round(unique_vals), equal_nan=True)):
                    is_classification = True
                    y_for_model = y_for_model.astype(int)
    else:
        is_classification = True
        y_for_model = y_for_model.astype(str)

    if is_classification:
        class_counts = pd.Series(y_for_model).value_counts(dropna=True)
        if class_counts.shape[0] < 2:
            is_classification = False

    stratify = y_for_model if (is_classification and pd.Series(y_for_model).nunique() >= 2) else None

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_for_model, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
    )

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    if is_classification:
        model = LogisticRegression(
            max_iter=200,
            solver="liblinear",
            n_jobs=1,
        )
        clf = Pipeline(steps=[("prep", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        reg = Pipeline(steps=[("prep", preprocessor), ("model", model)])
        y_train_num = pd.to_numeric(y_train, errors="coerce")
        y_test_num = pd.to_numeric(y_test, errors="coerce")
        train_mask = np.isfinite(np.asarray(y_train_num, dtype=float))
        test_mask = np.isfinite(np.asarray(y_test_num, dtype=float))
        if train_mask.sum() < 2 or test_mask.sum() < 1:
            mean_val = float(np.nanmean(np.asarray(y_train_num, dtype=float))) if train_mask.sum() > 0 else 0.0
            y_pred = np.full(shape=(test_mask.sum(),), fill_value=mean_val, dtype=float)
            accuracy = _make_accuracy_proxy_regression(np.asarray(y_test_num, dtype=float)[test_mask], y_pred)
        else:
            reg.fit(X_train.loc[train_mask].reset_index(drop=True), y_train_num.loc[train_mask].reset_index(drop=True))
            y_pred = reg.predict(X_test.loc[test_mask].reset_index(drop=True))
            accuracy = _make_accuracy_proxy_regression(np.asarray(y_test_num, dtype=float)[test_mask], y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression liblinear / Ridge) for CPU-friendly training and inference.
# - Robust schema handling: normalizes headers, drops Unnamed columns, and selects/derives a valid target if unknown.
# - Efficient preprocessing via ColumnTransformer: median imputation + StandardScaler(with_mean=False) for numerics and sparse OneHotEncoder for categoricals.
# - Avoids heavy feature engineering/ensembles; keeps max_iter modest to limit compute.
# - Regression fallback prints an "accuracy" proxy in [0,1] by mapping R^2 to (R^2+1)/2 and clipping; if variance is zero, uses 1/(1+MAE).