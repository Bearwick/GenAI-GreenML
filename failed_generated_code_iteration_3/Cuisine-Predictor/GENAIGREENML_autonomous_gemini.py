# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
import os
import json
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, r2_score

def load_dataset(path):
    # Robust CSV/JSON parsing with fallback
    if not os.path.exists(path):
        return pd.DataFrame()
    
    try:
        # Attempt to read as JSON first given the project context
        if path.endswith('.json'):
            df = pd.read_json(path)
        else:
            try:
                df = pd.read_csv(path)
            except:
                df = pd.read_csv(path, sep=';', decimal=',')
    except Exception:
        return pd.DataFrame()
    
    # Normalize column names: strip, single space, remove 'Unnamed'
    df.columns = [str(c).strip() for c in df.columns]
    df.columns = [" ".join(str(c).split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def preprocess_and_train():
    df = load_dataset('train.json')
    
    if df.empty:
        # Minimal fallback if file is missing/empty
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Derive target column (prefer 'cuisine' as per context)
    target_col = None
    for col in df.columns:
        if col.lower() in ['cuisine', 'target', 'label', 'category']:
            target_col = col
            break
    if not target_col:
        # Fallback: find a column with string/object types and multiple classes
        potential_targets = [c for c in df.columns if df[c].nunique() > 1 and df[c].nunique() < 100]
        target_col = potential_targets[0] if potential_targets else df.columns[-1]

    # Derive feature column (prefer 'ingredients')
    feature_col = None
    for col in df.columns:
        if col.lower() in ['ingredients', 'text', 'features']:
            feature_col = col
            break
    if not feature_col:
        feature_col = [c for c in df.columns if c != target_col][0]

    # Data transformation: ingredients often come as lists in JSON
    def process_features(x):
        if isinstance(x, list):
            return " ".join([str(i).replace(" ", "_") for i in x])
        return str(x)

    df['processed_features'] = df[feature_col].apply(process_features)
    
    # Handle target encoding/nulls
    df = df.dropna(subset=[target_col, 'processed_features'])
    
    # Check if we have enough data
    if len(df) < 2:
        print(f"ACCURACY={0.000000:.6f}")
        return

    X = df['processed_features']
    y = df[target_col]

    # Check for single class
    if y.nunique() < 2:
        print(f"ACCURACY={1.000000:.6f}")
        return

    # Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Energy-efficient pipeline: Tfidf with limited features + Multinomial NB
    # MultinomialNB is extremely lightweight (O(n) training/inference)
    # TfidfVectorizer is more efficient than deep embeddings
    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(max_features=3000, lowercase=True, stop_words='english')),
        ('clf', MultinomialNB())
    ])

    # Determine task type
    if y.dtype == 'object' or y.nunique() < 20:
        # Classification
        pipeline.fit(X_train, y_train)
        preds = pipeline.predict(X_test)
        accuracy = accuracy_score(y_test, preds)
    else:
        # Regression Fallback (if target is continuous)
        # Convert to a proxy accuracy in [0,1]
        from sklearn.linear_model import Ridge
        reg_pipeline = Pipeline([
            ('tfidf', TfidfVectorizer(max_features=3000)),
            ('reg', Ridge())
        ])
        y_train_num = pd.to_numeric(y_train, errors='coerce').fillna(0)
        y_test_num = pd.to_numeric(y_test, errors='coerce').fillna(0)
        reg_pipeline.fit(X_train, y_train_num)
        r2 = r2_score(y_test_num, reg_pipeline.predict(X_test))
        accuracy = max(0, min(1, r2)) # Clamp R2 to 0-1 range for accuracy proxy

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    preprocess_and_train()

# Optimization Summary:
# 1. Model Choice: Multinomial Naive Bayes was chosen for its minimal CPU and memory footprint compared to ensembles or deep learning.
# 2. Vectorization: TfidfVectorizer used with max_features=3000 to prevent memory bloat and high-dimensional computation.
# 3. Data Processing: List ingredients are joined with underscores to preserve multi-word ingredient entities as single tokens.
# 4. Energy Efficiency: Scikit-learn Pipeline ensures minimal data movement; Multinomial NB is a single-pass probability learner.
# 5. Robustness: Implemented multi-step schema inference and fallback mechanisms for CSV/JSON parsing to handle unexpected input formats.
# 6. Fallback: If classification is not viable, the script falls back to a Ridge regression and maps R^2 to an [0,1] scale.