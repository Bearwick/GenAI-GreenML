# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier
from sklearn.metrics import accuracy_score, r2_score

def load_data(path):
    df = None
    if path.lower().endswith('.json'):
        try:
            df = pd.read_json(path)
        except Exception:
            try:
                df = pd.read_json(path, lines=True)
            except Exception:
                df = None
    if df is None:
        try:
            df = pd.read_csv(path)
            if df.shape[1] == 1:
                sample = df.iloc[0, 0] if df.shape[0] > 0 else ''
                if isinstance(sample, str) and ';' in sample:
                    df_alt = pd.read_csv(path, sep=';', decimal=',')
                    if df_alt.shape[1] > 1:
                        df = df_alt
        except Exception:
            try:
                df = pd.read_csv(path, sep=';', decimal=',')
            except Exception as e:
                raise e
    if isinstance(df, pd.Series):
        df = df.to_frame()
    return df

def normalize_columns(df):
    new_cols = []
    for c in df.columns:
        if not isinstance(c, str):
            c = str(c)
        c = re.sub(r'\s+', ' ', c.strip())
        new_cols.append(c)
    df.columns = new_cols
    df = df.loc[:, ~df.columns.str.match(r'^Unnamed')]
    df = df.loc[:, df.columns != '']
    return df

def is_missing(val):
    return val is None or (isinstance(val, float) and np.isnan(val))

def detect_list_like_cols(df):
    list_cols = []
    for c in df.columns:
        series = df[c]
        found = False
        for v in series.head(20):
            if is_missing(v):
                continue
            if isinstance(v, (list, tuple, set, dict, np.ndarray)):
                found = True
            break
        if found:
            list_cols.append(c)
    return list_cols

def safe_nunique(series):
    try:
        return series.nunique(dropna=True)
    except Exception:
        try:
            return series.astype(str).nunique(dropna=True)
        except Exception:
            return len(series)

def select_target(df, list_like_cols):
    cols = list(df.columns)
    if not cols:
        return None
    lower_map = {c.lower(): c for c in cols}
    priority = ['target', 'label', 'labels', 'class', 'y', 'cuisine']
    for p in priority:
        if p in lower_map:
            return lower_map[p]
    obj_candidates = []
    for c in cols:
        if c in list_like_cols:
            continue
        if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_categorical_dtype(df[c]):
            nunique = safe_nunique(df[c])
            if nunique > 1 and nunique < max(50, int(len(df) * 0.5) + 1):
                obj_candidates.append((nunique, c))
    if obj_candidates:
        obj_candidates.sort()
        return obj_candidates[0][1]
    num_candidates = []
    for c in cols:
        if c in list_like_cols:
            continue
        if pd.api.types.is_numeric_dtype(df[c]) or pd.api.types.is_bool_dtype(df[c]):
            nunique = safe_nunique(df[c])
            if nunique > 1 and nunique < max(20, int(len(df) * 0.5) + 1):
                num_candidates.append((nunique, c))
    if num_candidates:
        num_candidates.sort()
        return num_candidates[0][1]
    numeric_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c]) or pd.api.types.is_bool_dtype(df[c])]
    if numeric_cols:
        variances = []
        for c in numeric_cols:
            if safe_nunique(df[c]) > 1:
                variances.append((df[c].var(skipna=True), c))
        if variances:
            variances.sort(reverse=True)
            return variances[0][1]
    return cols[-1]

def detect_numeric_like(df, exclude_cols):
    numeric_cols = list(df.select_dtypes(include=[np.number, 'bool']).columns)
    for c in df.columns:
        if c in exclude_cols or c in numeric_cols:
            continue
        if pd.api.types.is_object_dtype(df[c]):
            coerced = pd.to_numeric(df[c], errors='coerce')
            if coerced.notna().mean() > 0.9:
                df[c] = coerced
                numeric_cols.append(c)
    return df, numeric_cols

def combine_text(df_sub):
    def row_to_str(row):
        parts = []
        for v in row:
            if is_missing(v):
                continue
            if isinstance(v, (list, tuple, set, np.ndarray)):
                parts.append(" ".join([str(i) for i in v]))
            elif isinstance(v, dict):
                parts.append(" ".join([str(i) for i in v.values()]))
            else:
                parts.append(str(v))
        return " ".join(parts)
    return df_sub.apply(row_to_str, axis=1)

def infer_task(y):
    if pd.api.types.is_numeric_dtype(y):
        nunique = safe_nunique(y)
        if nunique <= 20:
            return 'classification'
        else:
            return 'regression'
    else:
        return 'classification'

path = 'train.json'
df = load_data(path)
df = normalize_columns(df)
df = df.replace([np.inf, -np.inf], np.nan)
assert df.shape[0] > 0 and df.shape[1] > 0
list_like_cols = detect_list_like_cols(df)
target_col = select_target(df, list_like_cols)
if target_col is None or target_col not in df.columns:
    target_col = df.columns[-1]
if pd.api.types.is_object_dtype(df[target_col]):
    coerced_target = pd.to_numeric(df[target_col], errors='coerce')
    if coerced_target.notna().mean() > 0.9:
        df[target_col] = coerced_target
exclude_cols = list_like_cols + [target_col]
df, _ = detect_numeric_like(df, exclude_cols)
df = df.replace([np.inf, -np.inf], np.nan)
df = df.loc[~df[target_col].isna()]
assert df.shape[0] > 0
y = df[target_col]
task = infer_task(y)
if task == 'classification':
    y = y.astype(str)
feature_cols = [c for c in df.columns if c != target_col]
if not feature_cols:
    df['_dummy'] = 0
    feature_cols = ['_dummy']
    list_like_cols = []
list_like_cols = [c for c in list_like_cols if c != target_col]
numeric_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c]) or pd.api.types.is_bool_dtype(df[c])]
text_cols = []
cat_cols = []
for c in feature_cols:
    if c in numeric_cols:
        continue
    if c in list_like_cols:
        text_cols.append(c)
    else:
        if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_categorical_dtype(df[c]):
            series = df[c].dropna()
            nunique = safe_nunique(series)
            if len(series) > 0:
                avg_len = series.astype(str).str.len().mean()
            else:
                avg_len = 0
            if (avg_len is not None and avg_len > 20) or nunique > 50:
                text_cols.append(c)
            else:
                cat_cols.append(c)
        else:
            cat_cols.append(c)
X = df[feature_cols]
is_classification = task == 'classification'
n_classes = safe_nunique(y) if is_classification else None
text_pipeline = Pipeline([
    ('combine', FunctionTransformer(combine_text, validate=False)),
    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 1), min_df=1))
])
cat_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
])
num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler(with_mean=False))
])
if is_classification and n_classes is not None and n_classes < 2:
    preprocessor = FunctionTransformer(lambda X: np.zeros((len(X), 1)), validate=False)
    model = DummyClassifier(strategy='most_frequent')
    pipeline = Pipeline([('preprocess', preprocessor), ('model', model)])
elif is_classification:
    if len(text_cols) > 0:
        transformers = []
        transformers.append(('text', text_pipeline, text_cols))
        if len(cat_cols) > 0:
            transformers.append(('cat', cat_pipeline, cat_cols))
        preprocessor = ColumnTransformer(transformers, remainder='drop')
        model = MultinomialNB()
        pipeline = Pipeline([('preprocess', preprocessor), ('model', model)])
    else:
        transformers = []
        if len(cat_cols) > 0:
            transformers.append(('cat', cat_pipeline, cat_cols))
        if len(numeric_cols) > 0:
            transformers.append(('num', num_pipeline, numeric_cols))
        if not transformers:
            preprocessor = FunctionTransformer(lambda X: np.zeros((len(X), 1)), validate=False)
        else:
            preprocessor = ColumnTransformer(transformers, remainder='drop')
        model = LogisticRegression(max_iter=200, solver='liblinear')
        pipeline = Pipeline([('preprocess', preprocessor), ('model', model)])
else:
    transformers = []
    if len(text_cols) > 0:
        transformers.append(('text', text_pipeline, text_cols))
    if len(cat_cols) > 0:
        transformers.append(('cat', cat_pipeline, cat_cols))
    if len(numeric_cols) > 0:
        transformers.append(('num', num_pipeline, numeric_cols))
    if not transformers:
        preprocessor = FunctionTransformer(lambda X: np.zeros((len(X), 1)), validate=False)
    else:
        preprocessor = ColumnTransformer(transformers, remainder='drop')
    model = Ridge(alpha=1.0)
    pipeline = Pipeline([('preprocess', preprocessor), ('model', model)])
n_samples = len(df)
if n_samples >= 5:
    test_size = 0.2
elif n_samples == 4:
    test_size = 0.25
elif n_samples == 3:
    test_size = 1 / 3
elif n_samples == 2:
    test_size = 0.5
else:
    test_size = 0.0
if test_size > 0:
    stratify = None
    if is_classification and n_classes is not None and n_classes > 1:
        vc = y.value_counts()
        if vc.min() > 1:
            stratify = y
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )
else:
    X_train, X_test, y_train, y_test = X, X, y, y
assert X_train.shape[0] > 0 and X_test.shape[0] > 0
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
if is_classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    try:
        score = r2_score(y_test, y_pred)
    except Exception:
        score = 0.0
    if np.isnan(score):
        score = 0.0
    accuracy = max(0.0, min(1.0, (score + 1) / 2))
print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# Used lightweight MultinomialNB for text and linear models otherwise to minimize CPU load.
# Limited TF-IDF vocabulary and applied simple imputers/encoders for efficient preprocessing.
# Fixed random_state ensures reproducibility with a minimal train/test split.
# Regression fallback maps R2 to a bounded [0,1] accuracy proxy for consistent reporting.