# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

def load_and_preprocess(filepath):
    # Robust CSV parsing
    try:
        df = pd.read_csv(filepath)
        if df.shape[1] <= 1:
            raise ValueError
    except Exception:
        df = pd.read_csv(filepath, sep=';', decimal=',')

    # Normalize column names
    df.columns = [str(col).strip() for col in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    
    if df.empty:
        return None, None

    # Derive target and features
    # Priority 1: Column named 'label' (standard for MNIST)
    # Priority 2: First column
    target_col = 'label' if 'label' in df.columns else df.columns[0]
    
    # Separate features and target
    y = df[target_col]
    X = df.drop(columns=[target_col])

    # Ensure all features are numeric and handle missing values
    X = X.apply(pd.to_numeric, errors='coerce')
    
    # Defensive check: ensure data is not empty
    if X.empty or len(y) == 0:
        return None, None
        
    return X, y

def run_pipeline():
    X, y = load_and_preprocess('MNIST_train.csv')
    
    if X is None or y is None:
        # Fallback if file is missing or corrupted for the sake of end-to-end execution
        print("ACCURACY=0.000000")
        return

    # Check for number of classes
    unique_classes = np.unique(y.dropna())
    if len(unique_classes) < 2:
        # Trivial baseline if only one class exists
        print("ACCURACY=1.000000")
        return

    # Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if len(unique_classes) > 1 else None
    )

    # Lightweight Pipeline:
    # 1. Imputer for safety
    # 2. Scaler (critical for Logistic Regression convergence/speed)
    # 3. Logistic Regression (Linear, CPU-friendly, fast)
    # Using 'lbfgs' and limiting max_iter for energy efficiency
    model_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(
            max_iter=100, 
            multi_class='auto', 
            solver='lbfgs', 
            tol=0.1, # High tolerance for faster, "greener" convergence
            random_state=42,
            n_jobs=1 # Avoid overhead of multi-processing for simple tasks
        ))
    ])

    # Train
    model_pipeline.fit(X_train, y_train)

    # Evaluate
    y_pred = model_pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary
# 1. Model Choice: Logistic Regression was selected over Deep Learning (CNNs) or Ensembles (Random Forest).
#    Linear models have significantly lower FLOPs (Floating Point Operations) and lower memory footprints.
# 2. Green Convergence: The 'tol' (tolerance) parameter in LogisticRegression was set to 0.1. 
#    This forces the solver to stop early once the improvement becomes marginal, saving CPU cycles.
# 3. Scaling: StandardScaler ensures that the gradient descent-based solver (lbfgs) converges in fewer iterations.
# 4. CPU Efficiency: Used scikit-learn's built-in lbfgs solver, which is highly optimized for CPU.
# 5. Preprocessing: Implemented a robust parsing and normalization logic to avoid manual data cleaning steps.
# 6. Memory Management: Avoided creating multiple copies of the dataset; used a simple Pipeline for reproducibility.
# 7. Robustness: Included fallbacks for schema mismatch and single-class targets to ensure the script never crashes.