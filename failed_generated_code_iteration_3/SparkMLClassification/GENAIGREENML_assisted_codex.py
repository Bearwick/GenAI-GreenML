# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd
import warnings
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFpr, chi2
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.exceptions import ConvergenceWarning

DATASET_PATH = "diabetes.csv"
DATASET_HEADERS = "Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age,Outcome"
SEED = 12345

warnings.filterwarnings("ignore", category=ConvergenceWarning)

def read_csv_robust(path, headers):
    df = pd.read_csv(path)
    if df.shape[1] == 1 or df.shape[1] != len(headers) or any(";" in str(c) for c in df.columns):
        df = pd.read_csv(path, sep=";", decimal=",")
    df.columns = [str(c).strip() for c in df.columns]
    if df.shape[1] == len(headers):
        if set(df.columns) != set(headers):
            df.columns = headers
    else:
        df_no_header = pd.read_csv(path, header=None)
        if df_no_header.shape[1] == len(headers):
            df_no_header.columns = headers
            df = df_no_header
    return df

def prepare_data(df, headers):
    df = df.apply(pd.to_numeric, errors="coerce")
    label_col = "Outcome" if "Outcome" in df.columns else headers[-1] if headers and headers[-1] in df.columns else df.columns[-1]
    zero_candidates = set(headers).intersection({"Glucose", "BloodPressure", "SkinThickness", "BMI", "Insulin"})
    zero_cols = [c for c in df.columns if c in zero_candidates]
    if zero_cols:
        df[zero_cols] = df[zero_cols].replace(0, np.nan)
    y = df.pop(label_col).to_numpy()
    X = df.to_numpy()
    return X, y.astype(int, copy=False)

def train_model(X, y, seed):
    imputer = SimpleImputer(strategy="mean")
    scaler = StandardScaler(with_mean=False)
    X_processed = scaler.fit_transform(imputer.fit_transform(X))
    X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=seed)
    selector = SelectFpr(score_func=chi2, alpha=0.05)
    X_train_sel = selector.fit_transform(X_train, y_train)
    X_test_sel = selector.transform(X_test)
    dataset_size = y_train.size
    num_pos = np.count_nonzero(y_train == 1)
    num_neg = dataset_size - num_pos
    balance = num_neg / dataset_size if dataset_size else 0.0
    sample_weights = np.where(y_train == 1, balance, 1.0 - balance)
    model = LogisticRegression(max_iter=10, solver="liblinear", random_state=seed)
    model.fit(X_train_sel, y_train, sample_weight=sample_weights)
    preds = model.predict(X_test_sel)
    return accuracy_score(y_test, preds)

def main():
    np.random.seed(SEED)
    headers = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]
    df = read_csv_robust(DATASET_PATH, headers)
    X, y = prepare_data(df, headers)
    accuracy = train_model(X, y, SEED)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# Replaced distributed Spark processing with pandas and scikit-learn to reduce overhead on a small dataset.
# Used vectorized numeric conversion and column-wise zero replacement to avoid row-wise operations.
# Combined imputation and scaling into a single preprocessing pass and reused fitted transformers.
# Fit the feature selector once and applied it to both splits to remove redundant computation.
# Switched to NumPy arrays after preprocessing to minimize data movement and memory usage.
# Set a fixed random seed for deterministic splits and model training.