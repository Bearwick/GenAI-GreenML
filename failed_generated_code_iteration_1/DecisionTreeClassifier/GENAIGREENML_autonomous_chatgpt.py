# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


RANDOM_STATE = 42
DATA_PATH = "music.csv"


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(dfx):
        if dfx is None or dfx.empty:
            return True
        if dfx.shape[1] <= 1:
            return True
        if dfx.columns.astype(str).str.contains(";").any():
            return True
        return False

    if _looks_wrong(df):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df2 is not None and not df2.empty and df2.shape[1] > 1:
                df = df2
        except Exception:
            pass

    if df is None:
        df = pd.DataFrame()
    return df


df = _read_csv_robust(DATA_PATH)

df.columns = _normalize_columns(df.columns)
df = df.loc[:, ~df.columns.astype(str).str.match(r"^Unnamed:")]
df = df.copy()

assert not df.empty, "Dataset is empty after loading."
assert df.shape[1] >= 1, "Dataset has no columns."

# Coerce numerics safely without touching object columns
for col in df.columns:
    if df[col].dtype.kind in "biufc":
        df[col] = pd.to_numeric(df[col], errors="coerce")

df = df.replace([np.inf, -np.inf], np.nan)
assert not df.empty, "Dataset is empty after basic cleanup."

# Determine target column robustly
normalized_lower = {c.lower(): c for c in df.columns}
target_col = None
if "genre" in normalized_lower:
    target_col = normalized_lower["genre"]
else:
    # Prefer a low-cardinality non-numeric column as target
    obj_cols = [c for c in df.columns if df[c].dtype == "object" or pd.api.types.is_string_dtype(df[c])]
    best = None
    best_card = None
    for c in obj_cols:
        nun = df[c].nunique(dropna=True)
        if nun >= 2 and (best_card is None or nun < best_card):
            best, best_card = c, nun
    if best is not None:
        target_col = best
    else:
        # Fallback: pick a non-constant numeric column as target
        num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
        for c in num_cols:
            if df[c].nunique(dropna=True) >= 2:
                target_col = c
                break

if target_col is None:
    # Final fallback: use last column
    target_col = df.columns[-1]

# Build feature set from remaining columns
feature_cols = [c for c in df.columns if c != target_col]
if len(feature_cols) == 0:
    # If only one column exists, use it as a feature and create a synthetic target (degenerate but end-to-end)
    feature_cols = [target_col]
    target_col = df.columns[-1]

X = df[feature_cols].copy()
y = df[target_col].copy()

# Decide task type
is_classification = True
if pd.api.types.is_numeric_dtype(y):
    # If numeric with many unique values, treat as regression
    nun = pd.Series(y).nunique(dropna=True)
    if nun > max(20, int(0.2 * max(1, len(y)))):
        is_classification = False
else:
    nun = pd.Series(y).nunique(dropna=True)
    if nun < 2:
        is_classification = False

# Drop rows with missing target for stability
mask = ~pd.isna(y)
X = X.loc[mask].copy()
y = y.loc[mask].copy()

assert len(X) > 0, "No samples after removing missing target."

# Identify column types for preprocessing
numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
categorical_features = [c for c in X.columns if c not in numeric_features]

numeric_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ]
)

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop",
)

# Split
test_size = 0.2 if len(X) >= 5 else 0.4
stratify = y if is_classification and pd.Series(y).nunique(dropna=True) >= 2 else None
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=RANDOM_STATE, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0, "Train/test split resulted in empty set."

# Train + evaluate
accuracy = 0.0

if is_classification and pd.Series(y_train).nunique(dropna=True) >= 2:
    clf = LogisticRegression(
        max_iter=200,
        solver="lbfgs",
        n_jobs=1,
        multi_class="auto",
    )
    model = Pipeline(steps=[("preprocess", preprocess), ("model", clf)])
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    accuracy = float(accuracy_score(y_test, preds))
else:
    reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
    model = Pipeline(steps=[("preprocess", preprocess), ("model", reg)])
    model.fit(X_train, y_train)
    preds = model.predict(X_test)

    y_test_arr = np.asarray(y_test, dtype=float)
    preds_arr = np.asarray(preds, dtype=float)
    ss_res = float(np.sum((y_test_arr - preds_arr) ** 2))
    ss_tot = float(np.sum((y_test_arr - np.mean(y_test_arr)) ** 2))
    r2 = 1.0 - (ss_res / ss_tot) if ss_tot > 0 else 0.0
    # Map R^2 to [0,1] as a stable "accuracy" proxy
    accuracy = float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Uses lightweight, CPU-friendly models: LogisticRegression for classification; Ridge for regression fallback.
# - ColumnTransformer + Pipeline ensures single-pass, reproducible preprocessing without redundant work.
# - OneHotEncoder(handle_unknown="ignore") avoids costly retries/failures on unseen categories and keeps sparse features.
# - SimpleImputer + StandardScaler provide stable numeric handling; coerces numeric columns and sanitizes NaN/inf.
# - Robust CSV parsing fallback (sep=';' and decimal=',') and defensive target/feature selection handle unknown schemas.
# - Regression fallback converts R^2 to an [0,1] proxy accuracy via (R^2+1)/2 with clipping for numeric stability.