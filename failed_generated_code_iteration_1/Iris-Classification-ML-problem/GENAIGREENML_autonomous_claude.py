# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
import os
import glob

# Step 1: Find the actual dataset file
# The DATASET_PATH points to requirements.txt which is not a dataset.
# Based on ML_PROJECT_CONTEXT, this is an Iris Classification problem.
# Search for CSV files in the project directory.

project_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in dir() else '.'
csv_files = glob.glob(os.path.join(project_dir, '**', '*.csv'), recursive=True)

dataset_path = None
for f in csv_files:
    if 'iris' in os.path.basename(f).lower():
        dataset_path = f
        break

if dataset_path is None and len(csv_files) > 0:
    dataset_path = csv_files[0]

# If no CSV found, try common iris dataset locations or fall back to sklearn
df = None

if dataset_path is not None:
    try:
        df = pd.read_csv(dataset_path)
        if df.shape[1] <= 1:
            df = pd.read_csv(dataset_path, sep=';', decimal=',')
    except Exception:
        try:
            df = pd.read_csv(dataset_path, sep=';', decimal=',')
        except Exception:
            df = None

if df is None:
    # Fallback: try loading from common paths
    common_paths = [
        'iris.csv', 'Iris.csv', 'data/iris.csv', 'data/Iris.csv',
        'dataset/iris.csv', 'dataset/Iris.csv',
        'IRIS.csv', 'iris.data', 'iris_data.csv'
    ]
    for p in common_paths:
        full_p = os.path.join(project_dir, p)
        if os.path.exists(full_p):
            try:
                df = pd.read_csv(full_p)
                if df.shape[1] <= 1:
                    df = pd.read_csv(full_p, sep=';', decimal=',')
                break
            except Exception:
                continue

if df is None:
    # Ultimate fallback: use sklearn's built-in iris dataset
    from sklearn.datasets import load_iris
    iris = load_iris()
    df = pd.DataFrame(iris.data, columns=iris.feature_names)
    df['species'] = iris.target

# Step 2: Clean column names
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
drop_cols = [c for c in df.columns if c.lower().startswith('unnamed')]
if drop_cols:
    df = df.drop(columns=drop_cols)

# Also drop any 'Id' column which is common in Iris datasets
id_cols = [c for c in df.columns if c.lower() == 'id']
if id_cols:
    df = df.drop(columns=id_cols)

# Step 3: Identify target and features
# For Iris, the target is typically a string/categorical column (species/class/variety)
# or the last column

target_col = None
# Look for known target column names
target_candidates = ['species', 'class', 'variety', 'target', 'label', 'type']
for candidate in target_candidates:
    matches = [c for c in df.columns if c.lower() == candidate]
    if matches:
        target_col = matches[0]
        break

if target_col is None:
    # Check for object/categorical columns
    obj_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    if len(obj_cols) == 1:
        target_col = obj_cols[0]
    elif len