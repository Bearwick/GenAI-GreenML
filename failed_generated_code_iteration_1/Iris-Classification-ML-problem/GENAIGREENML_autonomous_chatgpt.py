# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge


SEED = 4096
np.random.seed(SEED)


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = "" if c is None else str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = []
    for c in df.columns:
        if isinstance(c, str) and re.match(r"^Unnamed:\s*\d+$", c):
            drop_cols.append(c)
        if c == "":
            drop_cols.append(c)
    if drop_cols:
        df = df.drop(columns=list(dict.fromkeys(drop_cols)), errors="ignore")
    return df


def _read_csv_robust(path):
    last_err = None
    for kwargs in ({}, {"sep": ";", "decimal": ","}):
        try:
            df = pd.read_csv(path, **kwargs)
            if df is None or df.shape[0] == 0:
                continue
            if df.shape[1] == 1:
                continue
            return df
        except Exception as e:
            last_err = e
            continue
    raise RuntimeError(f"Could not parse CSV: {path}. Last error: {last_err}")


def _read_iris_data_fallback(path):
    # Handles classic iris.data (no header) robustly while keeping the main CSV loader generic.
    df = None
    try:
        df = pd.read_csv(path, header=None)
    except Exception:
        df = None

    if df is None or df.shape[0] == 0:
        raise RuntimeError(f"Dataset is empty or unreadable: {path}")

    if df.shape[1] == 1:
        # Try delimiter variants
        try:
            df2 = pd.read_csv(path, header=None, sep=";")
            if df2.shape[1] > 1:
                df = df2
        except Exception:
            pass

    if df.shape[1] >= 5:
        df = df.iloc[:, :5].copy()
        df.columns = ["SLength", "SWidth", "PLength", "PWidth", "class"]
    else:
        # Create generic names if unexpected shape
        df = df.copy()
        df.columns = [f"col_{i}" for i in range(df.shape[1])]
    return df


def _choose_target(df):
    cols = list(df.columns)
    if len(cols) == 0:
        return None

    preferred = [c for c in cols if str(c).strip().lower() in ("target", "label", "class", "y")]
    for c in preferred:
        if c in df.columns:
            nun = df[c].nunique(dropna=True)
            if nun >= 2:
                return c

    # Prefer low-cardinality non-numeric columns for classification if available
    for c in cols[::-1]:
        if c not in df.columns:
            continue
        nun = df[c].nunique(dropna=True)
        if nun >= 2 and nun <= max(20, int(0.2 * max(1, len(df)))):
            return c

    # Else pick a non-constant numeric column
    numeric_cols = []
    for c in cols[::-1]:
        s = pd.to_numeric(df[c], errors="coerce")
        nun = s.nunique(dropna=True)
        if nun >= 2:
            numeric_cols.append(c)
    if numeric_cols:
        return numeric_cols[0]

    # Else last column
    return cols[-1]


def _is_classification_target(y):
    if y is None or len(y) == 0:
        return False
    if pd.api.types.is_numeric_dtype(y):
        y_nonan = pd.Series(y).dropna()
        if y_nonan.empty:
            return False
        nun = y_nonan.nunique()
        if nun < 2:
            return False
        # Heuristic: small number of unique integer-like classes => classification
        if nun <= 20:
            y_vals = y_nonan.values
            if np.all(np.isfinite(y_vals)) and np.all(np.abs(y_vals - np.round(y_vals)) < 1e-9):
                return True
        return False
    # object/category/bool => classification if >=2 classes
    nun = pd.Series(y).nunique(dropna=True)
    return nun >= 2


def _bounded_regression_accuracy(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() < 2:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    denom = np.var(yt)
    if denom <= 0:
        return 0.0
    r2 = 1.0 - (np.mean((yt - yp) ** 2) / denom)
    acc = float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))
    return acc


# ---- Load dataset ----
candidate_paths = ["iris.data", "iris.csv", "data.csv", "dataset.csv", "train.csv"]
file_path = None
for p in candidate_paths:
    if os.path.exists(p):
        file_path = p
        break
if file_path is None:
    file_path = "iris.data"

if os.path.basename(file_path).lower() == "iris.data":
    df = _read_iris_data_fallback(file_path)
else:
    df = _read_csv_robust(file_path)

df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)

# Remove fully empty rows
df = df.dropna(how="all").reset_index(drop=True)
assert df.shape[0] > 0 and df.shape[1] > 0

# Choose target and features defensively
target_col = _choose_target(df)
if target_col is None or target_col not in df.columns:
    # Fallback to last column
    target_col = df.columns[-1]

X = df.drop(columns=[target_col], errors="ignore")
y = df[target_col].copy()

# If no features left, create a constant feature (keeps pipeline valid)
if X.shape[1] == 0:
    X = pd.DataFrame({"const": np.ones(len(df), dtype=float)})

# Identify column types robustly
numeric_features = []
categorical_features = []
for c in X.columns:
    # Determine numeric-ness by coercion ratio
    s = pd.to_numeric(X[c], errors="coerce")
    non_na = X[c].notna().sum()
    num_non_na = s.notna().sum()
    if non_na > 0 and (num_non_na / non_na) >= 0.8:
        numeric_features.append(c)
        X[c] = s  # store coerced numeric to avoid object medians
    else:
        categorical_features.append(c)

# Build preprocessing
numeric_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ]
)

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Decide task
task_is_classification = _is_classification_target(y)

# Defensive label handling
if task_is_classification:
    y_series = pd.Series(y)
    if not pd.api.types.is_numeric_dtype(y_series):
        y_series = y_series.astype("category").cat.codes
    else:
        y_series = pd.to_numeric(y_series, errors="coerce")
        # If integer-like, keep as int codes; else treat as regression
        y_nonan = y_series.dropna()
        if y_nonan.empty:
            task_is_classification = False
        else:
            if not (np.all(np.abs(y_nonan.values - np.round(y_nonan.values)) < 1e-9) and y_nonan.nunique() >= 2):
                task_is_classification = False
    y = y_series
else:
    y = pd.to_numeric(pd.Series(y), errors="coerce")

# Remove rows with missing target
mask = pd.Series(y).notna()
X = X.loc[mask].reset_index(drop=True)
y = pd.Series(y).loc[mask].reset_index(drop=True)
assert len(X) > 0 and len(y) > 0

# If classification but <2 classes after cleaning, fallback to regression
if task_is_classification:
    if pd.Series(y).nunique(dropna=True) < 2:
        task_is_classification = False
        y = pd.to_numeric(pd.Series(y), errors="coerce").fillna(0.0)

# Split (stratify if classification and feasible)
stratify = None
if task_is_classification:
    if pd.Series(y).nunique(dropna=True) >= 2 and len(y) >= 10:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.4,
    random_state=SEED,
    stratify=stratify,
)

assert len(X_train) > 0 and len(X_test) > 0

if task_is_classification:
    # Logistic regression is a strong, lightweight baseline for small datasets
    model = LogisticRegression(
        max_iter=200,
        solver="lbfgs",
        n_jobs=1,
        multi_class="auto",
    )
    clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Ridge regression is CPU-friendly and stable under collinearity
    reg = Ridge(alpha=1.0, random_state=SEED)
    rgs = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
    rgs.fit(X_train, y_train)
    y_pred = rgs.predict(X_test)
    accuracy = _bounded_regression_accuracy(y_test.values, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Avoided deep learning (torch) and used lightweight sklearn linear models (LogisticRegression/Ridge) for CPU efficiency.
# - Used a single Pipeline + ColumnTransformer to prevent redundant preprocessing and ensure reproducibility.
# - Robust CSV ingestion with delimiter/decimal fallback; special-case iris.data (headerless) to run end-to-end.
# - Defensive schema handling: normalized column names, dropped Unnamed columns, auto-selected target, and handled missing/constant targets.
# - Numeric coercion (errors='coerce') before imputation/scaling prevents costly dtype issues and median on object dtype.
# - For regression fallback, reported a bounded accuracy proxy: ACCURACY = clip((R2 + 1)/2, 0, 1) to keep output stable in [0,1].