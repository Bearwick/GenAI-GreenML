# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import glob
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols = []
    for c in df.columns:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        cols.append(c2)
    df.columns = cols
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", c)]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _try_read_csv(path: str) -> pd.DataFrame:
    # Attempt 1: default parser
    try:
        df1 = pd.read_csv(path)
    except Exception:
        df1 = None

    # Attempt 2: common EU formatting
    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
    except Exception:
        df2 = None

    candidates = [d for d in [df1, df2] if isinstance(d, pd.DataFrame)]
    if not candidates:
        raise RuntimeError("Failed to read CSV with available fallbacks.")

    # Heuristic: pick the parse with more columns and more non-empty cells
    def score(d: pd.DataFrame) -> tuple:
        if d is None or d.empty:
            return (-1, -1, -1)
        non_empty = int(d.notna().sum().sum())
        return (int(d.shape[1]), non_empty, int(d.shape[0]))

    df = max(candidates, key=score)
    return df


def _discover_dataset_path() -> str:
    # Prefer common iris dataset filenames, else any csv in repo.
    preferred = [
        "iris.csv",
        "Iris.csv",
        "IRIS.csv",
        "data/iris.csv",
        "data/Iris.csv",
        "dataset/iris.csv",
        "datasets/iris.csv",
        "input/iris.csv",
    ]
    for p in preferred:
        if os.path.exists(p) and os.path.isfile(p):
            return p

    csvs = []
    for pattern in ["*.csv", "**/*.csv"]:
        csvs.extend(glob.glob(pattern, recursive=True))
    # Exclude obvious non-dataset files
    csvs = [p for p in csvs if os.path.isfile(p) and "requirements" not in os.path.basename(p).lower()]
    if not csvs:
        raise FileNotFoundError("No CSV dataset found in the project directory.")
    # Choose smallest reasonable CSV first for CPU/IO efficiency
    csvs_sorted = sorted(csvs, key=lambda p: (os.path.getsize(p), len(p)))
    return csvs_sorted[0]


def _choose_target_and_features(df: pd.DataFrame):
    # Prefer typical iris target names; otherwise pick a non-constant column.
    cols = list(df.columns)
    lower_map = {c: c.lower() for c in cols}

    preferred_targets = ["species", "target", "label", "class", "y"]
    for pt in preferred_targets:
        for c in cols:
            if lower_map[c] == pt:
                target = c
                features = [x for x in cols if x != target]
                return target, features

    # If there are 5 columns, iris often has 4 features + 1 label (string)
    if df.shape[1] >= 2:
        # Prefer last column if it looks categorical-like (object or few uniques)
        last = cols[-1]
        nun = df[last].nunique(dropna=True)
        if df[last].dtype == "object" or nun <= max(20, int(0.2 * len(df))):
            if nun >= 2:
                target = last
                features = [x for x in cols if x != target]
                return target, features

    # Otherwise choose a column that varies and is not an obvious ID
    def is_id_like(s: pd.Series, name: str) -> bool:
        name_l = name.lower()
        if name_l in ("id", "index", "rowid"):
            return True
        if "id" in name_l and len(name_l) <= 6:
            return True
        # If integer-like and all unique, likely ID
        if pd.api.types.is_integer_dtype(s) and s.nunique(dropna=True) == len(s):
            return True
        return False

    candidates = []
    for c in cols:
        s = df[c]
        nun = s.nunique(dropna=True)
        if nun <= 1:
            continue
        if is_id_like(s, c):
            continue
        candidates.append((nun, c))
    if not candidates:
        # fallback: just pick last column
        target = cols[-1]
        features = [x for x in cols if x != target]
        return target, features

    # Prefer a categorical-ish target (few uniques) else numeric target (many uniques)
    candidates_sorted = sorted(candidates, key=lambda t: t[0])
    target = candidates_sorted[0][1]  # fewest uniques among non-constant non-ID
    features = [x for x in cols if x != target]
    if not features:
        # if only one column, create empty features (will be handled)
        features = []
    return target, features


def _coerce_numeric_safely(df: pd.DataFrame, numeric_cols):
    df2 = df.copy()
    for c in numeric_cols:
        df2[c] = pd.to_numeric(df2[c], errors="coerce")
    return df2


def _bounded_regression_score(y_true, y_pred) -> float:
    # Stable proxy in [0,1]: 1 / (1 + MAPE_clipped), with epsilon for zeros.
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    eps = 1e-9
    denom = np.maximum(np.abs(y_true), eps)
    ape = np.abs(y_true - y_pred) / denom
    ape = np.clip(ape, 0.0, 1e6)
    mape = float(np.nanmean(ape)) if ape.size else 1.0
    score = 1.0 / (1.0 + mape)
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    dataset_path = _discover_dataset_path()
    df = _try_read_csv(dataset_path)
    df = _normalize_columns(df)

    # Drop fully empty rows/cols early
    df = df.dropna(axis=0, how="all").dropna(axis=1, how="all")
    assert df is not None and isinstance(df, pd.DataFrame)
    assert not df.empty

    target_col, feature_cols = _choose_target_and_features(df)

    # Ensure we have some features; if none, create a constant feature column.
    if not feature_cols:
        df["_const_feature_"] = 1.0
        feature_cols = ["_const_feature_"]

    X = df[feature_cols].copy()
    y = df[target_col].copy()

    # Identify column types robustly (after copying).
    # Convert obvious numeric-looking objects to numeric for numeric pipeline.
    numeric_candidate_cols = []
    for c in X.columns:
        if pd.api.types.is_numeric_dtype(X[c]):
            numeric_candidate_cols.append(c)
        else:
            # Try coercion; if enough numeric values, treat as numeric
            coerced = pd.to_numeric(X[c], errors="coerce")
            non_na = int(coerced.notna().sum())
            if non_na >= max(3, int(0.6 * len(coerced))):
                X[c] = coerced
                numeric_candidate_cols.append(c)

    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    # Coerce numeric safely (defensive)
    X = _coerce_numeric_safely(X, numeric_features)

    # Determine if classification task is viable
    # Prefer classification if y is non-numeric or has small number of classes.
    y_is_numeric = pd.api.types.is_numeric_dtype(y)
    if not y_is_numeric:
        y_classes = y.astype("object").nunique(dropna=True)
    else:
        y_classes = y.nunique(dropna=True)

    classification_viable = (y_classes >= 2) and ((not y_is_numeric) or (y_classes <= 20))

    # Prepare preprocessors
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.0,
    )

    # Split
    # Use stratify only when classification with >=2 classes and not too many missing labels.
    y_for_split = y
    if classification_viable:
        y_for_split = y.astype("object")
        # Remove rows with missing target for classification to avoid split errors
        mask = y_for_split.notna()
        X2, y2 = X.loc[mask].copy(), y_for_split.loc[mask].copy()
        assert len(X2) > 1
        n_classes = int(y2.nunique(dropna=True))
        if n_classes < 2:
            classification_viable = False
            X2, y2 = X.copy(), y.copy()
    else:
        # Regression: drop missing targets
        y2 = pd.to_numeric(y, errors="coerce")
        mask = y2.notna()
        X2, y2 = X.loc[mask].copy(), y2.loc[mask].copy()
        if len(X2) <= 1:
            # Fallback: keep all and fill y with zeros (trivial baseline path)
            X2, y2 = X.copy(), pd.Series(np.zeros(len(X), dtype=float), index=X.index)

    assert len(X2) > 1

    stratify = None
    if classification_viable:
        # Only stratify if every class has at least 2 samples
        vc = y2.value_counts(dropna=True)
        if (vc.min() >= 2) and (len(vc) >= 2):
            stratify = y2

    X_train, X_test, y_train, y_test = train_test_split(
        X2,
        y2,
        test_size=0.2,
        random_state=42,
        stratify=stratify,
    )
    assert len(X_train) > 0 and len(X_test) > 0

    if classification_viable:
        # Logistic regression is CPU-friendly and strong for small tabular tasks.
        clf = LogisticRegression(
            solver="lbfgs",
            max_iter=200,
            n_jobs=1,
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Regression fallback with lightweight linear model; print bounded proxy as "accuracy".
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight, CPU-friendly models: LogisticRegression for classification; Ridge for regression fallback.
# - Robust CSV parsing with a low-cost two-pass fallback (default, then sep=';' and decimal=',') to avoid manual edits.
# - Defensive schema handling: normalizes column names, drops 'Unnamed' columns, selects target/features without hard-coded headers.
# - Minimal preprocessing via ColumnTransformer: median imputation + standard scaling for numeric, most-frequent + one-hot for categoricals.
# - Avoids heavy feature engineering and deep learning; keeps n_jobs=1 for predictable CPU use and reduced contention/energy.
# - Regression fallback reports a bounded [0,1] proxy score: 1/(1+MAPE) to maintain a stable "ACCURACY" output format.