# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    df0 = pd.read_csv(path)
    # Heuristic: if only 1 column but the raw line likely contains separators, try alternative parsing.
    if df0.shape[1] <= 1:
        try:
            df1 = pd.read_csv(path, sep=";", decimal=",")
            if df1.shape[1] > df0.shape[1]:
                return df1
        except Exception:
            pass
    return df0


def _pick_target_column(df, preferred_names=None):
    cols = list(df.columns)
    if preferred_names:
        pref_lower = {p.lower(): p for p in preferred_names}
        for c in cols:
            if str(c).strip().lower() in pref_lower:
                return c

    # Prefer a non-constant numeric-like column (coerced) with small-ish number of unique values (classification-friendly)
    best = None
    best_score = -1

    for c in cols:
        s = df[c]
        if s.name is None:
            continue
        if s.dtype == "O" or str(s.dtype).startswith("string"):
            # Try numeric coercion on object; if mostly numeric, consider it
            sn = pd.to_numeric(s, errors="coerce")
            non_na = sn.notna().mean()
            if non_na < 0.8:
                continue
            nun = sn.dropna().nunique()
            if nun < 2:
                continue
            score = 10.0 / (1.0 + nun) + non_na
        else:
            sn = pd.to_numeric(s, errors="coerce")
            non_na = sn.notna().mean()
            if non_na < 0.8:
                continue
            nun = sn.dropna().nunique()
            if nun < 2:
                continue
            score = 10.0 / (1.0 + nun) + non_na

        if score > best_score:
            best_score = score
            best = c

    if best is not None:
        return best

    # Fallback: last column
    return cols[-1] if cols else None


def _bounded_regression_accuracy(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    if ss_tot <= 0:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    # Bound to [0,1] for stable "accuracy" proxy
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


# Locate dataset with robust defaults
candidate_paths = [
    "fetal_health.csv",
    os.path.join("data", "fetal_health.csv"),
    os.path.join("dataset", "fetal_health.csv"),
    os.path.join("datasets", "fetal_health.csv"),
]
csv_path = None
for p in candidate_paths:
    if os.path.exists(p):
        csv_path = p
        break
if csv_path is None:
    csv_path = "fetal_health.csv"

df = _read_csv_robust(csv_path)

# Normalize columns and drop unnamed placeholders
df.columns = _normalize_columns(df.columns)
df = df.loc[:, ~df.columns.astype(str).str.match(r"^Unnamed:\s*\d+$", na=False)]

assert df.shape[0] > 0 and df.shape[1] > 0

# Prefer the known target name if present, but do not hard-require it
DATASET_HEADERS = [
    "baseline value",
    "accelerations",
    "fetal_movement",
    "uterine_contractions",
    "light_decelerations",
    "severe_decelerations",
    "prolongued_decelerations",
    "abnormal_short_term_variability",
    "mean_value_of_short_term_variability",
    "percentage_of_time_with_abnormal_long_term_variability",
    "mean_value_of_long_term_variability",
    "histogram_width",
    "histogram_min",
    "histogram_max",
    "histogram_number_of_peaks",
    "histogram_number_of_zeroes",
    "histogram_mode",
    "histogram_mean",
    "histogram_median",
    "histogram_variance",
    "histogram_tendency",
    "fetal_health",
]

target_col = _pick_target_column(df, preferred_names=["fetal_health"])
assert target_col is not None

y_raw = df[target_col].copy()
X = df.drop(columns=[target_col], errors="ignore")

# If no features remain, create a constant feature to keep pipeline valid
if X.shape[1] == 0:
    X = pd.DataFrame({"_bias": np.ones(len(df), dtype=float)})

# Identify numeric/categorical features defensively
numeric_cols = []
categorical_cols = []
for c in X.columns:
    s = X[c]
    if pd.api.types.is_numeric_dtype(s):
        numeric_cols.append(c)
    else:
        # attempt to treat as numeric if mostly coercible
        sn = pd.to_numeric(s, errors="coerce")
        if sn.notna().mean() >= 0.9:
            X[c] = sn
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)

# Build preprocessing with lightweight steps
numeric_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False)),
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ]
)

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_cols),
        ("cat", categorical_transformer, categorical_cols),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Decide classification vs regression based on target properties
y_num = pd.to_numeric(y_raw, errors="coerce")
y_non_na = y_num.dropna()

is_classification = False
if len(y_non_na) > 0:
    nunique = int(y_non_na.nunique())
    # If looks like small set of integer labels -> classification
    if nunique >= 2 and nunique <= 20:
        # Ensure near-integer labels (common in this dataset)
        frac = np.abs(y_non_na - np.round(y_non_na))
        if float(np.nanmean(frac)) < 1e-6:
            is_classification = True

# Prepare target vectors with safe handling
if is_classification:
    y = y_num.round().astype("Int64")
    valid_mask = y.notna()
    X2 = X.loc[valid_mask].copy()
    y2 = y.loc[valid_mask].astype(int).to_numpy()
    # If still degenerate, fallback to regression
    if len(np.unique(y2)) < 2:
        is_classification = False
else:
    valid_mask = y_num.notna()
    X2 = X.loc[valid_mask].copy()
    y2 = y_num.loc[valid_mask].to_numpy(dtype=float)

assert X2.shape[0] > 2

# Train/test split (fixed for reproducibility)
stratify = y2 if is_classification else None
X_train, X_test, y_train, y_test = train_test_split(
    X2, y2, test_size=0.3, random_state=8, stratify=stratify
)

assert X_train.shape[0] > 0 and X_test.shape[0] > 0

if is_classification:
    # Energy-efficient linear baseline; small iterations; CPU-friendly
    model = LogisticRegression(
        solver="lbfgs",
        max_iter=200,
        n_jobs=1,
        multi_class="auto",
    )
    clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Regression fallback with bounded score in [0,1]
    model = Ridge(alpha=1.0, random_state=8)
    reg = Pipeline(steps=[("preprocess", preprocess), ("model", model)])
    reg.fit(X_train, y_train)
    y_pred = reg.predict(X_test)
    accuracy = _bounded_regression_accuracy(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Uses lightweight, CPU-friendly models: LogisticRegression (classification) or Ridge (regression fallback) to avoid heavy ensembles/deep nets.
# - Robust CSV parsing fallback (default then ;/decimal=,) to reduce failures without manual edits.
# - Column normalization and dropping "Unnamed:*" prevents wasted memory/processing on placeholder columns.
# - Minimal preprocessing via ColumnTransformer + Pipeline ensures reproducibility and avoids repeated transforms.
# - Numeric coercion for mostly-numeric object columns reduces one-hot expansion (energy/memory savings).
# - Sparse OneHotEncoder for categoricals limits RAM and CPU usage.
# - Fixed random_state and simple train/test split for deterministic runs with low computational overhead.
# - Regression fallback returns a bounded proxy accuracy = clip((R2+1)/2,0,1) to keep a stable [0,1] metric when classification is not viable.