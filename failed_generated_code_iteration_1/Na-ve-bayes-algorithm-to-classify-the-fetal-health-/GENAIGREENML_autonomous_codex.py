# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier

dataset_path = "fetal_health.csv"
dataset_headers_str = "baseline value,accelerations,fetal_movement,uterine_contractions,light_decelerations,severe_decelerations,prolongued_decelerations,abnormal_short_term_variability,mean_value_of_short_term_variability,percentage_of_time_with_abnormal_long_term_variability,mean_value_of_long_term_variability,histogram_width,histogram_min,histogram_max,histogram_number_of_peaks,histogram_number_of_zeroes,histogram_mode,histogram_mean,histogram_median,histogram_variance,histogram_tendency,fetal_health"

def load_data(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    def looks_wrong(d):
        if d is None:
            return True
        if d.shape[1] <= 1:
            return True
        col_str = [str(c) for c in d.columns]
        if any(';' in c for c in col_str):
            return True
        return False
    if looks_wrong(df):
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt is not None and df_alt.shape[1] > 1:
                df = df_alt
        except Exception:
            pass
    if df is None:
        df = pd.DataFrame()
    return df

def normalize_colname(c):
    c = str(c)
    c = c.strip()
    c = re.sub(r'\s+', ' ', c)
    return c

def simplify_colname(c):
    return re.sub(r'[^a-z0-9]+', '', str(c).lower())

df = load_data(dataset_path)
df.columns = [normalize_colname(c) for c in df.columns]
df = df.loc[:, ~df.columns.str.match(r'^Unnamed')]

header_candidates = [normalize_colname(h) for h in dataset_headers_str.split(',') if h.strip()]
if len(df.columns) == len(header_candidates):
    if all(str(c).isdigit() for c in df.columns):
        df.columns = header_candidates

df.replace([np.inf, -np.inf], np.nan, inplace=True)

simplified_cols = {col: simplify_colname(col) for col in df.columns}
target_col = None
for col, simp in simplified_cols.items():
    if simp == 'fetalhealth' or ('fetal' in simp and 'health' in simp):
        target_col = col
        break
if target_col is None:
    for col, simp in simplified_cols.items():
        if any(k in simp for k in ['target', 'label', 'class', 'outcome', 'y']):
            target_col = col
            break
if target_col is None:
    numeric_candidates = []
    for col in df.columns:
        series = pd.to_numeric(df[col], errors='coerce')
        if series.notna().sum() > 0 and series.nunique(dropna=True) > 1:
            numeric_candidates.append(col)
    if numeric_candidates:
        target_col = numeric_candidates[-1]
    elif df.shape[1] > 0:
        target_col = df.columns[-1]

feature_cols = [c for c in df.columns if c != target_col] if target_col in df.columns else []

numeric_features = []
categorical_features = []
for col in feature_cols:
    series = df[col]
    if pd.api.types.is_numeric_dtype(series):
        numeric_features.append(col)
    else:
        coerced = pd.to_numeric(series, errors='coerce')
        if coerced.notna().mean() >= 0.5:
            df[col] = coerced
            numeric_features.append(col)
        else:
            categorical_features.append(col)

numeric_features = [col for col in numeric_features if df[col].notna().sum() > 0]
categorical_features = [col for col in categorical_features if df[col].notna().sum() > 0]

if not numeric_features and not categorical_features:
    df['constant_feature'] = 1
    numeric_features = ['constant_feature']
    feature_cols = ['constant_feature']
else:
    feature_cols = numeric_features + categorical_features

if target_col in df.columns:
    y_raw = df[target_col]
else:
    y_raw = pd.Series(dtype=float)

if pd.api.types.is_numeric_dtype(y_raw):
    y = y_raw
else:
    y_num = pd.to_numeric(y_raw, errors='coerce')
    if y_num.notna().mean() >= 0.5:
        y = y_num
    else:
        y = y_raw.astype(str)

mask = y.notna()
df = df.loc[mask].copy()
y = y.loc[mask]

numeric_features = [col for col in numeric_features if col in df.columns and df[col].notna().sum() > 0]
categorical_features = [col for col in categorical_features if col in df.columns and df[col].notna().sum() > 0]

if not numeric_features and not categorical_features:
    df['constant_feature'] = 1
    numeric_features = ['constant_feature']
    categorical_features = []

feature_cols = numeric_features + categorical_features
X = df[feature_cols] if feature_cols else pd.DataFrame(index=df.index)

assert len(df) > 0 and len(X) > 0 and target_col is not None

if pd.api.types.is_numeric_dtype(y):
    unique_vals = pd.unique(y)
    n_unique = len(unique_vals)
    integer_like = np.all(np.isclose(unique_vals, np.round(unique_vals)))
    if n_unique <= 20 and integer_like:
        problem_type = 'classification'
    else:
        problem_type = 'regression'
else:
    problem_type = 'classification'
    n_unique = y.nunique(dropna=True)
    integer_like = False

if problem_type == 'classification':
    if not pd.api.types.is_numeric_dtype(y):
        le = LabelEncoder()
        y = le.fit_transform(y.astype(str))
    else:
        if integer_like:
            y = y.astype(int)
    n_unique = len(np.unique(y))
    if n_unique < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    model = Ridge(alpha=1.0)

numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median'))])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

transformers = []
if numeric_features:
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    transformers.append(('cat', categorical_transformer, categorical_features))

preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')

stratify = None
if problem_type == 'classification':
    try:
        counts = pd.Series(y).value_counts()
        if len(counts) >= 2 and counts.min() >= 2:
            stratify = y
    except Exception:
        stratify = None

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0

model_pipeline = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])

model_pipeline.fit(X_train, y_train)

y_pred = model_pipeline.predict(X_test)

if problem_type == 'classification':
    try:
        accuracy = accuracy_score(y_test, y_pred)
    except Exception:
        accuracy = 0.0
else:
    if len(np.unique(y_test)) < 2:
        r2 = 0.0
    else:
        try:
            r2 = r2_score(y_test, y_pred)
        except Exception:
            r2 = 0.0
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight preprocessing (simple imputers and optional one-hot encoding) with linear/logistic models for CPU efficiency.
# - Avoided heavy ensembles or deep models, keeping computation and memory use low for small datasets.
# - Added robust CSV parsing, column normalization, and defensive fallbacks to ensure reproducible end-to-end execution.
# - For regression fallback, converted R^2 into a bounded [0,1] accuracy proxy using (r2+1)/2 with clipping.