# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import pickle
import warnings
from typing import Tuple, Optional, List

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score

warnings.filterwarnings("ignore")


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols = []
    for c in df.columns:
        s = str(c)
        s = s.strip()
        s = re.sub(r"\s+", " ", s)
        cols.append(s)
    df.columns = cols
    drop_cols = [c for c in df.columns if str(c).startswith("Unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _try_read_csv(path: str) -> Optional[pd.DataFrame]:
    try:
        df = pd.read_csv(path)
        df = _normalize_columns(df)
        # if single column and many commas inside strings, likely wrong delimiter
        if df.shape[1] <= 1 and df.shape[0] > 0:
            raise ValueError("Suspicious CSV parse (single column).")
        return df
    except Exception:
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
            df = _normalize_columns(df)
            return df
        except Exception:
            return None


def _load_dataset(path: str):
    ext = os.path.splitext(path)[1].lower()
    if ext in [".csv", ".txt"]:
        df = _try_read_csv(path)
        if df is None:
            raise RuntimeError("Failed to read CSV dataset.")
        return df

    if ext in [".parquet"]:
        df = pd.read_parquet(path)
        df = _normalize_columns(df)
        return df

    if ext in [".pkl", ".pickle"]:
        with open(path, "rb") as f:
            obj = pickle.load(f)

        # Common patterns: dict with 'data' or 'df', or direct DataFrame, or dict of arrays/lists
        if isinstance(obj, pd.DataFrame):
            df = obj.copy()
            df = _normalize_columns(df)
            return df

        if isinstance(obj, dict):
            for k in ["df", "data", "dataset", "frame"]:
                if k in obj and isinstance(obj[k], pd.DataFrame):
                    df = obj[k].copy()
                    df = _normalize_columns(df)
                    return df
            # dict of columns
            try:
                df = pd.DataFrame(obj)
                df = _normalize_columns(df)
                return df
            except Exception:
                pass

        # list of dicts
        if isinstance(obj, list):
            try:
                df = pd.DataFrame(obj)
                df = _normalize_columns(df)
                return df
            except Exception:
                pass

        raise RuntimeError("Unsupported pickle contents for dataset.")

    raise RuntimeError("Unsupported dataset file extension.")


def _coerce_numeric_inplace(df: pd.DataFrame, cols: List[str]) -> None:
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")


def _choose_target(df: pd.DataFrame) -> Tuple[str, str]:
    # returns (target_col, task) where task is 'classification' or 'regression'
    cols = list(df.columns)
    if len(cols) == 0:
        raise RuntimeError("Empty dataframe (no columns).")

    # Heuristics: prefer columns named like label/target/y/class if present
    name_candidates = []
    for c in cols:
        lc = str(c).strip().lower()
        if lc in ["label", "target", "y", "class", "classes", "output"]:
            name_candidates.append(c)
        elif any(tok in lc for tok in ["label", "target", "class", "outcome"]):
            name_candidates.append(c)

    def is_nonconstant(s: pd.Series) -> bool:
        v = s.dropna()
        if v.empty:
            return False
        return v.nunique(dropna=True) > 1

    # Evaluate candidates first
    for c in name_candidates:
        if c in df.columns and is_nonconstant(df[c]):
            # Determine task by cardinality and dtype
            v = df[c].dropna()
            nunq = v.nunique()
            if nunq <= 20 and (df[c].dtype == "object" or nunq <= 10):
                return c, "classification"
            # if looks like integer classes (-1,0,1 etc.)
            if pd.api.types.is_numeric_dtype(df[c]) and nunq <= 10:
                return c, "classification"
            return c, "regression"

    # Otherwise: prefer a non-constant object/categorical with low cardinality for classification
    for c in cols:
        if c not in df.columns:
            continue
        if not is_nonconstant(df[c]):
            continue
        if df[c].dtype == "object":
            nunq = df[c].dropna().nunique()
            if 2 <= nunq <= 20:
                return c, "classification"

    # Next: prefer numeric with small number of unique values for classification
    numeric_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]
    for c in numeric_cols:
        if not is_nonconstant(df[c]):
            continue
        nunq = df[c].dropna().nunique()
        if 2 <= nunq <= 10:
            return c, "classification"

    # Fallback: pick any non-constant numeric for regression
    for c in numeric_cols:
        if is_nonconstant(df[c]):
            return c, "regression"

    # Last resort: pick first column as target (may be constant); will trigger trivial fallback
    return cols[0], "classification"


def _build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:
    # Identify columns by dtype
    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False)),  # with_mean=False keeps sparse-friendly
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )
    return preprocessor


def _safe_accuracy_proxy_regression(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    # Bounded [0,1] proxy using normalized MAE: acc = 1 - MAE / (range + eps)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = np.nanmean(np.abs(y_true - y_pred))
    rng = np.nanmax(y_true) - np.nanmin(y_true)
    denom = float(rng) if np.isfinite(rng) and rng > 0 else float(np.nanstd(y_true))
    if not np.isfinite(denom) or denom <= 0:
        denom = 1.0
    acc = 1.0 - (mae / (denom + 1e-12))
    if not np.isfinite(acc):
        acc = 0.0
    return float(np.clip(acc, 0.0, 1.0))


def main():
    path = "dict.pickle"
    df = _load_dataset(path)
    df = _normalize_columns(df)

    # Drop completely empty columns
    df = df.dropna(axis=1, how="all")

    assert df.shape[0] > 0 and df.shape[1] > 0, "Dataset empty after basic cleanup."

    target_col, task = _choose_target(df)

    # Prepare features/target
    y = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # If no features remain, create a dummy constant feature to keep pipeline valid
    if X.shape[1] == 0:
        X = pd.DataFrame({"__dummy__": np.zeros(len(df), dtype=float)})

    # Coerce likely numeric object columns in X when possible (lightweight attempt)
    # This avoids expensive per-column heuristics; we only try conversion if many values look numeric.
    for c in list(X.columns):
        if X[c].dtype == "object":
            s = X[c].astype(str)
            sample = s.head(200)
            # quick numeric-likeness check
            numlike = sample.str.match(r"^\s*[-+]?\d+([.,]\d+)?\s*$", na=False).mean()
            if numlike >= 0.9:
                X[c] = pd.to_numeric(s.str.replace(",", ".", regex=False), errors="coerce")

    # For numeric targets that are object, coerce
    if y.dtype == "object":
        y_num = pd.to_numeric(y.astype(str).str.replace(",", ".", regex=False), errors="coerce")
        # If conversion yields enough non-null and multiple unique, treat as numeric
        if y_num.notna().mean() > 0.8 and y_num.dropna().nunique() > 1:
            y = y_num
            # re-evaluate task
            nunq = y.dropna().nunique()
            task = "classification" if 2 <= nunq <= 10 else "regression"

    # Remove rows with missing target
    mask = y.notna()
    X = X.loc[mask].reset_index(drop=True)
    y = y.loc[mask].reset_index(drop=True)

    assert len(y) > 2, "Not enough samples after removing missing targets."

    # Decide classification viability
    if task == "classification":
        # If y is numeric with many unique values, switch to regression to avoid heavy multiclass
        if pd.api.types.is_numeric_dtype(y):
            nunq = int(y.nunique())
            if nunq > 20:
                task = "regression"

    # If classification but <2 classes, fallback to regression proxy / trivial
    if task == "classification":
        n_classes = int(pd.Series(y).nunique())
        if n_classes < 2:
            task = "regression"

    # Train/test split
    if task == "classification":
        # Stratify only if enough samples per class
        vc = pd.Series(y).value_counts()
        stratify = y if (vc.min() >= 2 and len(vc) >= 2) else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Empty train/test split."

    preprocessor = _build_preprocessor(X_train)

    if task == "classification":
        # Logistic Regression with saga is efficient for sparse one-hot; capped iterations for CPU-friendliness
        clf = LogisticRegression(
            solver="saga",
            penalty="l2",
            C=1.0,
            max_iter=500,
            n_jobs=1,
            multi_class="auto",
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Ridge regression is lightweight and stable; works well with standardized numeric + one-hot
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        # Coerce y to numeric safely
        y_train_num = pd.to_numeric(pd.Series(y_train), errors="coerce").astype(float)
        y_test_num = pd.to_numeric(pd.Series(y_test), errors="coerce").astype(float)

        # If too many NaNs after coercion, use trivial baseline
        if y_train_num.notna().sum() < max(2, int(0.5 * len(y_train_num))):
            # Trivial predictor: mean of available numeric labels
            y_mean = float(np.nanmean(y_train_num.values)) if np.isfinite(np.nanmean(y_train_num.values)) else 0.0
            y_pred = np.full(shape=(len(y_test_num),), fill_value=y_mean, dtype=float)
            accuracy = _safe_accuracy_proxy_regression(y_test_num.values, y_pred)
        else:
            # Filter NaN targets for training
            train_mask = y_train_num.notna().values
            X_train_f = X_train.loc[train_mask].reset_index(drop=True)
            y_train_f = y_train_num.loc[train_mask].reset_index(drop=True)

            test_mask = y_test_num.notna().values
            X_test_f = X_test.loc[test_mask].reset_index(drop=True)
            y_test_f = y_test_num.loc[test_mask].reset_index(drop=True)

            # If test has no valid numeric targets, fall back to trivial 0.0 accuracy
            if len(y_test_f) == 0 or len(y_train_f) < 2:
                accuracy = 0.0
            else:
                model.fit(X_train_f, y_train_f)
                y_pred = model.predict(X_test_f)
                accuracy = _safe_accuracy_proxy_regression(y_test_f.values, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight classical models (LogisticRegression/Ridge) suitable for CPU-only inference/training; avoids heavy ensembles/deep nets.
# - Employs a single sklearn Pipeline + ColumnTransformer to prevent redundant preprocessing and ensure reproducibility.
# - OneHotEncoder(handle_unknown="ignore") provides robust categorical handling without expensive text embeddings.
# - StandardScaler(with_mean=False) is sparse-friendly, reducing memory and CPU overhead when one-hot features are present.
# - Defensive schema handling: normalizes column names, drops 'Unnamed:' columns, auto-selects a reasonable target, and falls back to regression if classification is ill-posed.
# - Regression accuracy proxy is bounded in [0,1] via normalized MAE: 1 - MAE/(range or std + eps), ensuring stable ACCURACY printing.