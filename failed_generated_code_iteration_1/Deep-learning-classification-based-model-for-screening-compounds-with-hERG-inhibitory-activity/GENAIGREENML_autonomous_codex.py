# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, mean_absolute_error

path = "herg_train_activity.csv"

def load_data(p):
    try:
        df_local = pd.read_csv(p)
    except Exception:
        df_local = pd.read_csv(p, sep=';', decimal=',')
    if df_local.shape[1] == 1:
        try:
            df_alt = pd.read_csv(p, sep=';', decimal=',')
            if df_alt.shape[1] > 1:
                df_local = df_alt
        except Exception:
            pass
    if df_local.shape[1] == 1:
        col0 = str(df_local.columns[0])
        if ';' in col0:
            try:
                df_alt = pd.read_csv(p, sep=';', decimal=',')
                if df_alt.shape[1] > 1:
                    df_local = df_alt
            except Exception:
                pass
    return df_local

df = load_data(path)

df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
df = df.loc[:, [c for c in df.columns if not str(c).lower().startswith('unnamed')]]

assert df.shape[0] > 0 and df.shape[1] > 0

expected_headers = ['a_acc', 'a_don', 'b_rotN', 'density', 'logP(o/w)', 'logS', 'rings', 'Weight', 'Activity_value']
expected_headers = [re.sub(r'\s+', ' ', str(c).strip()) for c in expected_headers]

target = None
for col in df.columns:
    col_lower = col.lower()
    if col_lower == 'activity_value' or col_lower == 'activity value':
        target = col
        break
if target is None:
    for col in df.columns:
        col_lower = col.lower()
        if col_lower in {'target', 'label', 'class', 'y'} or 'activity' in col_lower:
            target = col
            break
if target is None and expected_headers[-1] in df.columns:
    target = expected_headers[-1]
if target is None:
    best_col = None
    best_unique = -1
    for col in df.columns:
        series = pd.to_numeric(df[col], errors='coerce')
        nunique = series.nunique(dropna=True)
        if nunique > 1 and nunique > best_unique:
            best_unique = nunique
            best_col = col
    if best_col is not None:
        target = best_col
    else:
        target = df.columns[-1]

feature_cols = [c for c in df.columns if c != target]
if len(feature_cols) == 0:
    df['dummy_feature'] = 0.0
    feature_cols = ['dummy_feature']

X = df[feature_cols].copy()
y_raw = df[target].copy()

X = X.replace([np.inf, -np.inf], np.nan)
y_raw = y_raw.replace([np.inf, -np.inf], np.nan)

y_numeric = pd.to_numeric(y_raw, errors='coerce')
numeric_ratio = y_numeric.notna().mean()
if isinstance(numeric_ratio, float) and np.isnan(numeric_ratio):
    numeric_ratio = 0.0

task = None
y_temp = None
y_type_numeric = False

if y_raw.dtype.kind in 'biufc':
    unique_vals = y_numeric.dropna().unique()
    integer_like = unique_vals.size > 0 and np.all(np.isclose(unique_vals, np.round(unique_vals)))
    if integer_like and len(unique_vals) <= 20:
        task = 'classification'
        y_temp = y_numeric
        y_type_numeric = True
    else:
        task = 'regression'
        y_temp = y_numeric
else:
    if numeric_ratio >= 0.8:
        unique_vals = y_numeric.dropna().unique()
        integer_like = unique_vals.size > 0 and np.all(np.isclose(unique_vals, np.round(unique_vals)))
        if integer_like and len(unique_vals) <= 20:
            task = 'classification'
            y_temp = y_numeric
            y_type_numeric = True
        else:
            task = 'regression'
            y_temp = y_numeric
    else:
        task = 'classification'
        y_temp = y_raw
        y_type_numeric = False

valid_mask = pd.Series(y_temp).notna()
X = X.loc[valid_mask].reset_index(drop=True)
y = pd.Series(y_temp).loc[valid_mask].reset_index(drop=True)

if task == 'classification':
    if y_type_numeric:
        y = np.round(pd.to_numeric(y, errors='coerce')).astype(int)
    else:
        y = y.astype(str)

if task == 'classification':
    classes = pd.Series(y).unique()
    if len(classes) < 2:
        task = 'regression'
        y_numeric_fallback = pd.to_numeric(y, errors='coerce')
        if y_numeric_fallback.notna().sum() > 0:
            y = y_numeric_fallback.astype(float)
        else:
            y = pd.Series(pd.Categorical(y).codes).astype(float)

assert len(X) > 0

numeric_features = []
categorical_features = []
drop_cols = []
for col in X.columns:
    series = X[col]
    if series.dtype.kind in 'biufc':
        if series.notna().sum() == 0:
            drop_cols.append(col)
            continue
        numeric_features.append(col)
    else:
        coerced = pd.to_numeric(series, errors='coerce')
        if coerced.notna().sum() == 0:
            drop_cols.append(col)
            continue
        if coerced.notna().mean() >= 0.8:
            X[col] = coerced
            numeric_features.append(col)
        else:
            categorical_features.append(col)

if drop_cols:
    X = X.drop(columns=drop_cols)
    numeric_features = [c for c in numeric_features if c not in drop_cols]
    categorical_features = [c for c in categorical_features if c not in drop_cols]

if len(numeric_features) + len(categorical_features) == 0:
    X['dummy_feature'] = 0.0
    numeric_features = ['dummy_feature']
    categorical_features = []

for col in categorical_features:
    X[col] = X[col].astype(str)

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler(with_mean=False))
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
])

transformers = []
if len(numeric_features) > 0:
    transformers.append(('num', numeric_transformer, numeric_features))
if len(categorical_features) > 0:
    transformers.append(('cat', categorical_transformer, categorical_features))
preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')

if task == 'classification':
    model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[('preprocessor', preprocessor),
                     ('model', model)])

n_samples = len(X)
if n_samples > 1:
    test_size = 0.2 if n_samples >= 5 else 0.5
    stratify = None
    if task == 'classification':
        class_counts = pd.Series(y).value_counts()
        if len(class_counts) > 1 and class_counts.min() >= 2:
            stratify = y
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )
else:
    X_train = X.copy()
    y_train = y.copy()
    X_test = X.copy()
    y_test = y.copy()

assert len(X_train) > 0 and len(X_test) > 0

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

if task == 'classification':
    try:
        accuracy = accuracy_score(y_test, y_pred)
    except Exception:
        accuracy = 0.0
else:
    y_test_arr = np.array(y_test, dtype=float)
    y_pred_arr = np.array(y_pred, dtype=float)
    mask = ~np.isnan(y_test_arr)
    if mask.sum() == 0:
        accuracy = 0.0
    else:
        y_test_arr = y_test_arr[mask]
        y_pred_arr = y_pred_arr[mask]
        mae = mean_absolute_error(y_test_arr, y_pred_arr)
        y_range = np.max(y_test_arr) - np.min(y_test_arr)
        if y_range == 0:
            accuracy = 1.0 if mae < 1e-12 else 0.0
        else:
            accuracy = 1.0 - (mae / (y_range + 1e-12))
            if accuracy < 0:
                accuracy = 0.0
            elif accuracy > 1:
                accuracy = 1.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Used lightweight linear models (LogisticRegression/Ridge) to keep CPU usage low.
# Implemented minimal imputation, scaling, and one-hot encoding with ColumnTransformer for reproducibility and efficiency.
# Robust schema detection and coercion reduce preprocessing overhead while handling unknown inputs safely.
# Regression fallback uses MAE-to-range normalized accuracy in [0,1] to provide a stable proxy metric.