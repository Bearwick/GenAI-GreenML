# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score
from sklearn.dummy import DummyClassifier, DummyRegressor

data_path = "data.csv"

def read_dataset(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > df.shape[1]:
                df = df_alt
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    return df

def clean_column_name(name):
    name = str(name).strip()
    name = " ".join(name.split())
    return name

def add_constant_feature(df):
    const_col = "__constant__"
    while const_col in df.columns:
        const_col = "_" + const_col
    df[const_col] = 1.0
    return const_col

df = read_dataset(data_path)
df.columns = [clean_column_name(c) for c in df.columns]
df = df.loc[:, ~df.columns.str.match(r'^Unnamed', case=False, na=False)]
df = df.loc[:, [c for c in df.columns if c != '' and c is not None]]
df = df.replace(r'^\s*$', np.nan, regex=True)
df = df.replace([np.inf, -np.inf], np.nan)
df = df.dropna(axis=1, how='all')
assert not df.empty

candidate_targets = ['diagnosis', 'target', 'label', 'class', 'y', 'outcome']
lower_map = {c.lower(): c for c in df.columns}
target_col = None
for cand in candidate_targets:
    if cand in lower_map:
        target_col = lower_map[cand]
        break
if target_col is None:
    object_cols = [c for c in df.columns if df[c].dtype == object or pd.api.types.is_bool_dtype(df[c]) or pd.api.types.is_categorical_dtype(df[c])]
    obj_candidates = []
    for c in object_cols:
        uniq = df[c].nunique(dropna=True)
        if uniq > 1:
            obj_candidates.append((uniq, c))
    if obj_candidates:
        obj_candidates.sort(key=lambda x: x[0])
        target_col = obj_candidates[0][1]
    else:
        num_candidates = []
        for c in df.columns:
            uniq = pd.to_numeric(df[c], errors='coerce').nunique(dropna=True)
            if uniq > 1:
                num_candidates.append((uniq, c))
        if num_candidates:
            num_candidates.sort(key=lambda x: x[0])
            target_col = num_candidates[0][1]
        else:
            target_col = df.columns[-1]

df = df.loc[df[target_col].notna()].copy()
assert not df.empty

target_series = df[target_col]
target_numeric = pd.to_numeric(target_series, errors='coerce')
numeric_ratio = target_numeric.notna().mean() if len(target_numeric) > 0 else 0.0
if pd.api.types.is_numeric_dtype(target_series) or numeric_ratio >= 0.9:
    df[target_col] = target_numeric
    target_series = df[target_col]
    target_is_numeric = True
else:
    target_is_numeric = False

if not target_is_numeric:
    task = 'classification'
else:
    n_unique = target_series.nunique(dropna=True)
    if n_unique <= 20:
        vals = target_series.dropna().values
        if len(vals) > 0 and np.all(np.isclose(vals, np.round(vals))):
            task = 'classification'
        else:
            task = 'regression'
    else:
        task = 'regression'

if task == 'classification':
    if target_series.nunique(dropna=True) < 2:
        task = 'regression'
        if not target_is_numeric:
            df[target_col], _ = pd.factorize(df[target_col])
            df[target_col] = pd.to_numeric(df[target_col], errors='coerce')
            target_is_numeric = True
else:
    if not target_is_numeric:
        df[target_col], _ = pd.factorize(df[target_col])
        df[target_col] = pd.to_numeric(df[target_col], errors='coerce')
        target_is_numeric = True

if target_is_numeric:
    df = df.loc[pd.to_numeric(df[target_col], errors='coerce').notna()].copy()

assert not df.empty

features = [c for c in df.columns if c != target_col]

id_like = []
for c in features:
    cname = str(c).lower()
    if cname == 'id' or cname.endswith('id'):
        uniq_ratio = df[c].nunique(dropna=True) / max(len(df), 1)
        if uniq_ratio > 0.9:
            id_like.append(c)
features = [c for c in features if c not in id_like]

if not features:
    const_col = add_constant_feature(df)
    features = [const_col]

numeric_features = []
categorical_features = []
for c in features:
    s = df[c]
    if pd.api.types.is_numeric_dtype(s):
        numeric_features.append(c)
    else:
        coerced = pd.to_numeric(s, errors='coerce')
        non_nan_ratio = coerced.notna().mean()
        if non_nan_ratio >= 0.8:
            df[c] = coerced
            numeric_features.append(c)
        else:
            categorical_features.append(c)

numeric_features = [c for c in numeric_features if df[c].nunique(dropna=True) > 1]
categorical_features = [c for c in categorical_features if df[c].nunique(dropna=True) > 1]

if not numeric_features and not categorical_features:
    const_col = add_constant_feature(df)
    numeric_features = [const_col]
    categorical_features = []

for c in numeric_features:
    df[c] = pd.to_numeric(df[c], errors='coerce')

features = numeric_features + categorical_features
X = df[features]
y = df[target_col]

n_samples = len(df)
assert n_samples > 0

if n_samples < 2:
    X_train = X.copy()
    X_test = X.copy()
    y_train = y.copy()
    y_test = y.copy()
else:
    test_size = 0.2 if n_samples >= 5 else 1
    stratify = None
    if task == 'classification':
        class_counts = y.value_counts(dropna=True)
        if len(class_counts) > 1 and class_counts.min() >= 2 and n_samples >= 4:
            stratify = y
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )

assert len(X_train) > 0 and len(X_test) > 0

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler(with_mean=False))
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
])

transformers = []
if numeric_features:
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    transformers.append(('cat', categorical_transformer, categorical_features))
assert len(transformers) > 0

preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')

if task == 'classification':
    if y_train.nunique(dropna=True) < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear', random_state=42)
    clf = Pipeline(steps=[('preprocess', preprocessor),
                         ('model', model)])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
else:
    if y_train.nunique(dropna=True) < 2:
        model = DummyRegressor(strategy='mean')
    else:
        model = LinearRegression()
    reg = Pipeline(steps=[('preprocess', preprocessor),
                         ('model', model)])
    reg.fit(X_train, y_train)
    y_pred = reg.predict(X_test)
    y_test_arr = np.array(y_test, dtype=float)
    y_pred_arr = np.array(y_pred, dtype=float)
    mask = np.isfinite(y_test_arr) & np.isfinite(y_pred_arr)
    if mask.sum() == 0:
        accuracy = 0.0
    else:
        mae = np.mean(np.abs(y_test_arr[mask] - y_pred_arr[mask]))
        accuracy = 1.0 / (1.0 + mae)
    accuracy = float(np.clip(accuracy, 0.0, 1.0))

# Optimization Summary
# - Chose lightweight linear/logistic models and simple preprocessing for CPU efficiency.
# - Used ColumnTransformer with basic imputation and one-hot encoding to handle mixed schemas robustly.
# - Applied scaling without centering to preserve sparse efficiency and reduce compute.
# - Added fallbacks (dummy models or MAE-based bounded score) for stability on degenerate targets.
print(f"ACCURACY={accuracy:.6f}")