# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings("ignore")

# Robust CSV loading
file_candidates = [
    "heart/heart_dataset_.csv",
    "heart_dataset_.csv",
    "heart.csv",
    "heart/heart.csv",
]

df = None
for fpath in file_candidates:
    try:
        df = pd.read_csv(fpath)
        if df.shape[1] < 2:
            df = pd.read_csv(fpath, sep=';', decimal=',')
        break
    except Exception:
        continue

if df is None:
    import glob
    csv_files = glob.glob("**/*.csv", recursive=True)
    for fpath in csv_files:
        try:
            df = pd.read_csv(fpath)
            if df.shape[1] < 2:
                df = pd.read_csv(fpath, sep=';', decimal=',')
            if df.shape[1] >= 2:
                break
        except Exception:
            continue

assert df is not None and df.shape[0] > 0, "No valid CSV found."

# Normalize column names
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
df = df.loc[:, ~df.columns.str.startswith('Unnamed')]

# Expected schema
expected_features = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',
                     'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']
target_col = 'target'

# Check target column exists
if target_col not in df.columns:
    # Try to find a suitable target
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if len(numeric_cols) >= 2:
        target_col = numeric_cols[-1]
    else:
        target_col = df.columns[-1]

# Determine available feature columns
available_features = [c for c in expected_features if c in df.columns]
if len(available_features) == 0:
    available_features = [c for c in df.columns if c != target_col]

# Coerce all to numeric
for c in available_features + [target_col]:
    df[c] = pd.to_numeric(df[c], errors='coerce')

# Drop rows with NaN/inf in target or features
df = df.replace([np.inf, -np.inf], np.nan)
df = df.dropna(subset=available_features + [target_col])

assert df.shape[0] > 0, "Dataset empty after cleaning."

X = df[list(available_features)].values
y = df[target_col].values

# Determine if classification is viable
unique_classes = np.unique(y)
is_classification = len(unique_classes) >= 2 and len(unique_classes) <= 50

if is_classification:
    # Binarize if needed (original source treats 1=no disease, 2=disease; 
    # standard heart dataset uses 0/1; handle both)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=0, stratify=y if len(unique_classes) <= 30 else None
    )

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    # Lightweight pipeline: StandardScaler + LogisticRegression
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('clf', LogisticRegression(solver='lbfgs', max_iter=1000, random_state