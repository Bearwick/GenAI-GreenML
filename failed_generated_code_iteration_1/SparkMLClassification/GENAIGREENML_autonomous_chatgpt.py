# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler


warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)


DATASET_PATH = "diabetes.csv"
DATASET_HEADERS = [
    "Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin",
    "BMI", "DiabetesPedigreeFunction", "Age", "Outcome"
]


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = re.sub(r"\s+", " ", c2.strip())
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:.*", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _try_read_csv(path):
    df1 = None
    try:
        df1 = pd.read_csv(path)
    except Exception:
        df1 = None

    def looks_wrong(df):
        if df is None:
            return True
        if df.shape[1] <= 1:
            return True
        # If the first column name contains many commas, likely delimiter issue
        if df.columns.size == 1 and isinstance(df.columns[0], str) and ("," in df.columns[0]):
            return True
        return False

    if not looks_wrong(df1):
        return df1

    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        if df2 is not None and df2.shape[1] > 1:
            return df2
    except Exception:
        pass

    # Last resort: try python engine with automatic separator sniffing
    try:
        df3 = pd.read_csv(path, sep=None, engine="python")
        if df3 is not None and df3.shape[1] > 1:
            return df3
    except Exception:
        pass

    if df1 is None:
        raise RuntimeError("Failed to read dataset.")
    return df1


def _pick_target(df, expected_headers):
    cols_lower = {c.lower(): c for c in df.columns}
    # Prefer known target name if present
    if "outcome" in cols_lower:
        return cols_lower["outcome"]

    # Try other common target names
    for cand in ["target", "label", "y", "class"]:
        if cand in cols_lower:
            return cols_lower[cand]

    # Prefer non-constant numeric column with small number of unique values (classification-like)
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    best = None
    best_score = None
    for c in numeric_cols:
        s = df[c]
        s_non_nan = s.dropna()
        if s_non_nan.empty:
            continue
        nunique = int(s_non_nan.nunique())
        if nunique <= 1:
            continue
        # classification heuristic: few unique values relative to size
        score = (nunique, -len(s_non_nan))
        if best is None or score < best_score:
            best = c
            best_score = score
    if best is not None:
        return best

    # Fallback: last column
    return df.columns[-1]


def _coerce_numeric_columns(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    denom = np.var(y_true)
    if not np.isfinite(denom) or denom <= 1e-12:
        return 0.0
    mse = np.mean((y_true - y_pred) ** 2)
    score = 1.0 - (mse / denom)
    if not np.isfinite(score):
        return 0.0
    return float(np.clip(score, 0.0, 1.0))


# Load
df = _try_read_csv(DATASET_PATH)
df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)

# If file has no header row and got integer columns, try applying provided headers when lengths match
if all(isinstance(c, (int, np.integer)) for c in df.columns) and len(df.columns) == len(DATASET_HEADERS):
    df.columns = DATASET_HEADERS

# Ensure non-empty
assert df.shape[0] > 0 and df.shape[1] > 0

# Normalize again after potential header assignment
df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)

# Choose target robustly
target_col = _pick_target(df, DATASET_HEADERS)

# Build features list (avoid strict header mismatch)
feature_cols = [c for c in df.columns if c != target_col]
if not feature_cols:
    # If only one column, create a trivial feature from index
    df["_index_feature"] = np.arange(len(df), dtype=float)
    feature_cols = ["_index_feature"]

# Coerce numeric where possible (safe even for object; turns invalid to NaN)
df = _coerce_numeric_columns(df, df.columns.tolist())

# Drop rows where target is NaN (cannot learn)
df = df.dropna(subset=[target_col]).reset_index(drop=True)
assert df.shape[0] > 0

X = df[feature_cols].copy()
y = df[target_col].copy()

# Identify column types based on current dtypes
numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = [c for c in feature_cols if c not in numeric_features]

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=True, with_std=True)),
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Determine classification suitability
y_vals = y.dropna().values
unique_vals = pd.unique(y_vals)
unique_vals = [uv for uv in unique_vals if pd.notna(uv)]
n_unique = len(unique_vals)

is_classification = False
if n_unique >= 2:
    # Prefer classification if target is integer-like with small number of classes
    if pd.api.types.is_integer_dtype(y) or np.all(np.isclose(y_vals, np.round(y_vals), equal_nan=True)):
        if n_unique <= 20:
            is_classification = True
    # Common diabetes outcome is 0/1
    if set(unique_vals).issubset({0, 1, 0.0, 1.0}):
        is_classification = True

# Train/test split
if is_classification and n_unique >= 2:
    stratify = y if y.nunique(dropna=True) >= 2 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=stratify
    )
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

assert X_train.shape[0] > 0 and X_test.shape[0] > 0

if is_classification and y_train.nunique(dropna=True) >= 2:
    # Use a small, CPU-friendly linear model
    clf = LogisticRegression(
        solver="liblinear",
        max_iter=200,
        C=1.0,
    )
    model = Pipeline(steps=[("preprocess", preprocess), ("model", clf)])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Regression fallback: lightweight ridge regression; report bounded score in [0,1]
    reg = Ridge(alpha=1.0, random_state=42)
    model = Pipeline(steps=[("preprocess", preprocess), ("model", reg)])
    model.fit(X_train, y_train.astype(float))
    y_pred = model.predict(X_test)
    accuracy = _bounded_regression_score(y_test.astype(float), y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Uses pandas+scikit-learn with simple train/test split for reproducibility (random_state=42), avoiding costly CV by default.
# - Robust CSV loading: tries default read_csv first, then retries with ';' separator and ',' decimal to handle locale variants.
# - Defensive schema handling: normalizes column names, drops 'Unnamed' columns, and auto-selects a reasonable target if headers differ.
# - Lightweight preprocessing via ColumnTransformer: median imputation + StandardScaler for numerics; most-frequent + OneHotEncoder for categoricals.
# - CPU-friendly baseline model: LogisticRegression (liblinear) for classification; Ridge regression fallback if target is unsuitable for classification.
# - Regression fallback metric: bounded R2-like proxy clipped to [0,1] to satisfy ACCURACY output requirement while remaining stable.