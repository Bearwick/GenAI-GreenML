# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import chi2, SelectFpr
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, accuracy_score
from sklearn.model_selection import train_test_split

SEED = 12345

def load_data(path="diabetes.csv"):
    try:
        df = pd.read_csv(path)
        if df.shape[1] < 2:
            raise ValueError("Too few columns")
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df

def diabetes():
    np.random.seed(SEED)
    df = load_data()

    expected_cols = ["Pregnancies", "Glucose", "BloodPressure", "SkinThickness",
                     "Insulin", "BMI", "DiabetesPedigreeFunction", "Age", "Outcome"]
    col_map = {c.strip(): c.strip() for c in df.columns}
    df.columns = [c.strip() for c in df.columns]

    zero_replace_cols = ["Glucose", "BloodPressure", "SkinThickness", "BMI", "Insulin"]
    for col in zero_replace_cols:
        df[col] = df[col].replace(0, np.nan)

    imputer = SimpleImputer(strategy="mean")
    df[zero_replace_cols] = imputer.fit_transform(df[zero_replace_cols])

    feature_cols = [c for c in df.columns if c != "Outcome"]
    X = df[feature_cols].values
    y = df["Outcome"].values

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=SEED
    )

    dataset_size = float(len(y_train))
    num_positives = float(np.sum(y_train == 1))
    num_negatives = dataset_size - num_positives
    balancing_ratio = num_negatives / dataset_size

    sample_weights = np.where(y_train == 1, balancing_ratio, 1.0 - balancing_ratio)

    selector = SelectFpr(chi2, alpha=0.05)
    X_train_abs = np.abs(X_train)
    X_test_abs = np.abs(X_test)
    X_train_sel = selector.fit_transform(X_train_abs, y_train)
    X_test_sel = selector.transform(X_test_abs)

    X_train_sel_signed = X_train[:, selector.get_support()]
    X_test_sel_signed = X_test[:, selector.get_support()]

    lr = LogisticRegression(max_iter=10, random_state=SEED, solver="lbfgs")
    lr.fit(X_train_sel_signed, y_train, sample_weight=sample_weights)

    y_pred_test = lr.predict(X_test_sel_signed)
    y_prob_test = lr.predict_proba(X_test_sel_signed)[:, 1]

    accuracy = accuracy_score(y_test, y_pred_test)
    auc_test = roc_auc_score(y_test, y_prob_test)

    print(f"ACCURACY={accuracy:.6f}")

def main():
    diabetes()

if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced PySpark with scikit-learn and pandas to eliminate JVM startup overhead and distributed computing overhead for a small dataset, drastically reducing energy consumption and runtime.
# - Removed all print statements, plots, and intermediate .show() calls that served no computational purpose.
# - Removed redundant imports and unused code paths (