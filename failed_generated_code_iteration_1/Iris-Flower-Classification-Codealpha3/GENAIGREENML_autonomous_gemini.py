# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score

def load_dataset(path):
    # Robust CSV parsing logic
    try:
        df = pd.read_csv(path)
        # Check if delimiter is likely semicolon based on column count
        if df.shape[1] <= 1:
            df = pd.read_csv(path, sep=';', decimal=',')
    except Exception:
        # Fallback for extreme cases
        df = pd.DataFrame()
    
    if df.empty:
        return df

    # Normalize column names: strip, remove internal whitespace, drop 'Unnamed'
    df.columns = [str(c).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    
    # Drop trivial ID columns
    id_cols = [c for c in df.columns if c.lower() in ['id', 'index', 'idx']]
    if id_cols:
        df = df.drop(columns=id_cols)
        
    return df

def solve():
    dataset_path = 'Iris.csv'
    df = load_dataset(dataset_path)
    
    if df.empty:
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Identify potential target (last column or 'Species')
    target_candidates = [c for c in df.columns if 'species' in c.lower() or 'target' in c.lower()]
    target_col = target_candidates[0] if target_candidates else df.columns[-1]
    
    y_raw = df[target_col]
    X_raw = df.drop(columns=[target_col])

    # Pre-clean features: keep only numeric or convertible to numeric
    for col in X_raw.columns:
        X_raw[col] = pd.to_numeric(X_raw[col], errors='coerce')
    
    # Drop columns that are entirely NaN after coercion
    X_raw = X_raw.dropna(axis=1, how='all')
    
    if X_raw.empty:
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Ensure y is categorical for classification
    le = LabelEncoder()
    try:
        y = le.fit_transform(y_raw.fillna(method='ffill').astype(str))
        is_regression = False
    except:
        # Trivial fallback for weird target types
        y = y_raw.fillna(0).values
        is_regression = True

    # Check for enough classes
    if not is_regression and len(np.unique(y)) < 2:
        print(f"ACCURACY={1.000000:.6f}")
        return

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X_raw, y, test_size=0.2, random_state=42, stratify=y if not is_regression else None
    )

    # Pipeline: SimpleImputer (Mean) -> Scaler -> Lightweight Logistic Regression
    # Logistic Regression is highly efficient for O(N) training/inference
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler()),
        ('clf', LogisticRegression(max_iter=200, solver='lbfgs', multi_class='auto', random_state=42))
    ])

    try:
        pipeline.fit(X_train, y_train)
        preds = pipeline.predict(X_test)
        
        if is_regression:
            # Proxy accuracy for regression fallback: 1 - Normalized Mean Absolute Error
            # Bound it [0, 1]
            mae = np.mean(np.abs(preds - y_test))
            denom = np.mean(np.abs(y_test)) if np.mean(np.abs(y_test)) != 0 else 1.0
            accuracy = max(0, 1 - (mae / denom))
        else:
            accuracy = accuracy_score(y_test, preds)
    except:
        accuracy = 0.0

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# Optimization Summary:
# 1. Model Choice: Logistic Regression was selected as it provides a convex optimization problem 
#    that converges quickly on small datasets like Iris, minimizing CPU cycles and energy usage.
# 2. Preprocessing: Standard Scaling is used to ensure the gradient-based solver converges in 
#    fewer iterations, further reducing computational time.
# 3. Memory Management: Robust schema detection avoids loading redundant ID columns or string 
#    data that doesn't contribute to the predictive power of the model.
# 4. CPU-Friendliness: No multi-threading or GPU-acceleration is required, making the solution 
#    suitable for low-power environment deployments.
# 5. Robustness: Defensive checks for empty datasets and missing columns ensure the script 
#    runs end-to-end without manual intervention, saving development and debug energy.
# 6. Accuracy Proxy: In the rare event the target is treated as regression, a bounded 1-NMAE 
#    score is used to provide a consistent [0,1] metric output.