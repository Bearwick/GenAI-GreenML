# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import os
import glob
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")

def find_csv():
    possible_dirs = ['.', '/home/oai/share', '/home/oai/share/data', '/home/oai/share/datasets']
    files = []
    for d in possible_dirs:
        if os.path.isdir(d):
            files.extend(glob.glob(os.path.join(d, '*.csv')))
            files.extend(glob.glob(os.path.join(d, '*.CSV')))
    if not files:
        return None
    for f in files:
        if os.path.basename(f).lower() == 'iris.csv':
            return f
    for f in files:
        if 'iris' in os.path.basename(f).lower():
            return f
    return files[0]

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
        return df
    try_alt = False
    if df.shape[1] == 1:
        try_alt = True
    else:
        if any(pd.Series(df.columns).astype(str).str.contains(';')):
            try_alt = True
    if try_alt:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > 1:
                df = df_alt
        except Exception:
            pass
    return df

csv_path = find_csv()
if csv_path is None:
    raise FileNotFoundError("No CSV file found.")
df = read_csv_robust(csv_path)

df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
mask = ~pd.Series(df.columns).astype(str).str.contains(r'^Unnamed', case=False, na=False)
df = df.loc[:, mask.values]
df = df.dropna(axis=1, how='all')
df = df.replace([np.inf, -np.inf], np.nan)

assert df.shape[0] > 0 and df.shape[1] > 0

cols = list(df.columns)
lower_map = {c.lower(): c for c in cols}
preferred_keys = ['species', 'target', 'label', 'class', 'y']
target_col = None
for key in preferred_keys:
    for lc, orig in lower_map.items():
        if lc == key or lc.endswith('_' + key) or lc.startswith(key + '_'):
            target_col = orig
            break
    if target_col:
        break
if target_col is None:
    candidates = []
    for col in cols:
        if col.lower() in ['id', 'index']:
            continue
        n_unique = df[col].nunique(dropna=True)
        if n_unique > 1:
            candidates.append((n_unique, col))
    if candidates:
        obj_candidates = [(df[col].nunique(dropna=True), col) for col in cols if df[col].dtype == object and df[col].nunique(dropna=True) > 1 and col.lower() not in ['id', 'index']]
        if obj_candidates:
            target_col = sorted(obj_candidates, key=lambda x: x[0])[0][1]
        else:
            target_col = sorted(candidates, key=lambda x: x[0])[0][1]
    else:
        target_col = cols[-1]

df[target_col] = df[target_col].replace(r'^\s*$', np.nan, regex=True)
df = df.dropna(subset=[target_col])

feature_cols = [c for c in df.columns if c != target_col]
feature_cols = [c for c in feature_cols if c.lower() not in ['id', 'index']]
if not feature_cols:
    df['__constant__'] = 1.0
    feature_cols = ['__constant__']

X_df = df[feature_cols].copy()
y_raw = df[target_col].copy()

is_obj = y_raw.dtype == object or str(y_raw.dtype).startswith('category')
if is_obj:
    y_num = pd.to_numeric(y_raw, errors='coerce')
    if y_num.notna().mean() > 0.8:
        y_raw = y_num
        is_obj = False

n_unique = y_raw.nunique(dropna=True)
if is_obj:
    task = 'classification' if n_unique >= 2 else 'regression'
else:
    threshold = max(20, int(0.2 * len(y_raw)))
    if n_unique <= threshold:
        task = 'classification' if n_unique >= 2 else 'regression'
    else:
        task = 'regression'

if task == 'classification':
    if is_obj:
        y_encoded, _ = pd.factorize(y_raw.astype(str), sort=True)
        y = pd.Series(y_encoded, index=y_raw.index)
    else:
        y = pd.to_numeric(y_raw, errors='coerce')
    mask = y.notna()
    X_df = X_df.loc[mask].copy()
    y = y.loc[mask]
    if y.nunique() < 2:
        task = 'regression'
else:
    y = pd.to_numeric(y_raw, errors='coerce')
    mask = y.notna()
    X_df = X_df.loc[mask].copy()
    y = y.loc[mask]

assert len(X_df) > 0 and len(y) > 0

numeric_cols = []
categorical_cols = []
for col in X_df.columns:
    col_data = X_df[col]
    if col_data.dtype.kind in 'bifc':
        X_df[col] = pd.to_numeric(col_data, errors='coerce')
        numeric_cols.append(col)
    else:
        coerced = pd.to_numeric(col_data, errors='coerce')
        if coerced.notna().mean() >= 0.5:
            X_df[col] = coerced
            numeric_cols.append(col)
        else:
            categorical_cols.append(col)
            X_df[col] = col_data.astype(str)

numeric_cols = [c for c in numeric_cols if X_df[c].notna().any()]
categorical_cols = [c for c in categorical_cols if X_df[c].notna().any()]

if not numeric_cols and not categorical_cols:
    X_df['__constant__'] = 1.0
    numeric_cols = ['__constant__']

transformers = []
if numeric_cols:
    transformers.append(('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_cols))
if categorical_cols:
    transformers.append(('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))]), categorical_cols))

preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')

if task == 'classification':
    model = LogisticRegression(max_iter=200, n_jobs=1)
else:
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])

n_samples = len(y)
test_size = 0.2
if n_samples * test_size < 1:
    test_size = max(0.2, 1.0 / n_samples)
stratify = None
if task == 'classification':
    counts = y.value_counts()
    if y.nunique() > 1 and counts.min() >= 2:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=test_size, random_state=42, stratify=stratify)
assert len(X_train) > 0 and len(X_test) > 0

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    try:
        r2 = r2_score(y_test, y_pred)
    except Exception:
        r2 = np.nan
    if not np.isfinite(r2):
        mae = np.mean(np.abs(y_test - y_pred))
        range_y = np.nanmax(y_test) - np.nanmin(y_test)
        if not np.isfinite(range_y) or range_y == 0:
            accuracy = 1.0 if mae == 0 else 0.0
        else:
            accuracy = 1 - mae / range_y
            accuracy = max(0.0, min(1.0, accuracy))
    else:
        accuracy = (r2 + 1) / 2
        accuracy = max(0.0, min(1.0, accuracy))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Used lightweight linear models (logistic or ridge) to keep CPU usage low while providing a solid baseline.
# Employed a minimal ColumnTransformer with simple imputation, scaling, and optional one-hot encoding for reproducibility.
# Implemented robust CSV parsing and schema inference to handle unknown headers without expensive processing.
# For regression fallback, mapped R2 into [0,1] to provide a stable ACCURACY proxy when classification is not applicable.