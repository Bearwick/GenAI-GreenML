# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, FunctionTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression, SGDRegressor, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, r2_score

DATASET_PATH = "emails.csv"
DATASET_HEADERS = ["text", "spam"]

try:
    df = pd.read_csv(DATASET_PATH)
    if df.shape[1] <= 1:
        raise ValueError("Bad parse")
except Exception:
    df = pd.read_csv(DATASET_PATH, sep=";", decimal=",")

def normalize_col(c):
    c = str(c).strip()
    c = " ".join(c.split())
    return c

normalized_cols = [normalize_col(c) for c in df.columns]
new_cols = []
counts = {}
for c in normalized_cols:
    if c.lower().startswith("unnamed"):
        new_cols.append(None)
        continue
    if c in counts:
        counts[c] += 1
        new_cols.append(f"{c}_{counts[c]}")
    else:
        counts[c] = 0
        new_cols.append(c)
df = df.loc[:, [i for i, c in enumerate(new_cols) if c is not None]]
df.columns = [c for c in new_cols if c is not None]

assert not df.empty

def choose_target(df, headers):
    col_lower_map = {col.lower(): col for col in df.columns}
    candidates = []
    for name in ["spam", "target", "label", "y", "class"]:
        key = name.lower()
        if key in col_lower_map and col_lower_map[key] not in candidates:
            candidates.append(col_lower_map[key])
    for name in headers:
        key = str(name).strip().lower()
        if key in col_lower_map and col_lower_map[key] not in candidates:
            candidates.append(col_lower_map[key])
    for col in candidates:
        if df[col].nunique(dropna=True) > 1:
            return col
    if candidates:
        return candidates[0]
    for col in df.columns:
        s = pd.to_numeric(df[col], errors="coerce")
        if s.notna().sum() > 0 and s.nunique(dropna=True) > 1:
            return col
    return df.columns[-1] if len(df.columns) > 0 else None

target_col = choose_target(df, DATASET_HEADERS)
if target_col is None or target_col not in df.columns:
    target_col = df.columns[-1]

y = df[target_col]
X = df.drop(columns=[target_col], errors="ignore")

mask = y.notna()
X = X.loc[mask].copy()
y = y.loc[mask].copy()

assert len(y) > 0

if X.shape[1] == 0:
    X = pd.DataFrame({"const": np.ones(len(y))}, index=y.index)

n_samples = len(y)
accuracy = None

if n_samples < 2:
    accuracy = 1.0
else:
    y_non_null = y.dropna()
    n_unique = y_non_null.nunique()
    if n_unique <= 20:
        task = "classification"
    else:
        task = "regression"
    if task == "regression":
        y = pd.to_numeric(y, errors="coerce")
        mask = y.notna()
        X = X.loc[mask].copy()
        y = y.loc[mask].copy()
        n_samples = len(y)
        if n_samples < 2:
            accuracy = 1.0
    if accuracy is None:
        def detect_columns(X):
            X = X.copy()
            obj_cols = [c for c in X.columns if pd.api.types.is_object_dtype(X[c]) or pd.api.types.is_string_dtype(X[c]) or pd.api.types.is_categorical_dtype(X[c])]
            numeric_like_cols = []
            text_cols = []
            cat_cols = []
            for col in obj_cols:
                s = X[col]
                s_numeric = pd.to_numeric(s, errors="coerce")
                numeric_ratio = s_numeric.notna().mean()
                if numeric_ratio > 0.9:
                    numeric_like_cols.append(col)
                    X[col] = s_numeric
                    continue
                avg_len = s.astype(str).str.len().mean()
                uniq = s.nunique(dropna=True)
                if avg_len > 30 or uniq > 30:
                    text_cols.append(col)
                else:
                    cat_cols.append(col)
            numeric_cols = [c for c in X.columns if c not in obj_cols] + numeric_like_cols
            for col in numeric_cols:
                X[col] = pd.to_numeric(X[col], errors="coerce")
                X[col] = X[col].replace([np.inf, -np.inf], np.nan)
            numeric_cols = [c for c in numeric_cols if X[c].notna().sum() > 0]
            if len(text_cols) > 1:
                X["combined_text"] = X[text_cols].astype(str).agg(" ".join, axis=1)
                X.drop(columns=text_cols, inplace=True)
                text_cols = ["combined_text"]
            return X, text_cols, cat_cols, numeric_cols

        X, text_cols, cat_cols, numeric_cols = detect_columns(X)

        if len(text_cols) == 0 and len(cat_cols) == 0 and len(numeric_cols) == 0:
            X = pd.DataFrame({"const": np.ones(len(y))}, index=y.index)
            text_cols = []
            cat_cols = []
            numeric_cols = ["const"]

        has_text = len(text_cols) > 0
        has_cat = len(cat_cols) > 0
        has_num = len(numeric_cols) > 0
        has_sparse = has_text or has_cat

        if task == "classification":
            if y.nunique(dropna=True) < 2:
                model = DummyClassifier(strategy="most_frequent")
                model_hint = "linear"
            else:
                if has_text:
                    model = MultinomialNB()
                    model_hint = "nb"
                else:
                    model = LogisticRegression(max_iter=200, solver="liblinear")
                    model_hint = "linear"
        else:
            if y.nunique(dropna=True) < 2:
                model = DummyRegressor(strategy="mean")
                model_hint = "linear"
            else:
                if has_sparse:
                    model = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)
                    model_hint = "sgd"
                else:
                    model = Ridge(alpha=1.0)
                    model_hint = "linear"

        transformers = []
        if has_text:
            text_pipeline = Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="constant", fill_value="")),
                ("flatten", FunctionTransformer(lambda x: x.ravel(), validate=False)),
                ("vectorizer", CountVectorizer(max_features=2000))
            ])
            transformers.append(("text", text_pipeline, text_cols))
        if has_cat:
            cat_pipeline = Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
                ("onehot", OneHotEncoder(handle_unknown="ignore"))
            ])
            transformers.append(("cat", cat_pipeline, cat_cols))
        if has_num:
            num_steps = [("imputer", SimpleImputer(strategy="median"))]
            if model_hint == "nb":
                num_steps.append(("scaler", MinMaxScaler()))
            elif model_hint in ["linear", "sgd"]:
                num_steps.append(("scaler", StandardScaler(with_mean=not has_sparse)))
            num_pipeline = Pipeline(steps=num_steps)
            transformers.append(("num", num_pipeline, numeric_cols))
        preprocessor = ColumnTransformer(transformers, remainder="drop", sparse_threshold=0.3)

        test_size = 0.2 if n_samples >= 5 else 0.5
        stratify = None
        if task == "classification" and y.nunique(dropna=True) > 1:
            counts = y.value_counts()
            if counts.min() >= 2:
                stratify = y
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=stratify
        )
        assert len(X_train) > 0 and len(X_test) > 0

        clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        preds = clf.predict(X_test)

        if task == "classification":
            accuracy = accuracy_score(y_test, preds)
        else:
            if len(y_test) < 2:
                denom = np.mean(np.abs(y_test))
                if denom == 0:
                    accuracy = 1.0 if np.mean(np.abs(y_test - preds)) == 0 else 0.0
                else:
                    accuracy = max(0.0, 1.0 - np.mean(np.abs(y_test - preds)) / (denom + 1e-9))
            else:
                r2 = r2_score(y_test, preds)
                accuracy = float(np.clip(r2, 0.0, 1.0))

if accuracy is None:
    accuracy = 0.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used a capped CountVectorizer and simple linear/Naive Bayes models to minimize compute while handling text effectively.
# - Built a reproducible ColumnTransformer pipeline with lightweight imputers and one-hot encoding for robust preprocessing.
# - Chose sparse-friendly models and minimal scaling; regression accuracy is a clipped R2 or normalized error proxy within [0,1].
# - Combined multiple text fields into one to reduce dimensionality and processing overhead.