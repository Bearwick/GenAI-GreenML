# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import json
import csv
import random
from typing import List, Dict, Any, Optional

import numpy as np
import torch
from transformers import pipeline


SEED = 42


def _set_reproducibility(seed: int = SEED) -> None:
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    try:
        torch.use_deterministic_algorithms(True)
    except Exception:
        pass


class IntentDetector:
    def __init__(self) -> None:
        self.intent_options = (
            "Book Appointment",
            "Product Inquiry",
            "Pricing Negotiation",
            "Support Request",
            "Follow-Up",
        )

        device = 0 if torch.cuda.is_available() else -1
        dtype = torch.float16 if torch.cuda.is_available() else torch.float32

        self.intent_pipeline = pipeline(
            task="zero-shot-classification",
            model="cross-encoder/nli-distilroberta-base",
            device=device,
            torch_dtype=dtype,
        )

    def classify_intent(self, dialogue: str) -> Dict[str, str]:
        classification = self.intent_pipeline(dialogue, self.intent_options)
        top_intent = classification["labels"][0]
        rationale = f"Based on the conversation, the customer is likely interested in '{top_intent.lower()}'."
        return {"predicted_intent": top_intent, "rationale": rationale}


def create_conversation(messages: List[Dict[str, Any]], max_messages: Optional[int] = None) -> str:
    if max_messages is not None and len(messages) > max_messages:
        messages = messages[-max_messages:]
    return "\n".join(
        f"{(m.get('sender') or '').capitalize()}: {m.get('text') or ''}" for m in messages
    )


def _safe_load_json(path: str) -> List[Dict[str, Any]]:
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    if isinstance(data, list):
        return data
    if isinstance(data, dict):
        return [data]
    return []


def predict_intents(
    input_file: str,
    json_output: str,
    csv_output: str,
    intent_model: IntentDetector,
) -> float:
    conversations = _safe_load_json(input_file)

    output_data: List[Dict[str, Any]] = []
    correct = 0
    total = 0

    for entry in conversations:
        conv_id = entry.get("conversation_id")
        formatted_text = create_conversation(entry.get("messages") or [])
        intent_result = intent_model.classify_intent(formatted_text)

        pred = intent_result["predicted_intent"]
        output_data.append(
            {
                "conversation_id": conv_id,
                "predicted_intent": pred,
                "rationale": intent_result["rationale"],
            }
        )

        true_label = entry.get("intent") or entry.get("label") or entry.get("ground_truth") or entry.get("true_intent")
        if true_label is not None:
            total += 1
            if str(true_label).strip() == str(pred).strip():
                correct += 1

    os.makedirs(os.path.dirname(json_output) or ".", exist_ok=True)

    with open(json_output, "w", encoding="utf-8") as jf:
        json.dump(output_data, jf, indent=2, ensure_ascii=False)

    with open(csv_output, "w", newline="", encoding="utf-8") as cf:
        fieldnames = ["conversation_id", "predicted_intent", "rationale"]
        writer = csv.DictWriter(cf, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(output_data)

    accuracy = (correct / total) if total else 0.0
    return accuracy


def main() -> None:
    _set_reproducibility(SEED)
    intent_model = IntentDetector()
    accuracy = predict_intents(
        input_file="data/input.json",
        json_output="data/output/predictions.json",
        csv_output="data/output/predictions.csv",
        intent_model=intent_model,
    )
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused imports and unused preprocessing (emoji/re/cleaning) to avoid extra dependencies and CPU work while preserving outputs (original code never applied cleaning).
# - Reused a single pipeline instance and stored intent labels as an immutable tuple to reduce repeated allocations.
# - Selected device automatically (GPU if available) and used float16 on GPU to reduce energy and latency without changing the predicted label logic.
# - Ensured reproducibility by setting fixed seeds across random/NumPy/PyTorch and enabling deterministic algorithms when available.
# - Reduced intermediate data movement by formatting conversations with a generator expression and avoiding temporary lists beyond the required output records.
# - Added lightweight accuracy computation using any available ground-truth field (if present) without affecting prediction outputs or file formats.