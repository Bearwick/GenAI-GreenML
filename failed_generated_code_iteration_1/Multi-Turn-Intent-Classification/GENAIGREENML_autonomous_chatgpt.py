# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import json
import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


RANDOM_STATE = 42


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [re.sub(r"\s+", " ", str(c).strip()) for c in df.columns]
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", c)]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _read_csv_robust(path: str) -> pd.DataFrame:
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError("Likely wrong separator; retrying.")
        return df
    except Exception:
        return pd.read_csv(path, sep=";", decimal=",")


def _flatten_json_to_df(obj) -> pd.DataFrame:
    if isinstance(obj, list):
        if len(obj) == 0:
            return pd.DataFrame()
        if all(isinstance(x, dict) for x in obj):
            return pd.json_normalize(obj)
        return pd.DataFrame({"value": obj})
    if isinstance(obj, dict):
        for key in ["data", "items", "records", "conversations", "rows", "examples", "samples", "inputs"]:
            if key in obj and isinstance(obj[key], list):
                return _flatten_json_to_df(obj[key])
        return pd.json_normalize(obj)
    return pd.DataFrame({"value": [obj]})


def _extract_text_from_row(row: pd.Series) -> str:
    parts = []
    for v in row.values:
        if v is None:
            continue
        if isinstance(v, (list, tuple)):
            for item in v:
                if isinstance(item, (dict, list, tuple)):
                    parts.append(json.dumps(item, ensure_ascii=False))
                else:
                    s = str(item).strip()
                    if s:
                        parts.append(s)
        elif isinstance(v, dict):
            parts.append(json.dumps(v, ensure_ascii=False))
        else:
            s = str(v).strip()
            if s and s.lower() not in ("nan", "none"):
                parts.append(s)
    text = " ".join(parts)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def _pick_target_column(df: pd.DataFrame):
    cols = list(df.columns)
    if len(cols) == 0:
        return None

    # Prefer explicit label-like names
    preferred = [
        "label", "intent", "target", "y", "class", "category", "categories", "tag", "annotation", "annotations"
    ]
    lower_map = {c.lower(): c for c in cols}
    for p in preferred:
        if p in lower_map:
            return lower_map[p]

    # Else: choose a good categorical target (few unique, non-constant)
    best_col = None
    best_score = -1
    n = len(df)
    for c in cols:
        s = df[c]
        nun = s.nunique(dropna=True)
        if nun <= 1:
            continue
        # Avoid high-cardinality ids for classification targets
        ratio = nun / max(n, 1)
        score = 0
        if ratio <= 0.5:
            score += 2
        if ratio <= 0.2:
            score += 2
        if s.dtype == "object":
            score += 1
        if score > best_score:
            best_score = score
            best_col = c
    if best_col is not None:
        return best_col

    # Fallback: choose a numeric non-constant column for regression
    for c in cols:
        sn = pd.to_numeric(df[c], errors="coerce")
        if sn.nunique(dropna=True) > 1:
            return c
    return cols[-1]


def _prepare_xy(df: pd.DataFrame, target_col: str):
    df = df.copy()
    if target_col not in df.columns:
        target_col = _pick_target_column(df)

    if target_col is None or target_col not in df.columns:
        # Build a trivial dataset from any available content
        X_text = df.apply(_extract_text_from_row, axis=1) if len(df) else pd.Series([], dtype=str)
        y = pd.Series(np.zeros(len(df), dtype=int))
        return X_text, y, True, "text_only_trivial"

    y_raw = df[target_col]
    X_df = df.drop(columns=[target_col], errors="ignore")

    # Decide task type
    y_num = pd.to_numeric(y_raw, errors="coerce")
    is_numeric_target = y_num.notna().mean() >= 0.9  # mostly numeric

    if not is_numeric_target:
        y = y_raw.astype(str).fillna("NA")
        # Build a text column if needed
        if X_df.shape[1] == 0:
            X_text = df.drop(columns=[target_col], errors="ignore").apply(_extract_text_from_row, axis=1)
            return X_text, y, True, "classification_text"
        return X_df, y, True, "classification_tabular_or_mixed"

    y = y_num
    if X_df.shape[1] == 0:
        X_text = df.drop(columns=[target_col], errors="ignore").apply(_extract_text_from_row, axis=1)
        return X_text, y, False, "regression_text"
    return X_df, y, False, "regression_tabular_or_mixed"


def _build_model_for_text(is_classification: bool):
    if is_classification:
        clf = LogisticRegression(max_iter=400, solver="liblinear", random_state=RANDOM_STATE)
        return Pipeline(
            steps=[
                ("tfidf", TfidfVectorizer(lowercase=True, max_features=20000, ngram_range=(1, 2))),
                ("clf", clf),
            ]
        )
    reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
    return Pipeline(
        steps=[
            ("tfidf", TfidfVectorizer(lowercase=True, max_features=20000, ngram_range=(1, 2))),
            ("reg", reg),
        ]
    )


def _build_model_for_tabular(X: pd.DataFrame, is_classification: bool):
    Xc = X.copy()
    Xc = _normalize_columns(Xc)

    numeric_cols = []
    categorical_cols = []

    for c in Xc.columns:
        s = Xc[c]
        # detect numeric by coercion
        sn = pd.to_numeric(s, errors="coerce")
        if sn.notna().mean() >= 0.9:
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if is_classification:
        model = LogisticRegression(max_iter=400, solver="liblinear", random_state=RANDOM_STATE)
        return Pipeline(steps=[("preprocess", preprocessor), ("clf", model)])
    model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
    return Pipeline(steps=[("preprocess", preprocessor), ("reg", model)])


def _safe_accuracy_proxy_regression(y_true, y_pred) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    denom = np.var(y_true)
    if not np.isfinite(denom) or denom <= 1e-12:
        return 0.0
    r2 = 1.0 - (np.mean((y_true - y_pred) ** 2) / denom)
    r2 = float(np.clip(r2, -1.0, 1.0))
    return float((r2 + 1.0) / 2.0)


def main():
    path = "data/input.json"
    if not os.path.exists(path):
        # fallback to common alternatives
        for alt in ["data/input.csv", "data/input.tsv", "data/input.jsonl"]:
            if os.path.exists(alt):
                path = alt
                break

    if path.endswith(".csv") or path.endswith(".tsv"):
        df = _read_csv_robust(path)
    elif path.endswith(".jsonl"):
        rows = []
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    rows.append(json.loads(line))
                except Exception:
                    rows.append({"text": line})
        df = _flatten_json_to_df(rows)
    else:
        with open(path, "r", encoding="utf-8") as f:
            obj = json.load(f)
        df = _flatten_json_to_df(obj)

    df = _normalize_columns(df)

    # If df is a single column with nested structures, attempt to expand
    if df.shape[1] == 1:
        col = df.columns[0]
        if df[col].apply(lambda x: isinstance(x, (dict, list))).any():
            try:
                expanded = pd.json_normalize(df[col])
                if expanded.shape[1] > 1:
                    df = expanded
                    df = _normalize_columns(df)
            except Exception:
                pass

    assert df is not None and isinstance(df, pd.DataFrame)
    assert len(df) > 0, "Dataset is empty after loading."

    target_col = _pick_target_column(df)
    X, y, is_classification, mode = _prepare_xy(df, target_col)

    # Handle degenerate classification target
    if is_classification:
        y_series = pd.Series(y)
        if y_series.nunique(dropna=True) < 2:
            is_classification = False  # fallback to regression proxy
            y = pd.to_numeric(y_series, errors="coerce").fillna(0.0)

    # Split
    if is_classification:
        stratify = y if pd.Series(y).nunique(dropna=True) >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE
        )

    assert len(y_train) > 0 and len(y_test) > 0, "Train/test split failed."

    # Build pipeline
    if isinstance(X_train, pd.Series) or mode.endswith("_text") or (not hasattr(X_train, "columns")):
        model = _build_model_for_text(is_classification)
    else:
        model = _build_model_for_tabular(X_train, is_classification)

    # Fit & evaluate
    if is_classification:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Regression + bounded proxy score
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _safe_accuracy_proxy_regression(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight scikit-learn models (LogisticRegression liblinear / Ridge) for CPU-efficient baselines.
# - Robust schema handling: infers target, supports JSON/JSONL/CSV with fallback separators, normalizes headers, drops 'Unnamed' columns.
# - Minimal preprocessing with ColumnTransformer + imputers + one-hot encoding; avoids heavy feature engineering.
# - Text handling uses TF-IDF with capped max_features to limit memory/compute; no deep learning/embeddings.
# - Defensive checks prevent empty datasets/splits; if classification target is degenerate, falls back to regression and reports a bounded R2-based proxy in [0,1].