# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings("ignore")

# Robust CSV loading
df = None
for sep, decimal in [(',', '.'), (';', ','), ('\t', '.')]:
    try:
        df = pd.read_csv('Cancer_Data.csv', sep=sep, decimal=decimal)
        if df.shape[1] > 2:
            break
    except Exception:
        continue

if df is None or df.shape[1] <= 2:
    try:
        df = pd.read_csv('Cancer_Data.csv', sep=';', decimal=',')
    except Exception:
        df = pd.read_csv('Cancer_Data.csv')

# Normalize column names
df.columns = df.columns.str.strip()
df.columns = [' '.join(c.split()) for c in df.columns]

# Drop unnamed columns
df = df[[c for c in df.columns if not c.lower().startswith('unnamed')]]

# Identify target column
target_col = None
if 'diagnosis' in df.columns:
    target_col = 'diagnosis'
else:
    for c in df.columns:
        if 'diagnosis' in c.lower() or 'target' in c.lower() or 'class' in c.lower() or 'label' in c.lower():
            target_col = c
            break

if target_col is None:
    # Fallback: pick last column or first non-numeric
    for c in df.columns:
        if df[c].dtype == object:
            target_col = c
            break
    if target_col is None:
        target_col = df.columns[-1]

# Encode target if needed
y = df[target_col].copy()
if y.dtype == object:
    le = LabelEncoder()
    y = pd.Series(le.fit_transform(y.dropna().values), index=y.dropna().index)
    df = df.loc[y.index]
else:
    y = pd.to_numeric(y, errors='coerce')
    mask = y.notna()
    y = y[mask]
    df = df.loc[mask]

# Determine if classification
n_classes = y.nunique()
is_classification = n_classes <= 20

# Feature columns: drop target and id-like columns
id_cols = [c for c in df.columns if c.lower() in ('id', 'index', 'unnamed')]
feature_cols = [c for c in df.columns if c != target_col and c not in id_cols]

# Preferred feature names from schema
preferred_features = [
    "texture_worst", "radius_se", "symmetry_worst", "concave points_mean",
    "area_se", "area_worst", "radius_worst", "concave points_worst",
    "concavity_mean", "fractal_dimension_se"
]
available_preferred = [f for f in preferred_features if f in feature_cols]

# If we have most preferred features, use them; otherwise use all available numeric
if len(available_preferred) >= 5:
    selected_features = available_preferred
else:
    selected_features = feature_cols

# Separate numeric and categorical
numeric_features = []
for c in selected_features:
    df[c] = pd.to_numeric(df[c], errors='coerce')
    if df[c].notna().sum() > 0 and df[c].nunique() > 1:
        numeric_features.append(c)

# Use only numeric features for simplicity and efficiency
X = df[list(numeric_features)].copy()

# Handle NaN/inf
X = X.replace([np.inf, -np.inf], np.nan)
X = X.fillna(X.median())

# Align X and y
common_idx = X.index.intersection(