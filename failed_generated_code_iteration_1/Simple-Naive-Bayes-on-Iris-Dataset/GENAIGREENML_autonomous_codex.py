# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, r2_score
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.naive_bayes import GaussianNB

warnings.filterwarnings("ignore")

file_path = "iris.csv"
df = None
try:
    df = pd.read_csv(file_path)
except Exception:
    df = None
if df is None or df.shape[1] <= 1:
    try:
        df_alt = pd.read_csv(file_path, sep=";", decimal=",")
        if df is None or df_alt.shape[1] > df.shape[1]:
            df = df_alt
    except Exception:
        if df is None:
            raise

if df is None:
    raise ValueError("Dataset could not be loaded")

def _clean_col(c):
    c = str(c).strip()
    c = " ".join(c.split())
    return c

df.columns = [_clean_col(c) for c in df.columns]
df = df.loc[:, [c for c in df.columns if not c.lower().startswith("unnamed")]]
df = df.dropna(axis=1, how="all")
if df.shape[1] == 0:
    raise ValueError("No columns after cleaning")

DATASET_HEADERS = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]

def _is_id_like(col):
    cl = col.lower()
    return cl == "id" or cl.endswith("id") or cl == "index" or cl.endswith("_id") or cl.startswith("id_")

target_col = None
possible_target_keywords = ["target", "label", "class", "species", "y"]
for col in df.columns:
    cl = col.lower()
    for kw in possible_target_keywords:
        if cl == kw or cl.endswith(kw) or kw in cl:
            target_col = col
            break
    if target_col is not None:
        break

if target_col is None:
    for h in reversed(DATASET_HEADERS):
        if h in df.columns:
            target_col = h
            break

if target_col is None:
    target_col = df.columns[-1]

if _is_id_like(target_col):
    for col in df.columns:
        if col != target_col and not _is_id_like(col):
            target_col = col
            break

if df[target_col].nunique(dropna=True) <= 1:
    candidate_cols = [c for c in df.columns if c != target_col]
    best = None
    for c in candidate_cols:
        if pd.to_numeric(df[c], errors="coerce").nunique(dropna=True) > 1:
            best = c
            break
    if best is None:
        for c in candidate_cols:
            if df[c].nunique(dropna=True) > 1:
                best = c
                break
    if best is not None:
        target_col = best

y = df[target_col]
if y.dtype == object:
    y_stripped = y.astype(str).str.strip()
    y_numeric = pd.to_numeric(y_stripped, errors="coerce")
    if y_numeric.notna().mean() > 0.8:
        y = y_numeric
    else:
        y = y_stripped.replace(["nan", "NaN", "None", "NONE", ""], np.nan)
else:
    y = pd.to_numeric(y, errors="coerce")
y = y.replace([np.inf, -np.inf], np.nan)

feature_cols = [c for c in df.columns if c != target_col]
id_cols = [c for c in feature_cols if _is_id_like(c)]
if len(feature_cols) - len(id_cols) >= 1:
    feature_cols = [c for c in feature_cols if c not in id_cols]
if len(feature_cols) == 0:
    df["__dummy__"] = 0
    feature_cols = ["__dummy__"]

X = df[feature_cols].copy()

for col in feature_cols:
    series = X[col]
    if series.dtype != object:
        X[col] = pd.to_numeric(series, errors="coerce")
    else:
        coerced = pd.to_numeric(series, errors="coerce")
        if coerced.notna().mean() > 0.5:
            X[col] = coerced
        else:
            X[col] = series.astype(str).str.strip()
            X[col] = X[col].replace(["nan", "NaN", "None", "NONE", ""], np.nan)
X.replace([np.inf, -np.inf], np.nan, inplace=True)

mask = y.notna()
X = X.loc[mask]
y = y.loc[mask]

assert len(X) > 0, "Dataset is empty after preprocessing"

if len(X) < 2:
    X = pd.concat([X, X], ignore_index=True)
    y = pd.concat([y, y], ignore_index=True)

numeric_features = X.select_dtypes(include=["number"]).columns.tolist()
categorical_features = [c for c in X.columns if c not in numeric_features]
if len(numeric_features) == 0 and len(categorical_features) == 0:
    X["__dummy__"] = 0
    numeric_features = ["__dummy__"]

y_unique = y.nunique(dropna=True)
n_samples = len(y)
if (y.dtype == object) or (str(y.dtype).startswith("category")):
    problem_type = "classification"
else:
    if y_unique <= 20 and y_unique / max(n_samples, 1) <= 0.2:
        problem_type = "classification"
    else:
        problem_type = "regression"
if problem_type == "classification" and y_unique < 2:
    problem_type = "regression"

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False))
])
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=True))
])
transformers = []
if len(numeric_features) > 0:
    transformers.append(("num", numeric_transformer, numeric_features))
if len(categorical_features) > 0:
    transformers.append(("cat", categorical_transformer, categorical_features))
if len(transformers) == 0:
    transformers.append(("num", numeric_transformer, numeric_features))
preprocessor = ColumnTransformer(transformers=transformers, remainder="drop")

stratify = None
if problem_type == "classification":
    class_counts = y.value_counts()
    if class_counts.min() >= 2:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0, "Train/test split resulted in empty set"

if problem_type == "classification":
    if y_train.nunique(dropna=True) < 2:
        model = DummyClassifier(strategy="most_frequent")
    else:
        if len(categorical_features) == 0:
            model = GaussianNB()
        else:
            model = LogisticRegression(max_iter=200, solver="liblinear")
else:
    if y_train.nunique(dropna=True) < 2:
        model = DummyRegressor(strategy="mean")
    else:
        model = LinearRegression()

pipeline = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("model", model)
])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if problem_type == "classification":
    accuracy = accuracy_score(y_test, y_pred)
else:
    if len(y_test) < 2:
        r2 = 0.0
    else:
        r2 = r2_score(y_test, y_pred)
        if np.isnan(r2) or np.isinf(r2):
            r2 = 0.0
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Lightweight linear/GaussianNB or dummy models are chosen to keep computation small and CPU-friendly.
# A minimal ColumnTransformer pipeline with simple imputation and one-hot encoding ensures reproducible preprocessing.
# Regression fallback converts R2 to a bounded (r2+1)/2 accuracy proxy in [0,1] for stable reporting.