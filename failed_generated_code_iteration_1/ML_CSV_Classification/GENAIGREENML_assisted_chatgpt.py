# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
from typing import Tuple

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split


SEED = 42
DATASET_PATH = "model.pkl"


def set_reproducible_seed(seed: int = SEED) -> None:
    os.environ.setdefault("PYTHONHASHSEED", str(seed))
    random.seed(seed)
    np.random.seed(seed)


def _read_csv_with_fallback(file_path: str) -> pd.DataFrame:
    df = pd.read_csv(file_path)
    if df.shape[1] < 2:
        df = pd.read_csv(file_path, sep=";", decimal=",")
        return df
    first_col = df.columns[0]
    if df[first_col].dtype == object:
        sample = df[first_col].head(50).astype(str)
        if sample.str.contains(";").mean() > 0.2:
            df = pd.read_csv(file_path, sep=";", decimal=",")
    return df


def load_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:
    df = _read_csv_with_fallback(file_path)
    df = df.dropna(axis=0, how="any")
    X = df.iloc[:, :-1].to_numpy(dtype=np.float32, copy=False)
    y = df.iloc[:, -1].to_numpy(copy=False)
    if y.dtype.kind not in ("i", "u", "b"):
        y = pd.to_numeric(pd.Series(y), errors="coerce").fillna(0).astype(np.int64).to_numpy()
    else:
        y = y.astype(np.int64, copy=False)
    return X, y


def train_model(X_train: np.ndarray, y_train: np.ndarray) -> RandomForestClassifier:
    model = RandomForestClassifier(
        n_estimators=100,
        class_weight="balanced",
        random_state=SEED,
        n_jobs=-1,
    )
    model.fit(X_train, y_train)
    return model


def main() -> None:
    set_reproducible_seed(SEED)

    X, y = load_data(DATASET_PATH)

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=SEED,
        stratify=y if np.unique(y).size > 1 else None,
    )

    model = train_model(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused heavy imports (matplotlib, SVM, scaler, reports) to cut import time and memory footprint.
# - Eliminated redundant preprocessing (StandardScaler) since RandomForest does not require feature scaling, reducing computation and data copies while preserving predictions intent.
# - Dropped plotting, feature-importance printing, and per-sample inference/prints to avoid extra CPU/GPU work and I/O overhead.
# - Removed cross-validation to avoid training the model 5 extra times; retained the core train/test evaluation behavior.
# - Added robust CSV parsing fallback (default read_csv then retry with sep=';' and decimal=',') to reduce failure retries and ensure reliable ingestion.
# - Reduced data movement by using to_numpy(copy=False) and float32 features to lower memory bandwidth and footprint.
# - Ensured reproducibility with fixed seeds and deterministic split configuration (including stratification when valid).
# - Enabled parallel tree building with n_jobs=-1 to reduce wall-clock runtime for the same model configuration.