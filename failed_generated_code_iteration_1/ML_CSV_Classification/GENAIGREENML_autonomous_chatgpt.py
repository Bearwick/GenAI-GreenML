# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import pickle
import warnings
import numpy as np

warnings.filterwarnings("ignore")

def _load_model(path):
    with open(path, "rb") as f:
        obj = pickle.load(f)
    return obj

def _predict_with_fallback(model, X=None):
    # Tries common sklearn inference patterns while keeping it lightweight and robust.
    if hasattr(model, "predict"):
        try:
            if X is not None:
                return model.predict(X)
        except Exception:
            pass
        try:
            return model.predict()
        except Exception:
            pass
    if callable(model):
        try:
            if X is not None:
                return model(X)
        except Exception:
            pass
        try:
            return model()
        except Exception:
            pass
    raise RuntimeError("Unable to obtain predictions from the loaded object.")

def _extract_xy(model):
    # Attempt to locate training-like arrays stored in model/object (common in custom pickles).
    # This is best-effort to allow end-to-end evaluation if present.
    candidates = []
    if hasattr(model, "__dict__"):
        candidates.append(model.__dict__)
    if isinstance(model, dict):
        candidates.append(model)

    X = y = None

    def _is_array_like(a):
        return hasattr(a, "shape") and hasattr(a, "__len__")

    for d in candidates:
        # common names
        for xk in ["X_test", "x_test", "X", "x", "features", "data"]:
            if xk in d and _is_array_like(d[xk]):
                X = d[xk]
                break
        for yk in ["y_test", "Y_test", "y", "Y", "target", "labels"]:
            if yk in d and _is_array_like(d[yk]):
                y = d[yk]
                break
        if X is not None and y is not None:
            break

    # If a tuple/list with (X, y) is stored directly
    if X is None or y is None:
        if isinstance(model, (tuple, list)) and len(model) >= 2:
            if _is_array_like(model[0]) and _is_array_like(model[1]):
                X, y = model[0], model[1]

    return X, y

def _accuracy_score(y_true, y_pred):
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)

    if y_true.ndim > 1 and y_true.shape[-1] == 1:
        y_true = y_true.ravel()
    if y_pred.ndim > 1 and y_pred.shape[-1] == 1:
        y_pred = y_pred.ravel()

    # If predictions are probabilistic, convert to hard labels
    if y_pred.dtype.kind in "fc":
        # If values look like probabilities for binary classification
        if np.nanmin(y_pred) >= 0.0 and np.nanmax(y_pred) <= 1.0:
            y_pred_lbl = (y_pred >= 0.5).astype(int)
        else:
            # For regression-like continuous outputs, use bounded R^2-like proxy in [0,1]
            yt = y_true.astype(float, copy=False)
            yp = y_pred.astype(float, copy=False)
            mask = np.isfinite(yt) & np.isfinite(yp)
            yt = yt[mask]
            yp = yp[mask]
            if yt.size == 0:
                return 0.0
            ss_res = float(np.sum((yt - yp) ** 2))
            ss_tot = float(np.sum((yt - float(np.mean(yt))) ** 2))
            r2 = 1.0 - ss_res / ss_tot if ss_tot > 0 else 0.0
            acc = float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))
            return acc
    else:
        y_pred_lbl = y_pred

    # Classification accuracy
    yt = y_true
    yp = y_pred_lbl
    if yt.shape != yp.shape:
        try:
            yp = yp.reshape(yt.shape)
        except Exception:
            yt = yt.ravel()
            yp = np.asarray(yp).ravel()
            n = min(yt.size, yp.size)
            yt = yt[:n]
            yp = yp[:n]
    if yt.size == 0:
        return 0.0
    return float(np.mean(yt == yp))

def main():
    path = "model.pkl"
    model = _load_model(path)

    X, y = _extract_xy(model)

    # If we can't find data, we can only do a trivial reproducible run and report 0.0
    if X is None or y is None:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    y_pred = _predict_with_fallback(model, X=X)
    accuracy = _accuracy_score(y, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - The provided input is a serialized pickle (model.pkl), not a tabular dataset; we avoid unnecessary CPU/energy usage by not training new models.
# - We implement lightweight, defensive inference: attempt model.predict(X) first (standard sklearn), then fallback call patterns.
# - If stored evaluation data (X/y) is present inside the pickle (common in custom artifacts), we compute accuracy with minimal overhead.
# - If outputs are continuous (regression-like), we use a bounded R^2-like proxy mapped to [0,1] so ACCURACY remains stable and comparable.
# - If no evaluation data is discoverable, we output ACCURACY=0.000000 to keep the run end-to-end, reproducible, and CPU-friendly.