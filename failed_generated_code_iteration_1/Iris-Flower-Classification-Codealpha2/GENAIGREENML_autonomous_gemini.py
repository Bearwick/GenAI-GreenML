# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, r2_score

def robust_load(filename):
    """Robustly loads CSV with delimiter and encoding fallbacks."""
    try:
        df = pd.read_csv(filename)
        if df.shape[1] <= 1:
            raise ValueError
    except:
        try:
            df = pd.read_csv(filename, sep=';', decimal=',')
        except:
            # Create a dummy dataframe if file reading fails completely to ensure end-to-end execution
            return pd.DataFrame()
    
    # Normalize column names: strip, single space, remove Unnamed
    df.columns = [" ".join(str(c).strip().split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def solve():
    # Load dataset
    df = robust_load("Iris.csv")
    
    if df.empty:
        # Emergency exit if file not found/readable
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Identify target column
    # Preference: 1. 'species', 2. Last column
    target_col = None
    for col in df.columns:
        if 'species' in col.lower():
            target_col = col
            break
    if not target_col:
        target_col = df.columns[-1]

    # Drop ID-like columns
    cols_to_drop = [target_col]
    for col in df.columns:
        if 'id' == col.lower() or 'index' == col.lower():
            cols_to_drop.append(col)
    
    # Identify feature columns (numeric only for efficiency/safety)
    X_raw = df.drop(columns=[c for c in cols_to_drop if c in df.columns])
    X_numeric = X_raw.select_dtypes(include=[np.number]).columns.tolist()
    
    # Preprocess Target
    y_raw = df[target_col]
    
    # Determine task type (classification vs regression)
    is_classification = True
    if y_raw.dtype == object or len(y_raw.unique()) < 20:
        le = LabelEncoder()
        # Coerce to string to handle mixed types in target
        y = le.fit_transform(y_raw.astype(str))
        num_classes = len(le.classes_)
    else:
        # Fallback to regression if many unique numeric values
        y = pd.to_numeric(y_raw, errors='coerce').fillna(0)
        is_classification = False

    # Check if we have features
    if not X_numeric:
        print(f"ACCURACY={0.000000:.6f}")
        return

    X = df[X_numeric].apply(pd.to_numeric, errors='coerce').fillna(0)

    # Train/Test Split
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
    except:
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Pipeline Design
    if is_classification:
        # Energy efficient logistic regression
        model = LogisticRegression(
            max_iter=1000, 
            solver='lbfgs', 
            multi_class='auto',
            penalty='l2'
        )
    else:
        # Energy efficient linear regression (Ridge)
        model = Ridge()

    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler()),
        ('model', model)
    ])

    # Fit and Predict
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)

    # Scoring
    if is_classification:
        accuracy = accuracy_score(y_test, y_pred)
    else:
        # Convert R2 to a 0-1 bounded accuracy proxy
        r2 = r2_score(y_test, y_pred)
        accuracy = max(0, min(1, r2))

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# Optimization Summary:
# 1. Model Choice: Logistic Regression was chosen over SVM/RandomForest as it provides 
#    a highly efficient linear decision boundary for the Iris dataset with minimal CPU cycles.
# 2. Preprocessing: Scikit-learn Pipeline ensures no data leakage and encapsulates 
#    scaling/imputation, which is critical for the stability of linear models.
# 3. Robustness: Implemented a robust CSV loader with delimiter detection and header 
#    normalization to prevent crashes on varying file formats.
# 4. Energy Efficiency: Avoided deep learning and ensemble methods (e.g., XGBoost) 
#    which would require more memory and FLOPs for a simple 4-feature task.
# 5. Numerical Stability: Used 'coerce' on numeric columns and SimpleImputer to 
#    handle potential data corruption or missing values without manual intervention.
# 6. Fallback Logic: The script detects if the target is continuous or categorical 
#    to automatically switch between Logistic and Ridge regression, ensuring completion.