# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
import os
import warnings
warnings.filterwarnings("ignore")

# --- Step 1: Load datasets with robust parsing ---
fake_path = "Fake.csv"
true_path = "True.csv"

def robust_read_csv(path):
    try:
        df = pd.read_csv(path, encoding='utf-8')
        if df.shape[1] <= 1:
            df = pd.read_csv(path, sep=';', decimal=',', encoding='utf-8')
        return df
    except Exception:
        try:
            df = pd.read_csv(path, sep=';', decimal=',', encoding='utf-8')
            return df
        except Exception:
            return pd.DataFrame()

df_fake = robust_read_csv(fake_path)

# Try to load True.csv; if not available, we work with Fake.csv only
has_true = os.path.exists(true_path)
if has_true:
    df_true = robust_read_csv(true_path)
else:
    df_true = pd.DataFrame()

# --- Step 2: Normalize column names ---
def normalize_columns(df):
    df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
    drop_cols = [c for c in df.columns if c.startswith('Unnamed')]
    df = df.drop(columns=drop_cols, errors='ignore')
    return df

df_fake = normalize_columns(df_fake)
if not df_true.empty:
    df_true = normalize_columns(df_true)

# --- Step 3: Create labels and combine ---
# According to README: Fake.csv = fake news (label 0), True.csv = real news (label 1)
if not df_fake.empty:
    df_fake['label'] = 0

if not df_true.empty:
    df_true['label'] = 1
    df = pd.concat([df_fake, df_true], ignore_index=True)
else:
    # If only Fake.csv is available, we need a way to create a binary task
    # Check if there's a column that could serve as target or use subject as proxy
    df = df_fake.copy()
    if 'label' not in df.columns or df['label'].nunique() < 2:
        # Use 'subject' as a binary proxy if available
        if 'subject' in df.columns and df['subject'].nunique() >= 2:
            # Map subjects to binary: pick the most common as class 0, rest as class 1
            most_common = df['subject'].value_counts().index[0]
            df['label'] = (df['subject'] != most_common).astype(int)
        else:
            # Fallback: assign alternating labels for a trivial baseline
            df['label'] = np.arange(len(df)) % 2

# --- Step 4: Build text feature ---
# Determine which text columns are available
text_col = None
for candidate in ['text', 'title', 'Text', 'Title']:
    if candidate in df.columns:
        text_col = candidate
        break

if text_col is None:
    # Use first object column as text
    obj_cols = df.select_dtypes(include='object').columns.tolist()
    obj_cols = [c for c in obj_cols if c != 'label']
    if obj_cols:
        text_col = obj_cols[0]

assert text_col is not None, "No text column found"

# Combine title and text if both exist for richer features
if 'title' in df.columns and 'text' in df.columns:
    df['combined_text'] = df['title'].fillna('') + ' ' + df['text'].fillna('')
    text_