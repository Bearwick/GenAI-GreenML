# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler


warnings.filterwarnings("ignore")


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    # Attempt 1: default CSV parsing
    try:
        df1 = pd.read_csv(path)
    except Exception:
        df1 = None

    # Attempt 2: common EU format parsing
    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
    except Exception:
        df2 = None

    # Choose the "better" parse: more columns and more non-empty cells
    def score(df):
        if df is None or not isinstance(df, pd.DataFrame) or df.empty:
            return (-1, -1, -1)
        ncols = df.shape[1]
        nn = int(df.notna().sum().sum())
        return (ncols, nn, -int(df.isna().sum().sum()))

    best = df1
    if score(df2) > score(df1):
        best = df2

    if best is None:
        raise RuntimeError("Failed to read CSV with robust fallback.")
    return best


def _drop_unnamed(df):
    cols = []
    for c in df.columns:
        if re.match(r"^Unnamed:\s*\d+$", str(c)):
            continue
        cols.append(c)
    return df[cols]


def _coerce_numeric_series(s):
    # Handles strings with comma decimals and typical boolean-like tokens.
    if s.dtype == object:
        s2 = s.astype(str).str.strip()
        s2 = s2.replace(
            {
                "": np.nan,
                "nan": np.nan,
                "NaN": np.nan,
                "None": np.nan,
                "TRUE": "1",
                "True": "1",
                "true": "1",
                "FALSE": "0",
                "False": "0",
                "false": "0",
                "YES": "1",
                "Yes": "1",
                "yes": "1",
                "NO": "0",
                "No": "0",
                "no": "0",
                "Y": "1",
                "N": "0",
            }
        )
        s2 = s2.str.replace(",", ".", regex=False)
        return pd.to_numeric(s2, errors="coerce")
    return pd.to_numeric(s, errors="coerce")


def _choose_target(df):
    # Prefer known target names if available (case-insensitive), but never hard-fail.
    col_map = {str(c).strip().lower(): c for c in df.columns}
    for cand in ["death_event", "death event", "target", "label", "y", "class", "outcome"]:
        if cand in col_map:
            return col_map[cand]

    # Otherwise choose a non-constant numeric/binary-like column with few unique values (classification-friendly).
    numeric_candidates = []
    for c in df.columns:
        s_num = _coerce_numeric_series(df[c])
        nunique = int(pd.Series(s_num).dropna().nunique())
        if nunique >= 2:
            numeric_candidates.append((c, nunique, float(pd.Series(s_num).dropna().var()) if nunique > 1 else 0.0))

    if numeric_candidates:
        # Prefer binary/low-cardinality columns; else choose highest variance numeric column.
        low_card = [t for t in numeric_candidates if t[1] <= 10]
        if low_card:
            # pick the one closest to binary, then higher variance
            low_card.sort(key=lambda x: (abs(x[1] - 2), -x[2]))
            return low_card[0][0]
        numeric_candidates.sort(key=lambda x: -x[2])
        return numeric_candidates[0][0]

    # Fallback: last column
    return df.columns[-1]


def _is_classification_target(y):
    y_clean = pd.Series(y).dropna()
    if y_clean.empty:
        return False
    # If it can be interpreted as numeric and has small unique count -> classification
    y_num = _coerce_numeric_series(y_clean)
    uniq = y_num.dropna().unique()
    if len(uniq) < 2:
        return False
    if len(uniq) <= 20:
        # Heuristic: likely classification if integer-like
        if np.all(np.isclose(uniq, np.round(uniq))):
            return True
    return False


def _bounded_regression_score(y_true, y_pred):
    # Stable "accuracy" proxy in [0,1] based on normalized MAE.
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = np.mean(np.abs(y_true - y_pred)) if y_true.size else 1.0
    rng = np.nanmax(y_true) - np.nanmin(y_true)
    scale = rng if np.isfinite(rng) and rng > 0 else (np.nanstd(y_true) if np.nanstd(y_true) > 0 else 1.0)
    score = 1.0 - (mae / (scale + 1e-12))
    return float(np.clip(score, 0.0, 1.0))


def main():
    dataset_path = "heart_failure.csv"
    if not os.path.exists(dataset_path):
        # Try relative to script directory if executed elsewhere
        dataset_path = os.path.join(os.path.dirname(__file__), "heart_failure.csv")

    df = _read_csv_robust(dataset_path)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # If the file had an index-like first column, try to drop it if it looks like a row id.
    # (Only drop if it is strictly increasing integers with high uniqueness.)
    if df.shape[1] >= 2:
        first = df.columns[0]
        s_first = _coerce_numeric_series(df[first])
        non_na = s_first.dropna()
        if len(non_na) == len(df) and non_na.nunique() == len(df):
            vals = non_na.to_numpy()
            if np.all(np.isclose(vals, np.round(vals))):
                diffs = np.diff(vals)
                if len(diffs) > 0 and np.all(diffs == 1):
                    df = df.drop(columns=[first])

    # Select target robustly
    target_col = _choose_target(df)
    if target_col not in df.columns:
        target_col = df.columns[-1]

    # Prepare X/y with defensive cleaning
    y_raw = df[target_col]
    X = df.drop(columns=[target_col])

    # Ensure non-empty after basic setup
    assert df.shape[0] > 0 and df.shape[1] > 0
    assert X.shape[1] > 0

    # Identify column types
    # Determine numeric by coercion success rate
    numeric_cols = []
    categorical_cols = []
    for c in X.columns:
        s = X[c]
        s_num = _coerce_numeric_series(s)
        non_na = s_num.notna().mean() if len(s_num) else 0.0
        # Consider numeric if most values can be parsed as numbers
        if non_na >= 0.8:
            numeric_cols.append(c)
            X[c] = s_num
        else:
            categorical_cols.append(c)

    # Clean y and decide task type
    y_num = _coerce_numeric_series(y_raw)
    y_for_task = y_raw
    classification = _is_classification_target(y_raw) or _is_classification_target(y_num)

    if classification:
        # Use numeric labels if possible; else factorize
        y_candidate = y_num
        if y_candidate.dropna().nunique() >= 2:
            y_for_task = y_candidate
        else:
            y_for_task = pd.Series(pd.factorize(y_raw.astype(str).fillna("nan"))[0], index=y_raw.index).astype(float)

        # Drop rows where y is NaN
        mask = pd.Series(y_for_task).notna()
        X2 = X.loc[mask].copy()
        y2 = pd.Series(y_for_task).loc[mask].copy()

        # Ensure at least 2 classes
        uniq = pd.Series(y2).dropna().unique()
        if len(uniq) < 2:
            classification = False
            X = X2
            y_for_task = y_num.loc[mask]
        else:
            X = X2
            y_for_task = y2
    else:
        # Regression path
        y_for_task = y_num
        mask = pd.Series(y_for_task).notna()
        X = X.loc[mask].copy()
        y_for_task = pd.Series(y_for_task).loc[mask].copy()

    # Final safety checks
    X.replace([np.inf, -np.inf], np.nan, inplace=True)
    assert len(X) > 1

    # Build preprocessing
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Train/test split
    stratify = None
    if classification:
        # Stratify only if each class has enough samples
        vc = pd.Series(y_for_task).value_counts(dropna=True)
        if (vc.min() >= 2) and (vc.shape[0] >= 2):
            stratify = y_for_task

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y_for_task,
        test_size=0.2,
        random_state=42,
        stratify=stratify,
    )
    assert len(X_train) > 0 and len(X_test) > 0

    if classification:
        # Lightweight linear classifier; LBFGS is efficient for small datasets.
        model = LogisticRegression(
            max_iter=200,
            solver="lbfgs",
            n_jobs=1,
        )
        clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Lightweight linear regressor; deterministic and CPU-friendly.
        model = Ridge(alpha=1.0, random_state=42)
        reg = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        accuracy = _bounded_regression_score(y_test.to_numpy(dtype=float), np.asarray(y_pred, dtype=float))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses robust CSV parsing fallback (default, then sep=';' & decimal=',') to avoid re-parsing overhead loops.
# - Prefers simple linear models (LogisticRegression / Ridge) for strong CPU efficiency and small memory footprint.
# - ColumnTransformer+Pipeline ensures single-pass preprocessing and reproducibility without redundant computation.
# - Coerces numeric columns defensively and imputes missing values to avoid expensive row dropping and retries.
# - OneHotEncoder with sparse output reduces memory/compute for categorical features on CPU.
# - If regression fallback is triggered, reports a bounded [0,1] "accuracy" proxy = 1 - normalized MAE for stability.