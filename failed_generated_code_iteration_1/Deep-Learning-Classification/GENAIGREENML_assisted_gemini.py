# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

tf.keras.utils.set_random_seed(0)

try:
    df = pd.read_csv('heart_failure.csv')
except (FileNotFoundError, pd.errors.ParserError):
    df = pd.read_csv('heart_failure.csv', sep=';', decimal=',')

target_col = 'death_event' if 'death_event' in df.columns else 'DEATH_EVENT'
feature_cols = [
    'age', 'anaemia', 'creatinine_phosphokinase', 'diabetes', 
    'ejection_fraction', 'high_blood_pressure', 'platelets', 
    'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time'
]
scaling_indices = [0, 2, 4, 6, 7, 8, 11]

X = df[feature_cols].values.astype(np.float32)
y = df[target_col].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

scaler = StandardScaler()
X_train[:, scaling_indices] = scaler.fit_transform(X_train[:, scaling_indices])
X_test[:, scaling_indices] = scaler.transform(X_test[:, scaling_indices])

y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes=2)

model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(12, activation='relu'),
    tf.keras.layers.Dense(2, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train_cat, epochs=100, batch_size=16, verbose=0)

_, accuracy = model.evaluate(X_test, y_test_cat, verbose=0)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Set TF_CPP_MIN_LOG_LEVEL to '3' to suppress environment logging overhead.
# - Replaced pd.get_dummies with direct numeric array handling, as the heart failure features are already encoded.
# - Removed redundant LabelEncoder and string conversion for target labels, using direct categorical encoding.
# - Used numpy array slicing for feature scaling instead of the more complex ColumnTransformer object.
# - Minimized data movement by converting the dataframe to float32 numpy arrays once before training.
# - Reduced computational noise by setting verbose=0 in training and evaluation.
# - Optimized inference by removing the redundant classification_report and predict() call, relying on evaluate() for accuracy.
# - Improved reproducibility by utilizing tf.keras.utils.set_random_seed for global determinism.
# - Implemented robust CSV parsing to handle different delimiters or decimal points.
# - Streamlined model definition using a list-based Sequential constructor for faster instantiation.