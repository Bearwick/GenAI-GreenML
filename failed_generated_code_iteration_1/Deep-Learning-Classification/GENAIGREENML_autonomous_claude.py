# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# --- Robust CSV Loading ---
try:
    df = pd.read_csv("heart_failure.csv")
    if df.shape[1] < 3:
        raise ValueError("Too few columns, try alternative separator")
except Exception:
    df = pd.read_csv("heart_failure.csv", sep=';', decimal=',')

# --- Column name normalization ---
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
df = df[[c for c in df.columns if not c.startswith('Unnamed')]]

# --- Target selection ---
# The dataset has both 'DEATH_EVENT' (numeric) and 'death_event' (string yes/no).
# Prefer 'DEATH_EVENT' as numeric target; fall back to 'death_event'.
target_col = None
if 'DEATH_EVENT' in df.columns:
    target_col = 'DEATH_EVENT'
elif 'death_event' in df.columns:
    target_col = 'death_event'
else:
    # Pick last column as target
    target_col = df.columns[-1]

# Encode target if it's categorical
y = df[target_col].copy()
if y.dtype == object:
    y = y.str.strip().str.lower().map({'yes': 1, 'no': 0})
    if y.isna().any():
        # Try label encoding as fallback
        from sklearn.preprocessing import LabelEncoder
        y = LabelEncoder().fit_transform(df[target_col].astype(str))
y = pd.Series(y, name=target_col)

# --- Feature selection ---
# Drop both target columns and any index-like first unnamed column
drop_cols = set()
if 'DEATH_EVENT' in df.columns:
    drop_cols.add('DEATH_EVENT')
if 'death_event' in df.columns:
    drop_cols.add('death_event')
# Drop the first column if it looks like an index (monotonically increasing integers)
first_col = df.columns[0]
if first_col == '' or first_col.lower() in ('', 'index', 'id'):
    drop_cols.add(first_col)
else:
    try:
        vals = pd.to_numeric(df[first_col], errors='coerce')
        if vals.is_monotonic_increasing and vals.notna().all() and len(vals.unique()) == len(vals):
            drop_cols.add(first_col)
    except Exception:
        pass

feature_cols = [c for c in df.columns if c not in drop_cols]
X = df[feature_cols].copy()

# --- Identify numeric vs categorical columns ---
cat_cols = []
num_cols = []
for c in X.columns:
    # Try to coerce to numeric
    coerced = pd.to_numeric(X[c], errors='coerce')
    if coerced.notna().mean() > 0.5:
        X[c] = coerced
        num_cols.append(c)
    else:
        cat_cols.append(c)

# --- Handle NaN/inf in numeric columns ---
for c in num_cols:
    X[c] = X[c].replace([np.inf, -np.inf], np.nan)
    X[c] = X[c].fillna(X[c].median())

# Handle NaN in categorical columns
for c in cat_cols:
    X[c] = X[c].astype(str).fillna('missing')

# Align X and y, drop any remaining NaN in y
mask = y.notna()
X = X.loc[mask].reset_index(drop=True)
y = y.loc[mask].