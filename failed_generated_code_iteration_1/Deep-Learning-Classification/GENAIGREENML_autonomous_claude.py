# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# Robust CSV loading
try:
    data = pd.read_csv('heart_failure.csv')
    if data.shape[1] < 3:
        data = pd.read_csv('heart_failure.csv', sep=';', decimal=',')
except Exception:
    data = pd.read_csv('heart_failure.csv', sep=';', decimal=',')

# Normalize column names
data.columns = [c.strip().replace('  ', ' ') for c in data.columns]
data = data[[c for c in data.columns if not c.startswith('Unnamed')]]

# Determine target column
target_col = None
for candidate in ['death_event', 'DEATH_EVENT']:
    if candidate in data.columns:
        target_col = candidate
        break

if target_col is None:
    # Fallback: pick last numeric column
    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
    if numeric_cols:
        target_col = numeric_cols[-1]
    else:
        raise ValueError("No suitable target column found")

# If both death_event and DEATH_EVENT exist, prefer death_event and drop the other
if 'death_event' in data.columns and 'DEATH_EVENT' in data.columns:
    target_col = 'death_event'
    data = data.drop(columns=['DEATH_EVENT'])

# Coerce numeric columns
for col in data.columns:
    if data[col].dtype == object:
        try:
            data[col] = pd.to_numeric(data[col], errors='coerce')
        except Exception:
            pass

# Drop rows with NaN in target
data = data.dropna(subset=[target_col])

# Features
feature_cols = [c for c in data.columns if c != target_col]

# Separate numeric and categorical
numeric_features = []
categorical_features = []
for c in feature_cols:
    if data[c].dtype in [np.float64, np.int64, np.float32, np.int32, float, int]:
        numeric_features.append(c)
    else:
        categorical_features.append(c)

# Drop categorical with too many unique values or handle via one-hot
if categorical_features:
    data = pd.get_dummies(data, columns=categorical_features, drop_first=True)
    feature_cols = [c for c in data.columns if c != target_col]
    numeric_features = feature_cols

# Handle NaN/inf in features
data[feature_cols] = data[list(feature_cols)].replace([np.inf, -np.inf], np.nan)
data = data.dropna()

assert len(data) > 0, "Dataset empty after preprocessing"

y = data[target_col].values
X = data[list(feature_cols)].values

# Determine if classification or regression
unique_classes = np.unique(y)
is_classification = len(unique_classes) >= 2 and len(unique_classes) <= 20

if len(unique_classes) < 2:
    # Trivial baseline
    accuracy = 1.0
    print(f"ACCURACY={accuracy:.6f}")
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0, stratify=y if is_classification else None
    )

    assert len(X_train) > 0 and len(X_test) > 0, "Train/test split produced empty sets"

    if is_classification:
        # Lightweight logistic regression with standardization
        pipeline = Pipeline([
            ('scaler', StandardScaler()),
            ('clf', LogisticRegression(max_iter=500, random_state=