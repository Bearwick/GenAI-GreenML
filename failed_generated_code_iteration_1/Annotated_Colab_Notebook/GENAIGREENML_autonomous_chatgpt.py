# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor

RANDOM_STATE = 42
DATASET_PATH = "dataset_adult.arff"


def _normalize_columns(cols):
    norm = []
    for c in cols:
        c = "" if c is None else str(c)
        c = re.sub(r"\s+", " ", c.strip())
        if c.lower().startswith("unnamed:"):
            norm.append(None)
        else:
            norm.append(c)
    return norm


def _try_read_arff(path):
    # Try scipy loader (lightweight) first; fallback to manual ARFF parsing if needed.
    try:
        from scipy.io import arff  # type: ignore
        data, meta = arff.loadarff(path)
        df = pd.DataFrame(data)
        # Decode bytes in object columns if present
        for col in df.columns:
            if df[col].dtype == object:
                df[col] = df[col].apply(lambda x: x.decode("utf-8", errors="ignore") if isinstance(x, (bytes, bytearray)) else x)
        return df
    except Exception:
        # Manual ARFF parser: supports comma-separated @data with simple quoting; robust to extra spaces.
        attr_names = []
        rows = []
        in_data = False
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("%"):
                    continue
                low = line.lower()
                if low.startswith("@attribute"):
                    # Format: @attribute name type
                    parts = line.split(None, 2)
                    if len(parts) >= 2:
                        name = parts[1].strip()
                        # remove quotes if present
                        if (name.startswith("'") and name.endswith("'")) or (name.startswith('"') and name.endswith('"')):
                            name = name[1:-1]
                        attr_names.append(name)
                elif low.startswith("@data"):
                    in_data = True
                elif in_data:
                    if line.startswith("{") and line.endswith("}"):
                        # sparse format not supported; skip (rare for this dataset)
                        continue
                    # Split by comma respecting simple quotes
                    fields = []
                    cur = ""
                    in_quote = False
                    quote_char = ""
                    for ch in line:
                        if ch in ("'", '"'):
                            if not in_quote:
                                in_quote = True
                                quote_char = ch
                            elif quote_char == ch:
                                in_quote = False
                            cur += ch
                        elif ch == "," and not in_quote:
                            fields.append(cur.strip())
                            cur = ""
                        else:
                            cur += ch
                    fields.append(cur.strip())

                    # Strip surrounding quotes
                    cleaned = []
                    for v in fields:
                        v = v.strip()
                        if len(v) >= 2 and ((v[0] == "'" and v[-1] == "'") or (v[0] == '"' and v[-1] == '"')):
                            v = v[1:-1]
                        cleaned.append(v)
                    rows.append(cleaned)

        if not rows:
            return pd.DataFrame()

        # If attribute names mismatch row width, pad/trim safely
        max_w = max(len(r) for r in rows)
        if not attr_names or len(attr_names) != max_w:
            attr_names = [f"col_{i}" for i in range(max_w)]
        fixed_rows = []
        for r in rows:
            if len(r) < max_w:
                r = r + [np.nan] * (max_w - len(r))
            elif len(r) > max_w:
                r = r[:max_w]
            fixed_rows.append(r)
        return pd.DataFrame(fixed_rows, columns=attr_names)


def _try_read_csv_with_fallback(path):
    # Primary read_csv; if parsing looks wrong, retry with European settings.
    try:
        df1 = pd.read_csv(path)
        if df1.shape[1] <= 1:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df2.shape[1] > df1.shape[1]:
                return df2
        return df1
    except Exception:
        try:
            return pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            return pd.DataFrame()


def load_dataset(path):
    ext = os.path.splitext(path)[1].lower()
    if ext == ".arff":
        df = _try_read_arff(path)
        if df is None or df.empty:
            # As a last resort, try CSV reader fallback
            df = _try_read_csv_with_fallback(path)
        return df
    else:
        return _try_read_csv_with_fallback(path)


def clean_dataframe(df):
    if df is None:
        return pd.DataFrame()

    df = df.copy()
    df.columns = _normalize_columns(df.columns)
    # Drop unnamed columns
    keep_cols = [c for c in df.columns if c is not None]
    df = df[keep_cols]

    # Convert common missing markers to NaN
    for col in df.columns:
        if df[col].dtype == object:
            df[col] = df[col].replace(
                {
                    "?": np.nan,
                    " ?": np.nan,
                    "": np.nan,
                    "NA": np.nan,
                    "N/A": np.nan,
                    "null": np.nan,
                    "None": np.nan,
                }
            )
            # Strip whitespace in strings
            df[col] = df[col].apply(lambda x: x.strip() if isinstance(x, str) else x)

    # Drop fully empty columns
    df = df.dropna(axis=1, how="all")
    return df


def choose_target_and_task(df):
    # Prefer a likely label column by heuristics; otherwise pick a non-constant column.
    if df.empty or df.shape[1] < 2:
        return None, None

    cols = list(df.columns)

    # Heuristic: prefer columns with typical label names
    preferred_names = ["class", "target", "label", "y", "income", "outcome"]
    for pn in preferred_names:
        for c in cols:
            if c and c.strip().lower() == pn:
                y = df[c]
                X = df.drop(columns=[c])
                return c, infer_task(y), X

    # Another heuristic: last column often is the target in ARFF
    last_col = cols[-1]
    y = df[last_col]
    X = df.drop(columns=[last_col])
    task = infer_task(y)
    if task is not None:
        return last_col, task, X

    # Fallback: find a non-constant numeric column as target (regression), else non-constant object (classification)
    nunique = df.nunique(dropna=True)
    candidate_cols = [c for c in cols if nunique.get(c, 0) >= 2]
    if not candidate_cols:
        return None, None, None

    numeric_candidates = []
    for c in candidate_cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() >= max(10, int(0.1 * len(df))):
            numeric_candidates.append((c, s.var(skipna=True)))
    if numeric_candidates:
        numeric_candidates.sort(key=lambda t: (-(0 if pd.isna(t[1]) else t[1])))
        c = numeric_candidates[0][0]
        X = df.drop(columns=[c])
        return c, "regression", X

    # Otherwise pick a non-constant object column
    c = candidate_cols[0]
    X = df.drop(columns=[c])
    return c, "classification", X


def infer_task(y):
    if y is None:
        return None
    # If object/categorical -> classification
    if y.dtype == object or str(y.dtype).startswith("category"):
        # Require at least 2 classes after dropping NA
        if y.dropna().nunique() >= 2:
            return "classification"
        return None
    # Numeric: if looks like small integer set, treat as classification; else regression
    y_num = pd.to_numeric(y, errors="coerce")
    y_nonan = y_num.dropna()
    if y_nonan.empty:
        return None
    uniq = y_nonan.nunique()
    if uniq >= 2 and uniq <= 20 and np.all(np.isclose(y_nonan, np.round(y_nonan))):
        return "classification"
    if uniq >= 2:
        return "regression"
    return None


def build_preprocessor(X):
    # Identify columns by dtype (robust after cleaning)
    Xc = X.copy()

    # Attempt numeric coercion for object columns where possible (without overwriting originals here)
    numeric_cols = []
    categorical_cols = []

    for c in Xc.columns:
        s = Xc[c]
        if s.dtype == object or str(s.dtype).startswith("category"):
            s_num = pd.to_numeric(s, errors="coerce")
            # If most values parse as numeric, treat as numeric
            if s_num.notna().mean() >= 0.9:
                numeric_cols.append(c)
            else:
                categorical_cols.append(c)
        else:
            numeric_cols.append(c)

    # Pipelines: impute then scale for numeric; impute then one-hot for categoricals
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False)),  # sparse-friendly and CPU-light
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )
    return preprocessor, numeric_cols, categorical_cols


def bounded_regression_score(y_true, y_pred):
    # Convert regression quality into [0,1] "accuracy" proxy using bounded R^2-like score.
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    if y_true.size < 2:
        return 0.0
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    if ss_tot <= 0:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    # Bound to [0,1] to behave like accuracy and be stable across datasets.
    return float(np.clip(r2, 0.0, 1.0))


def main():
    df = load_dataset(DATASET_PATH)
    df = clean_dataframe(df)

    assert df is not None and not df.empty, "Empty dataset after loading/cleaning."

    target_col, task, X = choose_target_and_task(df)
    if target_col is None or task is None or X is None or X.empty:
        # Trivial fallback: predict most frequent label from any available column
        # Choose first available column as y
        y = df.iloc[:, 0] if df.shape[1] >= 1 else pd.Series([0] * len(df))
        X = df.drop(columns=[df.columns[0]]) if df.shape[1] > 1 else pd.DataFrame({"bias": np.ones(len(df))})
        target_col = df.columns[0] if df.shape[1] >= 1 else "target"
        task = "classification" if y.dropna().nunique() >= 2 else "regression"
    else:
        y = df[target_col]

    # Prepare y and decide final task with defensiveness
    if task == "classification":
        # Ensure y is clean categorical
        y_clean = y.astype(object)
        y_clean = y_clean.replace({np.nan: None})
        y_nonan = y_clean.dropna()
        if y_nonan.nunique() < 2:
            task = "regression"
        else:
            y = y_clean
    if task == "regression":
        y_num = pd.to_numeric(y, errors="coerce")
        y = y_num

    # Drop rows with missing target
    mask_y = pd.notna(y)
    X = X.loc[mask_y].reset_index(drop=True)
    y = y.loc[mask_y].reset_index(drop=True)

    assert len(X) > 1 and len(y) > 1, "Not enough data after target cleaning."

    preprocessor, _, _ = build_preprocessor(X)

    if task == "classification":
        # Split with stratification when possible
        y_for_split = y.astype(str)
        if pd.Series(y_for_split).nunique() >= 2:
            strat = y_for_split
        else:
            strat = None

        X_train, X_test, y_train, y_test = train_test_split(
            X, y_for_split, test_size=0.2, random_state=RANDOM_STATE, stratify=strat
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

        # Lightweight linear classifier; fallback to dummy if needed
        clf = LogisticRegression(
            max_iter=200,
            solver="liblinear",  # CPU-friendly for small/medium sparse problems
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        try:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
        except Exception:
            dummy = Pipeline(steps=[("preprocess", preprocessor), ("model", DummyClassifier(strategy="most_frequent"))])
            dummy.fit(X_train, y_train)
            y_pred = dummy.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)

    else:
        # Regression path: bounded R^2-like score as accuracy proxy
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        try:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = bounded_regression_score(y_test, y_pred)
        except Exception:
            dummy = Pipeline(steps=[("preprocess", preprocessor), ("model", DummyRegressor(strategy="mean"))])
            dummy.fit(X_train, y_train)
            y_pred = dummy.predict(X_test)
            accuracy = bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Chose a CPU-friendly baseline: LogisticRegression(liblinear) for classification and Ridge for regression; both are linear, fast, and memory efficient.
# - Used ColumnTransformer + Pipeline to ensure reproducibility, avoid repeated preprocessing work, and keep transformations sparse where possible.
# - OneHotEncoder(handle_unknown="ignore") enables robust categorical handling without large embeddings or complex models.
# - StandardScaler(with_mean=False) is sparse-matrix compatible to reduce memory and CPU overhead.
# - Implemented robust ARFF loading with scipy fallback to a minimal manual parser, preventing fragile schema assumptions.
# - Defensive target selection: prefers common target names, otherwise last column, otherwise non-constant numeric/object column; avoids hard failure on schema mismatch.
# - Regression fallback reports a bounded R^2-like score mapped to [0,1] as an "accuracy" proxy for stable single-metric output.