# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score

path = "dataset_adult.arff"

def load_dataset(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    def bad_parse(d):
        if d is None:
            return True
        if d.shape[0] == 0:
            return True
        if d.shape[1] <= 1:
            return True
        cols = [str(c) for c in d.columns]
        if any(str(c).strip().startswith('@') for c in cols):
            return True
        return False
    if bad_parse(df):
        try:
            df2 = pd.read_csv(path, sep=';', decimal=',')
            if not bad_parse(df2):
                df = df2
        except Exception:
            pass
    if bad_parse(df):
        try:
            from scipy.io import arff
            data, meta = arff.loadarff(path)
            df = pd.DataFrame(data)
        except Exception:
            pass
    if bad_parse(df):
        try:
            df = pd.read_csv(path, comment='@', header=None)
        except Exception:
            pass
    return df

df = load_dataset(path)
assert df is not None and df.shape[0] > 0

def normalize_columns(df):
    cols = []
    for c in df.columns:
        if isinstance(c, (bytes, bytearray)):
            c = c.decode('utf-8', errors='ignore')
        c = str(c)
        c = ' '.join(c.strip().split())
        cols.append(c)
    df = df.copy()
    df.columns = cols
    df = df.loc[:, [c for c in df.columns if not c.lower().startswith('unnamed')]]
    return df

df = normalize_columns(df)

for c in df.columns:
    if df[c].dtype == object:
        if df[c].apply(lambda x: isinstance(x, (bytes, bytearray))).any():
            df[c] = df[c].apply(lambda x: x.decode('utf-8', errors='ignore') if isinstance(x, (bytes, bytearray)) else x)
        df[c] = df[c].apply(lambda x: x.strip() if isinstance(x, str) else x)

df = df.replace({'?': np.nan})

def select_target(df):
    for key in ['target', 'label', 'class', 'outcome', 'y']:
        for c in df.columns:
            cl = c.lower()
            if cl == key or key in cl:
                return c
    return df.columns[-1]

target = select_target(df)

if df[target].nunique(dropna=True) <= 1:
    numeric_candidates = []
    for c in df.columns:
        if c == target:
            continue
        s = pd.to_numeric(df[c], errors='coerce')
        if s.nunique(dropna=True) > 1:
            numeric_candidates.append(c)
    if numeric_candidates:
        target = numeric_candidates[-1]
    else:
        for c in df.columns:
            if c == target:
                continue
            if df[c].nunique(dropna=True) > 1:
                target = c
                break

features = [c for c in df.columns if c != target]
if len(features) == 0:
    df['dummy'] = 0
    features = ['dummy']

n_rows = len(df)
numeric_cols = []
categorical_cols = []
for c in features:
    if pd.api.types.is_numeric_dtype(df[c]):
        numeric_cols.append(c)
    else:
        coerced = pd.to_numeric(df[c], errors='coerce')
        non_na = coerced.notna().sum()
        if non_na > 0 and non_na / max(1, n_rows) >= 0.8:
            df[c] = coerced
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)

if len(numeric_cols) == 0 and len(categorical_cols) == 0:
    categorical_cols = features

target_series = df[target]
if target_series.dtype == object:
    if target_series.apply(lambda x: isinstance(x, (bytes, bytearray))).any():
        target_series = target_series.apply(lambda x: x.decode('utf-8', errors='ignore') if isinstance(x, (bytes, bytearray)) else x)

is_classification = False
if target_series.dtype == object or pd.api.types.is_categorical_dtype(target_series):
    is_classification = True
else:
    target_numeric = pd.to_numeric(target_series, errors='coerce')
    unique_vals = target_numeric.nunique(dropna=True)
    if unique_vals > 0 and (unique_vals <= 20 or unique_vals / max(1, len(target_numeric)) <= 0.05):
        is_classification = True

if is_classification:
    y = target_series
    mask = y.notna()
    df = df.loc[mask].copy()
    y = y.loc[mask]
    le = LabelEncoder()
    y_encoded = pd.Series(le.fit_transform(y.astype(str)), index=y.index)
    n_classes = len(le.classes_)
    if n_classes < 2:
        is_classification = False
        y_reg = pd.to_numeric(target_series, errors='coerce')
        y_reg = y_reg.loc[mask]
        if y_reg.notna().sum() == 0:
            y_reg = y_encoded
        y = y_reg
    else:
        y = y_encoded
else:
    y = pd.to_numeric(target_series, errors='coerce')
    mask = y.notna()
    df = df.loc[mask].copy()
    y = y.loc[mask]
    n_classes = None

features = [c for c in features if c in df.columns]
if len(features) == 0:
    df['dummy'] = 0
    features = ['dummy']
    numeric_cols = ['dummy']
    categorical_cols = []
else:
    numeric_cols = [c for c in numeric_cols if c in features]
    categorical_cols = [c for c in categorical_cols if c in features]

for col in list(numeric_cols):
    if df[col].notna().sum() == 0:
        numeric_cols.remove(col)
        if col in features:
            features.remove(col)
for col in list(categorical_cols):
    if df[col].notna().sum() == 0:
        categorical_cols.remove(col)
        if col in features:
            features.remove(col)

if len(features) == 0:
    df['dummy'] = 0
    features = ['dummy']
    numeric_cols = ['dummy']
    categorical_cols = []

for c in numeric_cols:
    df[c] = pd.to_numeric(df[c], errors='coerce')
    df[c] = df[c].replace([np.inf, -np.inf], np.nan)

X = df[features]
y = y.loc[X.index]
assert X.shape[0] > 0 and len(y) > 0

stratify = None
if is_classification:
    unique_vals, counts = np.unique(y, return_counts=True)
    if len(unique_vals) > 1 and counts.min() >= 2:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=stratify
)

assert X_train.shape[0] > 0 and X_test.shape[0] > 0

accuracy = None

if is_classification and len(np.unique(y_train)) < 2:
    majority = y_train.iloc[0] if len(y_train) > 0 else 0
    y_pred = np.full(shape=len(y_test), fill_value=majority)
    accuracy = accuracy_score(y_test, y_pred) if len(y_test) > 0 else 0.0
else:
    transformers = []
    if len(numeric_cols) > 0:
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler(with_mean=False))
        ])
        transformers.append(('num', numeric_transformer, numeric_cols))
    if len(categorical_cols) > 0:
        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
        ])
        transformers.append(('cat', categorical_transformer, categorical_cols))
    if len(transformers) > 0:
        preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')
    else:
        preprocessor = 'passthrough'
    if is_classification:
        if n_classes is None:
            n_classes = len(np.unique(y))
        if n_classes <= 2:
            model = LogisticRegression(max_iter=200, solver='liblinear')
        else:
            model = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='auto')
    else:
        model = Ridge(alpha=1.0, random_state=42)
    clf = Pipeline(steps=[('preprocess', preprocessor),
                         ('model', model)])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    if is_classification:
        accuracy = accuracy_score(y_test, y_pred)
    else:
        try:
            r2 = r2_score(y_test, y_pred)
            if not np.isfinite(r2):
                r2 = -1.0
        except Exception:
            r2 = -1.0
        accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) to keep CPU usage low while providing a reasonable baseline.
# - Employed a compact preprocessing pipeline with simple imputation, scaling, and one-hot encoding for robust schema handling.
# - Regression fallback reports a bounded (r2+1)/2 proxy to keep the accuracy metric stable in [0,1].