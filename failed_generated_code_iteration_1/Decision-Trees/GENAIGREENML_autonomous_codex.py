# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
        return df
    def looks_wrong(d):
        if d.shape[1] <= 1:
            return True
        for col in d.columns:
            if isinstance(col, str) and ';' in col:
                return True
        return False
    if looks_wrong(df):
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > df.shape[1]:
                df = df_alt
        except Exception:
            pass
    return df

def normalize_columns(df):
    cols = []
    keep_cols = []
    for orig in df.columns:
        c = str(orig).strip()
        c = " ".join(c.split())
        if c.lower().startswith('unnamed') or c == '':
            cols.append(None)
            continue
        cols.append(c)
        keep_cols.append(orig)
    df = df[keep_cols].copy()
    new_names = [c for c in cols if c is not None]
    seen = {}
    unique_names = []
    for name in new_names:
        if name in seen:
            seen[name] += 1
            unique_names.append(f"{name}_{seen[name]}")
        else:
            seen[name] = 0
            unique_names.append(name)
    df.columns = unique_names
    return df

def simplify_name(name):
    return ''.join(ch for ch in str(name).lower() if ch.isalnum())

def find_target_column(df):
    candidate_names = ['vax_level', 'vax level', 'vaxlevel', 'target', 'label', 'class']
    simplified_map = {simplify_name(c): c for c in df.columns}
    for cand in candidate_names:
        key = simplify_name(cand)
        if key in simplified_map:
            return simplified_map[key]
    for c in df.columns:
        if 'vax' in simplify_name(c):
            return c
    return None

def choose_numeric_target(df):
    best_col = None
    best_var = -np.inf
    for c in df.columns:
        s = pd.to_numeric(df[c], errors='coerce')
        non_na = s.notna().sum()
        if non_na < 2:
            continue
        if s.nunique(dropna=True) <= 1:
            continue
        var = s.var()
        if pd.notna(var) and var > best_var:
            best_var = var
            best_col = c
    return best_col

def choose_categorical_target(df):
    for c in reversed(df.columns):
        if df[c].nunique(dropna=True) > 1:
            return c
    return df.columns[0]

def detect_feature_types(df, exclude):
    numeric_cols = []
    categorical_cols = []
    n_rows = len(df)
    for c in df.columns:
        if c in exclude:
            continue
        s = df[c]
        s_num = pd.to_numeric(s, errors='coerce')
        non_na = s_num.notna().sum()
        if non_na > 0 and (pd.api.types.is_numeric_dtype(s) or non_na / n_rows >= 0.5):
            df[c] = s_num
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)
    return numeric_cols, categorical_cols

def filter_feature_columns(df, numeric_cols, categorical_cols):
    clean_num = []
    for c in numeric_cols:
        s = pd.to_numeric(df[c], errors='coerce')
        s = s.replace([np.inf, -np.inf], np.nan)
        df[c] = s
        if s.notna().sum() == 0:
            continue
        if s.nunique(dropna=True) <= 1:
            continue
        clean_num.append(c)
    clean_cat = []
    n_rows = len(df)
    for c in categorical_cols:
        n_unique = df[c].nunique(dropna=True)
        if n_unique <= 1:
            continue
        if n_unique > max(50, int(0.5 * n_rows)):
            continue
        clean_cat.append(c)
    return clean_num, clean_cat

def determine_task(y):
    y_non = y.dropna()
    if y_non.empty:
        return 'regression'
    if pd.api.types.is_object_dtype(y_non) or pd.api.types.is_bool_dtype(y_non) or pd.api.types.is_categorical_dtype(y_non):
        return 'classification'
    n_unique = y_non.nunique()
    if n_unique <= 20 and n_unique <= max(2, int(0.2 * len(y_non))):
        return 'classification'
    return 'regression'

path = 'town_vax_data.csv'
df = read_csv_robust(path)
df = normalize_columns(df)
df = df.dropna(axis=1, how='all')

target_col = find_target_column(df)
if target_col is None:
    target_col = choose_numeric_target(df)
if target_col is None:
    target_col = choose_categorical_target(df)

df = df[df[target_col].notna()].copy()
assert len(df) > 0

task = determine_task(df[target_col])

if task == 'regression':
    y = pd.to_numeric(df[target_col], errors='coerce')
    y = y.replace([np.inf, -np.inf], np.nan)
    mask = y.notna()
    df = df.loc[mask].copy()
    y = y.loc[mask]
else:
    y = df[target_col]

assert len(df) > 0

numeric_cols, categorical_cols = detect_feature_types(df, exclude=[target_col])
numeric_cols, categorical_cols = filter_feature_columns(df, numeric_cols, categorical_cols)

feature_cols = numeric_cols + categorical_cols
X = df[feature_cols] if feature_cols else pd.DataFrame(index=df.index)

n_samples = len(df)
test_size = max(1, int(0.25 * n_samples))
if n_samples - test_size < 1:
    test_size = n_samples - 1
assert test_size >= 1 and n_samples - test_size >= 1

stratify = None
if task == 'classification':
    try:
        if y.nunique() >= 2:
            class_counts = y.value_counts()
            if class_counts.min() > 1:
                stratify = y
    except Exception:
        stratify = None

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=42, shuffle=True, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0

use_features = len(feature_cols) > 0

if use_features:
    transformers = []
    if len(numeric_cols) > 0:
        num_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler(with_mean=False))
        ])
        transformers.append(('num', num_transformer, numeric_cols))
    if len(categorical_cols) > 0:
        cat_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
        ])
        transformers.append(('cat', cat_transformer, categorical_cols))
    preprocessor = ColumnTransformer(
        transformers=transformers,
        remainder='drop',
        sparse_threshold=0.3
    )
else:
    preprocessor = None

if task == 'classification':
    n_classes = y_train.nunique() if hasattr(y_train, 'nunique') else len(np.unique(y_train))
    if n_classes < 2 or not use_features:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='auto', random_state=42, n_jobs=1)
else:
    if y_train.nunique() < 2 or not use_features:
        model = DummyRegressor(strategy='mean')
    else:
        model = Ridge(alpha=1.0)

if use_features and preprocessor is not None:
    pipeline = Pipeline(steps=[
        ('preprocess', preprocessor),
        ('model', model)
    ])
else:
    pipeline = Pipeline(steps=[
        ('model', model)
    ])

try:
    pipeline.fit(X_train, y_train)
except Exception:
    if task == 'classification':
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = DummyRegressor(strategy='mean')
    if use_features and preprocessor is not None:
        pipeline = Pipeline(steps=[
            ('preprocess', preprocessor),
            ('model', model)
        ])
    else:
        pipeline = Pipeline(steps=[('model', model)])
    pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    if not np.isfinite(r2):
        r2 = 0.0
    accuracy = (r2 + 1.0) / 2.0
    if accuracy < 0.0:
        accuracy = 0.0
    if accuracy > 1.0:
        accuracy = 1.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Used lightweight linear models (LogisticRegression/Ridge) with Dummy fallbacks to minimize CPU load.
# Dropped constant and high-cardinality categorical features to reduce one-hot size and computation.
# Applied simple imputation and sparse-friendly scaling/encoding via a single Pipeline for reproducibility.
# Regression accuracy is a bounded proxy computed as (r2+1)/2 clipped to [0,1].