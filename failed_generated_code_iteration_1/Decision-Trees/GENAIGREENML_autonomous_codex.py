# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")
np.random.seed(42)

DATASET_PATH = "town_vax_data.csv"

DATASET_HEADERS = [
    "town",
    "apartments_condos_multis_per_residential_parcels_2011",
    "assessed_home_value_changes_2009-2013",
    "births_per_1000_residents_2010",
    "boaters_per_10000_residents_2012",
    "burglaries_per_10000_residents_2011",
    "cars_motorcycles_&_trucks_average_age_2012",
    "cars_per_1000_residents_2012",
    "class_size_in_school_district_2011-2012",
    "condos_as_perc_of_parcels_2012",
    "crashes_per_1000_residents_2007-2011",
    "culture_and_rec_spending_per_person_2012",
    "education_spending_as_a_percent_2012",
    "education_spending_per_resident_2012",
    "expenditures_per_resident_2012",
    "females_percent_in_community_2010",
    "fire_dept_spending_as_a_percent_2012",
    "firefighter_costs_per_resident_2012",
    "fixed_costs_percent_2012",
    "gun_licenses_per_1000_residents_2012",
    "historic_places_per_10000_2013",
    "home_schooled_per_1000_students_2011-2012",
    "homes_built_in_39_or_before",
    "household_member_who_is_2_races_or_more_per_1000_households_2010",
    "households_average_size_2010",
    "households_one-person_2010",
    "hybrid_cars_per_1000_vehicles_2013",
    "in_home_since_1969_or_earlier",
    "income_average_per_resident_2010",
    "income_change_per_resident_2007-2010",
    "inmates_in_state_prison_per_1000_residents",
    "liquor_licenses_per_10000_2011",
    "median_age_2011",
    "miles_driven_daily_per_household_05-07",
    "minority_students_per_district_2012-2013",
    "motorcycles_change_in_ownership_2000-2012",
    "motorcycles_per_1000_2012",
    "multi-generation_households_2010",
    "police_costs_per_resident_2013",
    "police_employees_per_10000_residents_2011",
    "police_spending_as_a_percent_2012",
    "population_change_1950-2010",
    "population_change_2010-2011",
    "presidential_fundraising_obama_vs_romney",
    "property_crimes_per_10000_residents_2012",
    "property_tax_change_09-13",
    "pupils_per_cost_average_by_district_2011-2012",
    "residential_taxes_as_percent_of_all_property_taxes_2013",
    "saltwater_fishing_licenses_per_1000_2013",
    "school_district_growth_09-13",
    "single-person_households_percent_65_and_older",
    "snowmobiles_per_10000_residents_2012",
    "state_aid_as_a_percent_of_town_budget_2012",
    "students_in_public_schools_2011",
    "tax-exempt_property_2012",
    "taxable_property_by_percent_2012",
    "teacher_salaries_by_average_2011",
    "teachers_percent_under_40_years_old_2011-2012",
    "trucks_per_1000_residents_2012",
    "violent_crimes_per_10000_residents_2012",
    "voters_as_a_percent_of_population_2012",
    "voters_change_in_registrations_between_1982-2012",
    "voters_democrats_as_a_percent_2012",
    "2020_votes",
    "2020_biden_margin",
    "population",
    "vax_level"
]

def read_csv_fallback(path):
    try:
        df_local = pd.read_csv(path)
    except Exception:
        df_local = pd.read_csv(path, sep=";", decimal=",")
        return df_local
    if df_local.shape[1] == 1:
        try:
            df_alt = pd.read_csv(path, sep=";", decimal=",")
            if df_alt.shape[1] > 1:
                df_local = df_alt
        except Exception:
            pass
    return df_local

def choose_target_column(df_local):
    cols = list(df_local.columns)
    if not cols:
        return None
    lower_map = {c.lower(): c for c in cols}
    for name in ["vax_level", "target", "label", "class", "y"]:
        if name in lower_map:
            return lower_map[name]
    n_rows = len(df_local)
    candidate = None
    best = None
    for c in cols:
        uniq = df_local[c].nunique(dropna=True)
        if uniq <= 1:
            continue
        dtype = df_local[c].dtype
        if dtype == object or str(dtype).startswith("category"):
            if uniq <= min(20, max(2, int(0.1 * n_rows))):
                if best is None or uniq < best:
                    candidate = c
                    best = uniq
    if candidate is not None:
        return candidate
    for c in cols:
        uniq = pd.to_numeric(df_local[c], errors="coerce").nunique(dropna=True)
        if uniq > 1 and uniq <= min(20, max(2, int(0.1 * n_rows))):
            return c
    for c in cols:
        if pd.to_numeric(df_local[c], errors="coerce").nunique(dropna=True) > 1:
            return c
    return cols[-1]

df = read_csv_fallback(DATASET_PATH)

clean_expected = [re.sub(r"\s+", " ", str(c).strip()) for c in DATASET_HEADERS]
if len(df.columns) == len(clean_expected):
    mismatch = 0
    for c in df.columns:
        c_clean = re.sub(r"\s+", " ", str(c).strip())
        if c_clean not in clean_expected:
            mismatch += 1
    if mismatch > len(df.columns) * 0.5:
        df.columns = clean_expected

df.columns = [re.sub(r"\s+", " ", str(c).strip()) for c in df.columns]
df = df.loc[:, ~df.columns.str.match(r"^Unnamed", na=False)]
unique_cols = []
counts = {}
for c in df.columns:
    if c in counts:
        counts[c] += 1
        unique_cols.append(f"{c}_{counts[c]}")
    else:
        counts[c] = 0
        unique_cols.append(c)
df.columns = unique_cols
df = df.dropna(how="all")

target_col = choose_target_column(df)
if target_col is None:
    df["target"] = 0
    target_col = "target"

y_raw = df[target_col]
n_rows = len(df)
if y_raw.dtype == object or str(y_raw.dtype).startswith("category"):
    uniq = y_raw.nunique(dropna=True)
    classification = uniq <= min(20, max(2, int(0.1 * n_rows)))
else:
    y_num_tmp = pd.to_numeric(y_raw, errors="coerce")
    uniq = y_num_tmp.nunique(dropna=True)
    classification = uniq <= 20 and uniq / max(1, len(y_num_tmp)) < 0.2

if classification:
    mask = y_raw.notna()
    if mask.sum() == 0:
        classification = False
        y = pd.Series(np.zeros(len(df)), index=df.index, dtype=float)
    else:
        df = df.loc[mask].copy()
        y = y_raw.loc[mask]
else:
    y_numeric = pd.to_numeric(y_raw, errors="coerce")
    if y_numeric.notna().sum() == 0:
        y_numeric = pd.Series(pd.factorize(y_raw)[0], index=y_raw.index).astype(float)
    mask = y_numeric.notna()
    if mask.sum() == 0:
        y_numeric = pd.Series(np.zeros(len(df)), index=df.index, dtype=float)
        mask = y_numeric.notna()
    df = df.loc[mask].copy()
    y = y_numeric.loc[mask]

assert df.shape[0] > 0

feature_cols = [c for c in df.columns if c != target_col]
if len(feature_cols) == 0:
    df["dummy_feature"] = 0
    feature_cols = ["dummy_feature"]

numeric_cols = []
categorical_cols = []
orig_categorical_cols = []
for col in feature_cols:
    s = df[col]
    if s.isna().all():
        continue
    if s.dtype == object or str(s.dtype).startswith("category"):
        s_str = s.astype(str)
        s_str[s.isna()] = np.nan
        converted = pd.to_numeric(s_str.str.replace(",", ".", regex=False), errors="coerce")
        non_na = s.notna().sum()
        if non_na > 0 and converted.notna().sum() / non_na > 0.8:
            df[col] = converted.replace([np.inf, -np.inf], np.nan)
            numeric_cols.append(col)
        else:
            df[col] = s_str
            categorical_cols.append(col)
            orig_categorical_cols.append(col)
    else:
        df[col] = pd.to_numeric(s, errors="coerce").replace([np.inf, -np.inf], np.nan)
        numeric_cols.append(col)

numeric_cols = [c for c in numeric_cols if df[c].notna().any() and df[c].nunique(dropna=True) > 1]
categorical_cols = [c for c in categorical_cols if df[c].notna().any() and df[c].nunique(dropna=True) > 1]

if len(df) > 0 and categorical_cols:
    filtered = []
    for c in categorical_cols:
        uniq = df[c].nunique(dropna=True)
        if uniq / len(df) > 0.9:
            continue
        filtered.append(c)
    categorical_cols = filtered

if not numeric_cols and not categorical_cols:
    if orig_categorical_cols:
        best_col = min(orig_categorical_cols, key=lambda c: df[c].nunique(dropna=True))
        categorical_cols = [best_col]
    else:
        df["dummy_feature"] = 0
        numeric_cols = ["dummy_feature"]

feature_cols = numeric_cols + categorical_cols
X = df[feature_cols]

transformers = []
if numeric_cols:
    num_pipe = Pipeline(steps=[("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler(with_mean=False))])
    transformers.append(("num", num_pipe, numeric_cols))
if categorical_cols:
    cat_pipe = Pipeline(steps=[("imputer", SimpleImputer(strategy="most_frequent")), ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=True, dtype=np.float32))])
    transformers.append(("cat", cat_pipe, categorical_cols))

preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

n_samples = len(df)
if n_samples > 1:
    test_size = 0.2 if n_samples >= 5 else 0.5
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
else:
    X_train = X_test = X.copy()
    y_train = y_test = y.copy()

assert len(X_train) > 0 and len(X_test) > 0

if classification:
    train_unique = pd.Series(y_train).nunique(dropna=True)
    if train_unique < 2:
        model = DummyClassifier(strategy="most_frequent")
    else:
        model = LogisticRegression(max_iter=200, solver="liblinear", random_state=42)
else:
    train_unique = pd.Series(y_train).nunique(dropna=True)
    if train_unique < 2:
        model = DummyRegressor(strategy="mean")
    else:
        model = Ridge(alpha=1.0)

pipe = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])
pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)

if classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    if len(y_test) < 2:
        r2 = 0.0
    else:
        r2 = r2_score(y_test, y_pred)
        if not np.isfinite(r2):
            r2 = 0.0
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# - Used lightweight linear models (logistic/ridge) with dummy fallbacks to keep CPU cost low.
# - Applied minimal preprocessing (imputation, scaling, one-hot encoding) via a single Pipeline/ColumnTransformer for reproducibility.
# - Converted regression R2 to a bounded [0,1] proxy to provide a stable accuracy-style metric when classification is not suitable.