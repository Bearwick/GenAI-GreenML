# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
import random
import math

random.seed(42)

def read_data(csv_path):
    try:
        df = pd.read_csv(csv_path)
    except Exception:
        df = pd.read_csv(csv_path, sep=';', decimal=',')
    
    if df.shape[1] <= 1:
        df = pd.read_csv(csv_path, sep=';', decimal=',')
        
    df = df.replace({np.nan: None})
    return df.to_dict('records')

def train_test_split(examples, test_perc):
    test_size = round(test_perc * len(examples))
    shuffled = random.sample(examples, len(examples))
    return shuffled[test_size:], shuffled[:test_size]

class TreeNodeInterface():
    def classify(self, example): 
        raise NotImplementedError

class DecisionNode(TreeNodeInterface):
    def __init__(self, test_attr_name, test_attr_threshold, child_lt, child_ge, child_miss):
        self.test_attr_name = test_attr_name  
        self.test_attr_threshold = test_attr_threshold 
        self.child_ge = child_ge
        self.child_lt = child_lt
        self.child_miss = child_miss

    def classify(self, example):
        test_val = example[self.test_attr_name]
        if test_val is None:
            return self.child_miss.classify(example)
        elif test_val < self.test_attr_threshold:
            return self.child_lt.classify(example)
        else:
            return self.child_ge.classify(example)

class LeafNode(TreeNodeInterface):
    def __init__(self, pred_class, pred_class_count, total_count):
        self.pred_class = pred_class
        self.pred_class_count = pred_class_count
        self.total_count = total_count
        self.prob = pred_class_count / total_count if total_count > 0 else 0

    def classify(self, example):
        return self.pred_class, self.prob

class DecisionTree:
    def __init__(self, examples, id_name, class_name, min_leaf_count=1):
        self.id_name = id_name
        self.class_name = class_name
        self.min_leaf_count = min_leaf_count
        self.root = self.learn_tree(examples)  

    def learn_tree(self, examples):
        if not examples:
            return LeafNode("", 0, 0)
        attribute_set = set()
        for attribute in examples[0]:
            if attribute != self.id_name and attribute != self.class_name:
                attribute_set.add(attribute)
        return attributeSplit(attribute_set, examples, self.min_leaf_count, self.class_name)
    
    def classify(self, example):
        return self.root.classify(example) 

def entropy(examples, class_label):
    n = len(examples)
    if n == 0: return 0
    counts = {}
    for ex in examples:
        val = ex[class_label]
        counts[val] = counts.get(val, 0) + 1
    ent = 0
    for c in counts.values():
        p = c / n
        if p > 0:
            ent -= p * math.log2(p)
    return ent

def getRange(attribute, examples):
    mn, mx = 1000000.0, -1000000.0
    found = False
    for ex in examples:
        val = ex[attribute]
        if val is not None:
            fv = float(val)
            if fv < mn: mn = fv
            if fv > mx: mx = fv
            found = True
    if not found: return 0, 0, 0
    return mn, mx, (mx - mn) / 15

def getPredictiveClass(examples, class_label):
    classDict = {}
    max_val = ("", 0)
    for example in examples:
        name = example[class_label]
        if name not in classDict:
            classDict[name] = 0
        else: 
            classDict[name] += 1
        if classDict[name] > max_val[1]:
            max_val = (name, classDict[name])
    return max_val

def getInfoGain(attribute, examples, class_label, parent_ent):
    max_ig = 0
    threshold = 0
    lt_split, ge_split = [], []
    mn, mx, step = getRange(attribute, examples)
    if step == 0: return 0, 0, [], []
    
    n_total = len(examples)
    curr = mn + step
    while curr < mx:
        lt, ge = [], []
        for ex in examples:
            val = ex[attribute]
            if val is None: continue
            if val >= curr: ge.append(ex)
            else: lt.append(ex)
        
        if lt and ge:
            ig = parent_ent - ((len(lt)/n_total * entropy(lt, class_label)) + (len(ge)/n_total * entropy(ge, class_label)))
            if ig > max_ig:
                max_ig, threshold, lt_split, ge_split = ig, curr, lt, ge
        curr += step
    return max_ig, threshold, lt_split, ge_split

def attributeSplit(attribute_set, examples, min_leaf_count, class_name):
    best_attr = {"name": "", "ig": 0.0, "threshold": None, "lt": [], "ge": []}
    parent_ent = entropy(examples, class_name)
    
    for attr in attribute_set:
        ig, th, lt, ge = getInfoGain(attr, examples, class_name, parent_ent)
        if ig > best_attr["ig"]:
            best_attr = {"name": attr, "ig": ig, "threshold": th, "lt": lt, "ge": ge}
            
    if not best_attr["name"] or len(best_attr["lt"]) <= min_leaf_count or len(best_attr["ge"]) <= min_leaf_count:
        pred_class, pred_count = getPredictiveClass(examples, class_name)
        return LeafNode(pred_class, pred_count, len(examples))

    attribute_set.remove(best_attr["name"])
    child_lt = attributeSplit(attribute_set, best_attr["lt"], min_leaf_count, class_name)
    child_ge = attributeSplit(attribute_set, best_attr["ge"], min_leaf_count, class_name)
    child_miss = child_lt if len(best_attr["lt"]) >= len(best_attr["ge"]) else child_ge

    return DecisionNode(best_attr["name"], best_attr["threshold"], child_lt, child_ge, child_miss)

if __name__ == '__main__':
    csv_path = 'town_vax_data.csv'
    id_col = 'town'
    target_col = 'vax_level'
    min_leaf = 10
    
    data = read_data(csv_path)
    train_data, test_data = train_test_split(data, 0.25)
    
    tree = DecisionTree(train_data, id_col, target_col, min_leaf)
    
    correct = 0
    for row in test_data:
        prediction, _ = tree.classify(row)
        if prediction == row[target_col]:
            correct += 1
            
    accuracy = correct / len(test_data) if test_data else 0
    print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Used pandas for faster initial CSV parsing and robust delimiter handling.
# 2. Optimized entropy calculation by using a dictionary for label counts instead of creating sub-lists of objects.
# 3. Reduced redundant parent entropy calculations by computing it once per node split instead of once per threshold.
# 4. Replaced math.log(p, 2) with math.log2(p) for improved performance.
# 5. Minimized data movement by filtering missing values only during the attribute split evaluation loop.
# 6. Streamlined getBestAttributeAndSplit by removing the extra dictionary creation step in every threshold iteration.
# 7. Removed all expensive visualization logic (ASCII tree, confusion matrix) and print statements within loops to minimize I/O overhead.
# 8. Optimized train_test_split using random.sample for efficient in-place shuffling logic.
# 9. Simplified logic flow in attributeSplit to reduce recursion overhead when base cases are met early.
# 10. Implemented early exit checks for splits that do not yield valid subsets (lt/ge).