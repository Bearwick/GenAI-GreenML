# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


RANDOM_STATE = 42
DATASET_PATH = "MNIST_train.csv"


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(dfx):
        if dfx is None or dfx.empty:
            return True
        if dfx.shape[1] == 1:
            first_col = str(dfx.columns[0])
            sample = str(dfx.iloc[0, 0]) if len(dfx) else ""
            if ("," in sample) or (";" in sample) or ("," in first_col) or (";" in first_col):
                return True
        return False

    if looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df = None

    if df is None:
        raise RuntimeError("Failed to read CSV with robust settings.")
    return df


def _pick_target_column(df):
    cols_lower = {c.lower(): c for c in df.columns}
    if "label" in cols_lower:
        return cols_lower["label"]

    nunique = df.nunique(dropna=True)
    candidates = [c for c in df.columns if nunique.get(c, 0) > 1]
    if not candidates:
        return df.columns[0]

    num_candidates = []
    for c in candidates:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().mean() >= 0.5:
            num_candidates.append(c)

    if num_candidates:
        return num_candidates[0]
    return candidates[0]


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    denom = np.var(y_true)
    if not np.isfinite(denom) or denom <= 0:
        mae = np.mean(np.abs(y_true - y_pred))
        score = 1.0 / (1.0 + float(mae))
        return float(np.clip(score, 0.0, 1.0))
    mse = np.mean((y_true - y_pred) ** 2)
    r2 = 1.0 - float(mse / denom)
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


df = _read_csv_robust(DATASET_PATH)

df.columns = _normalize_columns(df.columns)
df = df.loc[:, ~df.columns.str.match(r"^Unnamed:\s*\d+$", na=False)]

assert df is not None and not df.empty

target_col = _pick_target_column(df)

X_df = df.drop(columns=[target_col], errors="ignore")
y_raw = df[target_col].copy()

if X_df.shape[1] == 0:
    X_df = pd.DataFrame({"__dummy__": np.zeros(len(df), dtype=np.float32)})

num_cols = []
cat_cols = []
for c in X_df.columns:
    s_num = pd.to_numeric(X_df[c], errors="coerce")
    if s_num.notna().mean() >= 0.8:
        num_cols.append(c)
    else:
        cat_cols.append(c)

for c in num_cols:
    X_df[c] = pd.to_numeric(X_df[c], errors="coerce").replace([np.inf, -np.inf], np.nan)

y_num = pd.to_numeric(y_raw, errors="coerce")
is_classification = False
y = None

if y_num.notna().mean() >= 0.95:
    yn = y_num.dropna()
    uniq = pd.unique(yn)
    if len(uniq) >= 2:
        if len(uniq) <= 50 and np.all(np.isclose(uniq, np.round(uniq))):
            is_classification = True
            y = y_num.round().astype("Int64")
        else:
            is_classification = False
            y = y_num.astype(float)
    else:
        is_classification = False
        y = y_num.astype(float)
else:
    y_str = y_raw.astype(str)
    uniq = pd.unique(y_str)
    if len(uniq) >= 2:
        is_classification = True
        y = y_str
    else:
        y = y_num.astype(float)
        is_classification = False

mask = pd.Series(True, index=df.index)
if is_classification:
    mask &= y.notna()
else:
    mask &= pd.to_numeric(y, errors="coerce").notna()

if num_cols:
    mask &= X_df[num_cols].notna().any(axis=1)

X_df = X_df.loc[mask].reset_index(drop=True)
y = y.loc[mask].reset_index(drop=True)

assert len(X_df) > 0 and len(y) > 0

test_size = 0.2
stratify = y if (is_classification and y.nunique(dropna=True) >= 2) else None

X_train, X_test, y_train, y_test = train_test_split(
    X_df, y, test_size=test_size, random_state=RANDOM_STATE, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0

numeric_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="constant", fill_value=0.0)),
        ("scaler", MaxAbsScaler()),
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ]
)

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, num_cols),
        ("cat", categorical_transformer, cat_cols),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

if is_classification and y_train.nunique(dropna=True) >= 2:
    clf = LogisticRegression(
        solver="saga",
        penalty="l2",
        C=1.0,
        max_iter=200,
        tol=1e-3,
        n_jobs=1,
        random_state=RANDOM_STATE,
        multi_class="auto",
    )
    model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
else:
    reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
    model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
    y_train_reg = pd.to_numeric(y_train, errors="coerce").astype(float)
    y_test_reg = pd.to_numeric(y_test, errors="coerce").astype(float)
    model.fit(X_train, y_train_reg)
    y_pred = model.predict(X_test)
    accuracy = _bounded_regression_score(y_test_reg, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Uses CPU-friendly linear models (LogisticRegression with saga / Ridge) as strong baselines for high-dimensional sparse/tabular data.
# - MaxAbsScaler chosen to scale pixel-like features efficiently without centering (works well with sparse/high-dim and avoids extra compute).
# - SimpleImputer with constant/most_frequent minimizes expensive statistics and ensures robust handling of NaN/inf.
# - ColumnTransformer + Pipeline ensures single-pass, reproducible preprocessing without manual feature loops.
# - Robust schema handling: auto-normalizes headers, drops Unnamed columns, infers numeric vs categorical, and selects target defensively.
# - Regression fallback uses a bounded R2-like proxy mapped to [0,1] to keep ACCURACY stable even when classification is not feasible.