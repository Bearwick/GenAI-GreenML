# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
import os

def load_data(path):
    # Robust CSV loading with fallbacks for common delimiters
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    
    # Normalize column names: strip whitespace, handle case, remove Unnamed
    df.columns = [str(c).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def solve():
    dataset_path = 'MNIST_train.csv'
    
    if not os.path.exists(dataset_path):
        # Fallback for environment where file might not exist during initialization
        # Create a dummy structure for structural validation if needed
        return

    df = load_data(dataset_path)
    
    if df.empty:
        return

    # Derive schema: Search for 'label' or use first column as target
    target_candidates = [c for c in df.columns if 'label' in c.lower()]
    if target_candidates:
        target_col = target_candidates[0]
    else:
        target_col = df.columns[0]
        
    # Features are everything else
    feature_cols = [c for c in df.columns if c != target_col]
    
    # Ensure data is numeric and handle missing values
    X = df[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0)
    y = pd.to_numeric(df[target_col], errors='coerce').fillna(0).astype(int)

    # Check for classification validity
    unique_classes = np.unique(y)
    if len(unique_classes) < 2:
        # Fallback to a trivial "accuracy" if target is constant
        print(f"ACCURACY={1.000000:.6f}")
        return

    # Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if len(unique_classes) > 1 else None
    )

    # Pipeline: Scaling + Lightweight Logistic Regression
    # Logistic Regression with 'lbfgs' is CPU efficient for multiclass problems like MNIST
    # We use a limited max_iter to cap energy consumption/training time
    model = Pipeline([
        ('scaler', StandardScaler()),
        ('clf', LogisticRegression(
            multi_class='multinomial', 
            solver='lbfgs', 
            max_iter=100, 
            tol=1e-3,
            n_jobs=1, # Sequential processing to maintain a low thermal profile on single-core setups
            random_state=42
        ))
    ])

    # Model Training
    model.fit(X_train, y_train)

    # Evaluation
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# Optimization Summary:
# 1. Model Selection: Used Logistic Regression (LBFGS) as it is significantly more energy-efficient 
#    than Deep Learning or Ensembles for high-dimensional pixel data while providing a strong baseline.
# 2. Computational Efficiency: Limited 'max_iter' and increased 'tol' to prevent the solver 
#    from wasting CPU cycles on marginal convergence gains (Green Coding principle).
# 3. Memory Footprint: Used standard pandas/sklearn pipelines which are optimized for CPU/RAM efficiency.
# 4. Robustness: Implemented a schema-agnostic column selector and robust CSV parser to ensure the 
#    pipeline runs end-to-end regardless of minor formatting variations.
# 5. Scalability: Standardizing features (StandardScaler) ensures faster convergence for linear solvers.
# 6. Fallback logic included to handle edge cases like single-class targets without crashing.