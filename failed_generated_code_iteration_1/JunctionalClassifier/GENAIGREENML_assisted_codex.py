# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import pickle
import random
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

warnings.filterwarnings("ignore")

RANDOM_STATE = 42
os.environ["PYTHONHASHSEED"] = str(RANDOM_STATE)
random.seed(RANDOM_STATE)
np.random.seed(RANDOM_STATE)

DATASET_PATH = "dict.pickle"
DATASET_HEADERS = None


def read_csv_with_fallback(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] == 1:
            sample = str(df.iloc[0, 0]) if len(df) else ""
            header = str(df.columns[0])
            if ";" in header or ";" in sample:
                df = pd.read_csv(path, sep=";", decimal=",")
        return df
    except Exception:
        return pd.read_csv(path, sep=";", decimal=",")


def load_raw(path):
    if os.path.exists(path):
        try:
            with open(path, "rb") as f:
                return pickle.load(f)
        except Exception:
            pass
    return read_csv_with_fallback(path)


def resolve_columns(df):
    if isinstance(DATASET_HEADERS, (list, tuple)) and DATASET_HEADERS:
        cols = [c for c in DATASET_HEADERS if c in df.columns]
        if cols:
            return cols
    return list(df.columns)


def extract_from_dataframe(df, force_label=True):
    if df is None or df.empty:
        raise ValueError("Empty dataset")
    cols = resolve_columns(df)
    target_col = None
    for c in cols:
        if str(c).lower() in ("label", "labels", "target", "class", "y"):
            target_col = c
            break
    if target_col is None and not force_label:
        last_col = cols[-1]
        series = df[last_col]
        if pd.api.types.is_numeric_dtype(series):
            unique_vals = pd.unique(series.dropna())
            if unique_vals.size <= 10 and np.all(np.isclose(unique_vals, np.round(unique_vals))):
                target_col = last_col
    if target_col is None and force_label:
        target_col = cols[-1]
    if target_col is None:
        return df.to_numpy(), None
    y = df[target_col].to_numpy()
    X = df.drop(columns=[target_col]).to_numpy()
    return X, y


def extract_from_dict(d):
    lower_keys = {str(k).lower(): k for k in d.keys()}
    feature_key = None
    label_key = None
    for k in ("data", "x", "features", "feature", "inputs", "input"):
        if k in lower_keys:
            feature_key = lower_keys[k]
            break
    for k in ("target", "y", "labels", "label", "class", "classes", "output", "outputs"):
        if k in lower_keys:
            label_key = lower_keys[k]
            break
    if feature_key is not None and label_key is not None:
        return np.asarray(d[feature_key]), np.asarray(d[label_key])
    arrays = []
    for v in d.values():
        if isinstance(v, (np.ndarray, list, tuple, pd.Series, pd.DataFrame)):
            arrays.append(np.asarray(v))
    if len(arrays) == 1:
        arr = arrays[0]
        if arr.ndim == 2 and arr.shape[1] > 1:
            return arr[:, :-1], arr[:, -1]
    if len(arrays) >= 2:
        X_candidate = None
        y_candidate = None
        for arr in arrays:
            if arr.ndim == 2 and arr.shape[1] == 1:
                if y_candidate is None or arr.shape[0] > y_candidate.shape[0]:
                    y_candidate = arr.ravel()
            elif arr.ndim == 2:
                if X_candidate is None or arr.shape[1] > X_candidate.shape[1]:
                    X_candidate = arr
            elif arr.ndim == 1:
                if y_candidate is None or arr.shape[0] > y_candidate.shape[0]:
                    y_candidate = arr
        if X_candidate is not None and y_candidate is not None and X_candidate.shape[0] == y_candidate.shape[0]:
            return X_candidate, y_candidate
    raise ValueError("Unable to extract dataset from dictionary")


def extract_features_labels(obj):
    if isinstance(obj, pd.DataFrame):
        return extract_from_dataframe(obj, force_label=True)
    if isinstance(obj, np.ndarray):
        if obj.ndim == 2 and obj.shape[1] > 1:
            return obj[:, :-1], obj[:, -1]
        if obj.ndim == 1:
            raise ValueError("No feature matrix")
    if isinstance(obj, (list, tuple)):
        if len(obj) == 2:
            X, y = obj
            return np.asarray(X), np.asarray(y)
    if isinstance(obj, dict):
        return extract_from_dict(obj)
    if hasattr(obj, "data") and hasattr(obj, "target"):
        return np.asarray(obj.data), np.asarray(obj.target)
    raise ValueError("Unsupported dataset format")


def ensure_numeric(X):
    X = np.asarray(X)
    if X.dtype.kind in "iuf":
        return X.astype(np.float64, copy=False)
    if X.ndim == 1:
        return pd.to_numeric(pd.Series(X), errors="coerce").to_numpy()
    df = pd.DataFrame(X)
    df = df.apply(pd.to_numeric, errors="coerce")
    return df.to_numpy()


def reshape_features(X):
    X = np.asarray(X)
    if X.ndim == 1:
        X = X.reshape(-1, 1)
    return X


def transform_labels(y):
    y = np.asarray(y).ravel()
    if y.dtype.kind not in "iuf":
        y = pd.to_numeric(pd.Series(y), errors="coerce").to_numpy()
    y = y.astype(np.float64, copy=False)
    return np.sign(y)


def train_and_evaluate(X, y):
    X = reshape_features(ensure_numeric(X))
    y = transform_labels(y)
    valid_mask = ~np.isnan(y)
    if np.isnan(X).any():
        valid_mask &= ~np.isnan(X).any(axis=1)
    X = X[valid_mask]
    y = y[valid_mask]
    if X.shape[0] < 2:
        return 0.0
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=RANDOM_STATE, shuffle=True
    )
    scaler = StandardScaler(copy=False)
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    model = MLPClassifier(hidden_layer_sizes=(30, 30, 30, 30), max_iter=1000, random_state=RANDOM_STATE)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return accuracy_score(y_test, y_pred)


def load_input_data(path):
    if not os.path.exists(path):
        return None, None
    df = read_csv_with_fallback(path)
    return extract_from_dataframe(df, force_label=False)


def evaluate_model(model, X, y):
    X = reshape_features(ensure_numeric(X))
    scaler = StandardScaler(copy=False)
    X_scaled = scaler.fit_transform(X)
    preds = model.predict(X_scaled)
    if y is None:
        return 0.0
    y = transform_labels(y)
    valid_mask = ~np.isnan(y)
    if np.isnan(X_scaled).any():
        valid_mask &= ~np.isnan(X_scaled).any(axis=1)
        preds = preds[valid_mask]
    y = y[valid_mask]
    if y.size == 0:
        return 0.0
    return accuracy_score(y, preds)


def main():
    raw = load_raw(DATASET_PATH)
    accuracy = None
    try:
        X, y = extract_features_labels(raw)
        if y is None:
            raise ValueError("Labels missing")
        accuracy = train_and_evaluate(X, y)
    except Exception:
        if hasattr(raw, "predict"):
            X, y = load_input_data("input.csv")
            if X is not None:
                accuracy = evaluate_model(raw, X, y)
            else:
                accuracy = 0.0
        else:
            accuracy = 0.0
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced manual row-wise parsing with vectorized pandas loading and delimiter fallback to reduce Python overhead.
# - Centralized feature/label extraction and numeric conversion to avoid redundant data movement and copies.
# - Used vectorized label normalization with np.sign for efficient preprocessing.
# - Applied in-place scaling (copy=False) and fit_transform to limit intermediate allocations.
# - Added fixed seeds and random_state settings for deterministic, reproducible results.