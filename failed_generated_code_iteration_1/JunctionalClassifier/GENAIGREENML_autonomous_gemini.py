# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
import io
import os
import pickle

def run_pipeline():
    # 1. Robust Data Loading
    dataset_path = 'dict.pickle'
    df = pd.DataFrame()

    try:
        if os.path.exists(dataset_path):
            with open(dataset_path, 'rb') as f:
                data = pickle.load(f)
            
            if isinstance(data, dict):
                # Attempt to extract the most relevant dataframe (e.g., 14k as mentioned in README)
                if '14k.csv' in data:
                    df_raw = data['14k.csv']
                else:
                    # Fallback: largest dataframe or first parseable object
                    dfs = [v for v in data.values() if isinstance(v, pd.DataFrame)]
                    if dfs:
                        df_raw = max(dfs, key=len)
                    else:
                        df_raw = None
                        for v in data.values():
                            try:
                                content = v.decode('utf-8') if isinstance(v, bytes) else str(v)
                                df_raw = pd.read_csv(io.StringIO(content))
                                if not df_raw.empty: break
                            except: continue
                df = df_raw if df_raw is not None else pd.DataFrame()
            elif isinstance(data, pd.DataFrame):
                df = data
    except Exception:
        df = pd.DataFrame()

    # 2. Schema Normalization & Defensive Checks
    if df.empty:
        print(f"ACCURACY={0.0:.6f}")
        return

    # Clean headers: strip, normalize whitespace, remove Unnamed
    df.columns = [str(c).strip() for c in df.columns]
    df.columns = [" ".join(str(c).split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

    # 3. Identify Target Column
    # Context suggests -1, 0, 1 for Junctional Classification
    target_col = None
    
    # Check for columns matching values mentioned in README
    for col in df.columns:
        try:
            unique_vals = df[col].dropna().unique()
            if set(unique_vals).issubset({-1, 0, 1, -1.0, 0.0, 1.0}) and len(unique_vals) >= 2:
                target_col = col
                break
        except: continue

    # Fallback to keyword search
    if target_col is None:
        keywords = ['target', 'label', 'class', 'junction', 'output', 'type']
        for col in df.columns:
            if any(k in col.lower() for k in keywords):
                target_col = col
                break
    
    # Ultimate fallback to last column
    if target_col is None:
        target_col = df.columns[-1]

    # 4. Feature Selection & Data Cleaning
    # Drop rows where target is NaN
    df = df.dropna(subset=[target_col])
    
    # Process Target
    y_raw = df[target_col]
    if y_raw.dtype == object or len(np.unique(y_raw)) > 20:
        # If target is categorical or looks continuous but used for classification
        le = LabelEncoder()
        y = le.fit_transform(y_raw.astype(str))
    else:
        y = y_raw.astype(int)

    # Process Features (Numeric focus for efficiency)
    X = df.drop(columns=[target_col])
    X = X.select_dtypes(include=[np.number])
    X = X.apply(pd.to_numeric, errors='coerce')

    if X.empty or len(np.unique(y)) < 2:
        print(f"ACCURACY={0.0:.6f}")
        return

    # 5. Train/Test Split
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
    except:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

    # 6. Energy-Efficient Pipeline
    # Logistic Regression is used for its low CPU footprint and linear scaling
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')), # Lightweight imputation
        ('scaler', StandardScaler()),                  # Essential for linear models
        ('clf', LogisticRegression(
            max_iter=1000, 
            solver='lbfgs', 
            multi_class='auto', 
            random_state=42,
            n_jobs=1 # Minimize parallel overhead for energy efficiency
        ))
    ])

    # 7. Execution & Evaluation
    try:
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
    except:
        accuracy = 0.0

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary:
# 1. Model Selection: Logistic Regression was selected as the baseline because it provides a high-performance, CPU-friendly classification approach compared to computationally intensive ensembles or deep learning.
# 2. Resource Efficiency: The pipeline utilizes `n_jobs=1` and `lbfgs` solver to minimize multi-core energy spikes and memory usage during training on CPU.
# 3. Data Robustness: Implemented a robust schema detection logic that cleans headers and identifies the classification target based on data values (-1, 0, 1) or semantic naming.
# 4. Preprocessing: Used a scikit-learn Pipeline with `SimpleImputer` (median strategy) and `StandardScaler` for reproducible, low-latency feature engineering.
# 5. Reliability: Added multiple fallback layers for dataset loading and target identification to ensure end-to-end execution regardless of minor schema variations.
# 6. Evaluation: Used a standard stratified 80/20 train-test split to ensure the accuracy metric is robust and representative of model generalization.