# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier
from sklearn.metrics import accuracy_score, r2_score

data_path = "Data/Diabetes-Training.csv"

def load_dataset(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is None or df.shape[1] == 1:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df is None or df_alt.shape[1] > 1:
                df = df_alt
        except Exception:
            pass
    return df

def normalize_col_name(name):
    return re.sub(r'\s+', ' ', str(name).strip()).lower()

def coerce_numeric(series):
    if pd.api.types.is_numeric_dtype(series):
        return pd.to_numeric(series, errors='coerce')
    ser = series.astype(str).str.replace(',', '.', regex=False)
    return pd.to_numeric(ser, errors='coerce')

df = load_dataset(data_path)
if df is None:
    raise FileNotFoundError(f"Could not read data at {data_path}")

df.columns = [normalize_col_name(c) for c in df.columns]
df = df.loc[:, [c for c in df.columns if not c.startswith('unnamed')]]

expected_headers = ['preg', 'plas', 'pres', 'skin', 'insu', 'mass', 'pedi', 'age', 'class']
expected_headers = [normalize_col_name(c) for c in expected_headers]
if len(df.columns) == len(expected_headers):
    matches = sum(1 for c in df.columns if c in expected_headers)
    if matches < len(expected_headers) // 2:
        df.columns = expected_headers

df = df.replace([np.inf, -np.inf], np.nan)

def select_target_column(df):
    candidates = ['class', 'target', 'label', 'y', 'outcome']
    for c in df.columns:
        if c.lower() in candidates:
            return c
    non_constant = [c for c in df.columns if df[c].nunique(dropna=True) > 1]
    numeric_like = []
    for c in non_constant:
        coerced = coerce_numeric(df[c])
        if coerced.notna().mean() > 0.5:
            numeric_like.append(c)
    if numeric_like:
        return numeric_like[-1]
    if non_constant:
        return non_constant[-1]
    return df.columns[-1]

target_col = select_target_column(df)

feature_cols = [c for c in df.columns if c != target_col]
if feature_cols:
    df_features = df[feature_cols].copy()
else:
    df_features = pd.DataFrame(index=df.index)
    df_features['constant'] = 1.0
    feature_cols = ['constant']

numeric_features = []
categorical_features = []
for col in feature_cols:
    series = df_features[col]
    if pd.api.types.is_numeric_dtype(series):
        numeric_features.append(col)
    else:
        coerced = coerce_numeric(series)
        if coerced.notna().mean() > 0.9:
            df_features[col] = coerced
            numeric_features.append(col)
        else:
            categorical_features.append(col)

for col in numeric_features:
    df_features[col] = coerce_numeric(df_features[col])

all_nan_cols = [c for c in df_features.columns if df_features[c].isna().all()]
if all_nan_cols:
    df_features = df_features.drop(columns=all_nan_cols)
    feature_cols = [c for c in feature_cols if c not in all_nan_cols]
    numeric_features = [c for c in numeric_features if c not in all_nan_cols]
    categorical_features = [c for c in categorical_features if c not in all_nan_cols]

if not feature_cols:
    df_features['constant'] = 1.0
    feature_cols = ['constant']
    numeric_features = ['constant']
    categorical_features = []

y_raw = df[target_col]
y_numeric = coerce_numeric(y_raw)
if y_numeric.notna().mean() > 0.9:
    y = y_numeric
    y_is_numeric = True
else:
    y = y_raw
    y_is_numeric = False

mask = ~pd.isna(y)
if not y_is_numeric:
    mask &= y.astype(str).str.strip() != ''
df_features = df_features.loc[mask]
y = y.loc[mask]

assert len(df_features) > 0

if not y_is_numeric:
    task = 'classification'
else:
    unique_count = y.nunique(dropna=True)
    task = 'classification' if unique_count <= 20 else 'regression'

if task == 'classification':
    n_classes = int(pd.Series(y).nunique())
else:
    n_classes = 0

test_size = 0.2
if len(df_features) < 5:
    test_size = 0.5

stratify = None
if task == 'classification' and n_classes >= 2:
    counts = pd.Series(y).value_counts()
    if counts.min() >= 2:
        stratify = y

try:
    X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=test_size, random_state=42, stratify=stratify)
except ValueError:
    X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=test_size, random_state=42, stratify=None)

assert len(X_train) > 0 and len(X_test) > 0

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
])

transformers = []
if numeric_features:
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    transformers.append(('cat', categorical_transformer, categorical_features))

preprocessor = ColumnTransformer(transformers=transformers)

if task == 'classification':
    if n_classes < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        if n_classes > 2:
            model = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='auto')
        else:
            model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    model = Ridge(alpha=1.0)

model_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])
model_pipeline.fit(X_train, y_train)
y_pred = model_pipeline.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    if len(np.unique(y_test)) > 1:
        r2 = r2_score(y_test, y_pred)
    else:
        r2 = 0.0
    if not np.isfinite(r2):
        r2 = 0.0
    accuracy = float(np.clip(r2, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Lightweight linear models (LogisticRegression/Ridge) with DummyClassifier fallback keep CPU usage low.
# - Reproducible preprocessing via ColumnTransformer with simple imputers and one-hot encoding.
# - Minimal feature engineering, only standard scaling for numeric stability in linear models.
# - Regression fallback reports a clipped R2 score in [0,1] as an accuracy proxy.