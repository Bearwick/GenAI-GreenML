# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.exceptions import ConvergenceWarning
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler


warnings.filterwarnings("ignore", category=ConvergenceWarning)
warnings.filterwarnings("ignore", category=UserWarning)

RANDOM_STATE = 42
DATASET_PATH = os.path.join("datasets", "train.csv")


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    # Try default parsing
    df = pd.read_csv(path)
    # Heuristic: if only one column and it contains separators, retry with ';' and decimal ','
    if df.shape[1] <= 1:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        if df2.shape[1] > df.shape[1]:
            df = df2
    return df


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:.*", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _coerce_numeric_inplace(df):
    # Coerce object columns that look numeric; keep true categoricals for OHE
    for c in df.columns:
        if df[c].dtype == "object":
            s = df[c].astype(str).str.strip()
            # If most values are parseable as numbers, convert
            parsed = pd.to_numeric(s, errors="coerce")
            ratio = float(parsed.notna().mean()) if len(parsed) else 0.0
            if ratio >= 0.85:
                df[c] = parsed
    # Ensure numeric dtypes are clean
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    for c in num_cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _choose_target(df):
    # Prefer common target names, but never hard-fail if missing
    preferred = ["label", "target", "y", "class"]
    for name in preferred:
        if name in df.columns:
            return name

    # Otherwise pick a non-constant numeric column with few unique values (classification-likely)
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    best = None
    best_score = -1
    for c in num_cols:
        nun = int(df[c].nunique(dropna=True))
        if nun <= 1:
            continue
        # Prefer fewer unique values; avoid IDs by penalizing very high uniqueness
        score = 0
        if nun <= 20:
            score += 3
        if nun <= 100:
            score += 1
        # Penalize near-unique columns (likely identifiers)
        uniq_ratio = float(nun) / max(1, len(df))
        if uniq_ratio > 0.8:
            score -= 3
        if score > best_score:
            best_score = score
            best = c

    if best is not None:
        return best

    # Fallback: last column
    return df.columns[-1]


def _is_classification_target(y):
    # Classification if integer-like or few unique categories
    if y.dtype == "object":
        return True
    y_nonan = y.dropna()
    if y_nonan.empty:
        return False
    nun = int(y_nonan.nunique())
    if nun < 2:
        return False
    if nun <= 20:
        return True
    # If numeric but integer-like and limited distinct relative to n, likely classification
    if np.all(np.isclose(y_nonan.values, np.round(y_nonan.values))):
        if nun <= min(100, int(0.1 * len(y_nonan)) + 2):
            return True
    return False


def _bounded_regression_score(y_true, y_pred):
    # Stable "accuracy proxy" in [0,1]: 1 / (1 + NMAE) where NMAE = MAE / (IQR + eps)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = float(np.nanmean(np.abs(y_true - y_pred)))
    q75, q25 = np.nanpercentile(y_true, [75, 25])
    iqr = float(q75 - q25)
    denom = iqr if iqr > 1e-12 else float(np.nanstd(y_true))
    if not np.isfinite(denom) or denom <= 1e-12:
        denom = 1.0
    nmae = mae / denom
    score = 1.0 / (1.0 + nmae)
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Ensure not empty
    assert df.shape[0] > 0 and df.shape[1] > 0, "Empty dataset after loading."

    # Coerce numeric where appropriate
    df = _coerce_numeric_inplace(df)

    target_col = _choose_target(df)

    # Separate X/y with defensive fallbacks
    y = df[target_col] if target_col in df.columns else df.iloc[:, -1]
    X = df.drop(columns=[target_col], errors="ignore")

    # Drop obvious ID column from features (energy-saving and avoids leakage)
    for id_name in ["id", "ID", "Id"]:
        if id_name in X.columns:
            X = X.drop(columns=[id_name], errors="ignore")

    # Replace inf with nan
    X = X.replace([np.inf, -np.inf], np.nan)
    if isinstance(y, pd.Series):
        y = y.replace([np.inf, -np.inf], np.nan)

    # Drop rows with missing target
    mask = y.notna()
    X = X.loc[mask].copy()
    y = y.loc[mask].copy()

    assert len(X) > 1, "Not enough rows after dropping missing target."

    is_clf = _is_classification_target(y)

    # Minimal, CPU-friendly split
    stratify = None
    if is_clf:
        y_tmp = y.astype("category")
        # Only stratify if each class has at least 2 samples
        vc = y_tmp.value_counts()
        if (vc.min() if len(vc) else 0) >= 2:
            stratify = y_tmp

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
    )
    assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

    # Identify column types
    numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()
    categorical_features = [c for c in X_train.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Handle degenerate classification targets
    accuracy = 0.0
    if is_clf:
        y_train_nonan = pd.Series(y_train).dropna()
        n_classes = int(y_train_nonan.nunique())
        if n_classes < 2:
            # Trivial baseline: always predict the only class
            majority = y_train_nonan.iloc[0] if len(y_train_nonan) else 0
            y_pred = np.full(shape=len(y_test), fill_value=majority, dtype=object)
            accuracy = float(accuracy_score(y_test, y_pred))
        else:
            # Logistic regression: efficient linear baseline; 'saga' supports sparse one-hot
            model = LogisticRegression(
                max_iter=200,
                solver="saga",
                n_jobs=1,
                multi_class="auto",
                class_weight=None,
                tol=1e-3,
                C=1.0,
                random_state=RANDOM_STATE,
            )
            clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Regression fallback: Ridge is light, stable, and CPU-friendly
        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        reg = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        accuracy = _bounded_regression_score(np.asarray(y_test, dtype=float), np.asarray(y_pred, dtype=float))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Chose lightweight linear models (LogisticRegression / Ridge) for CPU efficiency and low energy use; avoided ensembles/deep learning.
# - Used sklearn Pipeline + ColumnTransformer for reproducible, single-pass preprocessing (impute + scale numeric; impute + one-hot categorical).
# - Robust schema handling: normalize headers, drop 'Unnamed' columns, infer target if missing, coerce numeric safely, handle NaN/inf.
# - Kept split simple with fixed random_state; stratify only when safe to avoid failures on rare classes.
# - Regression fallback uses bounded score in [0,1]: ACCURACY = 1/(1+NMAE), NMAE=MAE/(IQR+eps), providing stable printing format.