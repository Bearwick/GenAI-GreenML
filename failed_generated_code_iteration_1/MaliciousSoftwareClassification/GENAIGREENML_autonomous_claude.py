# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

data_path = './datasets/'

# Robust CSV loading
try:
    train = pd.read_csv(os.path.join(data_path, 'train.csv'))
    if train.shape[1] < 3:
        train = pd.read_csv(os.path.join(data_path, 'train.csv'), sep=';', decimal=',')
except Exception:
    train = pd.read_csv(os.path.join(data_path, 'train.csv'), sep=';', decimal=',')

# Normalize column names
train.columns = [c.strip().replace('  ', ' ') for c in train.columns]
train = train[[c for c in train.columns if not c.startswith('Unnamed')]]

# Determine target column
ycol = 'label'
if ycol not in train.columns:
    # Fallback: pick last column or first non-constant numeric column
    numeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()
    if numeric_cols:
        ycol = numeric_cols[-1]
    else:
        ycol = train.columns[-1]

# Determine feature columns
features = [c for c in train.columns if c not in [ycol, 'id']]

# Coerce features to numeric
for c in features:
    train[c] = pd.to_numeric(train[c], errors='coerce')

# Coerce target to numeric if possible
train[ycol] = pd.to_numeric(train[ycol], errors='coerce')

# Drop rows with NaN in target
train = train.dropna(subset=[ycol])

# Fill NaN in features with 0
train[features] = train[features].fillna(0)

# Replace inf
train[features] = train[features].replace([np.inf, -np.inf], 0)

assert len(train) > 0, "Dataset empty after preprocessing"
assert len(features) > 0, "No features available"

X = train[list(features)].values
y = train[ycol].values.astype(int)

num_classes = len(np.unique(y))
assert num_classes >= 2, "Need at least 2 classes for classification"

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

assert len(X_train) > 0 and len(X_test) > 0, "Empty split"

# Energy-efficient pipeline: StandardScaler + LogisticRegression
# LogisticRegression with saga solver handles multiclass efficiently
pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression(
        max_iter=1000,
        solver='saga',
        multi_class='multinomial',
        C=1.0,
        random_state=42,
        n_jobs=-1,
        tol=1e-3,
    ))
])

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Used LogisticRegression (multinomial, saga solver) instead of LightGBM ensemble
#    - Much lower energy consumption on CPU, fast convergence for this feature set
# 2. StandardScaler ensures numeric stability for the linear model
# 3. Simple train/test split (80/20) with stratification for reproducibility
# 4. No deep learning, no large ensembles - minimal computational footprint
# 5. Robust CSV parsing with fallback separators and column normalization
# 6. All features