# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import glob
import re
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, SGDRegressor
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, mean_absolute_error

dataset_headers = [
    "school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob",
    "reason","guardian","traveltime","studytime","failures","schoolsup","famsup",
    "paid","activities","nursery","higher","internet","romantic","famrel","freetime",
    "goout","Dalc","Walc","health","absences","G1","G2","G3"
]

def clean_column(name):
    name = str(name).strip()
    name = re.sub(r"\s+", " ", name)
    return name if name else "col"

def norm_key(name):
    return re.sub(r"[^0-9a-zA-Z]+", "", str(name).lower())

def get_header(path):
    try:
        df_head = pd.read_csv(path, nrows=0)
    except Exception:
        df_head = None
    if df_head is None or df_head.shape[1] <= 1:
        try:
            df_head = pd.read_csv(path, sep=";", decimal=",", nrows=0)
        except Exception:
            return []
    return list(df_head.columns)

def find_dataset():
    candidates = []
    env_path = os.environ.get("DATASET_PATH")
    if env_path and os.path.isfile(env_path):
        candidates.append(env_path)
    for p in ["./data.csv","./dataset.csv","./data/dataset.csv","./data/student-mat.csv","./student-mat.csv","./data/student-por.csv"]:
        if os.path.isfile(p):
            candidates.append(p)
    if not candidates:
        candidates = glob.glob("*.csv") + glob.glob("./data/*.csv")
    if not candidates:
        raise FileNotFoundError("No CSV file found.")
    if len(candidates) == 1:
        return candidates[0]
    norm_headers = set(norm_key(h) for h in dataset_headers)
    best_path = candidates[0]
    best_score = -1
    for path in candidates:
        cols = get_header(path)
        norm_cols = set(norm_key(c) for c in cols)
        score = sum(1 for h in norm_headers if h in norm_cols)
        if score > best_score:
            best_score = score
            best_path = path
    return best_path

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    need_alt = False
    if df is None:
        need_alt = True
    else:
        if df.shape[1] <= 1:
            col0 = df.columns[0] if len(df.columns) > 0 else ""
            sample = df.iloc[0, 0] if len(df) > 0 else ""
            if (isinstance(col0, str) and ";" in col0) or (isinstance(sample, str) and ";" in str(sample)):
                need_alt = True
    if need_alt:
        try:
            df_alt = pd.read_csv(path, sep=";", decimal=",")
            if df is None or df_alt.shape[1] > (df.shape[1] if df is not None else 0):
                df = df_alt
        except Exception:
            if df is None:
                raise
    return df

path = find_dataset()
df = read_csv_robust(path)

df.columns = [clean_column(c) for c in df.columns]
df = df.loc[:, ~df.columns.str.contains(r"^Unnamed", case=False, regex=True)]
cols = []
counts = {}
for c in df.columns:
    if c not in counts:
        counts[c] = 0
        cols.append(c)
    else:
        counts[c] += 1
        cols.append(f"{c}_{counts[c]}")
df.columns = cols

assert df.shape[0] > 0 and df.shape[1] > 0

numeric_cols = []
for col in df.columns:
    converted = pd.to_numeric(df[col], errors="coerce")
    non_na = converted.notna().sum()
    if non_na >= max(1, int(0.5 * len(df))):
        df[col] = converted
        numeric_cols.append(col)

df.replace([np.inf, -np.inf], np.nan, inplace=True)

norm_map = {norm_key(c): c for c in df.columns}
candidate_keys = []
if dataset_headers:
    candidate_keys.append(norm_key(dataset_headers[-1]))
candidate_keys += ["target","label","class","y","g3","finalscore","finalgrade","grade","score","outcome","result","gpa"]
unique_keys = []
for k in candidate_keys:
    if k not in unique_keys:
        unique_keys.append(k)
candidate_keys = unique_keys

target = None
for cand in candidate_keys:
    if cand in norm_map:
        target = norm_map[cand]
        break
if target is None:
    numeric_nonconst = [c for c in numeric_cols if df[c].nunique(dropna=True) > 1]
    if numeric_nonconst:
        n = len(df)
        best = None
        best_score = -1
        for c in numeric_nonconst:
            nun = df[c].nunique(dropna=True)
            ratio = nun / n if n > 0 else 0
            score = nun if ratio <= 0.9 else nun * 0.5
            if score > best_score:
                best_score = score
                best = c
        target = best
    else:
        best = None
        best_score = -1
        for c in df.columns:
            nun = df[c].nunique(dropna=True)
            if nun > best_score:
                best_score = nun
                best = c
        target = best

y_series = df[target]
if y_series.isna().all():
    if y_series.dtype.kind in "biufc":
        df[target] = y_series.fillna(0)
    else:
        df[target] = y_series.fillna("missing")
else:
    df = df[y_series.notna()]

if df.shape[0] == 0:
    df = pd.DataFrame({target: y_series.fillna(0)})

assert df.shape[0] > 0

feature_cols = [c for c in df.columns if c != target]
if feature_cols:
    feature_cols = [c for c in feature_cols if not df[c].isna().all()]
if not feature_cols:
    df["__dummy__"] = 0
    feature_cols = ["__dummy__"]
    if "__dummy__" not in numeric_cols:
        numeric_cols.append("__dummy__")

X = df[feature_cols]
y = df[target]

if y.dtype.kind in "biufc":
    unique_vals = y.nunique(dropna=True)
    is_classification = unique_vals <= 10
else:
    is_classification = True

if not is_classification:
    y_numeric = pd.to_numeric(y, errors="coerce")
    if y_numeric.isna().all():
        y_numeric = pd.Series(np.zeros(len(y)), index=y.index)
    else:
        med = y_numeric.median()
        if np.isnan(med):
            med = 0.0
        y_numeric = y_numeric.fillna(med)
    y = y_numeric
    df[target] = y
    X = df[feature_cols]

if len(df) < 2:
    df = pd.concat([df, df], ignore_index=True)
    X = df[feature_cols]
    y = df[target]

numeric_features = [c for c in numeric_cols if c in feature_cols]
categorical_features = [c for c in feature_cols if c not in numeric_features]

transformers = []
if numeric_features:
    num_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False))
    ])
    transformers.append(("num", num_pipe, numeric_features))
if categorical_features:
    cat_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=True))
    ])
    transformers.append(("cat", cat_pipe, categorical_features))
if transformers:
    preprocess = ColumnTransformer(transformers=transformers, remainder="drop")
else:
    preprocess = "passthrough"

if is_classification:
    if y.nunique(dropna=True) < 2:
        model = DummyClassifier(strategy="most_frequent")
    else:
        model = LogisticRegression(max_iter=200, solver="liblinear", random_state=42)
else:
    if y.nunique(dropna=True) < 2:
        model = DummyRegressor(strategy="mean")
    else:
        model = SGDRegressor(max_iter=500, tol=1e-3, random_state=42)

pipeline = Pipeline(steps=[("preprocess", preprocess), ("model", model)])

n_samples = len(X)
test_size = 0.2 if n_samples >= 5 else 0.5
stratify = None
if is_classification:
    vc = y.value_counts()
    if y.nunique(dropna=True) > 1 and vc.min() >= 2:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=42, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if is_classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    mae = mean_absolute_error(y_test, y_pred)
    if not np.isfinite(mae):
        accuracy = 0.0
    else:
        y_range = float(np.max(y_test) - np.min(y_test)) if len(y_test) > 0 else 0.0
        if y_range == 0:
            accuracy = 1.0 if mae == 0 else 0.0
        else:
            accuracy = 1.0 - float(mae) / y_range
            if accuracy < 0.0:
                accuracy = 0.0
            if accuracy > 1.0:
                accuracy = 1.0

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# - Used lightweight linear models (LogisticRegression/SGDRegressor) and dummy fallbacks for CPU efficiency.
# - Applied ColumnTransformer with simple imputation, scaling, and one-hot encoding for a reproducible pipeline.
# - Implemented robust CSV parsing and adaptive target selection to handle unknown schemas safely.
# - Regression accuracy uses a normalized MAE-based score clamped to [0,1] for stability.