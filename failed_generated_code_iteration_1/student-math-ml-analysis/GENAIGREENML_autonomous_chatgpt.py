# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _robust_read_csv(path):
    df = None
    # Try default
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(d):
        if d is None or not isinstance(d, pd.DataFrame) or d.shape[0] == 0:
            return True
        # If it came as a single wide string column, parsing likely wrong
        if d.shape[1] == 1:
            col0 = str(d.columns[0]).lower()
            if "school" in col0 and (";" in col0 or "," in col0):
                return True
            # also if many rows contain ';' in the only column
            try:
                s = d.iloc[:, 0].astype(str).head(20)
                if (s.str.contains(";", regex=False).mean() >= 0.5):
                    return True
            except Exception:
                return True
        return False

    if looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df = None

    if df is None:
        raise RuntimeError("Could not read dataset.")
    return df


def _pick_target(df, dataset_headers=None):
    cols = list(df.columns)

    # Prefer known target names if present
    preferred = []
    if dataset_headers:
        # common targets for this dataset, but do not assume; just prioritize if present
        preferred.extend(["G3", "final_score", "final grade", "final_grade", "target", "label", "y"])
    else:
        preferred.extend(["target", "label", "y"])

    for t in preferred:
        if t in cols:
            return t

    # Prefer last column if it is not an obvious index/unnamed and has variability
    non_junk = [c for c in cols if not str(c).lower().startswith("unnamed")]
    if non_junk:
        last = non_junk[-1]
        if df[last].nunique(dropna=True) > 1:
            return last

    # Otherwise: select a numeric column with highest cardinality and not constant
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    best = None
    best_score = -1
    for c in num_cols:
        nun = df[c].nunique(dropna=True)
        if nun > 1 and nun > best_score:
            best_score = nun
            best = c
    if best is not None:
        return best

    # Otherwise: any column with >1 unique
    for c in non_junk:
        if df[c].nunique(dropna=True) > 1:
            return c

    # Fallback: first column
    return non_junk[0] if non_junk else cols[0]


def _coerce_numeric_inplace(df):
    # Coerce object columns that look numeric
    for c in df.columns:
        if df[c].dtype == "object":
            s = df[c].astype(str)
            # if many values contain digits, try to_numeric
            digit_frac = s.str.contains(r"\d", regex=True, na=False).mean()
            if digit_frac >= 0.8:
                df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _is_classification_target(y):
    # If numeric with small number of unique values, treat as classification
    if pd.api.types.is_bool_dtype(y):
        return True
    nun = y.nunique(dropna=True)
    if nun < 2:
        return False
    if pd.api.types.is_numeric_dtype(y):
        return nun <= 20
    # object/category -> classification
    return True


def _regression_accuracy_proxy(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    ss_tot = float(np.sum((y_true - float(np.mean(y_true))) ** 2))
    r2 = 1.0 - (ss_res / ss_tot) if ss_tot > 0 else 0.0
    # Bound to [0,1] to serve as "accuracy" proxy
    acc = (max(-1.0, min(1.0, r2)) + 1.0) / 2.0
    return float(max(0.0, min(1.0, acc)))


def main():
    data_path = os.path.join(".", "data", "student-mat.csv")
    df = _robust_read_csv(data_path)

    # Normalize columns and drop unnamed
    df.columns = _normalize_columns(df.columns)
    df = df.loc[:, [c for c in df.columns if not str(c).lower().startswith("unnamed")]]

    # Coerce numeric-looking object columns
    df = _coerce_numeric_inplace(df)

    # Replace inf with nan
    df = df.replace([np.inf, -np.inf], np.nan)

    # Ensure not empty
    assert df.shape[0] > 0 and df.shape[1] > 0

    dataset_headers = [
        "school", "sex", "age", "address", "famsize", "Pstatus", "Medu", "Fedu", "Mjob", "Fjob", "reason",
        "guardian", "traveltime", "studytime", "failures", "schoolsup", "famsup", "paid", "activities", "nursery",
        "higher", "internet", "romantic", "famrel", "freetime", "goout", "Dalc", "Walc", "health", "absences",
        "G1", "G2", "G3"
    ]

    target_col = _pick_target(df, dataset_headers=dataset_headers)

    # Features: all columns except target
    feature_cols = [c for c in df.columns if c != target_col]
    if len(feature_cols) == 0:
        # If only one column, create a constant feature
        df["_constant_feature"] = 1.0
        feature_cols = ["_constant_feature"]

    X = df[feature_cols].copy()
    y = df[target_col].copy()

    # Drop rows where y is nan
    mask_y = ~pd.isna(y)
    X = X.loc[mask_y].copy()
    y = y.loc[mask_y].copy()

    assert X.shape[0] > 1

    is_clf = _is_classification_target(y)

    # If y is object and classification, keep as-is; if numeric classification, keep numeric
    # Build preprocessing
    numeric_selector = make_column_selector(dtype_include=np.number)
    categorical_selector = make_column_selector(dtype_exclude=np.number)

    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_selector),
            ("cat", categorical_transformer, categorical_selector),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Split (stratify only if classification and has enough members per class)
    stratify = None
    if is_clf:
        vc = y.value_counts(dropna=True)
        if (vc.min() >= 2) and (vc.shape[0] >= 2):
            stratify = y

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=stratify
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    # If classification but only 1 class in train/test, fallback
    accuracy = 0.0
    if is_clf:
        if (pd.Series(y_train).nunique(dropna=True) < 2) or (pd.Series(y_test).nunique(dropna=True) < 2):
            model = Pipeline(steps=[
                ("preprocess", preprocessor),
                ("clf", DummyClassifier(strategy="most_frequent")),
            ])
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
        else:
            # Lightweight, CPU-friendly baseline
            model = Pipeline(steps=[
                ("preprocess", preprocessor),
                ("clf", LogisticRegression(
                    max_iter=300,
                    solver="lbfgs",
                    n_jobs=1,
                    multi_class="auto",
                )),
            ])
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Regression baseline with bounded proxy accuracy
        if pd.Series(y_train).nunique(dropna=True) < 2:
            model = Pipeline(steps=[
                ("preprocess", preprocessor),
                ("reg", DummyRegressor(strategy="mean")),
            ])
        else:
            model = Pipeline(steps=[
                ("preprocess", preprocessor),
                ("reg", Ridge(alpha=1.0, random_state=42)),
            ])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        # Coerce y_test to numeric safely
        y_test_num = pd.to_numeric(pd.Series(y_test), errors="coerce").to_numpy()
        accuracy = _regression_accuracy_proxy(y_test_num, y_pred)

    accuracy = float(np.clip(accuracy, 0.0, 1.0))
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses robust CSV parsing (default then ';' with decimal=',') to avoid wasted retries and manual edits.
# - Lightweight models (LogisticRegression / Ridge) chosen for strong baselines with low CPU cost; avoids heavy ensembles/deep learning.
# - Pipeline + ColumnTransformer ensures single-pass, reproducible preprocessing and avoids duplicating encoding/scaling work.
# - OneHotEncoder(handle_unknown='ignore') prevents crashes on unseen categories without expensive schema alignment.
# - SimpleImputer (median/most_frequent) is cheap and stable; StandardScaler improves linear model conditioning.
# - Defensive target selection avoids hard failures on schema mismatch by selecting an available, non-constant target.
# - Regression fallback reports a bounded [0,1] proxy accuracy derived from clipped R^2: acc=(clip(R2,-1,1)+1)/2.