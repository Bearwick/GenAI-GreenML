# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn import metrics

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

def load_data(filepath):
    try:
        df = pd.read_csv(filepath, encoding='UTF-8')
        if df.shape[1] < 2:
            df = pd.read_csv(filepath, encoding='UTF-8', sep=';', decimal=',')
    except Exception:
        df = pd.read_csv(filepath, encoding='UTF-8', sep=';', decimal=',')
    text_col = [c for c in df.columns if c.strip().lower() == 'text'][0]
    label_col = [c for c in df.columns if c.strip().lower() == 'label'][0]
    return df[text_col], df[label_col]

def main():
    filepath = "dataset.csv"
    feature, classcol = load_data(filepath)

    train_x, test_x, train_y, test_y = train_test_split(
        feature, classcol, train_size=0.75, test_size=0.25, random_state=RANDOM_SEED
    )

    tf = TfidfVectorizer()
    train_x_tfidf = tf.fit_transform(train_x)
    test_x_tfidf = tf.transform(test_x)

    linearsvm = LinearSVC(random_state=RANDOM_SEED, dual='auto')
    linearsvm.fit(train_x_tfidf, train_y)

    pred_y = linearsvm.predict(test_x_tfidf)
    accuracy = metrics.accuracy_score(test_y, pred_y)
    print(f"ACCURACY={accuracy:.6f}")

main()

# Optimization Summary
# - Removed all unused imports (graphviz, matplotlib, pickle, csv, os, sys, IPython, joblib, etc.)
# - Removed unused example functions (naivebayes, linsupvec, knearneighbor) that were never called for the main task
# - Removed all prints, plots, visualizations, and logging except final accuracy
# - Removed interactive sys.argv parsing; hardcoded dataset path for end-to-end execution
# - Removed saving of model artifacts (SVM.pkl, tfidf.pkl) to avoid unnecessary disk I/O
# - Added robust CSV fallback: tries default separator, falls back to sep=';' and decimal=','
# - Column names derived dynamically from actual dataframe columns rather than hardcoded
# - Set fixed random seeds for reproducibility (numpy and sklearn estimators)
# - Eliminated redundant predict call (original code called predict twice on test set)
# - Used dual='auto' in LinearSVC to let sklearn pick optimal solver, reducing computation
# - Reduced memory footprint by not storing intermediate unnecessary dataframes