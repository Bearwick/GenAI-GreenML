# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings
from typing import Tuple, List, Optional

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler


warnings.filterwarnings("ignore")


def _normalize_columns(cols: List[str]) -> List[str]:
    normed = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        normed.append(c2)
    return normed


def _drop_unnamed(df: pd.DataFrame) -> pd.DataFrame:
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _read_csv_robust(path: str) -> pd.DataFrame:
    df1 = pd.read_csv(path)
    df1.columns = _normalize_columns(df1.columns.tolist())
    df1 = _drop_unnamed(df1)

    # If parsing likely wrong (single wide column with separators), retry.
    looks_wrong = False
    if df1.shape[1] <= 1:
        looks_wrong = True
    else:
        first_col = str(df1.columns[0])
        if ";" in first_col or "," in first_col:
            looks_wrong = True

    if looks_wrong:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        df2.columns = _normalize_columns(df2.columns.tolist())
        df2 = _drop_unnamed(df2)
        # Choose better parse (more columns, or more non-null cells)
        nn1 = int(df1.notna().sum().sum())
        nn2 = int(df2.notna().sum().sum())
        if df2.shape[1] > df1.shape[1] or nn2 > nn1:
            return df2
    return df1


def _coerce_numeric_columns(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    for c in cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _pick_target(df: pd.DataFrame, dataset_headers: Optional[List[str]] = None) -> Tuple[str, str]:
    # Returns (target_col, task) where task in {"classification","regression"}
    cols = df.columns.tolist()

    preferred_target_names = []
    if dataset_headers:
        # Try last header as target heuristic, but don't require it.
        preferred_target_names.append(dataset_headers[-1])
    preferred_target_names += ["target", "Target", "label", "Label", "y", "Y", "Outcome", "outcome"]

    for name in preferred_target_names:
        if name in cols:
            y = df[name]
            # Determine task
            if y.dtype == "O":
                return name, "classification"
            y_num = pd.to_numeric(y, errors="coerce")
            nunique = int(pd.Series(y_num).dropna().nunique())
            if nunique <= 20:
                return name, "classification"
            return name, "regression"

    # Fallback: choose a non-constant numeric column, preferring low cardinality for classification.
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    best_col = None
    best_score = None
    for c in numeric_cols:
        s = df[c]
        nunique = int(pd.Series(s).dropna().nunique())
        if nunique <= 1:
            continue
        # Score: prefer low-ish cardinality for classification; otherwise regression.
        score = abs(nunique - 2)  # closest to binary
        if best_score is None or score < best_score:
            best_score = score
            best_col = c

    if best_col is not None:
        nunique = int(pd.Series(df[best_col]).dropna().nunique())
        if nunique <= 20:
            return best_col, "classification"
        return best_col, "regression"

    # If all numeric constant or absent, pick any column and treat as classification.
    if cols:
        return cols[-1], "classification"
    raise ValueError("No columns available to select target.")


def _build_pipeline(numeric_features: List[str], categorical_features: List[str], task: str) -> Pipeline:
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if task == "classification":
        # CPU-friendly linear classifier; saga handles sparse and is efficient for OHE.
        model = LogisticRegression(
            solver="saga",
            penalty="l2",
            C=1.0,
            max_iter=500,
            n_jobs=1,
            random_state=42,
        )
    else:
        model = Ridge(alpha=1.0, random_state=42)

    return Pipeline(steps=[("preprocess", preprocessor), ("model", model)])


def _safe_accuracy_proxy_regression(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    # Map R^2 (can be negative) to [0,1] for stable "ACCURACY" proxy.
    r2 = r2_score(y_true, y_pred)
    acc = (r2 + 1.0) / 2.0
    if not np.isfinite(acc):
        return 0.0
    return float(np.clip(acc, 0.0, 1.0))


def main():
    dataset_headers = [
        "Pregnancies",
        "Glucose",
        "BloodPressure",
        "SkinThickness",
        "Insulin",
        "BMI",
        "DiabetesPedigreeFunction",
        "Age",
        "Outcome",
    ]

    path = "diabetes.csv"
    if not os.path.exists(path):
        # Fallback: attempt common alternate filename
        alt = "Diabetes.csv"
        if os.path.exists(alt):
            path = alt

    df = _read_csv_robust(path)
    df.columns = _normalize_columns(df.columns.tolist())
    df = _drop_unnamed(df)

    # Coerce numeric where possible to avoid object dtypes from parsing issues.
    df = _coerce_numeric_columns(df, df.columns.tolist())

    # If canonical diabetes columns exist, treat specific zeros as missing (domain-robust but low cost).
    zero_as_missing = ["Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI"]
    present_zero_cols = [c for c in zero_as_missing if c in df.columns]
    if present_zero_cols:
        for c in present_zero_cols:
            if pd.api.types.is_numeric_dtype(df[c]):
                df[c] = df[c].replace(0, np.nan)

    # Drop fully empty rows
    df = df.dropna(axis=0, how="all")
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col, task = _pick_target(df, dataset_headers=dataset_headers)

    # Build feature set: use all other columns, but keep at least one.
    feature_cols = [c for c in df.columns if c != target_col]
    if len(feature_cols) == 0:
        feature_cols = [target_col]

    X = df[feature_cols].copy()
    y = df[target_col].copy()

    # Determine categorical vs numeric features from X
    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
    categorical_features = [c for c in X.columns if c not in numeric_features]

    # Ensure numeric coercion (again) for numeric features only; avoids means/medians on object.
    X = _coerce_numeric_columns(X, numeric_features)

    # Clean y
    if task == "classification":
        # If y is numeric with NaNs, keep as is; impute by dropping rows where y missing to keep metrics meaningful.
        y_num = pd.to_numeric(y, errors="ignore")
        y = y_num
        data = pd.concat([X, pd.Series(y, name=target_col)], axis=1)
        data = data.dropna(subset=[target_col])
        assert data.shape[0] > 0
        X = data[feature_cols]
        y = data[target_col]

        # If numeric-ish labels, try to keep as integers when possible.
        if pd.api.types.is_numeric_dtype(y):
            y = pd.to_numeric(y, errors="coerce")
            data = pd.concat([X, pd.Series(y, name=target_col)], axis=1).dropna(subset=[target_col])
            X = data[feature_cols]
            y = data[target_col]
            # Convert near-integers to int for cleaner class handling.
            if np.all(np.isclose(y.values, np.round(y.values))):
                y = np.round(y.values).astype(int)

        # If only one class remains, fallback to regression proxy on numeric target or trivial baseline.
        y_series = pd.Series(y)
        n_classes = int(y_series.nunique(dropna=True))
        if n_classes < 2:
            task = "regression"

    if task == "regression":
        y = pd.to_numeric(y, errors="coerce")
        data = pd.concat([X, pd.Series(y, name=target_col)], axis=1)
        data = data.replace([np.inf, -np.inf], np.nan).dropna(subset=[target_col])
        assert data.shape[0] > 0
        X = data[feature_cols]
        y = data[target_col].values.astype(float)

    # Train/test split
    stratify = None
    if task == "classification":
        y_series = pd.Series(y)
        if int(y_series.nunique()) >= 2:
            # Stratify only if classes have enough samples
            vc = y_series.value_counts()
            if (vc.min() >= 2) and (len(vc) >= 2):
                stratify = y_series

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=stratify
    )
    assert len(X_train) > 0 and len(X_test) > 0

    pipe = _build_pipeline(numeric_features=numeric_features, categorical_features=categorical_features, task=task)

    # Fit/predict
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)

    # Score
    if task == "classification":
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        accuracy = _safe_accuracy_proxy_regression(np.asarray(y_test, dtype=float), np.asarray(y_pred, dtype=float))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses a lightweight, CPU-friendly baseline: LogisticRegression (linear) for classification; Ridge for regression fallback.
# - Uses sklearn Pipeline + ColumnTransformer for reproducible preprocessing and to avoid redundant transformations.
# - Minimal feature engineering: median imputation + StandardScaler for numerics; most_frequent + OneHotEncoder for categoricals.
# - Robust CSV ingestion: retries with sep=';' and decimal=',' if initial parsing looks wrong; normalizes headers and drops 'Unnamed:' columns.
# - Defensive schema handling: chooses target based on provided headers/name heuristics, otherwise selects a non-constant numeric column; never hard-fails.
# - Safe numeric handling: coerces numeric columns with errors='coerce', replaces inf, drops rows missing target.
# - Domain-aware but low-cost missingness: replaces zeros with NaN for common diabetes medical fields when present.
# - If classification target collapses to <2 classes, falls back to regression and reports a bounded [0,1] accuracy proxy via (R2+1)/2 clipped.