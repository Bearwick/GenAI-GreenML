# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
import random
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

DATASET_PATH = "diabetes.csv"
DATASET_HEADERS = ["Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "DiabetesPedigreeFunction", "Age", "Outcome"]
SEED = 42

np.random.seed(SEED)
random.seed(SEED)

HEADER_MAP = {h.lower(): h for h in DATASET_HEADERS}
GLUCOSE_COL = HEADER_MAP.get("glucose")
BP_COL = HEADER_MAP.get("bloodpressure")
SKIN_COL = HEADER_MAP.get("skinthickness")
INSULIN_COL = HEADER_MAP.get("insulin")
BMI_COL = HEADER_MAP.get("bmi")
TARGET_COL = HEADER_MAP.get("outcome")
ZERO_IMPUTE_COLS = [c for c in [GLUCOSE_COL, BP_COL, SKIN_COL, INSULIN_COL, BMI_COL] if c]
MEDIAN_IMPUTE_COLS = [c for c in [BP_COL, SKIN_COL, INSULIN_COL, BMI_COL] if c]

def columns_are_numeric(cols):
    for c in cols:
        try:
            float(str(c))
        except ValueError:
            return False
    return True

def parsing_looks_wrong(df, expected_headers):
    cols = list(df.columns)
    if len(cols) == 1 and len(expected_headers) > 1:
        return True
    expected_set = set(expected_headers)
    if expected_set.intersection(cols):
        return len(cols) < len(expected_headers)
    if len(cols) < len(expected_headers):
        return True
    if columns_are_numeric(cols):
        return True
    return False

def align_columns(df, expected_headers):
    lower_map = {str(c).strip().lower(): c for c in df.columns}
    rename_map = {}
    for exp in expected_headers:
        key = exp.lower()
        if key in lower_map and lower_map[key] != exp:
            rename_map[lower_map[key]] = exp
    if rename_map:
        df = df.rename(columns=rename_map)
    return df

def _read_csv(path, **kwargs):
    df = pd.read_csv(path, **kwargs)
    df.columns = [str(c).strip() for c in df.columns]
    return df

def load_dataset(path, expected_headers):
    df = _read_csv(path)
    df = align_columns(df, expected_headers)
    if parsing_looks_wrong(df, expected_headers):
        df = _read_csv(path, sep=";", decimal=",")
        df = align_columns(df, expected_headers)
    if parsing_looks_wrong(df, expected_headers):
        df = _read_csv(path, header=None, names=expected_headers)
        df = align_columns(df, expected_headers)
    if len(df.columns) == len(expected_headers) and not set(expected_headers).issubset(df.columns):
        df.columns = expected_headers
    return df

def preprocess_data(df, expected_headers):
    if set(expected_headers).issubset(df.columns):
        df = df.loc[:, expected_headers].copy()
    else:
        df = df.copy()
    zero_cols = [c for c in ZERO_IMPUTE_COLS if c in df.columns]
    if zero_cols:
        df.loc[:, zero_cols] = df.loc[:, zero_cols].replace(0, np.nan)
    if GLUCOSE_COL in df.columns:
        df.loc[:, GLUCOSE_COL] = df[GLUCOSE_COL].fillna(df[GLUCOSE_COL].mean())
    median_cols = [c for c in MEDIAN_IMPUTE_COLS if c in df.columns]
    if median_cols:
        df.loc[:, median_cols] = df[median_cols].fillna(df[median_cols].median())
    target_col = TARGET_COL if TARGET_COL in df.columns else (expected_headers[-1] if expected_headers[-1] in df.columns else df.columns[-1])
    feature_cols = [c for c in df.columns if c != target_col]
    X = df.loc[:, feature_cols].to_numpy()
    y = df.loc[:, target_col].to_numpy()
    return X, y

def train_and_evaluate(X, y, seed):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)
    scaler = StandardScaler(copy=False)
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    model = KNeighborsClassifier(n_neighbors=5, n_jobs=1)
    model.fit(X_train, y_train)
    return model.score(X_test, y_test)

def main():
    df = load_dataset(DATASET_PATH, DATASET_HEADERS)
    X, y = preprocess_data(df, DATASET_HEADERS)
    accuracy = train_and_evaluate(X, y, SEED)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# Removed exploratory plotting and hyperparameter search loops that did not affect the final accuracy output.
# Used vectorized imputations and in-place scaling to minimize redundant computation and memory copies.
# Implemented robust CSV parsing and column alignment for reliable, deterministic preprocessing.