# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score

def load_and_preprocess(filepath):
    # Robust parsing: try default then infer delimiter
    try:
        df = pd.read_csv(filepath)
        # If the file was parsed as a single column, it's likely a different separator
        if df.shape[1] <= 1:
            df = pd.read_csv(filepath, sep=None, engine='python', decimal='.')
    except Exception:
        # Fallback to simple whitespace/comma inference
        df = pd.read_csv(filepath, sep=r'\s+', engine='python')

    # Normalize column names: strip whitespace, remove Unnamed
    df.columns = [str(c).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

    # Check if first row was misinterpreted as header (if header consists of numbers)
    try:
        # Check if first column name is a float/int
        float(df.columns[0])
        # If no error, reload without header
        df = pd.read_csv(filepath, header=None, sep=None, engine='python')
        df.columns = [f"col_{i}" for i in range(df.shape[1])]
    except ValueError:
        pass

    # Basic data cleaning
    for col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Drop rows where target or most features are NaN
    df = df.dropna(thresh=int(df.shape[1] * 0.5))
    
    if df.empty:
        return None, None

    # Identify Target: Assume last column for heart.csv logic
    target_col = df.columns[-1]
    
    # Separate Features and Target
    X = df.drop(columns=[target_col])
    y = df[target_col]

    # Classification logic: convert target to discrete labels if needed
    # (Heart disease target is often 1, 2 or 0, 1)
    if y.dtype == 'float':
        y = y.round().astype(int)
    
    # Handle missing values in target
    mask = ~y.isna()
    X = X[mask]
    y = y[mask]

    return X, y

def build_pipeline(X):
    # Identify numeric columns
    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
    
    # Simple, low-energy preprocessing: Impute and Scale
    # We avoid OneHotEncoding here to keep the feature space small unless necessary
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features)
        ],
        remainder='drop' # Drop columns that aren't numeric to save computation
    )

    # Choice: Logistic Regression (extremely CPU efficient, robust baseline)
    model = LogisticRegression(
        max_iter=1000, 
        solver='lbfgs', 
        random_state=42, 
        n_jobs=1, # Single core is often enough and more energy efficient for small data
        tol=1e-4
    )

    return Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', model)
    ])

def main():
    dataset_path = 'heart.csv'
    
    X, y = load_and_preprocess(dataset_path)
    
    if X is None or len(X) < 10:
        # Trivial fallback accuracy if data is missing or too small
        print(f"ACCURACY=0.000000")
        return

    # Check class distribution
    unique_classes = np.unique(y)
    if len(unique_classes) < 2:
        # If only one class exists, accuracy is technically 1.0 but logically flawed
        print(f"ACCURACY=1.000000")
        return

    # Reproducible Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if len(unique_classes) > 1 else None
    )

    # Train
    clf = build_pipeline(X)
    clf.fit(X_train, y_train)

    # Predict and Evaluate
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary:
# 1. Model Choice: Logistic Regression with lbfgs was used. It is computationally lightweight, 
#    requires minimal memory, and performs well on small, structured clinical datasets like heart.csv.
# 2. Preprocessing: Used StandardScaler and SimpleImputer(median) within a Pipeline. 
#    This ensures no data leakage and minimizes redundant transformations.
# 3. Energy Efficiency: Avoided complex ensembles (Random Forest/XGBoost) and Deep Learning 
#    which require higher CPU/GPU cycles. The model training is near-instant on a standard CPU.
# 4. Robustness: The parser handles cases where headers are missing (common in heart.csv variants) 
#    by checking for numeric headers and re-reading if necessary.
# 5. Resource Management: Used n_jobs=1 to prevent unnecessary overhead of process spawning on small data.
# 6. Feature Selection: Restricted to numeric columns and used ColumnTransformer to avoid 
#    processing categorical data unless strictly defined, reducing the dimensionality of the weight matrix.