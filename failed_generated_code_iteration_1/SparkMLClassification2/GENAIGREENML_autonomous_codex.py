# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import numpy as np
import pandas as pd
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")

path = "heart.csv"

def normalize_columns(cols):
    return [" ".join(str(c).strip().split()) for c in cols]

def read_dataset(path):
    attempts = [
        {},
        {"sep": ";", "decimal": ","},
        {"delim_whitespace": True}
    ]
    last_df = None
    last_params = None
    last_exc = None
    for params in attempts:
        try:
            df = pd.read_csv(path, **params)
            last_df = df
            last_params = params
        except Exception as e:
            last_exc = e
            continue
        if df.shape[1] == 1:
            sample = df.iloc[0, 0] if not df.empty else ""
            if isinstance(sample, str) and ("," in sample or ";" in sample or " " in sample or "\t" in sample):
                continue
        return df, params
    if last_df is not None:
        return last_df, last_params
    raise last_exc

df, params_used = read_dataset(path)
df.columns = normalize_columns(df.columns)
df = df.loc[:, [c for c in df.columns if c and not str(c).lower().startswith("unnamed")]]

def columns_are_numeric(cols):
    for c in cols:
        try:
            float(str(c))
        except Exception:
            return False
    return True

if columns_are_numeric(df.columns):
    try:
        df = pd.read_csv(path, header=None, **params_used)
        df.columns = [f"col_{i}" for i in range(df.shape[1])]
    except Exception:
        pass
    df.columns = normalize_columns(df.columns)
    df = df.loc[:, [c for c in df.columns if c and not str(c).lower().startswith("unnamed")]]

df.replace([np.inf, -np.inf], np.nan, inplace=True)

for c in df.columns:
    if df[c].dtype == object:
        coerced = pd.to_numeric(df[c], errors="coerce")
        if coerced.notna().mean() >= 0.9:
            df[c] = coerced

def choose_target_col(df):
    for c in df.columns:
        cl = str(c).lower()
        if any(k in cl for k in ["target", "label", "class", "outcome", "y", "diagnosis"]):
            return c
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    for c in reversed(numeric_cols):
        if df[c].nunique(dropna=True) > 1:
            return c
    return df.columns[-1]

target_col = choose_target_col(df)
y = df[target_col]
X = df.drop(columns=[target_col])

if y.dtype == object:
    y_coerced = pd.to_numeric(y, errors="coerce")
    if y_coerced.notna().mean() >= 0.9:
        y = y_coerced

valid_mask = pd.Series(y).notna()
X = X.loc[valid_mask].reset_index(drop=True)
y = pd.Series(y).loc[valid_mask].reset_index(drop=True)

for c in X.columns.tolist():
    if X[c].isna().all():
        X.drop(columns=[c], inplace=True)

if X.shape[1] == 0:
    X = pd.DataFrame({"bias": np.ones(len(y))})

assert len(X) > 0 and len(y) > 0

numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = [c for c in X.columns if c not in numeric_features]

y_series = pd.Series(y)
y_unique = y_series.dropna().unique()
n_unique = len(y_unique)
is_classification = False
if y_series.dtype == object:
    is_classification = True
else:
    if n_unique <= 2:
        is_classification = True
    else:
        if n_unique <= min(20, max(2, int(0.1 * len(y_series)))):
            if np.all(np.isclose(y_unique, np.round(y_unique))):
                is_classification = True

transformers = []
if numeric_features:
    transformers.append(("num", Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ]), numeric_features))
if categorical_features:
    transformers.append(("cat", Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ]), categorical_features))
if transformers:
    preprocess = ColumnTransformer(transformers=transformers, remainder="drop")
else:
    preprocess = "passthrough"

test_size = 0.2
if len(y) < 5:
    test_size = 0.5 if len(y) <= 3 else 0.4
stratify = y if is_classification and n_unique > 1 and len(y) >= 5 else None
try:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )
except Exception:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=None
    )

assert len(X_train) > 0 and len(X_test) > 0

accuracy = 0.0
if is_classification:
    if pd.Series(y_train).nunique() < 2:
        majority_class = pd.Series(y_train).mode().iloc[0]
        y_pred = np.full(len(y_test), majority_class)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        clf = LogisticRegression(max_iter=200, solver="liblinear")
        model = Pipeline(steps=[("preprocess", preprocess), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
else:
    if pd.Series(y_train).nunique() < 2:
        y_pred = np.full(len(y_test), float(pd.Series(y_train).mean()))
        mse = np.mean((y_test - y_pred) ** 2)
        accuracy = 1.0 / (1.0 + mse)
    else:
        reg = Ridge(alpha=1.0)
        model = Pipeline(steps=[("preprocess", preprocess), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        try:
            r2 = r2_score(y_test, y_pred)
        except Exception:
            r2 = 0.0
        accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) with simple scaling/imputation for CPU efficiency.
# - Applied ColumnTransformer with one-hot encoding only for categoricals to keep preprocessing reproducible and minimal.
# - Implemented robust CSV parsing and header handling to avoid manual fixes while keeping operations lightweight.
# - For regression fallback, converted R2 to a bounded [0,1] proxy via (r2+1)/2 with clipping.