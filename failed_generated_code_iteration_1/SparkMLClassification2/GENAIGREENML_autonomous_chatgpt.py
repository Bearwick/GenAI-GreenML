# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "heart.csv"
DATASET_HEADERS_RAW = "70.0 1.0 4.0 130.0 322.0 0.0 2.0 109.0 0.0 2.4 2.0 3.0 3.0 2"


def _normalize_colname(c):
    c = str(c)
    c = c.strip()
    c = re.sub(r"\s+", " ", c)
    return c


def _read_csv_robust(path):
    df = None
    # Try default CSV parsing
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(d):
        if d is None or d.empty:
            return True
        # If all columns collapsed into one, likely wrong separator
        if d.shape[1] == 1:
            return True
        # If majority of column names look like a single long string with separators, suspect parsing
        col0 = str(d.columns[0])
        if ("," in col0) or (";" in col0) or ("\t" in col0):
            return True
        return False

    if looks_wrong(df):
        # Retry with semicolon separator and comma decimal
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df = None

    if df is None:
        # Last resort: try python engine with flexible separator
        try:
            df = pd.read_csv(path, sep=None, engine="python")
        except Exception:
            raise RuntimeError("Failed to read dataset with robust CSV parsing.")
    return df


def _drop_unnamed(df):
    cols = []
    for c in df.columns:
        if str(c).startswith("Unnamed:"):
            continue
        cols.append(c)
    return df[cols]


def _normalize_columns(df):
    df = df.copy()
    df.columns = [_normalize_colname(c) for c in df.columns]
    df = _drop_unnamed(df)
    return df


def _coerce_numeric_columns(df):
    df = df.copy()
    for c in df.columns:
        if df[c].dtype == "object":
            # Try coercing to numeric where possible; keep as object if mostly non-numeric
            coerced = pd.to_numeric(df[c], errors="coerce")
            non_na_ratio = float(coerced.notna().mean()) if len(coerced) else 0.0
            if non_na_ratio >= 0.8:
                df[c] = coerced
    return df


def _select_target(df):
    # Prefer last column as target if sensible; otherwise choose a non-constant numeric column.
    cols = list(df.columns)
    if not cols:
        return None

    candidate_last = cols[-1]
    s_last = df[candidate_last]
    nunique_last = s_last.nunique(dropna=True)
    if nunique_last >= 2:
        return candidate_last

    numeric_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]
    for c in reversed(numeric_cols):
        if df[c].nunique(dropna=True) >= 2:
            return c

    # Fallback: any column with >=2 unique values
    for c in reversed(cols):
        if df[c].nunique(dropna=True) >= 2:
            return c

    return candidate_last


def _is_classification_target(y):
    if not pd.api.types.is_numeric_dtype(y):
        return True
    y_non_na = y.dropna()
    if y_non_na.empty:
        return False
    unique_vals = y_non_na.unique()
    if len(unique_vals) < 2:
        return False
    # Heuristic: small number of unique values indicates classification (binary/multiclass)
    if len(unique_vals) <= 10:
        # Also check if close to integers
        if np.all(np.isfinite(unique_vals)):
            if np.all(np.isclose(unique_vals, np.round(unique_vals))):
                return True
    return False


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    r2 = 1.0 - (ss_res / ss_tot) if ss_tot > 0 else 0.0
    # Map to [0,1] for stable "accuracy" proxy
    acc = float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))
    return acc


# Load data
df = _read_csv_robust(DATASET_PATH)
df = _normalize_columns(df)

# If the file has no header and first row became header, compare with provided headers count and fix
# Detect numeric-like column names (common when header=None but pandas inferred header from first row)
all_cols_numericish = True
for c in df.columns:
    try:
        float(str(c))
    except Exception:
        all_cols_numericish = False
        break

if all_cols_numericish and df.shape[1] >= 2:
    # Re-read without header to treat first line as data
    try:
        df2 = pd.read_csv(DATASET_PATH, header=None)
        df2 = _normalize_columns(df2)
        df = df2
    except Exception:
        pass

# If still single column, try whitespace-delimited
if df.shape[1] == 1:
    try:
        df2 = pd.read_csv(DATASET_PATH, header=None, sep=r"\s+", engine="python")
        df2 = _normalize_columns(df2)
        df = df2
    except Exception:
        pass

# Apply header names if provided and lengths match (safe, not required)
provided_headers = [h.strip() for h in DATASET_HEADERS_RAW.split()]
if df.shape[1] == len(provided_headers):
    # Use generic feature names to avoid numeric headers; keep target as last
    new_cols = [f"f{i}" for i in range(df.shape[1] - 1)] + ["target"]
    df.columns = new_cols

df = _coerce_numeric_columns(df)

# Basic cleanup: replace inf with nan
df = df.replace([np.inf, -np.inf], np.nan)

assert df is not None and not df.empty, "Dataset is empty after loading."

target_col = _select_target(df)
if target_col is None or target_col not in df.columns:
    # Fallback: create a trivial target
    target_col = df.columns[-1]
    df[target_col] = 0

# Separate X/y
y = df[target_col]
X = df.drop(columns=[target_col])

# If no features remain, create a constant feature
if X.shape[1] == 0:
    X = pd.DataFrame({"const": np.ones(len(df), dtype=float)})

# Drop rows where y is missing
mask_y = y.notna()
X = X.loc[mask_y].copy()
y = y.loc[mask_y].copy()

assert len(X) > 1, "Not enough samples after dropping missing target."

# Determine task type; if classification but <2 classes, fallback to regression
classification = _is_classification_target(y)
if classification:
    y_non_na = y.dropna()
    if y_non_na.nunique() < 2:
        classification = False

# Define preprocessing
numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
categorical_features = [c for c in X.columns if c not in numeric_features]

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=True, with_std=True)),
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Train/test split
# For classification, stratify when possible
stratify = None
if classification:
    try:
        if y.nunique() >= 2:
            stratify = y
    except Exception:
        stratify = None

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

# Model
if classification:
    # Use a small, CPU-friendly linear model
    model = LogisticRegression(
        solver="lbfgs",
        max_iter=300,
        n_jobs=1,
    )
    clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Regression fallback: lightweight Ridge regression; report bounded R2 proxy as "accuracy"
    model = Ridge(alpha=1.0, random_state=42)
    reg = Pipeline(steps=[("preprocess", preprocess), ("model", model)])
    # Coerce y to numeric safely
    y_train_num = pd.to_numeric(y_train, errors="coerce")
    y_test_num = pd.to_numeric(y_test, errors="coerce")
    # Drop NaNs in train/test for regression fit/predict
    train_mask = y_train_num.notna()
    test_mask = y_test_num.notna()
    if train_mask.sum() < 2 or test_mask.sum() < 1:
        # Trivial baseline if insufficient numeric targets
        accuracy = 0.0
    else:
        reg.fit(X_train.loc[train_mask], y_train_num.loc[train_mask])
        y_pred = reg.predict(X_test.loc[test_mask])
        accuracy = _bounded_regression_score(y_test_num.loc[test_mask].values, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used a simple train/test split with fixed random_state for reproducibility and low compute.
# - Chose LogisticRegression (linear, CPU-friendly) as baseline classifier; Ridge as regression fallback.
# - Implemented ColumnTransformer with median/mode imputation + scaling + one-hot encoding for robust mixed-type schemas.
# - Added robust CSV parsing fallbacks (default, semicolon+decimal comma, python engine, whitespace-delimited) to avoid manual edits.
# - Normalized column names and dropped 'Unnamed:' columns to reduce schema brittleness.
# - Coerced object columns to numeric when mostly numeric, replaced inf with NaN, and avoided strict header assumptions.
# - Regression fallback reports a bounded [0,1] proxy: (R2+1)/2 clipped, to keep ACCURACY stable and comparable.