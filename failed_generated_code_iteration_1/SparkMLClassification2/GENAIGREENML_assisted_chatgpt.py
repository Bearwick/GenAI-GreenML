# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lit
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator


DATASET_PATH = "heart.csv"
DATASET_HEADERS = "70.0 1.0 4.0 130.0 322.0 0.0 2.0 109.0 0.0 2.4 2.0 3.0 3.0 2"


def _read_csv_with_fallback(path: str, headers_str: str) -> pd.DataFrame:
    expected_ncols = len(headers_str.split())

    def _try_read(**kwargs):
        df_local = pd.read_csv(path, **kwargs)
        if df_local.shape[1] == expected_ncols:
            return df_local
        if df_local.shape[1] == 1:
            return None
        return df_local

    df = _try_read(header=None)
    if df is None:
        df = _try_read(header=None, sep=";", decimal=",")
    if df is None:
        raise ValueError("CSV parsing failed: unable to infer correct delimiter/decimal format.")

    if df.shape[1] != expected_ncols:
        df = df.iloc[:, :expected_ncols]

    return df


def _safe_index_in_columns(cols, idx: int):
    if 0 <= idx < len(cols):
        return cols[idx]
    return None


def run():
    seed = 12345
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)

    spark = (
        SparkSession.builder.appName("SparkMLClassification_Heart_Green")
        .config("spark.sql.shuffle.partitions", "8")
        .config("spark.default.parallelism", "8")
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("ERROR")

    pdf = _read_csv_with_fallback(DATASET_PATH, DATASET_HEADERS)

    cols = [
        "age",
        "sex",
        "chest pain",
        "resting blood pressure",
        "serum cholesterol",
        "fasting blood sugar",
        "resting electrocardiographic results",
        "maximum heart rate achieved",
        "exercise induced angina",
        "ST depression induced by exercise relative to rest",
        "the slope of the peak exercise ST segment",
        "number of major vessels ",
        "thal",
        "last",
    ]

    if pdf.shape[1] == len(cols):
        pdf.columns = cols
    else:
        pdf.columns = [f"c{i}" for i in range(pdf.shape[1])]

    keep_n = min(13, pdf.shape[1])
    pdf = pdf.iloc[:, :keep_n].copy()

    if keep_n >= 13:
        base_cols = cols[:13]
        if list(pdf.columns) != base_cols and len(pdf.columns) == len(base_cols):
            pdf.columns = base_cols

    df = spark.createDataFrame(pdf)

    thal_col = "thal" if "thal" in df.columns else _safe_index_in_columns(df.columns, keep_n - 1)
    if thal_col is None:
        raise ValueError("Unable to locate 'thal' column for label generation.")

    label_expr = when(col(thal_col).isin([3, 7]), lit(0.0)).otherwise(lit(1.0))
    df = df.withColumn("label", label_expr)

    feature_names = [
        "age",
        "sex",
        "chest pain",
        "resting blood pressure",
        "serum cholesterol",
        "fasting blood sugar",
        "resting electrocardiographic results",
        "maximum heart rate achieved",
        "exercise induced angina",
        "ST depression induced by exercise relative to rest",
        "the slope of the peak exercise ST segment",
        "number of major vessels ",
    ]
    features = [c for c in feature_names if c in df.columns]
    if not features:
        raise ValueError("No feature columns found; unable to build feature vector.")

    assembler = VectorAssembler(inputCols=features, outputCol="features", handleInvalid="keep")
    df = assembler.transform(df)

    scaler = StandardScaler(inputCol="features", outputCol="Scaled_features", withMean=False, withStd=True)
    scaler_model = scaler.fit(df)
    df = scaler_model.transform(df).select("Scaled_features", "label")

    training, test = df.randomSplit([0.5, 0.5], seed=seed)

    rf = RandomForestClassifier(
        labelCol="label",
        featuresCol="Scaled_features",
        numTrees=200,
        seed=seed,
        featureSubsetStrategy="auto",
        impurity="gini",
    )
    model = rf.fit(training)
    predictions = model.transform(test)

    evaluator = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="rawPrediction", metricName="areaUnderROC")
    accuracy = float(evaluator.evaluate(predictions))

    print(f"ACCURACY={accuracy:.6f}")

    spark.stop()


if __name__ == "__main__":
    run()

# Optimization Summary
# - Removed unused imports and all intermediate .show(), prints, and plotting to avoid triggering expensive Spark actions and UI overhead.
# - Implemented robust CSV parsing with a fallback separator/decimal strategy to prevent mis-parses and reruns.
# - Eliminated Pandas row-wise .apply for label creation; replaced with Spark column expression to reduce Python overhead and data movement.
# - Selected only required columns after scaling to reduce memory footprint and shuffle size in subsequent stages.
# - Reduced Spark shuffle/parallelism defaults to lower task overhead for small/medium datasets, improving runtime/energy efficiency.
# - Fixed random seeds (NumPy + Spark split + RF seed) for reproducible, stable results without additional reruns.