# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

def solve():
    file_path = 'cancer_data.csv'
    
    # Robust CSV loading
    try:
        # Try default load
        df = pd.read_csv(file_path)
        # Check if first row was actually data (no headers)
        # If all column names are castable to float, it's likely data
        try:
            [float(c) for c in df.columns]
            df = pd.read_csv(file_path, header=None)
        except ValueError:
            pass
    except Exception:
        # Fallback for different locales/delimiters
        try:
            df = pd.read_csv(file_path, sep=';', decimal=',')
        except Exception:
            # Final fallback: empty dataframe to trigger safe exit
            df = pd.DataFrame()

    if df.empty:
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Clean and normalize column names
    df.columns = [str(c).strip() for c in df.columns]
    df.columns = [" ".join(str(c).split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

    # Force numeric conversion and handle missing values
    for col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Drop rows with NaNs in critical places
    df = df.dropna().reset_index(drop=True)

    if df.empty or len(df) < 5:
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Robust Target Selection Logic
    # 1. Prefer columns with exactly 2 unique values (standard for malignant/benign)
    # 2. Otherwise, look for the column with the fewest unique integers
    # 3. Default to the last column
    target_col = None
    unique_counts = df.nunique()
    
    # Check for binary columns
    binary_cols = unique_counts[unique_counts == 2].index.tolist()
    if binary_cols:
        # If multiple binary columns, prefer those that are not float-heavy (0.1, 0.2, etc)
        # or just pick the first one which is usually the diagnosis in these datasets
        target_col = binary_cols[0]
    else:
        # If no binary, look for low-cardinality integer columns
        potential_targets = unique_counts[unique_counts < 10].index.tolist()
        if potential_targets:
            target_col = potential_targets[0]
        else:
            # Fallback to the last column
            target_col = df.columns[-1]

    # Define features and target
    y = df[target_col]
    feature_cols = [c for c in df.columns if c != target_col]
    
    if not feature_cols:
        print(f"ACCURACY={0.000000:.6f}")
        return
        
    X = df[feature_cols]

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if y.nunique() >= 2 else None
    )

    # Defensive check: ensure target is classification-ready
    unique_y = np.unique(y_train)
    if len(unique_y) < 2:
        # Trivial baseline: 100% if test set matches train, else 0%
        acc = float(all(val == unique_y[0] for val in y_test))
        print(f"ACCURACY={acc:.6f}")
        return

    # Lightweight energy-efficient pipeline
    # Logistic Regression is CPU-friendly and mathematically robust for small/medium tabular data
    # StandardScaler ensures fast convergence for the solver
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', LogisticRegression(
            solver='liblinear', 
            max_iter=1000, 
            random_state=42,
            penalty='l2',
            C=1.0
        ))
    ])

    # Fit and Predict
    try:
        pipeline.fit(X_train, y_train)
        predictions = pipeline.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
    except Exception:
        # Fallback accuracy if model fails
        accuracy = 0.0

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# Optimization Summary:
# 1. Model Choice: Logistic Regression was selected as it is computationally inexpensive, 
#    requiring minimal CPU cycles compared to ensembles or deep learning.
# 2. Solver choice: 'liblinear' is efficient for small datasets and binary classification.
# 3. Scaler: StandardScaler included in a Pipeline to prevent data leakage and 
#    ensure the linear model converges faster (reducing compute time).
# 4. Data Handling: Robust CSV parsing with header detection and 'coerce' logic 
#    minimizes crashes and manual intervention.
# 5. Energy Efficiency: Avoids hyperparameter grid searches or complex cross-validation 
#    to maintain a low carbon footprint for the baseline.
# 6. Fallback Logic: Implemented checks for low-cardinality columns and empty datasets 
#    to ensure end-to-end execution regardless of input variations.
# 7. Classification Proxy: If the dataset is detected as regression-based by mistake, 
#    the logic attempts to treat low-cardinality integers as classes which is 
#    typical for UCI-style medical datasets.