# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder

# Robust data loading
data_path = 'mail_data.csv'
try:
    df = pd.read_csv(data_path, engine='python')
except Exception:
    df = pd.read_csv(data_path, sep=';', decimal=',', engine='python')

# Schema normalization
df.columns = [str(c).strip() for c in df.columns]
df.columns = [" ".join(str(c).split()) for c in df.columns]
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Robust column identification
cols_lower = [c.lower() for c in df.columns]
target_col = None
feature_col = None

# Identify target (prefer 'category' or 'label')
for target_name in ['category', 'label', 'type', 'target', 'class']:
    if target_name in cols_lower:
        target_col = df.columns[cols_lower.index(target_name)]
        break

# Identify feature (prefer 'message' or 'text' or 'content')
for feature_name in ['message', 'text', 'content', 'body', 'sms', 'mail']:
    if feature_name in cols_lower:
        feature_col = df.columns[cols_lower.index(feature_name)]
        break

# Fallback heuristics if columns not found
if target_col is None:
    # Choose first object column with low cardinality
    for c in df.columns:
        if df[c].dtype == 'object' and df[c].nunique() <= 10:
            target_col = c
            break
    if target_col is None:
        target_col = df.columns[0]

if feature_col is None:
    # Choose longest text column
    if len(df.columns) > 1:
        feature_col = [c for c in df.columns if c != target_col][0]
    else:
        feature_col = df.columns[0]

# Data Cleaning
df = df.dropna(subset=[target_col, feature_col])
df[feature_col] = df[feature_col].astype(str)

# Ensure target is valid for classification
if df[target_col].dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(df[target_col])
else:
    # Ensure binary/multiclass or fallback
    y = df[target_col].astype(int)

X = df[feature_col]

# Check for empty dataset
if len(df) == 0:
    print("ACCURACY=0.000000")
    exit()

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) > 1 else None)

# Lightweight Pipeline
# Logistic Regression is highly efficient for high-dimensional sparse text data
# TfidfVectorizer with max_features limits memory usage
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(
        stop_words='english', 
        max_features=3000, 
        min_df=2, 
        lowercase=True
    )),
    ('clf', LogisticRegression(
        max_iter=200, 
        class_weight='balanced', 
        solver='lbfgs',
        n_jobs=1
    ))
])

# Training
pipeline.fit(X_train, y_train)

# Evaluation
y_pred = pipeline.predict(X_test)

# Robust metric calculation
if len(np.unique(y)) > 1:
    accuracy = accuracy_score(y_test, y_pred)
else:
    # Constant target edge case
    accuracy = 1.0 if (y_pred == y_test).all() else 0.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary:
# 1. Model Choice: Logistic Regression was chosen over large ensembles (like Random Forest or XGBoost) or Deep Learning models because it is computationally lightweight, converges quickly on CPUs, and is highly effective for binary text classification.
# 2. Feature Extraction: TfidfVectorizer is used with a limit on max_features (3000). This reduces the memory footprint and the dimensionality of the input matrix, ensuring the pipeline runs efficiently on standard CPU hardware.
# 3. Preprocessing: Built-in scikit-learn Pipeline ensures minimal data copying. Label encoding and basic text normalization (lowercase, stop word removal) were used to minimize feature noise without expensive NLP parsing (like lemmatization).
# 4. Energy Efficiency: By avoiding complex architectures and limiting the iterations/feature space, we minimize CPU cycles and thermal output during training and inference.
# 5. Robustness: The script implements schema-agnostic column detection and robust CSV parsing to ensure end-to-end execution without manual intervention.
# 6. Fallback: If the target has only one class, the code handles it gracefully to prevent execution failure while still providing a valid accuracy metric.