# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import os
import sys

# Suppress warnings for clean output
import warnings
warnings.filterwarnings("ignore")

# Determine the dataset file to use
dataset_file = None
for candidate in ['14k.csv', 'input.csv', 'test.csv', 'test2.csv']:
    if os.path.exists(candidate):
        dataset_file = candidate
        break

if dataset_file is None:
    # Search for any CSV in current directory
    csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]
    if csv_files:
        dataset_file = csv_files[0]

assert dataset_file is not None, "No CSV file found in directory"

# Robust CSV reading
df = None
try:
    df = pd.read_csv(dataset_file)
    if df.shape[1] < 2:
        df = pd.read_csv(dataset_file, sep=';', decimal=',')
except Exception:
    try:
        df = pd.read_csv(dataset_file, sep=';', decimal=',')
    except Exception:
        pass

if df is None or df.shape[1] < 2:
    # Try the original source code's parsing approach
    import csv as csv_module
    features_list = []
    labels_list = []
    first = True
    with open(dataset_file, newline='') as csvfile:
        reader = csv_module.reader(csvfile, delimiter=' ', quotechar='|')
        for row in reader:
            cols = row[0].split(",") if len(row) > 0 else row
            if cols and cols[-1] == '':
                cols.pop()
            if not first:
                try:
                    both = np.asarray(cols, dtype=np.double).tolist()
                    num = both.pop()
                    features_list.append(both)
                    labels_list.append(num)
                except ValueError:
                    pass
            else:
                first = False
    if len(features_list) > 0:
        n_feat = len(features_list[0])
        col_names = [f'f{i}' for i in range(n_feat)] + ['target']
        all_data = [f + [l] for f, l in zip(features_list, labels_list)]
        df = pd.DataFrame(all_data, columns=col_names)

assert df is not None and len(df) > 0, "Failed to load dataset"

# Clean column names
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
drop_cols = [c for c in df.columns if c.startswith('Unnamed')]
if drop_cols:
    df.drop(columns=drop_cols, inplace=True)

assert df.shape[1] >= 2, "Dataset must have at least 2 columns"

# Identify target: last column as in the source code's convention
target_col = df.columns[-1]
feature_cols = list(df.columns[:-1])

# Coerce all columns to numeric
for c in df.columns:
    df[c] = pd.to_numeric(df[c], errors='coerce')

# Drop rows with NaN in target
df.dropna(subset=[target_col], inplace=True)
assert len(df) > 0, "No valid rows after cleaning"

# Replace inf with NaN then handle
df.replace([np.inf, -np.inf], np.nan, inplace=True)

# Prepare features and target
X = df[feature_cols].copy()
y = df[target_col].copy()

# Apply the same label transformation as in the source code:
# positive -> 1, negative -> -1, zero -> 0
# This makes it a classification task
y_transformed =