# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
import json
import os
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

def load_data(path):
    # Robust loading for JSON or CSV as specified in requirements
    try:
        if path.endswith('.json'):
            df = pd.read_json(path)
        else:
            try:
                df = pd.read_csv(path)
            except:
                df = pd.read_csv(path, sep=';', decimal=',')
    except Exception:
        # Final fallback: empty dataframe to trigger defensive checks
        return pd.DataFrame()
    return df

def preprocess_schema(df):
    if df.empty:
        return df, None, None
    
    # Normalize column names: strip, single space, drop Unnamed
    df.columns = [str(col).strip() for col in df.columns]
    df.columns = [" ".join(str(col).split()) for col in df.columns]
    df = df.drop(columns=[col for col in df.columns if 'Unnamed' in col], errors='ignore')
    
    # Identify target and features
    cols = df.columns.tolist()
    
    # Heuristic for Cuisine-Predictor dataset
    target_col = None
    feature_col = None
    
    # Preference for cuisine target
    for c in ['cuisine', 'category', 'type']:
        if c in [x.lower() for x in cols]:
            target_col = [x for x in cols if x.lower() == c][0]
            break
            
    # Preference for ingredients feature
    for c in ['ingredients', 'recipe', 'components']:
        if c in [x.lower() for x in cols]:
            feature_col = [x for x in cols if x.lower() == c][0]
            break
            
    # Fallback logic if names differ
    if not target_col and len(cols) > 1:
        # Choose the column with fewest unique strings as target, or first object col
        obj_cols = df.select_dtypes(include=['object']).columns
        target_col = obj_cols[0] if len(obj_cols) > 0 else cols[-1]
        
    if not feature_col:
        remaining = [c for c in cols if c != target_col]
        feature_col = remaining[0] if remaining else None
        
    return df, target_col, feature_col

def run_pipeline():
    dataset_path = 'train.json'
    df = load_data(dataset_path)
    
    df, target, feature = preprocess_schema(df)
    
    if df.empty or target is None or feature is None:
        # Trivial baseline if data is missing
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Handle 'ingredients' list type commonly found in cuisine datasets
    # Flatten lists to strings for vectorization
    def flatten_ingredients(x):
        if isinstance(x, list):
            return " ".join([str(i).replace(" ", "_") for i in x])
        return str(x)

    df[feature] = df[feature].apply(flatten_ingredients)
    
    # Clean rows with missing target
    df = df.dropna(subset=[target])
    
    # Ensure there are at least 2 classes for classification
    if df[target].nunique() < 2:
        print(f"ACCURACY={1.000000:.6f}")
        return

    X = df[feature]
    y = df[target]

    # Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if y.nunique() > 1 else None
    )

    # Lightweight Pipeline: CountVectorizer + MultinomialNB
    # MultinomialNB is extremely energy-efficient (linear time complexity, low memory)
    # Binary=True in CountVectorizer often works better for ingredient presence
    model = Pipeline([
        ('vectorizer', CountVectorizer(binary=True, min_df=2, max_features=5000)),
        ('classifier', MultinomialNB(alpha=0.1))
    ])

    # Fit model
    model.fit(X_train, y_train)

    # Predict and Evaluate
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary:
# 1. Model Choice: Multinomial Naive Bayes was selected as it is computationally inexpensive (O(n) training) 
#    and highly effective for high-dimensional sparse data like ingredient lists.
# 2. Vectorization: CountVectorizer with binary=True is used to represent presence/absence of ingredients. 
#    This is more energy-efficient than computing TF-IDF weights while maintaining high accuracy for this task.
# 3. Memory Efficiency: max_features=5000 and min_df=2 limit the vocabulary size, reducing the 
#    memory footprint during training and inference on CPU.
# 4. CPU-Friendly: No heavy matrix decompositions or iterative gradient-based optimizations (like SGD or Trees) 
#    were used, minimizing CPU cycles and energy consumption.
# 5. Robustness: The pipeline includes schema normalization and robust list-to-string parsing 
#    to handle the specific 'ingredients' format in JSON recipes.
# 6. Fallback Strategy: The code includes checks for empty datasets and single-class targets 
#    to ensure a non-breaking end-to-end execution.