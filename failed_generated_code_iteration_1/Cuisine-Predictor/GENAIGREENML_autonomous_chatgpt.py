# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

#!/usr/bin/env python
# coding: utf-8

import os
import re
import json
import warnings
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.impute import SimpleImputer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import r2_score

warnings.filterwarnings("ignore")


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed: ?", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _safe_read_table(path):
    # Robust CSV parsing with fallback
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(dfx):
        if dfx is None or dfx.empty:
            return True
        if dfx.shape[1] <= 1:
            return True
        # If a single column contains lots of separators, likely wrong delimiter
        first_col = dfx.columns[0]
        sample = dfx[first_col].astype(str).head(20).tolist()
        sep_hits = sum(1 for s in sample if s.count(";") >= 2)
        return sep_hits >= 5

    if _looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            pass

    if df is None:
        raise RuntimeError("Failed to read dataset.")
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)
    return df


def _safe_read_json(path):
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data


def _as_text_ingredients(x):
    if isinstance(x, (list, tuple, set)):
        parts = []
        for t in x:
            try:
                parts.append(str(t).strip().lower())
            except Exception:
                continue
        return " ".join([p for p in parts if p])
    if pd.isna(x):
        return ""
    return str(x).strip().lower()


def _bounded_accuracy_from_r2(y_true, y_pred):
    # Map R^2 to [0,1] to keep a stable "accuracy" proxy for regression fallback
    r2 = r2_score(y_true, y_pred)
    if not np.isfinite(r2):
        r2 = -1.0
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def _select_target_from_dataframe(df):
    cols = list(df.columns)
    if not cols:
        return None

    lower = {c: c.lower() for c in cols}
    preferred = ["target", "label", "class", "y", "cuisine"]
    for p in preferred:
        for c in cols:
            if lower[c] == p or lower[c].endswith(f"_{p}") or p in lower[c]:
                if df[c].nunique(dropna=True) >= 2:
                    return c

    # Prefer a non-constant object/categorical column (classification)
    obj_cols = [c for c in cols if df[c].dtype == "object"]
    for c in obj_cols:
        if df[c].nunique(dropna=True) >= 2:
            return c

    # Else a non-constant numeric column
    numeric_candidates = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        nun = s.nunique(dropna=True)
        if nun >= 2:
            numeric_candidates.append((c, nun))
    if numeric_candidates:
        numeric_candidates.sort(key=lambda x: x[1], reverse=True)
        return numeric_candidates[0][0]

    return None


def _build_pipeline_for_df(df, target_col):
    X = df.drop(columns=[target_col], errors="ignore")
    y = df[target_col]

    # Decide classification vs regression
    y_obj = (y.dtype == "object") or (str(y.dtype).startswith("category"))
    y_nonnull = y.dropna()
    unique = y_nonnull.nunique(dropna=True) if len(y_nonnull) else 0

    is_classification = bool(y_obj) or (unique > 1 and unique <= 50 and not str(y.dtype).startswith("float"))

    # Feature typing
    text_cols = []
    cat_cols = []
    num_cols = []

    for c in X.columns:
        s = X[c]
        if s.dtype == "object":
            # Heuristic: treat as text if average string length is sizeable
            ss = s.dropna().astype(str)
            avg_len = float(ss.map(len).mean()) if len(ss) else 0.0
            if avg_len >= 20:
                text_cols.append(c)
            else:
                cat_cols.append(c)
        else:
            num_cols.append(c)

    # Coerce numeric features safely
    for c in num_cols:
        X[c] = pd.to_numeric(X[c], errors="coerce")

    # Preprocess
    transformers = []

    if num_cols:
        num_pipe = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
        ])
        transformers.append(("num", num_pipe, num_cols))

    if cat_cols:
        cat_pipe = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ])
        transformers.append(("cat", cat_pipe, cat_cols))

    if text_cols:
        # CountVectorizer is CPU-friendly, sparse, and strong baseline for text
        for tc in text_cols:
            text_pipe = Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="constant", fill_value="")),
                ("vect", CountVectorizer(lowercase=True, max_features=20000, token_pattern=r"(?u)\b\w+\b")),
            ])
            transformers.append((f"txt_{tc}", text_pipe, tc))

    if not transformers:
        # If no usable features, create a constant numeric feature to keep pipeline valid
        X = X.copy()
        X["_const_"] = 1.0
        num_cols = ["_const_"]
        transformers = [("num", Pipeline(steps=[("imputer", SimpleImputer(strategy="median"))]), num_cols)]

    pre = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    if is_classification and unique >= 2:
        # MultinomialNB is very efficient for sparse count-like features and one-hot
        model = MultinomialNB(alpha=1.0)
        pipe = Pipeline(steps=[("pre", pre), ("model", model)])
        task = "classification"
    else:
        # Ridge is lightweight and stable baseline for regression
        model = Ridge(alpha=1.0, random_state=42)
        pipe = Pipeline(steps=[("pre", pre), ("model", model)])
        task = "regression"

    return X, y, pipe, task


def _run_json_cuisine_baseline(train_path):
    data = _safe_read_json(train_path)
    assert isinstance(data, list) and len(data) > 0

    # Build minimal dataframe from typical recipe schema, but robust to missing keys
    rows = []
    for d in data:
        if not isinstance(d, dict):
            continue
        rid = d.get("id", None)
        cuisine = d.get("cuisine", None)
        ingredients = d.get("ingredients", None)
        rows.append({"id": rid, "cuisine": cuisine, "ingredients": _as_text_ingredients(ingredients)})

    df = pd.DataFrame(rows)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Defensive cleanup
    df["ingredients"] = df["ingredients"].astype(str)
    df = df.dropna(subset=["cuisine"])
    df = df[df["cuisine"].astype(str).str.len() > 0]

    assert not df.empty

    X = df[["ingredients"]]
    y = df["cuisine"].astype(str)

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if y.nunique() > 1 else None
    )
    assert len(X_train) > 0 and len(X_test) > 0

    pre = ColumnTransformer(
        transformers=[
            ("txt", CountVectorizer(lowercase=True, max_features=25000, token_pattern=r"(?u)\b\w+\b"), "ingredients")
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # NB is very fast and strong for bag-of-words classification
    clf = Pipeline(steps=[
        ("pre", pre),
        ("model", MultinomialNB(alpha=1.0)),
    ])

    clf.fit(X_train, y_train)
    pred = clf.predict(X_test)
    accuracy = float(accuracy_score(y_test, pred))
    return accuracy


def main():
    # Locate dataset in a robust way
    cwd_files = set(os.listdir("."))
    train_json = "train.json" if "train.json" in cwd_files else None

    # Prefer JSON recipe dataset if present, otherwise generic CSV
    if train_json is not None:
        accuracy = _run_json_cuisine_baseline(train_json)
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Otherwise try a CSV in current directory (first one found)
    csv_files = [f for f in os.listdir(".") if f.lower().endswith(".csv")]
    if not csv_files:
        # Last-resort: try any .tsv
        tsv_files = [f for f in os.listdir(".") if f.lower().endswith(".tsv")]
        if tsv_files:
            csv_files = tsv_files

    if not csv_files:
        # Trivial fallback accuracy if no data available (keeps end-to-end)
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    path = sorted(csv_files)[0]
    df = _safe_read_table(path)
    assert df is not None and not df.empty

    # Normalize columns already done; coerce potentially numeric-looking object columns later
    target_col = _select_target_from_dataframe(df)
    if target_col is None:
        # Create a dummy target to keep pipeline running
        df["_target_"] = 0
        target_col = "_target_"

    # Drop fully empty columns
    nonempty_cols = [c for c in df.columns if df[c].notna().any()]
    df = df[nonempty_cols] if nonempty_cols else df
    assert not df.empty

    X, y, pipe, task = _build_pipeline_for_df(df, target_col)

    # Drop rows where y is NaN
    mask = ~pd.isna(y)
    X = X.loc[mask]
    y = y.loc[mask]
    assert len(X) > 1

    if task == "classification":
        y = y.astype(str)
        strat = y if y.nunique() >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=strat
        )
        assert len(X_train) > 0 and len(X_test) > 0

        # If only 1 class in train, fallback to simple majority accuracy
        if y_train.nunique() < 2:
            maj = y_train.iloc[0]
            pred = np.array([maj] * len(y_test), dtype=object)
            accuracy = float(accuracy_score(y_test.astype(str), pred.astype(str)))
        else:
            pipe.fit(X_train, y_train)
            pred = pipe.predict(X_test)
            accuracy = float(accuracy_score(y_test, pred))
    else:
        # Regression path
        y_num = pd.to_numeric(y, errors="coerce")
        mask2 = np.isfinite(y_num.values)
        X = X.loc[mask2]
        y_num = y_num.loc[mask2]
        assert len(X) > 1

        X_train, X_test, y_train, y_test = train_test_split(
            X, y_num, test_size=0.2, random_state=42
        )
        assert len(X_train) > 0 and len(X_test) > 0

        pipe.fit(X_train, y_train)
        pred = pipe.predict(X_test)
        accuracy = _bounded_accuracy_from_r2(y_test, pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses MultinomialNB + CountVectorizer for JSON recipe-style data: sparse bag-of-words is CPU-efficient and accurate.
# - Uses sklearn Pipeline/ColumnTransformer to avoid repeated preprocessing and ensure reproducibility.
# - Robust schema handling: auto-detect target; fallback to available non-constant column; handles missing/Unnamed columns.
# - Robust CSV parsing fallback (sep=';' and decimal=',') to reduce failures on European-formatted files.
# - Avoids heavy models/ensembles; chooses lightweight Naive Bayes / Ridge regression for CPU-friendly baselines.
# - Regression fallback reports "accuracy" as bounded proxy: ACC = clip((R2+1)/2, 0..1) for stable [0,1] scoring.