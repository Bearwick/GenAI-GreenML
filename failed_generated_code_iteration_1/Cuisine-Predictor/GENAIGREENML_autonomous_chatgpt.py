# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import json
import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.dummy import DummyClassifier, DummyRegressor


RANDOM_STATE = 42
DATASET_PATH = "train.json"


def _normalize_columns(cols):
    norm = []
    for c in cols:
        c = str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        norm.append(c)
    return norm


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if isinstance(c, str) and c.strip().lower().startswith("unnamed")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _read_json_robust(path):
    # Try pandas first; fallback to python json for robustness across json formats (array of dicts / json lines).
    try:
        df = pd.read_json(path)
        if isinstance(df, pd.Series):
            df = df.to_frame()
    except Exception:
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            df = pd.DataFrame(data)
        except Exception:
            # Final fallback: JSON Lines
            rows = []
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        rows.append(json.loads(line))
                    except Exception:
                        continue
            df = pd.DataFrame(rows)
    return df


def _is_non_constant(series):
    try:
        s = series.dropna()
        if s.empty:
            return False
        return s.nunique(dropna=True) > 1
    except Exception:
        return False


def _pick_target_and_task(df):
    # Prefer common target names, but never assume they exist.
    cols = list(df.columns)
    lower_map = {str(c).lower(): c for c in cols}

    preferred = ["cuisine", "target", "label", "class", "y"]
    for p in preferred:
        if p in lower_map:
            ycol = lower_map[p]
            y = df[ycol]
            # Determine classification viability
            nunique = y.dropna().nunique()
            if nunique >= 2:
                return ycol, "classification"
            break

    # Otherwise: pick best candidate column (object/categorical with >=2 classes), else numeric regression
    best_cls = None
    best_cls_n = -1
    for c in cols:
        if c is None:
            continue
        s = df[c]
        if s.dtype == "object" or str(s.dtype).startswith("string") or pd.api.types.is_categorical_dtype(s):
            n = s.dropna().nunique()
            if n >= 2 and n > best_cls_n:
                best_cls = c
                best_cls_n = n

    if best_cls is not None:
        return best_cls, "classification"

    # Numeric regression fallback: choose a non-constant numeric column
    numeric_candidates = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if _is_non_constant(s):
            numeric_candidates.append(c)
    if numeric_candidates:
        return numeric_candidates[0], "regression"

    # Absolute fallback: first column as target, trivial classification/regression later
    return cols[0], "classification" if df[cols[0]].dropna().nunique() >= 2 else "regression"


def _make_ingredients_text(x):
    # Handle lists of ingredients, strings, or other types robustly.
    if isinstance(x, list) or isinstance(x, tuple) or isinstance(x, set):
        parts = []
        for item in x:
            if item is None:
                continue
            s = str(item).strip()
            if s:
                parts.append(s)
        return " ".join(parts)
    if x is None or (isinstance(x, float) and not np.isfinite(x)):
        return ""
    return str(x)


def _build_pipeline(X, task, has_text):
    # Build lightweight, CPU-friendly pipelines.
    # Text: CountVectorizer + MultinomialNB for energy-efficient sparse linear classification.
    # Tabular: OneHotEncoder + LogisticRegression (small iterations) for robust baseline.

    if task == "classification":
        if has_text:
            model = MultinomialNB()
            pipe = Pipeline(
                steps=[
                    ("vect", CountVectorizer(lowercase=True, token_pattern=r"(?u)\b\w+\b", min_df=1)),
                    ("clf", model),
                ]
            )
            return pipe
        else:
            # Determine numeric/categorical columns from X
            numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
            categorical_cols = [c for c in X.columns if c not in numeric_cols]

            numeric_transformer = Pipeline(
                steps=[
                    ("imputer", SimpleImputer(strategy="median")),
                ]
            )
            categorical_transformer = Pipeline(
                steps=[
                    ("imputer", SimpleImputer(strategy="most_frequent")),
                    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
                ]
            )

            preprocessor = ColumnTransformer(
                transformers=[
                    ("num", numeric_transformer, numeric_cols),
                    ("cat", categorical_transformer, categorical_cols),
                ],
                remainder="drop",
                sparse_threshold=0.3,
            )

            model = LogisticRegression(
                max_iter=200,
                solver="lbfgs",
                n_jobs=1,
                multi_class="auto",
            )
            pipe = Pipeline(steps=[("pre", preprocessor), ("clf", model)])
            return pipe
    else:
        # Regression: use dummy regressor if no good features; otherwise linear ridge-like via SGD could be used,
        # but to stay lightweight & robust without tuning, use DummyRegressor baseline through same preprocessing.
        if has_text:
            # No strong text regression baseline; use simple counts + DummyRegressor to keep it safe
            from sklearn.linear_model import Ridge

            pipe = Pipeline(
                steps=[
                    ("vect", CountVectorizer(lowercase=True, token_pattern=r"(?u)\b\w+\b", min_df=1)),
                    ("reg", Ridge(alpha=1.0, random_state=RANDOM_STATE)),
                ]
            )
            return pipe
        else:
            numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
            categorical_cols = [c for c in X.columns if c not in numeric_cols]

            numeric_transformer = Pipeline(
                steps=[
                    ("imputer", SimpleImputer(strategy="median")),
                ]
            )
            categorical_transformer = Pipeline(
                steps=[
                    ("imputer", SimpleImputer(strategy="most_frequent")),
                    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
                ]
            )
            preprocessor = ColumnTransformer(
                transformers=[
                    ("num", numeric_transformer, numeric_cols),
                    ("cat", categorical_transformer, categorical_cols),
                ],
                remainder="drop",
                sparse_threshold=0.3,
            )

            from sklearn.linear_model import Ridge

            model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
            pipe = Pipeline(steps=[("pre", preprocessor), ("reg", model)])
            return pipe


def _accuracy_proxy_regression(y_true, y_pred):
    # Convert regression performance to stable bounded [0,1] score:
    # accuracy = 1 / (1 + MAPE_clipped), with safe handling around zeros.
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    eps = 1e-9
    denom = np.maximum(np.abs(y_true), eps)
    mape = np.mean(np.abs(y_true - y_pred) / denom)
    mape = float(np.clip(mape, 0.0, 1e6))
    acc = 1.0 / (1.0 + mape)
    return float(np.clip(acc, 0.0, 1.0))


def main():
    df = _read_json_robust(DATASET_PATH)
    if df is None or not isinstance(df, pd.DataFrame):
        df = pd.DataFrame()

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # If nested structures exist, keep as-is; only special-handle common ingredients field later.
    assert df.shape[0] > 0, "Empty dataset after loading"

    target_col, task = _pick_target_and_task(df)

    # Build X/y with defensive coercions
    y = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # Ensure X has at least one feature; if not, use id-like or create a constant feature
    if X.shape[1] == 0:
        X = pd.DataFrame({"__constant__": np.ones(len(df), dtype=np.float32)})

    # Detect common ingredients/text field
    lower_cols = {c.lower(): c for c in X.columns if isinstance(c, str)}
    ingredients_col = None
    for key in ["ingredients", "ingredient", "text", "description", "recipe", "instructions"]:
        if key in lower_cols:
            ingredients_col = lower_cols[key]
            break

    has_text = False
    X_text = None

    if ingredients_col is not None:
        # Use only ingredients text for a strong, efficient baseline and to avoid wide OHE.
        X_text = X[ingredients_col].apply(_make_ingredients_text)
        has_text = True

    # Clean y depending on task
    if task == "classification":
        # Avoid NaNs in y; align X accordingly
        valid = y.notna()
        if has_text:
            X_use = X_text[valid]
        else:
            X_use = X.loc[valid].copy()
        y_use = y.loc[valid].astype(str)

        # If still <2 classes, fallback to trivial baseline that runs end-to-end
        if y_use.nunique() < 2:
            # Create a dummy classification with constant prediction
            X_train, X_test, y_train, y_test = train_test_split(
                (X_use.to_frame(name="text") if has_text else X_use),
                y_use,
                test_size=0.2,
                random_state=RANDOM_STATE,
                stratify=None,
            )
            clf = DummyClassifier(strategy="most_frequent")
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            print(f"ACCURACY={accuracy:.6f}")
            return

        # Split; stratify for classification when possible
        strat = y_use if y_use.nunique() > 1 else None
        X_input = X_use if has_text else X_use
        X_train, X_test, y_train, y_test = train_test_split(
            X_input,
            y_use,
            test_size=0.2,
            random_state=RANDOM_STATE,
            stratify=strat,
        )

        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed"

        if has_text:
            pipeline = _build_pipeline(X=None, task="classification", has_text=True)
            pipeline.fit(X_train, y_train)
            y_pred = pipeline.predict(X_test)
        else:
            # Coerce numeric columns safely
            X_train = X_train.copy()
            X_test = X_test.copy()
            for c in X_train.columns:
                if pd.api.types.is_numeric_dtype(X_train[c]):
                    X_train[c] = pd.to_numeric(X_train[c], errors="coerce")
                    X_test[c] = pd.to_numeric(X_test[c], errors="coerce")
            pipeline = _build_pipeline(X_train, task="classification", has_text=False)
            pipeline.fit(X_train, y_train)
            y_pred = pipeline.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Regression path
    y_num = pd.to_numeric(y, errors="coerce")
    valid = y_num.notna() & np.isfinite(y_num)
    if has_text:
        X_use = X_text[valid]
    else:
        X_use = X.loc[valid].copy()
    y_use = y_num.loc[valid]

    assert len(y_use) > 0, "No valid regression targets after coercion"

    X_input = X_use
    X_train, X_test, y_train, y_test = train_test_split(
        X_input,
        y_use,
        test_size=0.2,
        random_state=RANDOM_STATE,
    )
    assert len(y_train) > 0 and len(y_test) > 0, "Train/test split failed"

    if has_text:
        pipeline = _build_pipeline(X=None, task="regression", has_text=True)
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        accuracy = _accuracy_proxy_regression(y_test.values, y_pred)
    else:
        X_train = X_train.copy()
        X_test = X_test.copy()
        # Coerce numeric columns safely
        for c in X_train.columns:
            if pd.api.types.is_numeric_dtype(X_train[c]):
                X_train[c] = pd.to_numeric(X_train[c], errors="coerce")
                X_test[c] = pd.to_numeric(X_test[c], errors="coerce")
        pipeline = _build_pipeline(X_train, task="regression", has_text=False)
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        accuracy = _accuracy_proxy_regression(y_test.values, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses CPU-friendly linear models: MultinomialNB for sparse bag-of-words cuisine classification; LogisticRegression for tabular classification; Ridge for regression fallback.
# - Prefers CountVectorizer over embeddings/NNs to minimize compute/memory while remaining effective for ingredient text.
# - Robust schema handling: normalizes headers, drops Unnamed columns, infers target and task without hardcoded column dependencies.
# - Defensive preprocessing: numeric coercion with errors='coerce', NaN-safe filtering/imputation via SimpleImputer, safe fallbacks (DummyClassifier) when classes are insufficient.
# - Reproducibility: fixed random_state and sklearn Pipelines/ColumnTransformer to avoid redundant work and ensure consistent transforms.
# - Regression "accuracy" uses a bounded proxy in [0,1]: 1/(1+MAPE) for stable reporting under unknown target scales.