# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "cars.csv"


def _read_csv_robust(path: str) -> pd.DataFrame:
    # Try default parsing first
    try:
        df0 = pd.read_csv(path)
    except Exception:
        df0 = None

    def _looks_wrong(df: pd.DataFrame) -> bool:
        if df is None or df.empty:
            return True
        # Heuristic: single column with many separators indicates wrong delimiter
        if df.shape[1] == 1:
            s = df.columns[0]
            if isinstance(s, str) and ("," in s or ";" in s or "\t" in s):
                return True
        # If most rows are NaN in all but one column, likely parsed wrong
        if df.shape[1] > 1:
            nonnull_counts = df.notnull().sum(axis=1)
            if (nonnull_counts <= 1).mean() > 0.8:
                return True
        return False

    if df0 is not None and not _looks_wrong(df0):
        return df0

    # Fallback: semicolon separator + decimal comma
    try:
        df1 = pd.read_csv(path, sep=";", decimal=",")
        if df1 is not None and not _looks_wrong(df1):
            return df1
        return df1 if df1 is not None else df0
    except Exception:
        return df0 if df0 is not None else pd.DataFrame()


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols = []
    for c in df.columns:
        s = str(c)
        s = s.strip()
        s = re.sub(r"\s+", " ", s)
        cols.append(s)
    df = df.copy()
    df.columns = cols
    # Drop unnamed index-like columns
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", c)]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _to_numeric_coerce(series: pd.Series) -> pd.Series:
    # Handle decimal commas and stray characters defensively
    s = series.astype(str).str.strip()
    s = s.str.replace(",", ".", regex=False)
    s = s.str.replace(r"[^0-9eE\.\-\+]+", "", regex=True)
    return pd.to_numeric(s, errors="coerce")


def _pick_target(df: pd.DataFrame, headers_hint=None) -> str:
    cols = list(df.columns)
    if not cols:
        return ""

    # Prefer a plausible classification target: brand if present and non-empty
    if "brand" in [c.lower() for c in cols]:
        for c in cols:
            if c.lower() == "brand":
                if df[c].astype(str).str.strip().nunique(dropna=True) >= 2:
                    return c

    # If headers hint provides a likely target, use if present and usable
    if headers_hint:
        for h in headers_hint:
            for c in cols:
                if c.lower() == str(h).strip().lower():
                    nun = df[c].nunique(dropna=True)
                    if nun >= 2:
                        return c

    # Otherwise pick a non-constant column; prefer non-numeric (classification)
    best = ""
    for c in cols:
        nun = df[c].nunique(dropna=True)
        if nun < 2:
            continue
        # Prefer object/categorical-like
        if df[c].dtype == object:
            return c
        if best == "":
            best = c
    return best


def _infer_task_and_prepare_target(df: pd.DataFrame, target_col: str):
    y_raw = df[target_col]
    # If object-like: treat as classification
    if y_raw.dtype == object:
        y = y_raw.astype(str).str.strip()
        y = y.replace({"": np.nan})
        return "classification", y

    # Try coerce numeric; if many unique integers small-ish, could still be classification
    y_num = _to_numeric_coerce(y_raw)
    nun = y_num.nunique(dropna=True)
    if nun >= 2 and nun <= 20 and float(nun) / max(1, len(y_num.dropna())) < 0.2:
        # Small number of classes; treat as classification by discretized numeric classes
        return "classification", y_num.astype("Int64").astype(str)

    return "regression", y_num


def _bounded_regression_score(y_true, y_pred) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() < 2:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    ss_res = np.sum((yt - yp) ** 2)
    ss_tot = np.sum((yt - np.mean(yt)) ** 2)
    r2 = 1.0 - (ss_res / ss_tot) if ss_tot > 0 else 0.0
    # Map to [0,1] for stable "accuracy" proxy
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    df = _read_csv_robust(DATASET_PATH)
    df = _normalize_columns(df)

    # If the dataset is still empty, create a minimal placeholder to avoid hard failure
    if df is None or df.empty or df.shape[1] == 0:
        # Trivial path with accuracy 0.0
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    headers_hint = ["mpg", "cylinders", "cubicinches", "hp", "weightlbs", "time-to-60", "year", "brand"]
    target_col = _pick_target(df, headers_hint=headers_hint)
    if not target_col:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Build X, y
    task, y = _infer_task_and_prepare_target(df, target_col)
    X = df.drop(columns=[target_col], errors="ignore").copy()

    # Normalize object columns slightly for stability
    for c in X.columns:
        if X[c].dtype == object:
            X[c] = X[c].astype(str).str.strip().replace({"": np.nan})

    # For numeric-looking columns that are objects, attempt coercion
    # (keep original object column if coercion would destroy high-cardinality text)
    for c in X.columns:
        if X[c].dtype == object:
            coerced = _to_numeric_coerce(X[c])
            # If coercion yields substantial numeric coverage, use numeric
            if coerced.notna().mean() >= 0.8:
                X[c] = coerced

    # Drop rows where y is missing
    valid = y.notna()
    X = X.loc[valid].reset_index(drop=True)
    y = y.loc[valid].reset_index(drop=True)

    # Replace inf values
    X = X.replace([np.inf, -np.inf], np.nan)

    assert len(X) > 0, "Dataset empty after target NaN filtering."

    # Identify feature types
    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Train/test split with defensive logic
    if task == "classification":
        y_classes = y.astype(str)
        if y_classes.nunique(dropna=True) < 2:
            task = "regression"
            y = _to_numeric_coerce(y_classes)
        else:
            # Stratify only if classes have sufficient members
            vc = y_classes.value_counts(dropna=True)
            stratify = y_classes if (vc.min() >= 2 and len(vc) >= 2) else None
            X_train, X_test, y_train, y_test = train_test_split(
                X,
                y_classes,
                test_size=0.2,
                random_state=42,
                stratify=stratify,
            )
            assert len(X_train) > 0 and len(X_test) > 0, "Empty train/test split."

            # Lightweight, CPU-friendly baseline
            clf = LogisticRegression(
                max_iter=300,
                solver="lbfgs",
                n_jobs=1,
                multi_class="auto",
            )
            model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

    # Regression fallback
    y_num = _to_numeric_coerce(y)
    valid2 = y_num.notna()
    X = X.loc[valid2].reset_index(drop=True)
    y_num = y_num.loc[valid2].reset_index(drop=True)

    if len(X) < 3:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y_num,
        test_size=0.2,
        random_state=42,
    )
    assert len(X_train) > 0 and len(X_test) > 0, "Empty train/test split."

    reg = Ridge(alpha=1.0, random_state=42)
    model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = _bounded_regression_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used lightweight linear models (LogisticRegression / Ridge) for CPU-friendly training and inference.
# - Employed a single sklearn Pipeline + ColumnTransformer to avoid redundant preprocessing and ensure reproducibility.
# - OneHotEncoder(handle_unknown="ignore") provides robust categorical handling with sparse matrices to reduce memory/compute.
# - Median/most_frequent imputers are cheap and stable; StandardScaler helps linear models converge quickly.
# - Implemented robust CSV reading fallback (sep=';' and decimal=',') and defensive schema/target selection to avoid failures.
# - If regression fallback is required, reports a bounded accuracy proxy: ACCURACY = clip((R2+1)/2, 0, 1).