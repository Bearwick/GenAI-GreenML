# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

def read_csv_fallback(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        return pd.read_csv(path, sep=';', decimal=',')
    if df.shape[1] == 1:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > 1:
                return df_alt
        except Exception:
            pass
    return df

def normalize_col(name):
    if not isinstance(name, str):
        name = str(name)
    return " ".join(name.strip().split())

def safe_to_numeric(series):
    if series.dtype == object:
        series = series.astype(str).str.replace(',', '.', regex=False)
    return pd.to_numeric(series, errors='coerce')

df = read_csv_fallback('cars.csv')
df.columns = [normalize_col(c) for c in df.columns]
df = df.loc[:, [c for c in df.columns if not c.lower().startswith('unnamed')]]
df = df.dropna(axis=1, how='all')
df = df.dropna(axis=0, how='all')
df = df.replace([np.inf, -np.inf], np.nan)
assert df.shape[0] > 0 and df.shape[1] > 0

numeric_ratio = {}
unique_counts = {}
for col in df.columns:
    series_num = safe_to_numeric(df[col])
    numeric_ratio[col] = series_num.notna().mean()
    unique_counts[col] = df[col].nunique(dropna=True)

preferred_names = ['target', 'label', 'class', 'brand']
target_col = None
for pref in preferred_names:
    for col in df.columns:
        if col.lower() == pref and unique_counts.get(col, 0) >= 2:
            target_col = col
            break
    if target_col is not None:
        break

n_rows = len(df)
max_cat_unique = min(20, max(2, int(0.5 * n_rows)))

if target_col is None:
    cat_candidates = [c for c in df.columns if unique_counts.get(c, 0) >= 2 and (numeric_ratio.get(c, 0) < 0.8 or df[c].dtype == object or df[c].dtype == bool) and unique_counts.get(c, 0) <= max_cat_unique]
    if cat_candidates:
        target_col = cat_candidates[0]
    else:
        num_candidates = [c for c in df.columns if unique_counts.get(c, 0) >= 2 and numeric_ratio.get(c, 0) >= 0.8]
        if num_candidates:
            def var_of(col):
                return safe_to_numeric(df[col]).var(skipna=True)
            target_col = max(num_candidates, key=var_of)
        else:
            target_col = df.columns[0]

if (numeric_ratio.get(target_col, 0) < 0.8) or (df[target_col].dtype == object) or (df[target_col].dtype == bool):
    problem_type = 'classification'
else:
    problem_type = 'regression'

if problem_type == 'regression':
    df[target_col] = safe_to_numeric(df[target_col])

df = df[df[target_col].notna()]
assert df.shape[0] > 0

feature_cols = [c for c in df.columns if c != target_col]
if not feature_cols:
    df['const_feature'] = 1.0
    feature_cols = ['const_feature']
    numeric_ratio['const_feature'] = 1.0

numeric_features = []
categorical_features = []
for col in feature_cols:
    series_num = safe_to_numeric(df[col])
    if series_num.notna().mean() >= 0.8:
        df[col] = series_num
        numeric_features.append(col)
    else:
        df[col] = df[col].astype(str)
        categorical_features.append(col)

X = df[feature_cols]
y = df[target_col]

test_size = 0.2
if len(df) < 5:
    test_size = 0.5

stratify = None
if problem_type == 'classification':
    class_counts = y.value_counts(dropna=True)
    if (class_counts >= 2).all() and len(class_counts) > 1 and len(df) >= 4:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=stratify)
assert len(X_train) > 0 and len(X_test) > 0

transformers = []
if numeric_features:
    num_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler(with_mean=False))])
    transformers.append(('num', num_transformer, numeric_features))
if categorical_features:
    cat_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))])
    transformers.append(('cat', cat_transformer, categorical_features))
if not transformers:
    num_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent'))])
    transformers.append(('num', num_transformer, feature_cols))

preprocess = ColumnTransformer(transformers=transformers, remainder='drop')

if problem_type == 'classification':
    if y_train.nunique(dropna=True) < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    if y_train.nunique(dropna=True) < 2:
        model = DummyRegressor(strategy='mean')
    else:
        model = Ridge(alpha=1.0)

pipeline = Pipeline(steps=[('preprocess', preprocess), ('model', model)])
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if problem_type == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    if np.isnan(r2) or np.isinf(r2):
        r2 = 0.0
    accuracy = max(0.0, min(1.0, r2))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Selected lightweight linear/dummy models to minimize CPU usage and energy consumption.
# - Used a compact ColumnTransformer with simple imputation, scaling, and one-hot encoding for reproducible preprocessing.
# - Applied safe numeric coercion and schema-agnostic target selection to handle uncertain inputs with minimal overhead.
# - Regression fallback reports clamped R^2 as a bounded accuracy proxy in [0,1].