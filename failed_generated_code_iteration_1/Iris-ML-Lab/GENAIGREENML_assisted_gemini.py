# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np

def run_analysis():
    np.random.seed(42)
    file_path = 'iris.csv'
    
    try:
        df = pd.read_csv(file_path)
        if len(df.columns) < 5:
            raise ValueError
    except Exception:
        df = pd.read_csv(file_path, sep=';', decimal=',')
    
    mapping = {'setosa': 0, 'versicolor': 1, 'virginica': 2}
    mapped_dict = {k.lower(): v for k, v in mapping.items()}
    
    x = df.iloc[:, [2, 3]].values.astype(float)
    y_raw = df.iloc[:, -1].astype(str).str.strip().str.lower().map(mapped_dict).values
    
    valid_mask = ~np.isnan(y_raw.astype(float))
    x = x[valid_mask]
    y = y_raw[valid_mask].astype(int)
    
    centroids = np.array([x[y == i].mean(axis=0) for i in range(3)])
    
    diff = x[:, np.newaxis, :] - centroids
    dist_sq = np.sum(diff**2, axis=2)
    predictions = np.argmin(dist_sq, axis=1)
    
    accuracy = np.mean(predictions == y)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_analysis()

# Optimization Summary
# 1. Replaced memory-intensive np.loadtxt(dtype='object') with pd.read_csv for faster, type-specific data loading.
# 2. Implemented a robust parsing fallback to handle different CSV delimiters and decimal formats automatically.
# 3. Eliminated all Matplotlib dependencies and visualization logic to drastically reduce runtime and energy consumption.
# 4. Sliced only the necessary feature columns (petal length and width) early to minimize memory footprint.
# 5. Replaced scipy.spatial.distance.cdist with vectorized NumPy broadcasting to reduce external dependencies and overhead.
# 6. Used squared Euclidean distance for classification to avoid computationally expensive square root operations.
# 7. Removed redundant loops, intermediate prints, and unnecessary data type conversions to streamline execution.
# 8. Optimized centroid calculation using a compact, vectorized array comprehension.