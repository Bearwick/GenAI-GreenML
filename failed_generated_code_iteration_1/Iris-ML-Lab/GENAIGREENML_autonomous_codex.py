# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
warnings.filterwarnings("ignore")

file_path = 'iris.csv'
try:
    df = pd.read_csv(file_path)
    if df.shape[1] <= 1:
        df = pd.read_csv(file_path, sep=';', decimal=',')
except Exception:
    df = pd.read_csv(file_path, sep=';', decimal=',')

df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
df = df.loc[:, ~df.columns.str.match(r'^Unnamed')]
df = df.dropna(axis=1, how='all')

assert df.shape[0] > 0 and df.shape[1] > 0

cols = list(df.columns)
lower_cols = [c.lower() for c in cols]
target_col = None
preferred = ['species', 'target', 'label', 'class', 'y']
for name in preferred:
    for col, lcol in zip(cols, lower_cols):
        if lcol == name or lcol.endswith(name):
            target_col = col
            break
    if target_col:
        break
if target_col is None:
    n_rows = len(df)
    for col in cols:
        series = df[col]
        nunique = series.dropna().nunique()
        if series.dtype == object or str(series.dtype).startswith('string'):
            if nunique > 1 and nunique <= max(20, int(0.1 * n_rows) + 1):
                target_col = col
                break
    if target_col is None:
        candidates = []
        for col in cols:
            series = df[col]
            nunique = series.dropna().nunique()
            if nunique > 1:
                candidates.append((nunique, col))
        if candidates:
            candidates.sort()
            target_col = candidates[0][1]
        else:
            target_col = cols[-1]

X_df = df.drop(columns=[target_col])
if X_df.shape[1] == 0:
    X_df = pd.DataFrame({'dummy': np.zeros(len(df))})

y_raw = df[target_col]
y_non_null = y_raw.dropna()
n_rows = len(df)
unique_count = y_non_null.nunique()
threshold = max(20, int(0.2 * n_rows))
if unique_count < 2:
    is_classification = False
elif y_raw.dtype.kind in 'biufc':
    is_classification = unique_count <= threshold
else:
    is_classification = True

if is_classification:
    mask = y_raw.notna()
    y_clean = y_raw[mask]
else:
    y_num = pd.to_numeric(y_raw, errors='coerce')
    mask = y_num.notna()
    y_clean = y_num[mask]

X_df = X_df.loc[mask].reset_index(drop=True)
y_clean = y_clean.reset_index(drop=True)

assert len(X_df) > 0

from sklearn.preprocessing import LabelEncoder
if is_classification:
    y = LabelEncoder().fit_transform(y_clean.astype(str))
else:
    y = y_clean.astype(float)

X = X_df.copy()
numeric_features = []
categorical_features = []
for col in X.columns:
    series = X[col]
    if series.dtype.kind in 'biufc':
        X[col] = pd.to_numeric(series, errors='coerce')
        numeric_features.append(col)
    else:
        converted = pd.to_numeric(series, errors='coerce')
        if converted.notna().sum() >= 0.5 * len(converted):
            X[col] = converted
            numeric_features.append(col)
        else:
            categorical_features.append(col)

X.replace([np.inf, -np.inf], np.nan, inplace=True)

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

transformers = []
if len(numeric_features) > 0:
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    transformers.append(('num', numeric_transformer, numeric_features))
if len(categorical_features) > 0:
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    transformers.append(('cat', categorical_transformer, categorical_features))

if transformers:
    preprocess = ColumnTransformer(transformers)
else:
    preprocess = 'passthrough'

from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier
from sklearn.model_selection import train_test_split

stratify = None
if is_classification and len(np.unique(y)) > 1:
    class_counts = pd.Series(y).value_counts()
    if class_counts.min() >= 2:
        stratify = y

n_samples = len(X)
if n_samples < 2:
    X_train, X_test, y_train, y_test = X, X, y, y
else:
    test_size = 0.2 if n_samples >= 5 else 0.5
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )

assert len(X_train) > 0 and len(X_test) > 0

if is_classification:
    if len(np.unique(y_train)) < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[('preprocess', preprocess), ('model', model)])
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

from sklearn.metrics import accuracy_score, r2_score
if is_classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    if len(y_test) > 1:
        r2 = r2_score(y_test, y_pred)
        accuracy = (r2 + 1) / 2
        accuracy = max(0.0, min(1.0, accuracy))
        if not np.isfinite(accuracy):
            accuracy = 0.0
    else:
        mae = np.mean(np.abs(np.asarray(y_test) - np.asarray(y_pred)))
        denom = np.mean(np.abs(np.asarray(y_test))) + 1e-9
        accuracy = 1.0 - min(1.0, mae / (denom + 1e-9))
        if not np.isfinite(accuracy):
            accuracy = 0.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) with a Dummy fallback to minimize CPU usage.
# - Applied simple imputation, scaling, and one-hot encoding in a ColumnTransformer for reproducible preprocessing.
# - Implemented robust target inference and conservative splitting to handle unknown schemas without heavy computation.
# - Reported regression performance as a bounded [0,1] proxy via (R2+1)/2 when regression is selected.