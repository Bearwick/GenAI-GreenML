# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

def looks_like_header(cols):
    if cols is None or len(cols) == 0:
        return False
    col_list = [str(c) for c in cols]
    numeric_like = 0
    for c in col_list:
        try:
            float(c)
            numeric_like += 1
        except Exception:
            pass
    numeric_ratio = numeric_like / len(col_list)
    duplicate_ratio = 1 - len(set(col_list)) / len(col_list)
    if numeric_ratio > 0.5 or duplicate_ratio > 0.1:
        return False
    return True

def load_data(path):
    read_kwargs = {}
    try:
        df = pd.read_csv(path)
        read_kwargs = {}
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
        read_kwargs = {'sep': ';', 'decimal': ','}
    if df.shape[1] == 1:
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
            read_kwargs = {'sep': ';', 'decimal': ','}
        except Exception:
            df = pd.read_csv(path)
            read_kwargs = {}
    if not looks_like_header(df.columns):
        try:
            df = pd.read_csv(path, header=None, **read_kwargs)
        except Exception:
            df = pd.read_csv(path, header=None)
    return df

def normalize_columns(df):
    drop_cols = [c for c in df.columns if str(c).lower().startswith('unnamed')]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    new_cols = []
    seen = {}
    for i, c in enumerate(df.columns):
        c_str = str(c).strip()
        c_str = " ".join(c_str.split())
        if c_str == '' or c_str.lower().startswith('unnamed'):
            c_str = f'col_{i}'
        if c_str in seen:
            seen[c_str] += 1
            c_str = f"{c_str}_{seen[c_str]}"
        else:
            seen[c_str] = 0
        new_cols.append(c_str)
    df.columns = new_cols
    return df

def choose_target(df, numeric_likeness):
    cols = list(df.columns)
    if not cols:
        return None
    for col in cols:
        col_low = str(col).lower()
        if any(k in col_low for k in ['label', 'target', 'class', 'attack', 'outcome']):
            return col
    for col in cols:
        if not numeric_likeness.get(col, False):
            try:
                vals = df[col].astype(str).str.lower()
                if vals.str.contains('normal|attack|anomaly|malicious|intrusion', regex=True).any():
                    return col
            except Exception:
                pass
    for col in reversed(cols[-3:]):
        if not numeric_likeness.get(col, False):
            if df[col].nunique(dropna=True) >= 2:
                return col
    obj_cols = [c for c in cols if not numeric_likeness.get(c, False)]
    if obj_cols:
        candidates = []
        for c in obj_cols:
            n = df[c].nunique(dropna=True)
            if n >= 2:
                candidates.append((n, c))
        if candidates:
            within = [t for t in candidates if t[0] <= 50]
            if within:
                return max(within, key=lambda x: x[0])[1]
            return min(candidates, key=lambda x: x[0])[1]
    num_cols = [c for c in cols if numeric_likeness.get(c, False)]
    if num_cols:
        candidates = []
        for c in num_cols:
            n = pd.to_numeric(df[c], errors='coerce').nunique(dropna=True)
            if n >= 2:
                candidates.append((n, c))
        if candidates:
            return min(candidates, key=lambda x: x[0])[1]
    return cols[-1]

file_path = 'data/raw/Train.txt'
df = load_data(file_path)
df = normalize_columns(df)
df = df.dropna(axis=1, how='all')
assert df is not None and not df.empty

numeric_likeness = {}
for col in df.columns:
    coerced = pd.to_numeric(df[col], errors='coerce')
    ratio = coerced.notna().mean()
    numeric_likeness[col] = ratio > 0.9

target = choose_target(df, numeric_likeness)
if target is None or target not in df.columns:
    target = df.columns[-1]

y_raw = df[target]
target_numeric = numeric_likeness.get(target, False)
if target_numeric:
    y_clean = pd.to_numeric(y_raw, errors='coerce')
else:
    y_clean = y_raw

if not target_numeric:
    task = 'classification'
else:
    unique_vals = y_clean.nunique(dropna=True)
    task = 'classification' if unique_vals <= 20 else 'regression'

X = df.drop(columns=[target])
numeric_features = []
categorical_features = []
keep_cols = []

for col in X.columns:
    if numeric_likeness.get(col, False):
        X[col] = pd.to_numeric(X[col], errors='coerce')
        X[col] = X[col].replace([np.inf, -np.inf], np.nan)
        if X[col].notna().sum() > 0:
            numeric_features.append(col)
            keep_cols.append(col)
    else:
        if X[col].notna().sum() > 0:
            categorical_features.append(col)
            keep_cols.append(col)

X = X[keep_cols]
mask = y_clean.notna()
X = X.loc[mask]
y_clean = y_clean.loc[mask]

if task == 'classification' and not target_numeric:
    y_clean = y_clean.astype(str).str.strip()

had_no_features = X.shape[1] == 0
if had_no_features:
    X = pd.DataFrame({'const': np.zeros(len(y_clean))}, index=y_clean.index)
    numeric_features = ['const']
    categorical_features = []
    keep_cols = ['const']

assert len(y_clean) > 0
y_nunique = y_clean.nunique(dropna=True)

n_samples = len(y_clean)
if n_samples < 2:
    X_train = X_test = X
    y_train = y_test = y_clean
else:
    test_size = 0.2
    if n_samples * test_size < 1:
        test_size = 1
    stratify = None
    if task == 'classification' and y_nunique >= 2 and isinstance(test_size, float):
        vc = y_clean.value_counts()
        if (vc >= 2).all():
            stratify = y_clean
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_clean, test_size=test_size, random_state=42, stratify=stratify
    )

assert len(y_train) > 0 and len(y_test) > 0

use_features = X.shape[1] > 0
if task == 'classification':
    if y_nunique < 2 or had_no_features:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    if y_nunique < 2 or had_no_features:
        model = DummyRegressor(strategy='mean')
    else:
        model = Ridge(alpha=1.0)

if use_features:
    transformers = []
    if numeric_features:
        with_mean = False if categorical_features else True
        num_pipeline = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler(with_mean=with_mean))
        ])
        transformers.append(('num', num_pipeline, numeric_features))
    if categorical_features:
        cat_pipeline = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
        ])
        transformers.append(('cat', cat_pipeline, categorical_features))
    preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')
    clf = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])
else:
    clf = Pipeline(steps=[('model', model)])

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    accuracy = (r2 + 1.0) / 2.0
    if not np.isfinite(accuracy):
        accuracy = 0.0
    accuracy = float(np.clip(accuracy, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# - Used lightweight linear models with simple preprocessing to keep computation CPU-friendly and energy-efficient.
# - Implemented robust parsing, schema inference, and safe fallbacks to ensure end-to-end execution on unknown data formats.
# - Regression accuracy is derived from a bounded R^2 proxy via (R^2+1)/2 to keep scores in [0,1].