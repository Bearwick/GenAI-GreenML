# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

def load_and_preprocess():
    file_path = 'trainingfinal.csv'
    
    # Robust CSV parsing
    try:
        df = pd.read_csv(file_path)
        if df.shape[1] <= 1:
            raise ValueError
    except Exception:
        df = pd.read_csv(file_path, sep=';', decimal=',')

    # Clean column names
    df.columns = [str(c).strip() for c in df.columns]
    df.columns = [" ".join(c.split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

    if df.empty:
        return None, None

    # Identify target and features
    target_col = 'class'
    if target_col not in df.columns:
        # Fallback: find any column that might be a target (usually last)
        potential_targets = [c for c in df.columns if 'class' in c.lower() or 'target' in c.lower()]
        target_col = potential_targets[0] if potential_targets else df.columns[-1]

    # Clean features: coerce to numeric, handle target
    y = df[target_col]
    X = df.drop(columns=[target_col])

    # Ensure numeric types for features
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')

    # Drop columns that are entirely NaN after coercion
    X = X.dropna(axis=1, how='all')
    
    # Handle NaNs/Infs in X
    X = X.replace([np.inf, -np.inf], np.nan)
    
    # Ensure dataset is not empty
    if X.empty or len(y) == 0:
        return None, None

    # Sync X and y by dropping rows with NaN in target
    valid_idx = y.dropna().index
    X = X.loc[valid_idx]
    y = y.loc[valid_idx]

    return X, y

def main():
    X, y = load_and_preprocess()
    
    if X is None or len(np.unique(y)) < 2:
        # Trivial baseline if data is insufficient for classification
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) > 1 else None
    )

    # Energy-efficient Pipeline
    # 1. SimpleImputer: Mean is low-cost
    # 2. StandardScaler: Linear scaling is CPU-friendly
    # 3. RandomForest: Restricted trees (50) and depth (8) for CPU efficiency 
    #    relative to project context (EEG data)
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler()),
        ('classifier', RandomForestClassifier(
            n_estimators=50, 
            max_depth=8, 
            random_state=42, 
            n_jobs=1
        ))
    ])

    # Fit
    pipeline.fit(X_train, y_train)

    # Predict and Evaluate
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# 1. Model Choice: Used a lightweight Random Forest (50 trees, max_depth=8). 
#    While Random Forest is an ensemble, a constrained version balances 
#    predictive power for EEG signals with low CPU overhead.
# 2. Parallelization: Set n_jobs=1. For small-to-medium datasets, the overhead 
#    of spawning processes for parallelization often consumes more energy than 
#    sequential execution.
# 3. Preprocessing: Used a Scikit-Learn Pipeline to ensure data is processed 
#    once and to prevent memory leaks/redundant computations during inference.
# 4. Data Robustness: Implemented multi-step CSV parsing and column normalization 
#    to prevent runtime crashes without human intervention.
# 5. Energy Efficiency: Avoided Deep Learning and large Gradient Boosting 
#    libraries which require significant FLOPs for training and inference.
# 6. Evaluation: Used a standard 80/20 split rather than Leave-One-Out (LOO) 
#    cross-validation. LOO is O(N) in training iterations, making it highly 
#    energy-inefficient for non-trivial datasets.
# 7. Numerical Safety: Systematic coercion of types and handling of Inf/NaN 
#    ensures the pipeline runs on the CPU without encountering floating-point 
#    exceptions or memory-intensive error-handling loops.
[FILE: /Users/edvard/Documents/NTNU/Master/Master Thesis/GenAI-GreenML/repos/dyslexia-randomforest/README.md]
# dyslexia-randomforest
leave one out classification of the dyslexia project EEG feature csvs with Random Forest