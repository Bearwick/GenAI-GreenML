# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# --- Robust CSV loading ---
df = None
for kwargs in [dict(), dict(sep=';', decimal=',')]:
    try:
        df = pd.read_csv('trainingfinal.csv', **kwargs)
        if df.shape[1] < 3:
            raise ValueError("Too few columns, retry with different separator")
        break
    except Exception:
        continue

if df is None or df.empty:
    raise RuntimeError("Could not parse dataset")

# --- Normalize column names ---
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
df = df[[c for c in df.columns if not c.startswith('Unnamed')]]

# --- Identify target ---
target_col = None
if 'class' in df.columns:
    target_col = 'class'
elif 'Class' in df.columns:
    target_col = 'Class'
else:
    # Try to find a likely target column (last column or first low-cardinality column)
    for c in reversed(df.columns):
        nunique = df[c].nunique(dropna=True)
        if 2 <= nunique <= 20:
            target_col = c
            break
    if target_col is None:
        # fallback: use last column
        target_col = df.columns[-1]

# --- Separate features and target ---
feature_cols = [c for c in df.columns if c != target_col]

# Coerce target
df[target_col] = pd.to_numeric(df[target_col], errors='coerce')
df = df.dropna(subset=[target_col])

# Coerce features to numeric
for c in feature_cols:
    df[c] = pd.to_numeric(df[c], errors='coerce')

# Drop columns that are all NaN
valid_features = [c for c in feature_cols if df[c].notna().sum() > 0]
feature_cols = valid_features

# Fill remaining NaN in features with column median
df[feature_cols] = df[feature_cols].fillna(df[feature_cols].median())

# Replace inf with NaN then fill
df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)
df[feature_cols] = df[feature_cols].fillna(df[feature_cols].median())

# --- Determine task type ---
n_classes = df[target_col].nunique()
is_classification = n_classes >= 2 and n_classes <= 50

if is_classification:
    y = df[target_col].astype(int)
else:
    y = df[target_col].astype(float)

X = df[list(feature_cols)]

assert X.shape[0] > 0, "No samples after preprocessing"
assert len(feature_cols) > 0, "No features after preprocessing"

# --- Handle edge case: fewer than 2 classes ---
if is_classification and y.nunique() < 2:
    # Trivial baseline: predict the single class
    accuracy = 1.0
    print(f"ACCURACY={accuracy:.6f}")
    import sys
    sys.exit(0)

# --- Train/test split ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y if is_classification else None
)

assert X_train.shape[0] > 0, "Empty training set"
assert X_test.shape[0] > 0, "Empty test set"

# --- Build energy-efficient pipeline ---
if is_classification:
    # For EEG dyslexia classification: try logistic regression first (lightweight),