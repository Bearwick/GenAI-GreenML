# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
import warnings
import os
import sys

warnings.filterwarnings("ignore")

# --- Robust CSV loading ---
def load_csv_robust(filepath):
    try:
        df = pd.read_csv(filepath)
        if df.shape[1] < 2:
            df = pd.read_csv(filepath, sep=';', decimal=',')
    except Exception:
        df = pd.read_csv(filepath, sep=';', decimal=',')
    return df

# --- Try to find data files ---
candidate_files = [
    "entire brain_training_2.csv",
    "entire brain_test_2.csv",
    "trainingfinal.csv",
    "testfinal.csv",
    "data.csv",
    "dataset.csv",
    "train.csv",
    "test.csv",
]

found_files = [f for f in candidate_files if os.path.isfile(f)]

# Determine if we have separate train/test or a single file
train_file = None
test_file = None
single_file = None

if "entire brain_training_2.csv" in found_files and "entire brain_test_2.csv" in found_files:
    train_file = "entire brain_training_2.csv"
    test_file = "entire brain_test_2.csv"
elif "trainingfinal.csv" in found_files and "testfinal.csv" in found_files:
    train_file = "trainingfinal.csv"
    test_file = "testfinal.csv"
elif "train.csv" in found_files and "test.csv" in found_files:
    train_file = "train.csv"
    test_file = "test.csv"
else:
    # Look for any CSV in current directory
    all_csvs = [f for f in os.listdir('.') if f.endswith('.csv')]
    if len(all_csvs) >= 2:
        # Check for train/test pair
        train_candidates = [f for f in all_csvs if 'train' in f.lower()]
        test_candidates = [f for f in all_csvs if 'test' in f.lower()]
        if train_candidates and test_candidates:
            train_file = train_candidates[0]
            test_file = test_candidates[0]
        else:
            single_file = all_csvs[0]
    elif len(all_csvs) == 1:
        single_file = all_csvs[0]

# --- Load data ---
if train_file and test_file:
    df_train = load_csv_robust(train_file)
    df_test = load_csv_robust(test_file)
    df = pd.concat([df_train, df_test], ignore_index=True)
    separate_split = True
    split_index = len(df_train)
elif single_file:
    df = load_csv_robust(single_file)
    separate_split = False
    split_index = None
else:
    # Last resort: try the expected training file name
    df = load_csv_robust("entire brain_training_2.csv")
    separate_split = False
    split_index = None

# --- Normalize column names ---
df.columns = df.columns.str.strip()
df.columns = df.columns.str.replace(r'\s+', ' ', regex=True)

# Drop 'Unnamed' columns
unnamed_cols = [c for c in df.columns if c.lower().startswith('unnamed')]
if unnamed_cols:
    df.drop(columns=unnamed_cols, inplace=True)

# --- Drop columns that are clearly not features (from source code hints) ---
cols_to_drop_if_present = [
    'parts {1=akoustiko,2=optiko,3=mousiki}',
    '