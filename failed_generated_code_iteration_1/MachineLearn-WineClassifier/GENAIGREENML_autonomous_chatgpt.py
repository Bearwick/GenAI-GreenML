# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "data/wine_dataset.csv"


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d):
        if d is None or d.empty:
            return True
        # Heuristic: if only one column, likely wrong delimiter
        if d.shape[1] == 1:
            return True
        # If columns look like a single concatenated header line
        joined = " ".join(map(str, d.columns))
        if joined.count(",") > 3 and d.shape[1] <= 2:
            return True
        return False

    if _looks_wrong(df):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if not _looks_wrong(df2):
                df = df2
        except Exception:
            pass

    if df is None:
        raise RuntimeError("Failed to read dataset.")
    return df


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _coerce_numeric_columns(df, numeric_cols):
    for c in numeric_cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _pick_target(df):
    # Prefer common targets if present
    for cand in ["quality", "target", "label", "y"]:
        if cand in df.columns:
            return cand

    # Otherwise choose a non-constant numeric column with least missingness
    numeric_candidates = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() >= max(10, int(0.2 * len(df))):
            nunique = s.nunique(dropna=True)
            if nunique and nunique > 1:
                miss = s.isna().mean()
                numeric_candidates.append((miss, -nunique, c))
    if numeric_candidates:
        numeric_candidates.sort()
        return numeric_candidates[0][2]

    # Otherwise choose first non-constant column
    for c in df.columns:
        if df[c].nunique(dropna=True) > 1:
            return c

    return df.columns[0] if len(df.columns) else None


def _is_classification_target(y):
    if y is None:
        return False
    if y.dtype == "O" or str(y.dtype).startswith("category") or str(y.dtype).startswith("bool"):
        return True
    # Numeric: treat as classification if few distinct integer-like values
    y_num = pd.to_numeric(y, errors="coerce")
    if y_num.notna().sum() == 0:
        return True
    y_clean = y_num.dropna()
    nunique = y_clean.nunique()
    if nunique < 2:
        return False
    # If all close to integers and small number of classes, assume classification
    close_to_int = np.mean(np.isclose(y_clean.values, np.round(y_clean.values)))
    if nunique <= 20 and close_to_int >= 0.98:
        return True
    return False


def _make_preprocessor(X):
    # Identify numeric vs categorical from current X
    num_cols = []
    cat_cols = []
    for c in X.columns:
        if pd.api.types.is_numeric_dtype(X[c]):
            num_cols.append(c)
        else:
            cat_cols.append(c)

    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, num_cols),
            ("cat", categorical_transformer, cat_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )
    return preprocessor, num_cols, cat_cols


def main():
    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Normalize provided headers style if present as row data mismatch (defensive)
    # No strict dependency on DATASET_HEADERS; columns are discovered dynamically.

    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _pick_target(df)
    if target_col is None or target_col not in df.columns:
        # Extremely defensive fallback
        target_col = df.columns[-1]

    # Build X/y, drop rows where target missing after coercion if numeric
    y_raw = df[target_col]
    X = df.drop(columns=[target_col])

    # Coerce numeric columns in X safely before schema detection
    numeric_guess = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    # Also attempt coercion for object columns that might be numeric strings
    for c in X.columns:
        if c not in numeric_guess:
            coerced = pd.to_numeric(X[c], errors="coerce")
            # If many values become numeric, treat as numeric
            if coerced.notna().mean() >= 0.8:
                X[c] = coerced

    # Handle infinities
    X = X.replace([np.inf, -np.inf], np.nan)

    classification = _is_classification_target(y_raw)

    if classification:
        y = y_raw.astype("object")
        # Drop rows with missing target
        mask = y.notna()
        X = X.loc[mask].copy()
        y = y.loc[mask].copy()
        assert len(y) > 0

        # If too many unique classes, still proceed with multinomial logistic regression
        n_classes = y.nunique(dropna=True)
        if n_classes < 2:
            # Trivial baseline: predict the only class; accuracy=1.0
            accuracy = 1.0
            print(f"ACCURACY={accuracy:.6f}")
            return

        preprocessor, _, _ = _make_preprocessor(X)

        # CPU-friendly solver; cap iterations for energy efficiency
        clf = LogisticRegression(
            max_iter=200,
            solver="lbfgs",
            n_jobs=1,
            multi_class="auto",
        )

        model = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", clf),
        ])

        stratify = y if (y.nunique() >= 2 and y.value_counts(normalize=False).min() >= 2) else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify
        )
        assert X_train.shape[0] > 0 and X_test.shape[0] > 0

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Regression path
    y_num = pd.to_numeric(y_raw, errors="coerce").replace([np.inf, -np.inf], np.nan)
    mask = y_num.notna()
    X = X.loc[mask].copy()
    y_num = y_num.loc[mask].copy()
    assert len(y_num) > 0

    preprocessor, _, _ = _make_preprocessor(X)

    reg = Ridge(alpha=1.0, random_state=42)

    model = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", reg),
    ])

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_num, test_size=0.2, random_state=42
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Stable bounded "accuracy" proxy in [0,1] based on normalized MAE
    y_test_arr = y_test.to_numpy(dtype=float)
    y_pred_arr = np.asarray(y_pred, dtype=float)
    mae = np.mean(np.abs(y_test_arr - y_pred_arr))
    denom = np.std(y_test_arr) + 1e-12
    accuracy = float(np.clip(1.0 - (mae / denom), 0.0, 1.0))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) for strong baselines with low CPU/energy cost.
# - Preprocessing via ColumnTransformer+Pipeline avoids redundant passes and ensures reproducibility.
# - Robust CSV loading retries alternative delimiter/decimal to prevent costly manual debugging loops.
# - Defensive schema handling: normalizes headers, drops Unnamed columns, inf->NaN, coerces numeric-like strings.
# - Limits LogisticRegression iterations and uses a single-threaded solver (n_jobs=1) for predictable CPU usage.
# - Regression fallback reports a bounded [0,1] score (1 - normalized MAE) to keep "ACCURACY" stable across scales.