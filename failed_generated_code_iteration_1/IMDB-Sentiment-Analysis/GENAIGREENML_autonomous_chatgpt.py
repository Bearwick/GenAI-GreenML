# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    norm = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        norm.append(c2)
    return norm


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _try_read_csv(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(d):
        if d is None or d.empty:
            return True
        if d.shape[1] == 1:
            first_col = str(d.columns[0]).lower()
            sample = ""
            try:
                sample = str(d.iloc[0, 0])
            except Exception:
                sample = ""
            if (";" in first_col) or (";" in sample):
                return True
        return False

    if looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df = None

    if df is None:
        raise RuntimeError("Failed to read dataset.")

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)
    return df


def _choose_target(df, headers_hint=None):
    cols = list(df.columns)

    if headers_hint:
        hint = [h.strip() for h in headers_hint if h and isinstance(h, str)]
        for cand in hint:
            if cand in cols:
                # Prefer 'class' as in provided headers, else accept hint
                if cand.lower() in ("class", "target", "label", "y"):
                    return cand

    for cand in cols:
        if str(cand).strip().lower() in ("class", "target", "label", "y"):
            return cand

    # Prefer non-constant numeric
    numeric_candidates = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        nunq = s.nunique(dropna=True)
        if nunq >= 2:
            numeric_candidates.append((nunq, c))
    if numeric_candidates:
        numeric_candidates.sort(reverse=True)
        return numeric_candidates[0][1]

    # Else prefer non-constant object
    obj_candidates = []
    for c in cols:
        nunq = df[c].nunique(dropna=True)
        if nunq >= 2:
            obj_candidates.append((nunq, c))
    if obj_candidates:
        obj_candidates.sort(reverse=True)
        return obj_candidates[0][1]

    # Fallback: last column
    return cols[-1] if cols else None


def _is_classification_target(y):
    if y is None:
        return False
    # Heuristic: object/bool -> classification; numeric with small unique -> classification
    if y.dtype == "bool":
        return True
    if y.dtype == "object" or str(y.dtype).startswith("category"):
        nunq = y.nunique(dropna=True)
        return nunq >= 2
    y_num = pd.to_numeric(y, errors="coerce")
    nunq = y_num.nunique(dropna=True)
    if nunq >= 2 and nunq <= 50:
        # treat as classification if discrete-ish
        non_na = y_num.dropna()
        if non_na.empty:
            return False
        frac_int = np.mean(np.isclose(non_na.values, np.round(non_na.values)))
        return frac_int > 0.95
    return False


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    if ss_tot <= 0:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    dataset_path = "movie_review_train.csv"
    headers_hint = ["class", "text"]

    df = _try_read_csv(dataset_path)
    df = df.copy()
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _choose_target(df, headers_hint=headers_hint)
    if target_col is None or target_col not in df.columns:
        # Minimal trivial fallback if no usable columns
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    y = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # Identify feature types robustly
    text_cols = []
    cat_cols = []
    num_cols = []

    for c in X.columns:
        s = X[c]
        if str(s.dtype) == "object" or str(s.dtype).startswith("category"):
            # Detect likely text column: long average length
            sample = s.dropna().astype(str).head(200)
            avg_len = sample.map(len).mean() if len(sample) else 0.0
            if avg_len >= 30:
                text_cols.append(c)
            else:
                cat_cols.append(c)
        else:
            num_cols.append(c)

    # Coerce numeric safely
    for c in num_cols:
        X[c] = pd.to_numeric(X[c], errors="coerce")

    # If no features left, create a constant feature
    if X.shape[1] == 0:
        X = pd.DataFrame({"__bias__": np.ones(len(df), dtype=float)})
        num_cols = ["__bias__"]
        text_cols, cat_cols = [], []

    is_clf = _is_classification_target(y)

    # Ensure not empty after dropping problematic rows (only for regression numeric target)
    if not is_clf:
        y_num = pd.to_numeric(y, errors="coerce")
        mask = np.isfinite(y_num.values)
        X = X.loc[mask].reset_index(drop=True)
        y = y_num.loc[mask].reset_index(drop=True)
        assert len(y) > 0

    # Train/test split
    stratify = None
    if is_clf:
        nunq = y.nunique(dropna=True)
        if nunq >= 2:
            stratify = y
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=stratify
    )
    assert len(X_train) > 0 and len(X_test) > 0

    # Build preprocessors
    transformers = []

    if text_cols:
        # Use a single shared vectorizer for combined text columns (cheap and robust)
        def _join_text(df_in):
            out = pd.DataFrame(index=df_in.index)
            joined = []
            for c in text_cols:
                joined.append(df_in[c].astype(str))
            if len(joined) == 1:
                out["__text__"] = joined[0]
            else:
                out["__text__"] = joined[0]
                for s in joined[1:]:
                    out["__text__"] = out["__text__"].str.cat(s, sep=" ", na_rep="")
            return out

        text_pipeline = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="constant", fill_value="")),
                ("tfidf", TfidfVectorizer(lowercase=True, stop_words="english", max_features=20000, ngram_range=(1, 2))),
            ]
        )
        # ColumnTransformer expects column selection; we pass all text columns then join via a FunctionTransformer-like lambda
        from sklearn.preprocessing import FunctionTransformer

        transformers.append(
            ("text", Pipeline([("join", FunctionTransformer(_join_text, validate=False)), ("vec", text_pipeline)]), text_cols)
        )

    if num_cols:
        num_pipeline = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="median")),
            ]
        )
        transformers.append(("num", num_pipeline, num_cols))

    if cat_cols:
        cat_pipeline = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
            ]
        )
        transformers.append(("cat", cat_pipeline, cat_cols))

    preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    # If classification but <2 classes in train, fallback to regression-style trivial baseline accuracy
    accuracy = 0.0
    if is_clf and y_train.nunique(dropna=True) >= 2:
        clf = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            C=1.0,
        )
        model = Pipeline(steps=[("prep", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    elif not is_clf:
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("prep", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_score(y_test, y_pred)
    else:
        # Trivial baseline: predict most frequent label
        y_mode = y_train.mode(dropna=True)
        if len(y_mode) > 0:
            pred = y_mode.iloc[0]
            y_pred = np.array([pred] * len(y_test), dtype=object)
            accuracy = float(accuracy_score(y_test, y_pred))
        else:
            accuracy = 0.0

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - CPU-friendly baseline: TF-IDF + LogisticRegression (liblinear) for text classification; Ridge for regression fallback.
# - Energy-efficient choices: capped vocabulary (max_features=20000) and simple linear models; avoids heavy ensembles/deep nets.
# - Robust CSV parsing: retries with ';' separator and ',' decimal if default parsing looks wrong.
# - Schema-robust: normalizes column names, drops 'Unnamed' columns, auto-selects a reasonable target if expected missing.
# - Safe preprocessing: numeric coercion with errors='coerce', imputes missing values, one-hot for low-cardinality categoricals.
# - Reproducibility: fixed random_state in split and Ridge; deterministic pipeline via sklearn Pipeline/ColumnTransformer.
# - Regression accuracy proxy: uses bounded score (R2 mapped to [0,1]) to keep ACCURACY in [0,1] when falling back.