# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _robust_read_csv(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(d):
        if d is None or d.empty:
            return True
        if d.shape[1] <= 1:
            return True
        bad = 0
        for c in d.columns[: min(3, d.shape[1])]:
            if isinstance(c, str) and (";" in c or "\t" in c):
                bad += 1
        return bad > 0

    if looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            pass

    if df is None:
        df = pd.DataFrame()

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)
    return df


def _infer_text_column(df):
    if df is None or df.empty:
        return None
    lowered = {c.lower(): c for c in df.columns}
    for key in ["text", "review", "sentence", "comment", "content", "document", "body"]:
        if key in lowered:
            return lowered[key]
    obj_cols = [c for c in df.columns if df[c].dtype == "object"]
    if not obj_cols:
        return None
    best_col = None
    best_score = -1
    for c in obj_cols:
        s = df[c].astype(str)
        score = float(s.str.len().mean())
        if score > best_score:
            best_score = score
            best_col = c
    return best_col


def _infer_target_column(df, preferred=None):
    if df is None or df.empty:
        return None

    cols = list(df.columns)
    lowered = {c.lower(): c for c in cols}

    if preferred is not None and preferred in cols:
        return preferred
    for key in ["class", "label", "target", "y"]:
        if key in lowered:
            return lowered[key]

    nunique = df.nunique(dropna=True)
    candidate_cols = [c for c in cols if nunique.get(c, 0) > 1]
    if not candidate_cols:
        return None

    obj_candidates = [c for c in candidate_cols if df[c].dtype == "object"]
    if obj_candidates:
        best = None
        best_n = -1
        for c in obj_candidates:
            n = int(nunique.get(c, 0))
            if 2 <= n <= 50 and n > best_n:
                best_n = n
                best = c
        if best is not None:
            return best

    # Prefer numeric-ish if no good object target
    numeric_like = []
    for c in candidate_cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().mean() > 0.8:
            numeric_like.append(c)
    if numeric_like:
        best = None
        best_var = -1.0
        for c in numeric_like:
            s = pd.to_numeric(df[c], errors="coerce")
            v = float(np.nanvar(s.values))
            if np.isfinite(v) and v > best_var:
                best_var = v
                best = c
        return best

    return candidate_cols[0]


def _make_label(y_series):
    y = y_series.copy()
    if y.dtype == "object":
        y_str = y.astype(str).str.strip()
        low = y_str.str.lower()
        mapping = {
            "neg": 0,
            "negative": 0,
            "0": 0,
            "false": 0,
            "no": 0,
            "pos": 1,
            "positive": 1,
            "1": 1,
            "true": 1,
            "yes": 1,
        }
        mapped = low.map(mapping)
        if mapped.notna().mean() >= 0.7:
            y_num = mapped
        else:
            y_num = y_str.astype("category").cat.codes
    else:
        y_num = pd.to_numeric(y, errors="coerce")

    y_num = pd.to_numeric(y_num, errors="coerce")
    return y_num


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    if y_true.size == 0:
        return 0.0
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    y_mean = float(np.mean(y_true))
    ss_tot = float(np.sum((y_true - y_mean) ** 2))
    if ss_tot <= 1e-12:
        r2 = 0.0
    else:
        r2 = 1.0 - (ss_res / ss_tot)
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


# Load dataset(s)
train_path_candidates = ["movie_review_train.csv", "train.csv", "data.csv", "dataset.csv"]
test_path_candidates = ["movie_review_test.csv", "test.csv"]

train_path = next((p for p in train_path_candidates if os.path.exists(p)), None)
test_path = next((p for p in test_path_candidates if os.path.exists(p)), None)

df_train = _robust_read_csv(train_path) if train_path else pd.DataFrame()
df_test = _robust_read_csv(test_path) if test_path else pd.DataFrame()

# If both exist but one is empty, treat as single-file
if df_train.empty and not df_test.empty:
    df_train, df_test = df_test, pd.DataFrame()

# Normalize again after any odd parsing
df_train.columns = _normalize_columns(df_train.columns)
df_train = _drop_unnamed(df_train)
df_test.columns = _normalize_columns(df_test.columns) if not df_test.empty else df_test.columns
df_test = _drop_unnamed(df_test) if not df_test.empty else df_test

assert not df_train.empty, "Dataset is empty after loading."

# Infer schema
text_col = _infer_text_column(df_train)
target_col = _infer_target_column(df_train, preferred=None)

# If target col ends up being the same as text, try another
if target_col == text_col:
    other_cols = [c for c in df_train.columns if c != text_col]
    target_col = _infer_target_column(df_train[other_cols], preferred=None) if other_cols else None

# Fallback: if no clear target, try last column (not text)
if target_col is None:
    candidates = [c for c in df_train.columns if c != text_col]
    target_col = candidates[-1] if candidates else None

# Prepare X/y
use_external_test = (not df_test.empty) and (target_col in df_test.columns)

if use_external_test:
    train_df = df_train.copy()
    test_df = df_test.copy()
else:
    train_df = df_train.copy()
    test_df = None

assert target_col is not None and target_col in train_df.columns, "No usable target column found."

y_all = _make_label(train_df[target_col])
valid_y_mask = y_all.notna()
train_df = train_df.loc[valid_y_mask].copy()
y_all = y_all.loc[valid_y_mask].copy()

assert train_df.shape[0] > 1, "Not enough rows after target cleaning."

# Choose features defensively
feature_cols = [c for c in train_df.columns if c != target_col]
if not feature_cols:
    # Minimal feature path: create a constant feature
    train_df["_const_feature_"] = 1
    feature_cols = ["_const_feature_"]

# Identify text vs categorical/numeric columns
if text_col is None or text_col not in feature_cols:
    # If no explicit text col, pick best object col as text, else no text feature
    text_col = _infer_text_column(train_df[feature_cols]) if feature_cols else None
    if text_col is not None and text_col not in feature_cols:
        text_col = None

numeric_cols = []
categorical_cols = []
for c in feature_cols:
    if c == text_col:
        continue
    s_num = pd.to_numeric(train_df[c], errors="coerce")
    if s_num.notna().mean() >= 0.8 and train_df[c].dtype != "object":
        numeric_cols.append(c)
    else:
        # treat as categorical (including low-quality numeric)
        categorical_cols.append(c)

# Build preprocessing + model pipeline (prefer MultinomialNB for text)
transformers = []
if text_col is not None:
    transformers.append(
        ("text", CountVectorizer(stop_words="english", min_df=2, max_df=0.9), text_col)
    )
if numeric_cols:
    transformers.append(
        ("num", Pipeline(steps=[
            ("to_num", "passthrough"),
            ("impute", SimpleImputer(strategy="median")),
        ]), numeric_cols)
    )
if categorical_cols:
    transformers.append(
        ("cat", Pipeline(steps=[
            ("impute", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]), categorical_cols)
    )

if not transformers:
    # absolute fallback: constant feature
    train_df["_const_feature_"] = 1
    feature_cols = ["_const_feature_"]
    transformers = [("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=True), feature_cols)]

preprocess = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

y_unique = pd.Series(y_all).dropna().unique()
y_unique = np.array(y_unique)

is_classification = False
if y_unique.size >= 2:
    # Heuristic: classification if small number of discrete values and close to integers
    finite = np.isfinite(y_unique.astype(float, copy=False)) if y_unique.dtype != object else False
    if y_unique.size <= 50:
        is_classification = True

if is_classification:
    clf = MultinomialNB(alpha=1.0)
    model = Pipeline(steps=[("preprocess", preprocess), ("model", clf)])
else:
    reg = Ridge(alpha=1.0, random_state=42)
    model = Pipeline(steps=[("preprocess", preprocess), ("model", reg)])

# Split / fit / evaluate
if use_external_test:
    X_train = train_df[feature_cols]
    y_train = y_all.astype(float)
    X_test = test_df.copy()
    # Ensure expected columns exist
    for c in feature_cols:
        if c not in X_test.columns:
            X_test[c] = np.nan
    X_test = X_test[feature_cols]
    y_test = _make_label(test_df[target_col])
    y_test = pd.to_numeric(y_test, errors="coerce")
    valid_mask = y_test.notna()
    X_test = X_test.loc[valid_mask].copy()
    y_test = y_test.loc[valid_mask].astype(float).copy()
    assert X_test.shape[0] > 0, "No valid test samples."
else:
    X = train_df[feature_cols]
    y = y_all.astype(float)
    stratify = None
    if is_classification:
        y_int = y.round().astype(int)
        if y_int.nunique(dropna=True) >= 2:
            stratify = y_int
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=stratify
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Train/test split failed."

model.fit(X_train, y_train)

if is_classification and pd.Series(y_train).nunique(dropna=True) >= 2:
    y_pred = model.predict(X_test)
    # Ensure labels are integers when possible
    y_true_eval = pd.Series(y_test).round().astype(int)
    y_pred_eval = pd.Series(y_pred).round().astype(int)
    accuracy = float(accuracy_score(y_true_eval, y_pred_eval))
else:
    y_pred = model.predict(X_test)
    accuracy = _bounded_regression_score(y_test, y_pred)

accuracy = float(np.clip(accuracy, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Uses CountVectorizer + MultinomialNB for text classification: sparse + linear-time training, strong baseline, CPU-friendly.
# - Robust CSV loading with delimiter/decimal fallback to avoid costly manual fixes and reruns.
# - ColumnTransformer/Pipeline ensures single-pass, reproducible preprocessing without redundant transforms.
# - Minimal feature engineering: bag-of-words + optional one-hot for categoricals; avoids heavy embeddings/deep models.
# - Defensive schema inference selects text/target columns from actual df.columns, avoiding brittle hard-coded names.
# - If classification is ill-posed (<2 classes), falls back to Ridge regression and reports a bounded (R2+1)/2 proxy in [0,1].