# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

def load_data(path):
    try:
        data = pd.read_csv(path, sep=None, engine='python')
        if data.shape[1] < 2:
            raise ValueError
    except:
        data = pd.read_csv(path, sep=';', decimal='.')
    return data

df = load_data('data/students_graduate_predict.csv')

target_col = 'Graduated (target)'
if target_col not in df.columns:
    target_col = df.columns[-1]

X = df.drop(columns=[target_col]).astype('float32')
y = df[target_col].astype('int32')

x_train, x_test, y_train, y_test = train_test_split(
    X, y, test_size=0.15, random_state=1, stratify=y
)

model = MLPClassifier(
    hidden_layer_sizes=(5, 7),
    max_iter=800,
    random_state=1,
    early_stopping=True,
    n_iter_no_change=10
)
model.fit(x_train, y_train)

y_pred = model.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Removed heavy visualization libraries (matplotlib, seaborn) to reduce memory and startup time.
# 2. Implemented robust CSV parsing with engine='python' and sep=None to detect delimiters automatically/efficiently.
# 3. Cast features to float32 and target to int32 to reduce memory footprint.
# 4. Added 'early_stopping=True' and 'n_iter_no_change' to the MLPClassifier to prevent redundant iterations and save energy.
# 5. Fixed random_state in the model to ensure reproducibility without increasing computational cost.
# 6. Eliminated redundant calls to model.predict() and intermediate DataFrame creations for logging.
# 7. Used 'stratify' in train_test_split to ensure stable evaluation metrics across runs.
# 8. Cleaned up unused variables and intermediate structures to free up memory earlier.