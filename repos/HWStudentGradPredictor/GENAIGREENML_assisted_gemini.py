# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

def run_pipeline():
    # Load dataset using the faster C engine and specific delimiter
    df = pd.read_csv('data/students_graduate_predict.csv', delimiter=';', engine='c')

    # Reduce memory footprint by downcasting numerical types
    for col in df.columns:
        if df[col].dtype == 'int64':
            df[col] = df[col].astype('int16')
        elif df[col].dtype == 'float64':
            df[col] = df[col].astype('float32')

    # Efficiently split features and target without creating redundant copies
    X = df.drop(columns=['Graduated (target)'])
    y = df['Graduated (target)']

    # Maintain original split ratio and random state for reproducibility
    x_train, x_test, y_train, y_test = train_test_split(
        X, y, test_size=0.15, random_state=1
    )

    # MLP hyperparameter optimization: 
    # Enabled early_stopping to halt training once the validation score stops improving,
    # significantly reducing total iterations and energy consumption.
    model = MLPClassifier(
        hidden_layer_sizes=(5, 7), 
        max_iter=800, 
        early_stopping=True, 
        n_iter_no_change=10,
        random_state=1
    )
    
    model.fit(x_train, y_train)

    # Perform inference only once to save cycles
    y_pred = model.predict(x_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# OPTIMIZATION SUMMARY
# 1. Memory Management: Downcast 64-bit types to 16-bit integers and 32-bit floats to reduce RAM usage and data movement overhead.
# 2. Computational Efficiency: Integrated early_stopping into the MLPClassifier to prevent redundant epochs after convergence.
# 3. Code Streamlining: Removed all visualization libraries (matplotlib/seaborn) and redundant dataframe displays to minimize import overhead and CPU cycles.
# 4. Data Loading: Utilized the 'c' engine in pandas for faster CSV parsing.
# 5. Redundancy Removal: Eliminated intermediate prediction steps and unused metric calculations (precision, recall, F1) that were not required for the final output.
# 6. Algorithmic Stability: Preserved random_state to ensure deterministic behavior while improving energy profile via early exit.