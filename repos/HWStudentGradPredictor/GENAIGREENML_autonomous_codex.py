# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, r2_score
from sklearn.linear_model import LogisticRegression, Ridge

warnings.filterwarnings("ignore")

path = "data/students_graduate_predict.csv"

def load_data(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is None or df.shape[1] <= 1:
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            if df is None:
                df = pd.read_csv(path)
    return df

df = load_data(path)
if df is None:
    raise ValueError("Data not loaded")

df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
df = df.loc[:, ~df.columns.str.contains('^Unnamed', case=False, regex=True)]

for col in df.columns:
    if df[col].dtype == object:
        ser = df[col].astype(str).str.replace(',', '.', regex=False)
        num = pd.to_numeric(ser, errors='coerce')
        if num.notna().mean() > 0.9:
            df[col] = num

keywords = ['target', 'label', 'class', 'outcome', 'graduated']
target_col = None
for c in df.columns:
    if any(k in c.lower() for k in keywords):
        target_col = c
        break
if target_col is None:
    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    numeric_cols = [c for c in numeric_cols if df[c].nunique(dropna=True) > 1]
    if numeric_cols:
        target_col = numeric_cols[-1]
    else:
        non_constant = [c for c in df.columns if df[c].nunique(dropna=True) > 1]
        if non_constant:
            target_col = non_constant[-1]
        else:
            target_col = df.columns[-1]

y_raw = df[target_col]
y_num_candidate = pd.to_numeric(y_raw.astype(str).str.replace(',', '.', regex=False), errors='coerce')
numeric_ratio = y_num_candidate.notna().mean()
if numeric_ratio > 0.9:
    y_use = y_num_candidate
    numeric_target = True
else:
    y_use = y_raw
    numeric_target = pd.api.types.is_numeric_dtype(y_use)

if numeric_target:
    y_nonan = y_use.dropna()
    unique_vals = y_nonan.nunique()
    is_int_like = False
    if len(y_nonan) > 0:
        is_int_like = np.all(np.isclose(y_nonan, np.round(y_nonan)))
    if unique_vals <= 20 and is_int_like:
        task = 'classification'
    else:
        task = 'regression'
else:
    task = 'classification'

if task == 'classification' and y_use.nunique(dropna=True) < 2:
    task = 'regression'

y_use = y_use.replace([np.inf, -np.inf], np.nan)
if y_use.notna().sum() == 0:
    y_use = pd.Series(np.zeros(len(y_use)), index=y_use.index)
mask = y_use.notna()
df = df.loc[mask].copy()
y_use = y_use.loc[mask]

if task == 'regression':
    if not pd.api.types.is_numeric_dtype(y_use):
        y_use = pd.Series(pd.factorize(y_use)[0], index=y_use.index).astype(float)
    y_use = pd.to_numeric(y_use, errors='coerce')
    y_use = y_use.replace([np.inf, -np.inf], np.nan)
    if y_use.notna().sum() == 0:
        y_use = pd.Series(np.zeros(len(y_use)), index=y_use.index)
    mask = y_use.notna()
    df = df.loc[mask].copy()
    y_use = y_use.loc[mask]

assert df.shape[0] > 0

features = [c for c in df.columns if c != target_col]
all_nan_cols = [c for c in features if df[c].isna().all()]
if all_nan_cols:
    df = df.drop(columns=all_nan_cols)
    features = [c for c in features if c not in all_nan_cols]
if len(features) == 0:
    df['__dummy__'] = 0
    features = ['__dummy__']

for col in features:
    if pd.api.types.is_numeric_dtype(df[col]):
        df[col] = pd.to_numeric(df[col], errors='coerce')
        df[col] = df[col].replace([np.inf, -np.inf], np.nan)

X = df[features]
y = y_use

assert len(X) > 0

test_size = 0.2
if len(df) < 5:
    test_size = 1

stratify = None
if task == 'classification':
    if y.nunique() > 1 and y.value_counts().min() >= 2 and not isinstance(test_size, int) and len(df) >= 10:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=42, stratify=stratify
)
assert len(X_train) > 0 and len(X_test) > 0

all_nan_train = [c for c in X_train.columns if X_train[c].isna().all()]
if all_nan_train:
    X_train = X_train.drop(columns=all_nan_train)
    X_test = X_test.drop(columns=all_nan_train)
    features = [c for c in features if c not in all_nan_train]
if len(features) == 0:
    X_train = X_train.assign(__dummy__=0)
    X_test = X_test.assign(__dummy__=0)
    features = ['__dummy__']

numeric_features = [c for c in features if pd.api.types.is_numeric_dtype(X_train[c])]
categorical_features = [c for c in features if c not in numeric_features]

transformers = []
if numeric_features:
    num_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    transformers.append(('num', num_transformer, numeric_features))
if categorical_features:
    cat_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ])
    transformers.append(('cat', cat_transformer, categorical_features))

preprocessor = ColumnTransformer(transformers, remainder='drop')

if task == 'classification':
    model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    score = r2_score(y_test, y_pred)
    if not np.isfinite(score):
        score = 0.0
    accuracy = max(0.0, min(1.0, score))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) for CPU-efficient training and inference.
# - Applied simple imputers, optional scaling, and one-hot encoding to keep preprocessing minimal yet robust.
# - Included schema-agnostic target selection, numeric coercion, and fallback handling to ensure end-to-end execution.
# - Regression accuracy proxy is clipped R2 in [0,1] for stable, bounded reporting.