# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Robust CSV Loading logic
DATA_PATH = "data/students_graduate_predict.csv"

def load_data(path):
    try:
        # Try standard comma separator
        df = pd.read_csv(path)
        # If only one column, likely wrong separator
        if df.shape[1] <= 1:
            df = pd.read_csv(path, sep=';', decimal=',')
    except Exception:
        # Fallback for complex parsing errors
        df = pd.read_csv(path, sep=None, engine='python')
    
    # Normalize column names
    df.columns = [str(c).strip() for c in df.columns]
    df.columns = [" ".join(str(c).split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

df = load_data(DATA_PATH)

if df.empty:
    import sys
    sys.exit(0)

# Identify target column
target_name = None
potential_targets = [c for c in df.columns if 'target' in c.lower() or 'graduated' in c.lower()]
if potential_targets:
    target_name = potential_targets[0]
else:
    # Use last non-constant column if target not found by name
    for col in reversed(df.columns):
        if df[col].nunique() > 1:
            target_name = col
            break

if not target_name:
    target_name = df.columns[-1]

# Feature selection: drop target and non-informative columns
X = df.drop(columns=[target_name])
y = df[target_name]

# Ensure y is categorical/integer for classification
if y.dtype == 'float' or y.dtype == 'object':
    # Try to factorize if it's strings or handle as classification
    y, _ = pd.factorize(y)
else:
    # Ensure it's clean for sklearn
    y = y.astype(int)

# Defensive check for single class
if len(np.unique(y)) < 2:
    # Trivial baseline if data is degenerate
    print(f"ACCURACY={1.000000:.6f}")
    import sys
    sys.exit(0)

# Preprocessing: split features by type
numeric_features = []
categorical_features = []

for col in X.columns:
    # Coerce numeric-like objects to floats
    if X[col].dtype == 'object':
        converted = pd.to_numeric(X[col], errors='coerce')
        if converted.isna().sum() < (len(X) * 0.5):
            X[col] = converted
        else:
            categorical_features.append(col)
            continue
    
    if pd.api.types.is_numeric_dtype(X[col]):
        numeric_features.append(col)
    else:
        categorical_features.append(col)

# Pipelines for numeric and categorical data
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Energy-efficient model: Logistic Regression
# Fast convergence, low CPU cycles, no GPU needed
clf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000, solver='lbfgs', random_state=42))
])

# Split and Train
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

if len(X_train) > 0:
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
else:
    accuracy = 0.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary:
# 1. Used Logistic Regression (LBFGS): Highly energy-efficient, linear complexity, 
#    converges quickly on CPU.
# 2. Robust Schema Handling: Implemented flexible CSV parsing (sep check, column normalization) 
#    to prevent pipeline failure on unknown input formats.
# 3. Memory Efficiency: Used sklearn Pipelines to avoid redundant data copying 
#    and ensure consistent preprocessing between train/test.
# 4. Feature Engineering: Applied StandardScaler for numeric stability and 
#    OneHotEncoding for categorical features, maintaining low computational overhead.
# 5. Resource Constraints: Avoided deep learning/large ensembles (XGBoost/RandomForest) 
#    to minimize CPU/memory footprint, adhering to green coding principles.
# 6. Robustness: Added fallback logic for target detection and handling of 
#    degenerate datasets (single class).
# 7. Preprocessing: Automated numeric coercion for object-type columns that 
#    contain numerical data.