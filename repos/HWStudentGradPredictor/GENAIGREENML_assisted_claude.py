# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

df = pd.read_csv('data/students_graduate_predict.csv', delimiter=';')

X = df.drop(columns=['Graduated (target)'])
Y = df['Graduated (target)']

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=1)

model = MLPClassifier(hidden_layer_sizes=(5, 7), max_iter=800, random_state=1)
model.fit(x_train, y_train)

y_pred = model.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Removed unused imports (confusion_matrix, precision_score, recall_score, f1_score, matplotlib, seaborn)
# Removed all plotting and visualization code to save computation and memory
# Removed unnecessary intermediate predictions (model.predict on iloc rows) that were unused
# Removed construction of intermediate DataFrames for display purposes
# Removed unused metric computations (precision, recall, f1) that were not required
# Used tuple instead of list for hidden_layer_sizes (canonical form, avoids list overhead)
# Added random_state to MLPClassifier for reproducibility
# Kept only essential computation: load, split, train, predict, accuracy