# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

try:
    df = pd.read_csv('data/students_graduate_predict.csv')
    if df.shape[1] < 2:
        raise ValueError
except (ValueError, Exception):
    df = pd.read_csv('data/students_graduate_predict.csv', sep=';', decimal=',')

target_col = [c for c in df.columns if 'graduate' in c.lower() or 'target' in c.lower()]
if not target_col:
    target_col = [df.columns[-1]]
target_col = target_col[0]

X = df.drop(columns=[target_col])
y = df[target_col]

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)

model = MLPClassifier(hidden_layer_sizes=(5, 7), max_iter=800, random_state=1)
model.fit(x_train, y_train)

y_pred = model.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Removed matplotlib/seaborn imports and all plotting code to eliminate unnecessary computation and dependencies.
# Removed unused metric computations (precision, recall, f1) and intermediate DataFrames that were never consumed.
# Removed redundant predict call on X.iloc[[0,1,2,3]] which served no purpose beyond display.
# Removed all print/display statements and interactive outputs per requirements.
# Used robust CSV parsing with fallback from default delimiter to sep=';' and decimal=','.
# Dynamically detected target column from headers instead of hardcoding exact name.
# Set random_state on MLPClassifier for reproducibility.
# Used tuple instead of list for hidden_layer_sizes (canonical form, avoids list copy internally).
# Minimal memory footprint: no intermediate DataFrames or unused variables retained.