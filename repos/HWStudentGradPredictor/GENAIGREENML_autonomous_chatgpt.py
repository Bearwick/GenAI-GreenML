# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import sys
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _find_dataset_path() -> str:
    # Prefer a provided path; otherwise search common filenames in current directory.
    if len(sys.argv) > 1 and sys.argv[1]:
        return sys.argv[1]

    candidates = [
        "data.csv",
        "dataset.csv",
        "train.csv",
        "students.csv",
        "student.csv",
        "data.txt",
        "dataset.txt",
    ]
    for name in candidates:
        if os.path.isfile(name):
            return name

    for name in os.listdir("."):
        lower = name.lower()
        if lower.endswith(".csv") or lower.endswith(".txt"):
            return name

    raise FileNotFoundError("Dataset file not found. Provide path as the first command-line argument.")


def _read_dataset(path: str) -> pd.DataFrame:
    # Use semicolon separator as specified by headers.
    df = pd.read_csv(path, sep=";")
    return df


def main() -> None:
    path = _find_dataset_path()
    df = _read_dataset(path)

    target_col = "Graduated (target)"
    if target_col not in df.columns:
        raise KeyError(f"Target column '{target_col}' not found in dataset.")

    y = df[target_col]
    X = df.drop(columns=[target_col])

    # Detect categorical vs numeric without assumptions about encoding quality.
    # Object/category treated as categorical; the rest numeric for lightweight preprocessing.
    cat_cols = X.select_dtypes(include=["object", "category"]).columns.tolist()
    num_cols = [c for c in X.columns if c not in cat_cols]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, num_cols),
            ("cat", categorical_transformer, cat_cols),
        ],
        sparse_threshold=0.3,
        remainder="drop",
    )

    # Lightweight linear classifier; good baseline; converges fast on CPU with sparse input.
    # Using saga supports sparse and multinomial; small iteration budget keeps energy low.
    clf = LogisticRegression(
        solver="saga",
        max_iter=300,
        n_jobs=1,
        C=1.0,
        penalty="l2",
        random_state=42,
    )

    model = Pipeline(
        steps=[
            ("preprocess", preprocessor),
            ("clf", clf),
        ]
    )

    # Stratified split for stable accuracy estimate on classification targets.
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if y.nunique() > 1 else None
    )

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Chose LogisticRegression (linear model) to avoid heavy compute and memory use; strong baseline for tabular data.
# - Used a single Pipeline + ColumnTransformer for reproducible preprocessing and to prevent data leakage.
# - Median/mode imputation is computationally cheap and robust; avoids iterative/matrix factorization methods.
# - StandardScaler improves optimizer convergence; OneHotEncoder uses sparse matrices to minimize RAM/CPU.
# - Limited max_iter and forced n_jobs=1 for predictable CPU usage and reduced energy consumption.
# - Avoided deep learning, embeddings, plotting, interactive inputs, and model serialization to keep runtime lean.