# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge

warnings.filterwarnings("ignore")


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        if c2.lower().startswith("unnamed:"):
            c2 = ""
        normed.append(c2)
    return normed


def _try_read_csv(path):
    df_local = None
    # Attempt 1: default
    try:
        df_local = pd.read_csv(path)
    except Exception:
        df_local = None

    def _looks_wrong(dfx):
        if dfx is None or not isinstance(dfx, pd.DataFrame):
            return True
        if dfx.shape[0] == 0 or dfx.shape[1] <= 1:
            return True
        # Heuristic: if only one column and it contains many ';', parsing likely wrong
        if dfx.shape[1] == 1:
            try:
                sample = dfx.iloc[:10, 0].astype(str).str.contains(";").mean()
                if sample > 0.5:
                    return True
            except Exception:
                return True
        return False

    if _looks_wrong(df_local):
        # Attempt 2: semicolon + decimal comma
        try:
            df_local = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df_local = None

    if df_local is None:
        raise FileNotFoundError(f"Could not read CSV at path: {path}")

    df_local.columns = _normalize_columns(df_local.columns)
    # Drop empty/unnamed columns after normalization
    df_local = df_local.loc[:, [c for c in df_local.columns if c != ""]]
    return df_local


def _pick_target(df):
    # Prefer a column containing 'target'
    cols = list(df.columns)
    target_candidates = []
    for c in cols:
        if "target" in c.lower():
            target_candidates.append(c)
    # Prefer specific known header if present but do not require it
    preferred = None
    for c in cols:
        if c.strip().lower() == "graduated (target)":
            preferred = c
            break
    if preferred is not None:
        return preferred

    if target_candidates:
        return target_candidates[-1]

    # Otherwise choose last column
    return cols[-1] if cols else None


def _coerce_numeric_safe(series):
    return pd.to_numeric(series, errors="coerce")


def _build_and_eval(df):
    assert df is not None and isinstance(df, pd.DataFrame)
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _pick_target(df)
    if target_col is None or target_col not in df.columns:
        # Fallback: just pick last available column
        target_col = df.columns[-1]

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # Drop fully empty columns
    X = X.dropna(axis=1, how="all")

    # If no features remain, create a constant feature to keep pipeline running
    if X.shape[1] == 0:
        X = pd.DataFrame({"_constant": np.ones(len(df))})

    # Identify numeric-like columns by coercion success ratio
    numeric_cols = []
    categorical_cols = []

    for c in X.columns:
        s = X[c]
        if pd.api.types.is_numeric_dtype(s):
            numeric_cols.append(c)
            continue
        # Try coercion for object columns
        s_num = _coerce_numeric_safe(s)
        non_na_ratio = float(s_num.notna().mean()) if len(s_num) else 0.0
        # If many values become numeric, treat as numeric
        if non_na_ratio >= 0.85:
            X[c] = s_num
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)

    # Prepare target: attempt classification first (binary/multiclass)
    y = y_raw.copy()
    # Try numeric coercion for target if it looks numeric
    y_num = _coerce_numeric_safe(y)
    y_num_ratio = float(y_num.notna().mean()) if len(y_num) else 0.0
    if y_num_ratio >= 0.85:
        y = y_num

    # Remove rows with missing target
    mask = pd.Series(y).notna().to_numpy()
    X = X.loc[mask].reset_index(drop=True)
    y = pd.Series(y).loc[mask].reset_index(drop=True)

    assert X.shape[0] > 0 and len(y) > 0

    # Determine classification viability
    is_classification = False
    if pd.api.types.is_numeric_dtype(y):
        # If numeric but with few unique values, treat as classification
        uniq = pd.Series(y).dropna().unique()
        if len(uniq) >= 2 and len(uniq) <= max(20, int(0.05 * len(y))):
            is_classification = True
    else:
        uniq = pd.Series(y).dropna().unique()
        if len(uniq) >= 2:
            is_classification = True

    # Train/test split with defensive handling of small datasets
    test_size = 0.2
    random_state = 42

    if is_classification:
        # Ensure y is label-encodable and stable
        y_cls = y.astype(str)
        n_classes = y_cls.nunique(dropna=True)
        if n_classes < 2:
            is_classification = False
        else:
            stratify = y_cls if (len(y_cls) >= 10 and n_classes > 1 and y_cls.value_counts().min() >= 2) else None
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_cls, test_size=test_size, random_state=random_state, stratify=stratify
            )

            assert X_train.shape[0] > 0 and X_test.shape[0] > 0

            numeric_transformer = Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler(with_mean=False)),  # safe with sparse outputs
            ])
            categorical_transformer = Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
            ])

            preprocessor = ColumnTransformer(
                transformers=[
                    ("num", numeric_transformer, list(numeric_cols)),
                    ("cat", categorical_transformer, list(categorical_cols)),
                ],
                remainder="drop",
                sparse_threshold=0.3,
            )

            # CPU-friendly linear model; use saga for sparse + multinomial/binary
            clf = LogisticRegression(
                solver="saga",
                max_iter=300,
                n_jobs=1,
                C=1.0,
                tol=1e-3,
            )

            model = Pipeline(steps=[
                ("preprocess", preprocessor),
                ("model", clf),
            ])

            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
            return accuracy

    # Regression fallback: produce a bounded "accuracy proxy" in [0,1]
    # Use Ridge (lightweight) and score via 1/(1+RMSE/std) which is stable and bounded.
    y_reg = pd.Series(y)
    if not pd.api.types.is_numeric_dtype(y_reg):
        # Coerce to numeric; if impossible, use ordinal codes
        y_reg_num = _coerce_numeric_safe(y_reg)
        if float(y_reg_num.notna().mean()) >= 0.5:
            y_reg = y_reg_num
        else:
            y_reg = y_reg.astype("category").cat.codes.astype(float)

    # Remove any remaining missing targets
    mask2 = y_reg.notna().to_numpy()
    X = X.loc[mask2].reset_index(drop=True)
    y_reg = y_reg.loc[mask2].reset_index(drop=True)

    assert X.shape[0] > 1

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_reg, test_size=test_size, random_state=random_state
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False)),
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, list(numeric_cols)),
            ("cat", categorical_transformer, list(categorical_cols)),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    reg = Ridge(alpha=1.0, random_state=random_state)

    model = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", reg),
    ])

    model.fit(X_train, y_train)
    preds = model.predict(X_test)

    y_test_arr = np.asarray(y_test, dtype=float)
    preds_arr = np.asarray(preds, dtype=float)

    # Safe RMSE
    err = preds_arr - y_test_arr
    rmse = float(np.sqrt(np.mean(err * err))) if len(err) else 0.0
    std = float(np.std(y_test_arr)) if len(y_test_arr) else 0.0
    denom = std if std > 1e-12 else (abs(float(np.mean(y_test_arr))) + 1.0)
    accuracy_proxy = 1.0 / (1.0 + (rmse / max(denom, 1e-12)))
    accuracy_proxy = float(np.clip(accuracy_proxy, 0.0, 1.0))
    return accuracy_proxy


def main():
    # Try common paths
    candidate_paths = [
        "data/students_graduate_predict.csv",
        "students_graduate_predict.csv",
        "data.csv",
        "dataset.csv",
        "train.csv",
    ]
    df = None
    for p in candidate_paths:
        if os.path.exists(p):
            df = _try_read_csv(p)
            break
    if df is None:
        # Last resort: first CSV in current tree (shallow)
        for root, _, files in os.walk("."):
            for fn in files:
                if fn.lower().endswith(".csv"):
                    df = _try_read_csv(os.path.join(root, fn))
                    break
            if df is not None:
                break
    if df is None:
        raise FileNotFoundError("No CSV dataset found.")

    # Replace inf with NaN early
    df = df.replace([np.inf, -np.inf], np.nan)

    accuracy = _build_and_eval(df)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses a lightweight linear model (LogisticRegression with saga) instead of MLP to reduce CPU cycles and energy.
# - Robust CSV ingestion with fallback delimiter/decimal prevents expensive manual debugging and reruns.
# - ColumnTransformer + Pipeline avoids repeated preprocessing work and ensures reproducibility with a single fit/transform graph.
# - Numeric detection via coercion ratio handles unknown schemas while keeping transformations simple (median impute + scaler).
# - OneHotEncoder(handle_unknown="ignore") enables sparse representation for categorical features, reducing memory/compute.
# - n_jobs=1 prevents uncontrolled parallelism that can increase power draw on shared/limited CPU environments.
# - If classification is not viable, falls back to Ridge regression (closed-form-ish, efficient) and reports a bounded accuracy proxy:
#   accuracy_proxy = 1 / (1 + RMSE / scale), which stays in [0,1] and is stable across target magnitudes.