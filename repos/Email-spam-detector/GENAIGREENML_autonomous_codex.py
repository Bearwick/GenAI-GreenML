# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, r2_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor

path = "email_analysis.csv"
try:
    df = pd.read_csv(path)
    if df.shape[1] == 1:
        df_alt = pd.read_csv(path, sep=';', decimal=',')
        if df_alt.shape[1] > 1:
            df = df_alt
except Exception:
    df = pd.read_csv(path, sep=';', decimal=',')

df.columns = [re.sub(r'\s+', ' ', str(c)).strip() for c in df.columns]
df = df.loc[:, ~df.columns.str.match(r'^Unnamed', case=False, na=False)]
df = df.loc[:, df.columns != '']
df.replace([np.inf, -np.inf], np.nan, inplace=True)
assert df.shape[0] > 0 and df.shape[1] > 0

candidates = ['label', 'target', 'y', 'class', 'spam', 'is_spam', 'category']
target_col = None
lower_cols = {col.lower(): col for col in df.columns}
for cand in candidates:
    if cand in lower_cols:
        target_col = lower_cols[cand]
        break
if target_col is None:
    for col in df.columns:
        low = col.lower()
        if any(cand in low for cand in candidates):
            target_col = col
            break
if target_col is None:
    numeric_candidate = None
    for col in df.columns:
        conv = pd.to_numeric(df[col], errors='coerce')
        if conv.notna().sum() == 0:
            continue
        unique = conv.nunique(dropna=True)
        if unique > 1:
            numeric_candidate = col
            break
        if numeric_candidate is None:
            numeric_candidate = col
    if numeric_candidate is not None:
        target_col = numeric_candidate
    else:
        target_col = df.columns[-1]

y_raw = df[target_col]
X_raw = df.drop(columns=[target_col])

y_numeric = pd.to_numeric(y_raw, errors='coerce')
numeric_ratio = y_numeric.notna().mean()
if np.isnan(numeric_ratio):
    numeric_ratio = 0.0
if numeric_ratio > 0.9:
    y_clean = y_numeric
    y_is_numeric = True
else:
    y_clean = y_raw.astype(str)
    y_is_numeric = False

if y_is_numeric:
    unique_count = y_clean.nunique(dropna=True)
    threshold = max(2, min(20, len(y_clean) // 2 if len(y_clean) > 0 else 2))
    if unique_count <= threshold:
        task = 'classification'
    else:
        task = 'regression'
else:
    task = 'classification'

if task == 'classification':
    if y_is_numeric:
        if y_clean.isna().any():
            mode = y_clean.dropna().mode()
            fill_val = mode.iloc[0] if not mode.empty else 0
            y_clean = y_clean.fillna(fill_val)
    else:
        y_clean = y_clean.fillna('missing').astype(str)
else:
    if y_clean.isna().any():
        med = y_clean.dropna().median() if y_clean.notna().sum() > 0 else 0.0
        y_clean = y_clean.fillna(med)

X_processed = X_raw.copy()
X_processed.replace([np.inf, -np.inf], np.nan, inplace=True)
numeric_cols = []
text_cols = []
for col in X_processed.columns:
    series = X_processed[col]
    if series.dtype.kind in 'biufc':
        X_processed[col] = pd.to_numeric(series, errors='coerce')
        numeric_cols.append(col)
    else:
        converted = pd.to_numeric(series, errors='coerce')
        non_na_ratio = converted.notna().mean()
        if np.isnan(non_na_ratio):
            non_na_ratio = 0.0
        if non_na_ratio > 0.9:
            X_processed[col] = converted
            numeric_cols.append(col)
        else:
            text_cols.append(col)

numeric_cols_filtered = []
for col in numeric_cols:
    if X_processed[col].notna().any():
        numeric_cols_filtered.append(col)
    else:
        X_processed = X_processed.drop(columns=[col])
numeric_cols = numeric_cols_filtered

text_col_name = None
has_text = False
if len(text_cols) > 0:
    text_data = X_processed[text_cols].fillna('').astype(str)
    combined_text = text_data.agg(' '.join, axis=1)
    text_col_name = '__combined_text__'
    while text_col_name in X_processed.columns:
        text_col_name = text_col_name + '_'
    X_processed[text_col_name] = combined_text
    X_processed = X_processed.drop(columns=text_cols)
    if X_processed[text_col_name].astype(str).str.strip().replace('', np.nan).notna().any():
        has_text = True
    else:
        X_processed = X_processed.drop(columns=[text_col_name])
        text_col_name = None
        has_text = False

numeric_cols = [col for col in X_processed.columns if col != text_col_name]

if X_processed.shape[1] == 0:
    const_name = '__constant__'
    while const_name in X_processed.columns:
        const_name = const_name + '_'
    X_processed[const_name] = 1.0
    numeric_cols = [const_name]

use_text = text_col_name is not None and has_text

if task == 'classification':
    n_classes = y_clean.nunique(dropna=False)
    if use_text:
        preprocess = ColumnTransformer([('text', TfidfVectorizer(max_features=2000, dtype=np.float32), text_col_name)], remainder='drop')
        model = MultinomialNB()
        X_final = X_processed[[text_col_name]]
    else:
        numeric_transformer = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])
        preprocess = ColumnTransformer([('num', numeric_transformer, numeric_cols)], remainder='drop')
        model = LogisticRegression(max_iter=200, solver='liblinear', random_state=42)
        X_final = X_processed[numeric_cols]
    if n_classes < 2:
        model = DummyClassifier(strategy='most_frequent')
else:
    if use_text:
        preprocess = ColumnTransformer([('text', TfidfVectorizer(max_features=2000, dtype=np.float32), text_col_name)], remainder='drop')
        model = Ridge(alpha=1.0, random_state=42)
        X_final = X_processed[[text_col_name]]
    else:
        numeric_transformer = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])
        preprocess = ColumnTransformer([('num', numeric_transformer, numeric_cols)], remainder='drop')
        model = Ridge(alpha=1.0, random_state=42)
        X_final = X_processed[numeric_cols]
    if y_clean.nunique(dropna=True) <= 1:
        model = DummyRegressor(strategy='mean')

n_samples = len(y_clean)
if n_samples < 2:
    X_train, X_test, y_train, y_test = X_final, X_final, y_clean, y_clean
else:
    stratify = None
    if task == 'classification' and y_clean.nunique() > 1:
        value_counts = y_clean.value_counts()
        if value_counts.min() >= 2:
            stratify = y_clean
    X_train, X_test, y_train, y_test = train_test_split(
        X_final, y_clean, test_size=0.2, random_state=42, stratify=stratify
    )
assert len(X_train) > 0 and len(X_test) > 0

pipeline = Pipeline([('preprocess', preprocess), ('model', model)])
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    try:
        r2 = r2_score(y_test, y_pred)
    except Exception:
        r2 = 0.0
    if np.isnan(r2):
        r2 = 0.0
    accuracy = max(0.0, min(1.0, 0.5 * (r2 + 1.0)))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight models (MultinomialNB for text, Logistic/Ridge for numeric) to stay CPU-friendly and energy-efficient.
# - Limited TF-IDF vocabulary size and combined text fields to reduce feature dimensionality and processing overhead.
# - Applied simple imputation/scaling in a single pipeline for reproducible, low-overhead preprocessing.
# - Added robust fallbacks (Dummy models/constant features) to keep the pipeline running under schema uncertainty.
# - Regression accuracy is a clipped linear transform of R2 into [0,1] for a stable accuracy-like proxy.