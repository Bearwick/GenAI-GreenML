# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

def robust_load(path):
    # Robust CSV parsing with fallbacks
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError
    except:
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except:
            # Fallback for very malformed files
            df = pd.read_csv(path, on_bad_lines='skip', engine='python')
    
    # Normalize column names: strip, single space, remove Unnamed
    df.columns = [" ".join(str(c).strip().split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed', case=False)]
    return df

def solve():
    dataset_path = 'email_analysis.csv'
    df = robust_load(dataset_path)

    if df.empty:
        return

    # Identify Target
    # Priority: 'label', 'spam', then any column with 'target' or 'class'
    target_col = None
    potential_targets = ['label', 'spam', 'target', 'class', 'classification']
    for pt in potential_targets:
        for col in df.columns:
            if pt in col.lower():
                target_col = col
                break
        if target_col: break
    
    if not target_col:
        # Fallback: find a column with 2 unique values, or the last column
        for col in df.columns:
            if 1 < df[col].nunique() <= 10:
                target_col = col
                break
        if not target_col:
            target_col = df.columns[-1]

    # Clean target
    df = df.dropna(subset=[target_col])
    y = df[target_col]
    
    # Identify features
    # Collect text columns for vectorization
    text_cols = [c for c in df.columns if c != target_col and df[c].dtype == 'object']
    # If specifically 'text', 'body', or 'subject' exist, prefer them
    preferred_text = [c for c in text_cols if any(x in c.lower() for x in ['text', 'body', 'subject'])]
    
    # Prepare X
    # We combine all available text info into one feature to minimize vectorizer overhead
    if preferred_text:
        X_raw = df[preferred_text].fillna('').astype(str).agg(' '.join, axis=1)
    elif text_cols:
        X_raw = df[text_cols].fillna('').astype(str).agg(' '.join, axis=1)
    else:
        # Fallback for numeric-only datasets
        X_raw = df.drop(columns=[target_col]).fillna(0)
        # Convert to string to pass through the text pipeline or handle as numeric
        # For simplicity in this lightweight baseline, we'll use the text path if possible
        # or a basic numeric path if no text exists.
        X_raw = X_raw.astype(str).agg(' '.join, axis=1)

    # Encode labels if they are categorical
    le = LabelEncoder()
    y_encoded = le.fit_transform(y.astype(str))
    
    # Check for trivial case (only 1 class)
    if len(np.unique(y_encoded)) < 2:
        print(f"ACCURACY={1.000000:.6f}")
        return

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X_raw, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded if len(np.unique(y_encoded)) > 1 else None
    )

    # Energy-efficient Pipeline:
    # 1. TfidfVectorizer: caps features at 1000 to save memory/CPU
    # 2. MultinomialNB: extremely fast, low-energy classifier for text
    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english', min_df=2)),
        ('clf', MultinomialNB())
    ])

    # Fit
    pipeline.fit(X_train, y_train)

    # Predict and Evaluate
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# Optimization Summary
# 1. Model Choice: Multinomial Naive Bayes was selected for its minimal computational complexity O(nd), 
#    making it ideal for CPU-only environments and energy-efficient inference.
# 2. Feature Engineering: TfidfVectorizer with a max_feature constraint of 1000 limits the dimensionality 
#    of the feature space, reducing memory footprint and power consumption during training.
# 3. Robustness: Implemented a flexible schema discovery mechanism to handle variations in CSV formatting 
#    and column naming, ensuring the pipeline runs end-to-end without manual intervention.
# 4. Efficiency: Text features are concatenated into a single stream before vectorization to reduce 
#    the number of independent transformations and dictionary lookups.
# 5. Resource Management: Avoided heavy ensembles (like Random Forests) or Deep Learning (Transformers) 
#    as they require significantly more FLOPs and energy for marginal gains on small-scale text tasks.