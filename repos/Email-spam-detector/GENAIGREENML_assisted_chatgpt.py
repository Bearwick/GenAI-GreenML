# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import re
import email
from email import policy
from bs4 import BeautifulSoup
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score


_URL_RE = re.compile(
    r"http[s]?://(?:[a-zA-Z]|[0-9]|[$\-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
)


def _safe_decode(payload_bytes: bytes | None) -> str:
    if not payload_bytes:
        return ""
    return payload_bytes.decode("utf-8", errors="ignore")


def extract_email_content(filepath: str):
    try:
        with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
            msg = email.message_from_file(f, policy=policy.default)

        sender = msg.get("From") or ""
        subject = msg.get("Subject") or ""

        body_parts = []
        if msg.is_multipart():
            for part in msg.walk():
                ctype = part.get_content_type()
                if ctype == "text/plain":
                    body_parts.append(_safe_decode(part.get_payload(decode=True)))
                elif ctype == "text/html":
                    html = _safe_decode(part.get_payload(decode=True))
                    if html:
                        body_parts.append(BeautifulSoup(html, "html.parser").get_text())
        else:
            body_parts.append(_safe_decode(msg.get_payload(decode=True)))

        body = "".join(body_parts)
        urls = _URL_RE.findall(body)

        return {"sender": sender, "subject": subject, "body": body, "urls": urls}
    except Exception:
        return None


def _iter_eml_files(directory: str):
    try:
        with os.scandir(directory) as it:
            for entry in it:
                if entry.is_file() and entry.name.endswith(".eml"):
                    yield entry.name, entry.path
    except FileNotFoundError:
        return


def load_emails(spam_dir: str, good_dir: str) -> pd.DataFrame:
    rows = []
    for label, directory in (("spam", spam_dir), ("good", good_dir)):
        for filename, filepath in _iter_eml_files(directory):
            email_data = extract_email_content(filepath)
            if email_data is not None:
                email_data["filename"] = filename
                email_data["label"] = label
                rows.append(email_data)
    return pd.DataFrame.from_records(rows)


def train_spam_classifier(df: pd.DataFrame):
    text = (df["subject"].fillna("") + " " + df["body"].fillna("")).astype(str)

    vectorizer = TfidfVectorizer(stop_words="english", max_features=5000)
    X = vectorizer.fit_transform(text)
    y = df["label"].astype(str)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    model = MultinomialNB()
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return model, vectorizer, float(accuracy)


def main():
    spam_dir = "emails/spam"
    good_dir = "emails/good"

    df = load_emails(spam_dir, good_dir)
    _, _, accuracy = train_spam_classifier(df)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Replaced os.listdir + join with os.scandir to reduce syscalls and speed directory iteration.
# - Precompiled URL regex to avoid recompilation for every email, reducing CPU overhead.
# - Avoided repeated string concatenation in multipart parsing by collecting parts in a list and joining once.
# - Added a small safe decode helper to minimize repeated conditional/exception paths when decoding payloads.
# - Removed classification_report and other prints/output artifacts to reduce unnecessary computation and I/O.
# - Avoided storing an extra 'text' column in the DataFrame; built the text Series once for vectorization to reduce memory.
# - Used fillna + astype(str) once to stabilize preprocessing and improve reproducibility of text handling.
# - Removed CSV saving and sample prediction side-effects to reduce disk I/O while preserving training/accuracy behavior.