# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

RANDOM_SEED = 42

def load_dataset(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            df = pd.read_csv(path, sep=';', decimal=',')
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    return df

def train_and_evaluate(df):
    text_col = 'text'
    label_col = 'label'

    if text_col not in df.columns:
        subject_col = [c for c in df.columns if c.strip().lower() == 'subject']
        body_col = [c for c in df.columns if c.strip().lower() == 'body']
        s = df[subject_col[0]].fillna('') if subject_col else ''
        b = df[body_col[0]].fillna('') if body_col else ''
        df[text_col] = s + ' ' + b
    else:
        df[text_col] = df[text_col].fillna('')

    if label_col not in df.columns:
        label_col = [c for c in df.columns if c.strip().lower() == 'label'][0]

    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000, dtype='float32')
    X = vectorizer.fit_transform(df[text_col])
    y = df[label_col]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_SEED
    )

    model = MultinomialNB()
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    return accuracy

if __name__ == "__main__":
    df = load_dataset("email_analysis.csv")
    accuracy = train_and_evaluate(df)
    print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Removed all file I/O for .eml parsing since the CSV dataset already exists with pre-extracted features.
# 2. Removed BeautifulSoup, email, os, and re imports that are no longer needed.
# 3. Removed all print statements, logging, and classification report generation.
# 4. Removed visualization and artifact saving (no CSV or model saving).
# 5. Used float32 dtype in TfidfVectorizer to reduce memory footprint of the sparse matrix.
# 6. Removed redundant sample prediction code that was not part of evaluation.
# 7. Added robust CSV fallback parsing with sep=';' and decimal=',' as required.
# 8. Set fixed random_state for reproducibility.
# 9. Removed interactive inputs and kept code modular with minimal intermediate structures.
# 10. Dynamically detect column names from actual dataframe rather than hardcoding.