# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer
import warnings
warnings.filterwarnings("ignore")

# Load dataset - try common names
import os
data_path = None
for candidate in ["dataset.csv", "data.csv", "train.csv", "spam.csv", "emails.csv"]:
    if os.path.exists(candidate):
        data_path = candidate
        break

if data_path is None:
    for root, dirs, files in os.walk("."):
        for f in files:
            if f.endswith(".csv"):
                data_path = os.path.join(root, f)
                break
        if data_path:
            break

df = pd.read_csv(data_path)

# Use 'text' column if available, otherwise combine text features for a richer representation
if "text" in df.columns and df["text"].notna().sum() > 0:
    df["combined_text"] = df["text"].fillna("")
else:
    text_cols = ["sender", "subject", "body", "urls", "filename"]
    available = [c for c in text_cols if c in df.columns]
    df["combined_text"] = df[available].fillna("").astype(str).agg(" ".join, axis=1)

# Ensure combined_text is string
df["combined_text"] = df["combined_text"].astype(str)

# Target column
target_col = "label"
df = df.dropna(subset=[target_col]).reset_index(drop=True)

X = df["combined_text"]
y = df[target_col]

# Encode labels if they are strings
if y.dtype == object:
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y = le.fit_transform(y)

# Train/test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Lightweight pipeline: TF-IDF (limited features) + Logistic Regression
# TF-IDF with sublinear_tf is energy-efficient and effective for text classification
# Logistic Regression is CPU-friendly, fast to train, and works well on small datasets
pipeline = Pipeline([
    ("tfidf", TfidfVectorizer(
        max_features=10000,       # Cap features for efficiency
        sublinear_tf=True,        # Dampens term frequency, improves performance
        min_df=2,                 # Remove very rare terms
        max_df=0.95,              # Remove very common terms
        ngram_range=(1, 2),       # Unigrams + bigrams for better context
        strip_accents="unicode",
        lowercase=True,
        dtype=np.float32          # Use float32 to save memory
    )),
    ("clf", LogisticRegression(
        solver="saga",            # Efficient for larger datasets
        max_iter=1000,
        C=1.0,
        random_state=42,
        n_jobs=-1                 # Use available cores
    ))
])

pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. TF-IDF with max_features=10000 limits vocabulary size for memory and compute efficiency.
# 2. float32 dtype in TF-IDF reduces memory footprint by ~50% vs float64.
# 3. sublinear_tf=True improves classification quality without extra computation.
# 4. Logistic Regression is chosen as a lightweight, interpretable, CPU-efficient classifier.
# 5.