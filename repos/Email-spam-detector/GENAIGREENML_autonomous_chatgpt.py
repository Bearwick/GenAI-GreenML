# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "email_analysis.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = "" if c is None else str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        normed.append(c2)
    return normed


def _read_csv_robust(path):
    # Try default CSV parsing; if schema looks wrong, retry with common EU settings.
    df1 = pd.read_csv(path)
    if df1.shape[1] <= 1:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        if df2.shape[1] > df1.shape[1]:
            return df2
    return df1


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if isinstance(c, str) and c.strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _choose_target(df):
    # Prefer columns that look like labels; else pick a non-constant numeric column.
    candidates = ["label", "target", "y", "class", "spam", "is_spam"]
    for c in candidates:
        if c in df.columns:
            s = df[c]
            if s.notna().sum() > 0 and s.nunique(dropna=True) >= 2:
                return c

    # Else choose any non-constant column with few unique values (classification-like), preferring object/bool/int
    best = None
    best_score = -1
    for c in df.columns:
        s = df[c]
        nunq = s.nunique(dropna=True)
        if nunq < 2:
            continue
        # score: prefer low cardinality, non-float types
        dtype = s.dtype
        score = 0
        if nunq <= 20:
            score += 3
        if dtype == "bool":
            score += 3
        if pd.api.types.is_integer_dtype(dtype):
            score += 2
        if pd.api.types.is_object_dtype(dtype) or pd.api.types.is_string_dtype(dtype):
            score += 2
        if score > best_score:
            best, best_score = c, score
    if best is not None:
        return best

    # Else pick a numeric non-constant (regression fallback)
    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    for c in num_cols:
        if df[c].nunique(dropna=True) >= 2:
            return c
    return None


def _infer_task(y):
    # Return ("classification"|"regression", y_processed)
    if y is None:
        return "regression", None

    y0 = y.copy()
    # If numeric with many unique values -> regression
    if pd.api.types.is_numeric_dtype(y0):
        nunq = y0.nunique(dropna=True)
        if nunq >= 20:
            return "regression", y0
        # small unique numeric => classification
        return "classification", y0

    # Non-numeric => classification if >=2 classes
    if y0.nunique(dropna=True) >= 2:
        return "classification", y0

    return "regression", y0


def _safe_accuracy_from_regression(y_true, y_pred):
    # Convert regression quality to bounded [0,1] "accuracy" proxy:
    # 1 / (1 + normalized MAE), normalized by (IQR + eps) for scale robustness.
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    mae = np.mean(np.abs(yt - yp))
    q75, q25 = np.percentile(yt, [75, 25])
    iqr = float(q75 - q25)
    denom = iqr if iqr > 1e-12 else (np.std(yt) if np.std(yt) > 1e-12 else 1.0)
    acc = 1.0 / (1.0 + (mae / (denom + 1e-12)))
    return float(np.clip(acc, 0.0, 1.0))


def main():
    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Ensure non-empty
    df = df.dropna(axis=0, how="all")
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _choose_target(df)
    if target_col is None:
        # Trivial path: no usable target; define constant "accuracy" as 0
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # Normalize object columns that might contain only whitespace
    for c in X.columns:
        if pd.api.types.is_object_dtype(X[c]) or pd.api.types.is_string_dtype(X[c]):
            X[c] = X[c].astype("string")

    task, y = _infer_task(y_raw)

    # Identify column types robustly
    # Coerce numerics safely: attempt conversion for non-text features where possible.
    # Keep original text-like columns for TF-IDF.
    preferred_text_cols = [c for c in X.columns if c.lower() in {"subject", "body", "text", "sender", "urls", "filename"}]
    text_cols = []
    for c in preferred_text_cols:
        if c in X.columns and (pd.api.types.is_object_dtype(X[c]) or pd.api.types.is_string_dtype(X[c])):
            text_cols.append(c)

    # Add any other object columns as categorical (excluding chosen text cols)
    cat_cols = []
    for c in X.columns:
        if c in text_cols:
            continue
        if pd.api.types.is_object_dtype(X[c]) or pd.api.types.is_string_dtype(X[c]) or pd.api.types.is_bool_dtype(X[c]):
            cat_cols.append(c)

    # Numeric columns: attempt coercion
    num_cols = []
    for c in X.columns:
        if c in text_cols or c in cat_cols:
            continue
        s = X[c]
        if pd.api.types.is_numeric_dtype(s):
            num_cols.append(c)
        else:
            # Try to coerce to numeric; if it yields enough finite values, treat as numeric.
            coerced = pd.to_numeric(s, errors="coerce")
            finite_ratio = np.isfinite(coerced.to_numpy(dtype=float, na_value=np.nan)).mean()
            if finite_ratio >= 0.5:
                X[c] = coerced
                num_cols.append(c)
            else:
                cat_cols.append(c)

    # Build preprocessing: TF-IDF for combined text, OneHot for categoricals, scaling for numeric.
    # Combine text columns into one lightweight TF-IDF space by concatenation via a FunctionTransformer-like approach.
    def _join_text_frame(frame):
        if frame.shape[1] == 0:
            return np.array([""] * len(frame), dtype=object)
        # Ensure string and fill missing
        joined = None
        for col in frame.columns:
            s = frame[col].astype("string").fillna("")
            joined = s if joined is None else (joined + " " + s)
        return joined.to_numpy(dtype=object)

    from sklearn.preprocessing import FunctionTransformer

    transformers = []
    if text_cols:
        text_pipe = Pipeline(steps=[
            ("select_join", FunctionTransformer(lambda d: _join_text_frame(pd.DataFrame(d, columns=text_cols)), validate=False)),
            ("tfidf", TfidfVectorizer(
                lowercase=True,
                strip_accents="unicode",
                stop_words=None,
                max_features=5000,
                ngram_range=(1, 2),
                min_df=2,
                dtype=np.float32
            )),
        ])
        transformers.append(("text", text_pipe, text_cols))

    if cat_cols:
        cat_pipe = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ])
        transformers.append(("cat", cat_pipe, cat_cols))

    if num_cols:
        num_pipe = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False)),  # keep sparse-friendly behavior
        ])
        transformers.append(("num", num_pipe, num_cols))

    if not transformers:
        # No usable features -> trivial baseline
        if task == "classification":
            # Majority class accuracy
            y_series = y_raw.dropna()
            if y_series.nunique() < 2:
                accuracy = 0.0
            else:
                accuracy = float((y_raw == y_raw.mode(dropna=True).iloc[0]).mean())
        else:
            accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    preprocessor = ColumnTransformer(
        transformers=transformers,
        remainder="drop",
        sparse_threshold=0.3
    )

    # Prepare y for modeling
    if task == "classification":
        y_series = y_raw.copy()
        # Map common spam labels to 0/1 if possible (but do not assume)
        if pd.api.types.is_object_dtype(y_series) or pd.api.types.is_string_dtype(y_series):
            y_str = y_series.astype("string").str.strip().str.lower()
            mapping = {
                "spam": 1, "ham": 0, "good": 0, "legit": 0, "legitimate": 0,
                "not spam": 0, "not_spam": 0, "phishing": 1
            }
            y_mapped = y_str.map(mapping)
            # Use mapped if it yields at least 2 classes; else keep original
            if y_mapped.notna().sum() > 0 and y_mapped.nunique(dropna=True) >= 2:
                y_model = y_mapped
            else:
                y_model = y_series
        else:
            y_model = y_series
    else:
        y_model = pd.to_numeric(y_raw, errors="coerce")

    # Drop rows with missing target
    mask = y_model.notna()
    X = X.loc[mask].reset_index(drop=True)
    y_model = y_model.loc[mask].reset_index(drop=True)

    assert len(X) > 1 and len(y_model) > 1

    # If classification has <2 classes after cleaning, fallback to regression proxy
    if task == "classification" and pd.Series(y_model).nunique(dropna=True) < 2:
        task = "regression"
        y_model = pd.to_numeric(y_raw.loc[mask].reset_index(drop=True), errors="coerce")
        mask2 = y_model.notna()
        X = X.loc[mask2].reset_index(drop=True)
        y_model = y_model.loc[mask2].reset_index(drop=True)

    assert len(X) > 1 and len(y_model) > 1

    stratify = y_model if (task == "classification" and pd.Series(y_model).nunique(dropna=True) >= 2) else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_model, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
    )
    assert len(X_train) > 0 and len(X_test) > 0

    if task == "classification":
        # Lightweight linear classifier, good baseline for TF-IDF sparse features.
        clf = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            C=1.0
        )
        model = Pipeline(steps=[("prep", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        model = Pipeline(steps=[("prep", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _safe_accuracy_from_regression(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses CPU-friendly, sparse linear models (LogisticRegression/Ridge) that scale well on TF-IDF and one-hot features.
# - TF-IDF is capped (max_features=5000, min_df=2, float32) to reduce memory/compute while preserving signal.
# - Single ColumnTransformer+Pipeline ensures reproducible, one-pass preprocessing without redundant copies.
# - Robust CSV reading fallback (default then sep=';' and decimal=',') and defensive schema handling avoids hard failures.
# - Regression fallback uses a bounded [0,1] proxy accuracy = 1/(1+normalized MAE) for stable reporting when classification isn't viable.