# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.preprocessing import FunctionTransformer
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score


def _find_dataset_path() -> str:
    candidates = []
    for fn in os.listdir("."):
        low = fn.lower()
        if low.endswith(".csv"):
            candidates.append(fn)
    preferred = []
    for fn in candidates:
        low = fn.lower()
        if any(k in low for k in ["spam", "phish", "email", "mail", "message", "messages", "dataset", "data", "train"]):
            preferred.append(fn)
    for fn in preferred + candidates:
        try:
            df_head = pd.read_csv(fn, nrows=5)
        except Exception:
            continue
        cols = {c.strip().lower() for c in df_head.columns}
        if "label" in cols and any(c in cols for c in ["text", "body", "subject", "sender", "urls", "filename"]):
            return fn
    raise FileNotFoundError("No suitable CSV dataset found in current directory.")


def _clean_text_series(x: pd.Series) -> pd.Series:
    x = x.fillna("").astype(str)
    x = x.str.lower()
    x = x.str.replace(r"\s+", " ", regex=True)
    x = x.str.replace(r"[^\w\s:/\.-]", " ", regex=True)
    x = x.str.replace(r"\s+", " ", regex=True).str.strip()
    return x


def _combine_text_columns(df: pd.DataFrame) -> pd.Series:
    cols = [c for c in ["sender", "subject", "body", "urls", "filename", "text"] if c in df.columns]
    if not cols:
        return pd.Series([""] * len(df), index=df.index)
    parts = []
    for c in cols:
        parts.append(_clean_text_series(df[c]))
    out = parts[0]
    for p in parts[1:]:
        out = out + " " + p
    return out


def _normalize_labels(y: pd.Series) -> np.ndarray:
    y = y.fillna("").astype(str).str.strip().str.lower()
    def to_bin(v: str) -> int:
        if v in {"1", "true", "yes", "y", "spam", "phish", "phishing", "malicious"}:
            return 1
        if v in {"0", "false", "no", "n", "ham", "legit", "legitimate", "benign"}:
            return 0
        try:
            f = float(v)
            return 1 if f >= 0.5 else 0
        except Exception:
            return 1 if "spam" in v or "phish" in v or "malicious" in v else 0
    return y.map(to_bin).to_numpy(dtype=np.int64)


def main():
    dataset_path = _find_dataset_path()
    df = pd.read_csv(dataset_path)

    df.columns = [c.strip() for c in df.columns]
    if "label" not in df.columns:
        raise KeyError("Dataset must contain a 'label' column.")

    X = df.drop(columns=["label"])
    y = _normalize_labels(df["label"])

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) > 1 else None
    )

    def featurize_frame(frame: pd.DataFrame) -> pd.Series:
        return _combine_text_columns(frame)

    text_extractor = FunctionTransformer(featurize_frame, validate=False)

    model = SGDClassifier(
        loss="log_loss",
        penalty="l2",
        alpha=1e-5,
        max_iter=1000,
        tol=1e-3,
        random_state=42,
    )

    pipeline = Pipeline(
        steps=[
            ("extract_text", text_extractor),
            (
                "vectorize",
                HashingVectorizer(
                    n_features=2**18,
                    alternate_sign=False,
                    norm="l2",
                    ngram_range=(1, 2),
                    token_pattern=r"(?u)\b\w+\b",
                ),
            ),
            ("clf", model),
        ]
    )

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()


# OPTIMIZATION SUMMARY
# - Uses HashingVectorizer to avoid building/storing a vocabulary (lower memory, faster startup, stream-friendly).
# - Keeps feature dimensionality moderate (2**18) and uses simple word+bigram features for strong baseline accuracy.
# - Chooses SGDClassifier (linear model) for efficient CPU training/inference; avoids deep learning and embeddings.
# - Preprocessing is deterministic and lightweight: lowercase, whitespace normalization, minimal character filtering.
# - Combines available text columns (sender/subject/body/urls/filename/text) to maximize signal without extra models.
# - No model saving, no plots, no interactive inputs; reproducible split with fixed random_state.