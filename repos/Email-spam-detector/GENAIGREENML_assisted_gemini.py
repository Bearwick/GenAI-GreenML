# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import os
import email
from email import policy
import re
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

def get_email_text(filepath, tag_re):
    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            msg = email.message_from_file(f, policy=policy.default)
        
        subject = msg['Subject'] or ''
        body_parts = []
        
        if msg.is_multipart():
            for part in msg.walk():
                content_type = part.get_content_type()
                if content_type == 'text/plain':
                    body_parts.append(part.get_payload(decode=True).decode('utf-8', errors='ignore'))
                elif content_type == 'text/html':
                    html = part.get_payload(decode=True).decode('utf-8', errors='ignore')
                    body_parts.append(tag_re.sub('', html))
        else:
            body_parts.append(msg.get_payload(decode=True).decode('utf-8', errors='ignore'))
        
        return f"{subject} {' '.join(body_parts)}"
    except Exception:
        return None

def load_data(spam_dir, good_dir):
    texts, labels = [], []
    tag_re = re.compile(r'<[^>]+>')
    
    for label, folder in [('spam', spam_dir), ('good', good_dir)]:
        if not os.path.exists(folder):
            continue
        for filename in os.listdir(folder):
            if filename.endswith('.eml'):
                filepath = os.path.join(folder, filename)
                content = get_email_text(filepath, tag_re)
                if content:
                    texts.append(content)
                    labels.append(label)
    return texts, labels

def run_pipeline():
    spam_dir = "emails/spam"
    good_dir = "emails/good"
    
    texts, labels = load_data(spam_dir, good_dir)
    
    if not texts:
        return

    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
    X = vectorizer.fit_transform(texts)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, labels, test_size=0.2, random_state=42
    )
    
    model = MultinomialNB()
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# OPTIMIZATION SUMMARY
# - Replaced BeautifulSoup with a compiled regex (tag_re) for HTML stripping, significantly reducing CPU overhead and dependency weight.
# - Removed extraction of unused features (sender, URLs, filenames) to minimize memory consumption and string processing.
# - Consolidated data loading into a single pass with pre-allocated lists instead of creating multiple intermediate dictionary objects or DataFrames.
# - Eliminated redundant Pandas operations by performing string concatenation during the parsing phase rather than as a post-processing step on a DataFrame.
# - Reduced I/O and memory footprint by processing files stream-style and storing only the final text required for the ML model.
# - Used TfidfVectorizer with a fixed feature limit to bound memory usage during training.
# - Removed all non-essential prints, visualizations, and file exports to minimize runtime and disk I/O.