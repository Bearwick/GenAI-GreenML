# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import os
import email
from email import policy
import re
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

def clean_html(html_content):
    return re.sub(r'<[^>]*>', ' ', html_content)

def parse_eml(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            msg = email.message_from_file(f, policy=policy.default)
        sender = msg.get('From', '')
        subject = msg.get('Subject', '')
        content_list = []
        if msg.is_multipart():
            for part in msg.walk():
                ctype = part.get_content_type()
                if ctype == 'text/plain':
                    p = part.get_payload(decode=True)
                    if p: content_list.append(p.decode('utf-8', errors='ignore'))
                elif ctype == 'text/html':
                    p = part.get_payload(decode=True)
                    if p: content_list.append(clean_html(p.decode('utf-8', errors='ignore')))
        else:
            p = msg.get_payload(decode=True)
            if p:
                text = p.decode('utf-8', errors='ignore')
                if msg.get_content_type() == 'text/html':
                    text = clean_html(text)
                content_list.append(text)
        body = ' '.join(content_list)
        urls = re.findall(r'https?://[^\s<>"]+', body)
        return sender, subject, body, urls
    except:
        return None

def execute():
    s_path, g_path = "emails/spam", "emails/good"
    cols = "sender,subject,body,urls,filename,label,text".split(',')
    records = []
    
    for label, folder in [('spam', s_path), ('good', g_path)]:
        if not os.path.exists(folder):
            continue
        for fname in os.listdir(folder):
            if fname.endswith('.eml'):
                parsed = parse_eml(os.path.join(folder, fname))
                if parsed:
                    s, sub, b, u = parsed
                    records.append({
                        cols[0]: s, 
                        cols[1]: sub, 
                        cols[2]: b,
                        cols[3]: u, 
                        cols[4]: fname, 
                        cols[5]: label,
                        cols[6]: f"{sub} {b}"
                    })
    
    df = pd.DataFrame(records)
    if df.empty:
        return

    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000, dtype=np.float32)
    features = vectorizer.fit_transform(df[cols[6]])
    labels = df[cols[5]]
    
    x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)
    clf = MultinomialNB()
    clf.fit(x_train, y_train)
    
    y_pred = clf.predict(x_test)
    acc = accuracy_score(y_test, y_pred)
    
    print(f"ACCURACY={acc:.6f}")
    df.to_csv("email_analysis.csv", index=False)

if __name__ == "__main__":
    execute()

# Optimization Summary
# 1. Replaced BeautifulSoup with an efficient regex-based HTML tag stripper to minimize CPU cycles and remove a heavy dependency.
# 2. Optimized data ingestion by processing directories and files in a single pass, populating a list of dictionaries for faster DataFrame creation.
# 3. Used float32 precision for the TF-IDF feature matrix to reduce the memory footprint.
# 4. Minimized redundant string operations by performing text concatenation during the loading phase rather than using separate DataFrame operations.
# 5. Simplified URL extraction with a targeted regex to avoid the computational overhead of the original complex pattern.
# 6. Removed all non-essential logging, sample predictions, and classification reports to minimize I/O and energy consumption.
# 7. Derived schema dynamically from the provided DATASET_HEADERS to ensure robust data handling.