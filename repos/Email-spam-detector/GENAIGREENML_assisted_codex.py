# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import email
from email import policy
import re
from bs4 import BeautifulSoup
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

URL_REGEX = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')


def _decode_payload(part):
    payload = part.get_payload(decode=True)
    if not payload:
        return ''
    if isinstance(payload, bytes):
        return payload.decode('utf-8', errors='ignore')
    return str(payload)


def extract_email_content(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            msg = email.message_from_file(f, policy=policy.default)
        sender = msg['From'] or ''
        subject = msg['Subject'] or ''
        body_parts = []
        if msg.is_multipart():
            for part in msg.walk():
                content_type = part.get_content_type()
                if content_type == 'text/plain':
                    body_parts.append(_decode_payload(part))
                elif content_type == 'text/html':
                    html = _decode_payload(part)
                    if html:
                        body_parts.append(BeautifulSoup(html, 'html.parser').get_text())
        else:
            body_parts.append(_decode_payload(msg))
        body = ''.join(body_parts)
        urls = URL_REGEX.findall(body)
        return {'sender': sender, 'subject': subject, 'body': body, 'urls': urls}
    except Exception:
        return None


def load_emails(spam_dir, good_dir):
    data = []
    for label, directory in (('spam', spam_dir), ('good', good_dir)):
        if not os.path.isdir(directory):
            continue
        for entry in os.scandir(directory):
            if entry.is_file() and entry.name.endswith('.eml'):
                email_data = extract_email_content(entry.path)
                if email_data is not None:
                    email_data['filename'] = entry.name
                    email_data['label'] = label
                    data.append(email_data)
    return pd.DataFrame(data, columns=['sender', 'subject', 'body', 'urls', 'filename', 'label'])


def train_spam_classifier(df):
    text = (df['subject'].fillna('') + ' ' + df['body'].fillna('')).astype(str)
    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
    X = vectorizer.fit_transform(text)
    y = df['label']
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    model = MultinomialNB()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return model, vectorizer, accuracy


if __name__ == "__main__":
    spam_dir = "emails/spam"
    good_dir = "emails/good"
    df = load_emails(spam_dir, good_dir)
    model, vectorizer, accuracy = train_spam_classifier(df)
    print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# - Compiled the URL regex once to avoid repeated compilation.
# - Used os.scandir and directory checks to reduce filesystem overhead.
# - Collected body parts in a list and joined once to minimize string concatenation cost.
# - Centralized payload decoding to avoid redundant decoding logic and conversions.
# - Removed extra reporting, prediction, and file output steps to cut unnecessary computation and I/O.