# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split

def confmatrix_of_random_forest(data_path, criterion="entropy"):
    try:
        df = pd.read_csv(data_path)
        if df.shape[1] <= 1:
            raise ValueError
    except (ValueError, pd.errors.ParserError):
        df = pd.read_csv(data_path, sep=';', decimal=',')

    drop_cols = ['parts {1=akoustiko,2=optiko,3=mousiki}', 'Subject ID']
    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)

    if 'class' in df.columns:
        if df['class'].dtype == object:
            df['class'] = df['class'].replace({'CN': 0, 'DYS': 1})
    
    df.dropna(axis=1, inplace=True)

    target_col = 'class' if 'class' in df.columns else df.columns[-1]
    
    y = df[target_col].astype(np.int32, copy=False)
    X = df.drop(columns=[target_col]).astype(np.float32, copy=False)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    total_conf = np.zeros((2, 2))
    
    for i in range(5):
        model = RandomForestClassifier(criterion=criterion, max_depth=8, random_state=42 + i)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        total_conf += confusion_matrix(y_test, y_pred, labels=[0, 1])

    mean_conf = total_conf / 5.0
    accuracy = (mean_conf[0, 0] + mean_conf[1, 1]) / np.sum(mean_conf)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == '__main__':
    confmatrix_of_random_forest("trainingfinal.csv")

# Optimization Summary:
# 1. Reduced redundant I/O by eliminating intermediate CSV file writes (to_csv).
# 2. Optimized memory footprint by using float32 for feature data and int32 for labels.
# 3. Enhanced computational efficiency by using inplace=True for dataframe modifications.
# 4. Improved robustness with a fallback CSV parsing mechanism for different delimiters.
# 5. Guaranteed reproducibility by implementing fixed random seeds for the model iterations.
# 6. Streamlined preprocessing by removing redundant loops and combining column filtering steps.
# 7. Reduced energy consumption by avoiding unnecessary data duplication during type casting.