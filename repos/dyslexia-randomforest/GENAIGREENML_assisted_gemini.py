# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix

def confmatrix_of_RandomForest(trainingdata_filepath, testdata_filepath, criterion="entropy"):
    df_train = pd.read_csv(trainingdata_filepath)
    df_test = pd.read_csv(testdata_filepath)

    unnecessary_cols = ['parts {1=akoustiko,2=optiko,3=mousiki}', 'Subject ID']
    df_train.drop(columns=unnecessary_cols, inplace=True, errors='ignore')
    df_test.drop(columns=unnecessary_cols, inplace=True, errors='ignore')

    label_map = {'CN': 0, 'DYS': 1}
    df_train.replace(label_map, inplace=True)
    df_test.replace(label_map, inplace=True)

    nan_cols = df_train.columns[df_train.isna().any()].union(df_test.columns[df_test.isna().any()])
    if not nan_cols.empty:
        df_train.drop(columns=nan_cols, inplace=True)
        df_test.drop(columns=nan_cols, inplace=True)

    df_train = df_train.astype(np.float32, copy=False)
    df_train['class'] = df_train['class'].astype(np.int32, copy=False)
    df_test = df_test.astype(np.float32, copy=False)
    df_test['class'] = df_test['class'].astype(np.int32, copy=False)

    y_train = df_train['class']
    x_train = df_train.drop(columns=['class'])
    y_test = df_test['class']
    x_test = df_test.drop(columns=['class'])

    results = np.zeros((5, 4))
    for i in range(5):
        model = RandomForestClassifier(criterion=criterion, max_depth=8, n_jobs=-1, random_state=42 + i)
        model.fit(x_train, y_train)
        y_pred = model.predict(x_test)
        results[i] = confusion_matrix(y_test, y_pred, labels=[0, 1]).flatten()

    mean_conf_1d = np.mean(results, axis=0)
    return mean_conf_1d

if __name__ == '__main__':
    conf = confmatrix_of_RandomForest("entire brain_training_2.csv", "entire brain_test_2.csv")
    accuracy = (conf[0] + conf[3]) / conf.sum()
    print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Utilized 'n_jobs=-1' in RandomForestClassifier to parallelize training, reducing runtime and energy consumption.
# 2. Replaced redundant 'replace' calls with a single dictionary mapping to minimize dataset passes.
# 3. Streamlined NaN column identification using Index union operations for faster preprocessing.
# 4. Eliminated redundant disk I/O by removing intermediate CSV saving operations.
# 5. Applied 'inplace=True' and optimized slicing to minimize memory footprint and avoid unnecessary data duplication.
# 6. Downcasted numerical features to 'float32' to reduce memory usage and speed up computations.
# 7. Fixed random seeds within the loop to ensure reproducibility and stable performance metrics.
# 8. Removed all visualization, print statements, and unused variables to minimize computational overhead.