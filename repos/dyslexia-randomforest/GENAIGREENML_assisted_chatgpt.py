# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix

SEED = 42


def _set_reproducible_seed(seed: int = SEED) -> None:
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)


def _robust_read_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _clean_and_align(train_df: pd.DataFrame, test_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:
    drop_candidates = ["parts {1=akoustiko,2=optiko,3=mousiki}", "Subject ID"]
    to_drop_train = [c for c in drop_candidates if c in train_df.columns]
    to_drop_test = [c for c in drop_candidates if c in test_df.columns]
    if to_drop_train:
        train_df = train_df.drop(columns=to_drop_train)
    if to_drop_test:
        test_df = test_df.drop(columns=to_drop_test)

    train_df = train_df.replace({"CN": 0, "DYS": 1})
    test_df = test_df.replace({"CN": 0, "DYS": 1})

    train_na_cols = set(train_df.columns[train_df.isna().any()].tolist())
    test_na_cols = set(test_df.columns[test_df.isna().any()].tolist())
    exclude_cols = list(train_na_cols | test_na_cols)

    if exclude_cols:
        exclude_cols_train = [c for c in exclude_cols if c in train_df.columns]
        exclude_cols_test = [c for c in exclude_cols if c in test_df.columns]
        if exclude_cols_train:
            train_df = train_df.drop(columns=exclude_cols_train)
        if exclude_cols_test:
            test_df = test_df.drop(columns=exclude_cols_test)

    common_cols = [c for c in train_df.columns if c in test_df.columns]
    train_df = train_df.loc[:, common_cols]
    test_df = test_df.loc[:, common_cols]

    if "class" in train_df.columns:
        feature_cols = [c for c in train_df.columns if c != "class"]
        dtype_map_train = {c: np.float32 for c in feature_cols}
        dtype_map_train["class"] = np.int32
        train_df = train_df.astype(dtype_map_train, copy=False)

    if "class" in test_df.columns:
        feature_cols = [c for c in test_df.columns if c != "class"]
        dtype_map_test = {c: np.float32 for c in feature_cols}
        dtype_map_test["class"] = np.int32
        test_df = test_df.astype(dtype_map_test, copy=False)

    return train_df, test_df


def confmatrix_of_RandomForest(trainingdata_filepath: str, testdata_filepath: str, criterion: str = "entropy") -> np.ndarray:
    _set_reproducible_seed(SEED)

    training_data = _robust_read_csv(trainingdata_filepath)
    test_data = _robust_read_csv(testdata_filepath)

    training_data, test_data = _clean_and_align(training_data, test_data)

    y_training = training_data["class"].to_numpy(copy=False)
    x_training = training_data.drop(columns=["class"]).to_numpy(copy=False)

    y_test = test_data["class"].to_numpy(copy=False)
    x_test = test_data.drop(columns=["class"]).to_numpy(copy=False)

    confs = np.empty((5, 4), dtype=np.float64)

    for i in range(5):
        model = RandomForestClassifier(
            criterion=criterion,
            max_depth=8,
            random_state=SEED + i,
            n_jobs=1,
        )
        model.fit(x_training, y_training)
        y_pred = model.predict(x_test)
        conf = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()
        confs[i] = conf

    conf_mean = confs.mean(axis=0)
    return conf_mean


if __name__ == "__main__":
    conf = confmatrix_of_RandomForest("entire brain_training_2.csv", "entire brain_test_2.csv")
    accuracy = (conf[0] + conf[3]) / conf.sum() if conf.sum() else 0.0
    print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Removed CSV saving side effects to avoid unnecessary I/O and energy use.
# - Added robust CSV parsing with fallback delimiter/decimal to prevent costly downstream errors/retries.
# - Dropped optional columns only if present and aligned train/test to common columns to avoid extra copies and schema mismatch.
# - Replaced multiple replace() calls with a single mapping to reduce redundant passes over data.
# - Computed NA-based excluded columns once and dropped in bulk to minimize repeated dataframe scans.
# - Cast feature columns to float32 and labels to int32 using a single dtype map to reduce memory footprint and conversions.
# - Avoided creating intermediate DataFrames for modeling by converting to NumPy arrays once (less overhead/data movement).
# - Preallocated the confusion-matrix accumulator and used mean(axis=0) directly to avoid redundant array construction.
# - Ensured reproducibility with fixed seeds and explicit random_state per run while keeping the 5-run averaging behavior.