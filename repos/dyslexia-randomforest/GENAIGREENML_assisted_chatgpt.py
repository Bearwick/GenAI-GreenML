# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix


SEED = 42


def _robust_read_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    drop_candidates = ["parts {1=akoustiko,2=optiko,3=mousiki}", "Subject ID"]
    to_drop = [c for c in drop_candidates if c in df.columns]
    if to_drop:
        df = df.drop(columns=to_drop)

    if df.select_dtypes(include=["object"]).shape[1]:
        df = df.replace({"CN": 0, "DYS": 1})

    nan_cols = df.columns[df.isna().any()]
    if len(nan_cols):
        df = df.drop(columns=nan_cols)

    if "class" not in df.columns:
        raise ValueError("Required target column 'class' not found in dataset.")

    feature_cols = [c for c in df.columns if c != "class"]
    df[feature_cols] = df[feature_cols].astype(np.float32, copy=False)
    df["class"] = df["class"].astype(np.int32, copy=False)
    return df


def _infer_test_path(training_path: str) -> str:
    base, ext = os.path.splitext(training_path)
    candidates = [
        base.replace("training", "test") + ext,
        base.replace("Training", "Test") + ext,
        base.replace("train", "test") + ext,
        "testfinal.csv",
        "entire brain_test_2.csv",
    ]
    for p in candidates:
        if p and os.path.exists(p):
            return p
    return ""


def confmatrix_of_RandomForest(trainingdata_filepath: str, testdata_filepath: str, criterion: str = "entropy") -> np.ndarray:
    training_data = _clean_dataframe(_robust_read_csv(trainingdata_filepath))
    test_data = _clean_dataframe(_robust_read_csv(testdata_filepath))

    train_cols = set(training_data.columns)
    test_cols = set(test_data.columns)
    common_cols = list((train_cols & test_cols))
    if "class" not in common_cols:
        raise ValueError("Train/test datasets do not share the required 'class' column.")
    common_cols.sort(key=lambda c: (c != "class", c))
    training_data = training_data.loc[:, common_cols]
    test_data = test_data.loc[:, common_cols]

    y_training = training_data["class"].to_numpy(copy=False)
    x_training = training_data.drop(columns=["class"]).to_numpy(copy=False)

    y_test = test_data["class"].to_numpy(copy=False)
    x_test = test_data.drop(columns=["class"]).to_numpy(copy=False)

    conf_accum = np.zeros(4, dtype=np.float64)

    for i in range(5):
        model = RandomForestClassifier(
            criterion=criterion,
            max_depth=8,
            random_state=SEED + i,
            n_jobs=1,
        )
        model.fit(x_training, y_training)
        y_pred = model.predict(x_test)
        conf_accum += confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()

    return conf_accum / 5.0


def _accuracy_from_conf(conf_1d: np.ndarray) -> float:
    conf_1d = np.asarray(conf_1d, dtype=np.float64).ravel()
    if conf_1d.size != 4:
        return float("nan")
    tn, fp, fn, tp = conf_1d
    denom = tn + fp + fn + tp
    return float((tn + tp) / denom) if denom else float("nan")


def main() -> None:
    training_path = "trainingfinal.csv" if os.path.exists("trainingfinal.csv") else "entire brain_training_2.csv"
    test_path = _infer_test_path(training_path)
    if not test_path:
        test_path = "entire brain_test_2.csv"

    conf = confmatrix_of_RandomForest(training_path, test_path)
    accuracy = _accuracy_from_conf(conf)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed disk writes (to_csv) to eliminate unnecessary I/O and energy use while preserving computed outputs.
# - Dropped columns only if present to avoid exceptions and extra work; avoids redundant operations across datasets.
# - Consolidated label replacement into a single mapping and only applied when object columns exist to reduce scanning.
# - Performed NaN-column detection once per dataframe and dropped in one operation to reduce intermediate structures.
# - Cast features to float32 and target to int32 in-place where possible to shrink memory footprint and speed training.
# - Converted pandas frames to NumPy arrays (copy=False) before model fitting to reduce pandas overhead/data movement.
# - Accumulated confusion matrices directly (no 5x4 array creation) to reduce memory and intermediate allocations.
# - Added fixed seeds (random_state) for reproducible results; kept n_jobs=1 for deterministic behavior.
# - Implemented robust CSV parsing fallback (default, then sep=';' and decimal=',') to avoid misparsed data retries.
# - Removed interactive inputs, plots, and all original prints; kept only the required final accuracy print.