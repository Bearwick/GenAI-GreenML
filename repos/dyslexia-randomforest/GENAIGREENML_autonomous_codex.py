# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier

warnings.filterwarnings("ignore")

DATASET_PATH = "trainingfinal.csv"
DATASET_HEADERS = "AF3_Shannon,AF3_Energy0_4,AF3_Energy4_8,AF3_Energy8_10,AF3_Energy10_12,AF3_Energy13_19,AF3_Energy20_30,AF3_Energy30_60,F3_Shannon,F3_Energy0_4,F3_Energy4_8,F3_Energy8_10,F3_Energy10_12,F3_Energy13_19,F3_Energy20_30,F3_Energy30_60,F7_Energy0_4,F7_Energy4_8,F7_Energy8_10,F7_Energy10_12,F7_Energy13_19,F7_Energy20_30,F7_Energy30_60,FC5_Shannon,FC5_Energy0_4,FC5_Energy4_8,FC5_Energy8_10,FC5_Energy10_12,FC5_Energy13_19,FC5_Energy20_30,FC5_Energy30_60,O1_Shannon,O1_Energy0_4,O1_Energy4_8,O1_Energy8_10,O1_Energy10_12,O1_Energy13_19,O1_Energy20_30,O1_Energy30_60,P7_Shannon,P7_Energy0_4,P7_Energy4_8,P7_Energy8_10,P7_Energy10_12,P7_Energy13_19,P7_Energy20_30,P7_Energy30_60,T7_Shannon,T7_Energy0_4,T7_Energy4_8,T7_Energy8_10,T7_Energy10_12,T7_Energy13_19,T7_Energy20_30,T7_Energy30_60,AF4_Shannon,AF4_Energy0_4,AF4_Energy4_8,AF4_Energy8_10,AF4_Energy10_12,AF4_Energy13_19,AF4_Energy20_30,AF4_Energy30_60,F4_Shannon,F4_Energy0_4,F4_Energy4_8,F4_Energy8_10,F4_Energy10_12,F4_Energy13_19,F4_Energy20_30,F4_Energy30_60,F8_Shannon,F8_Energy0_4,F8_Energy4_8,F8_Energy8_10,F8_Energy10_12,F8_Energy13_19,F8_Energy20_30,F8_Energy30_60,FC6_Shannon,FC6_Energy0_4,FC6_Energy4_8,FC6_Energy8_10,FC6_Energy10_12,FC6_Energy13_19,FC6_Energy20_30,FC6_Energy30_60,O2_Shannon,O2_Energy0_4,O2_Energy4_8,O2_Energy8_10,O2_Energy10_12,O2_Energy13_19,O2_Energy20_30,O2_Energy30_60,T8_Shannon,T8_Energy0_4,T8_Energy4_8,T8_Energy8_10,T8_Energy10_12,T8_Energy13_19,T8_Energy20_30,T8_Energy30_60,P8_Shannon,P8_Energy0_4,P8_Energy4_8,P8_Energy8_10,P8_Energy10_12,P8_Energy13_19,P8_Energy20_30,P8_Energy30_60,class"

def read_csv_with_fallback(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
        return df
    if df.shape[1] == 1:
        try:
            df2 = pd.read_csv(path, sep=';', decimal=',')
            if df2.shape[1] > df.shape[1]:
                df = df2
        except Exception:
            pass
    return df

def clean_name(name):
    return re.sub(r'\s+', ' ', str(name).strip())

def is_numeric_like(x):
    try:
        float(str(x))
        return True
    except Exception:
        return False

headers_list = [clean_name(h) for h in DATASET_HEADERS.split(',') if str(h).strip() != '']

df = read_csv_with_fallback(DATASET_PATH)
df.columns = [clean_name(c) for c in df.columns]
df = df.loc[:, [c for c in df.columns if not str(c).lower().startswith('unnamed')]]

if len(headers_list) == df.shape[1]:
    num_like_count = sum([is_numeric_like(c) for c in df.columns])
    if num_like_count >= len(df.columns) * 0.5 or ('class' in headers_list and 'class' not in df.columns):
        df.columns = headers_list

assert df.shape[0] > 0 and df.shape[1] > 0

df = df.replace([np.inf, -np.inf], np.nan)

for col in df.columns:
    if pd.api.types.is_numeric_dtype(df[col]):
        df[col] = pd.to_numeric(df[col], errors='coerce')
    else:
        converted = pd.to_numeric(df[col], errors='coerce')
        if converted.notna().mean() >= 0.5:
            df[col] = converted

def choose_target_column(df):
    for cand in ['class', 'target', 'label', 'y']:
        if cand in df.columns:
            return cand
    best_col = None
    best_unique = None
    for col in df.columns:
        uniq = df[col].nunique(dropna=True)
        if uniq <= 1:
            continue
        if best_unique is None or uniq < best_unique:
            best_unique = uniq
            best_col = col
    if best_col is None:
        best_col = df.columns[-1]
    return best_col

target_col = choose_target_column(df)

df = df.dropna(subset=[target_col])
assert df.shape[0] > 0

if df.shape[0] < 2:
    df = pd.concat([df, df], ignore_index=True)

X = df.drop(columns=[target_col])
y = df[target_col]

if X.shape[1] == 0:
    X = pd.DataFrame({'dummy': np.zeros(len(df))})

unique_vals = y.nunique(dropna=True)
is_classification = bool((y.dtype == object) or (unique_vals <= 20))

if is_classification:
    if y.dtype == object or pd.api.types.is_bool_dtype(y):
        le = LabelEncoder()
        y = le.fit_transform(y.astype(str))
else:
    y = pd.to_numeric(y, errors='coerce')
    mask = pd.Series(y).notna()
    X = X.loc[mask]
    y = pd.Series(y).loc[mask]
    if len(y) < 2:
        X = pd.concat([X, X], ignore_index=True)
        y = pd.concat([y, y], ignore_index=True)
    unique_vals = pd.Series(y).nunique(dropna=True)

stratify = None
if is_classification and unique_vals > 1:
    try:
        class_counts = pd.Series(y).value_counts()
        n_samples = len(y)
        n_classes = int(unique_vals)
        if (class_counts >= 2).all() and n_samples * 0.2 >= n_classes:
            stratify = y
    except Exception:
        stratify = None

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=stratify
)
assert len(X_train) > 0 and len(X_test) > 0

numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
categorical_features = [c for c in X.columns if c not in numeric_features]

transformers = []
if len(numeric_features) > 0:
    transformers.append(('num', Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler(with_mean=False))
    ]), numeric_features))
if len(categorical_features) > 0:
    transformers.append(('cat', Pipeline([
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ]), categorical_features))

preprocess = ColumnTransformer(transformers=transformers, remainder='drop', sparse_threshold=0.3)

if is_classification:
    if unique_vals < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        solver = 'lbfgs' if unique_vals > 2 else 'liblinear'
        model = LogisticRegression(max_iter=200, solver=solver)
else:
    model = Ridge(alpha=1.0)

pipeline = Pipeline(steps=[('preprocess', preprocess), ('model', model)])
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if is_classification:
    try:
        accuracy = accuracy_score(y_test, y_pred)
    except Exception:
        accuracy = 0.0
else:
    try:
        r2 = r2_score(y_test, y_pred)
    except Exception:
        r2 = np.nan
    if r2 is None or not np.isfinite(r2):
        try:
            mae = np.mean(np.abs(np.array(y_test) - np.array(y_pred)))
            accuracy = 1.0 / (1.0 + mae) if np.isfinite(mae) else 0.0
        except Exception:
            accuracy = 0.0
    else:
        accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

if accuracy is None or not np.isfinite(accuracy):
    accuracy = 0.0
accuracy = float(accuracy)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) for energy-efficient CPU training and inference.
# - Minimal preprocessing with imputation, scaling, and one-hot encoding via ColumnTransformer ensures reproducibility.
# - Robust schema handling and safe fallbacks (dummy model, bounded regression proxy) guarantee end-to-end execution.