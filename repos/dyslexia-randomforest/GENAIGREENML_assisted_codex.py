# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import random
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

DATASET_PATH = "trainingfinal.csv"
DATASET_HEADERS = "AF3_Shannon,AF3_Energy0_4,AF3_Energy4_8,AF3_Energy8_10,AF3_Energy10_12,AF3_Energy13_19,AF3_Energy20_30,AF3_Energy30_60,F3_Shannon,F3_Energy0_4,F3_Energy4_8,F3_Energy8_10,F3_Energy10_12,F3_Energy13_19,F3_Energy20_30,F3_Energy30_60,F7_Energy0_4,F7_Energy4_8,F7_Energy8_10,F7_Energy10_12,F7_Energy13_19,F7_Energy20_30,F7_Energy30_60,FC5_Shannon,FC5_Energy0_4,FC5_Energy4_8,FC5_Energy8_10,FC5_Energy10_12,FC5_Energy13_19,FC5_Energy20_30,FC5_Energy30_60,O1_Shannon,O1_Energy0_4,O1_Energy4_8,O1_Energy8_10,O1_Energy10_12,O1_Energy13_19,O1_Energy20_30,O1_Energy30_60,P7_Shannon,P7_Energy0_4,P7_Energy4_8,P7_Energy8_10,P7_Energy10_12,P7_Energy13_19,P7_Energy20_30,P7_Energy30_60,T7_Shannon,T7_Energy0_4,T7_Energy4_8,T7_Energy8_10,T7_Energy10_12,T7_Energy13_19,T7_Energy20_30,T7_Energy30_60,AF4_Shannon,AF4_Energy0_4,AF4_Energy4_8,AF4_Energy8_10,AF4_Energy10_12,AF4_Energy13_19,AF4_Energy20_30,AF4_Energy30_60,F4_Shannon,F4_Energy0_4,F4_Energy4_8,F4_Energy8_10,F4_Energy10_12,F4_Energy13_19,F4_Energy20_30,F4_Energy30_60,F8_Shannon,F8_Energy0_4,F8_Energy4_8,F8_Energy8_10,F8_Energy10_12,F8_Energy13_19,F8_Energy20_30,F8_Energy30_60,FC6_Shannon,FC6_Energy0_4,FC6_Energy4_8,FC6_Energy8_10,FC6_Energy10_12,FC6_Energy13_19,FC6_Energy20_30,FC6_Energy30_60,O2_Shannon,O2_Energy0_4,O2_Energy4_8,O2_Energy8_10,O2_Energy10_12,O2_Energy13_19,O2_Energy20_30,O2_Energy30_60,T8_Shannon,T8_Energy0_4,T8_Energy4_8,T8_Energy8_10,T8_Energy10_12,T8_Energy13_19,T8_Energy20_30,T8_Energy30_60,P8_Shannon,P8_Energy0_4,P8_Energy4_8,P8_Energy8_10,P8_Energy10_12,P8_Energy13_19,P8_Energy20_30,P8_Energy30_60,class"
RANDOM_SEED = 42
N_RUNS = 1

np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

def parse_headers(headers_str):
    return [h.strip() for h in headers_str.split(",") if h.strip()]

HEADERS = parse_headers(DATASET_HEADERS)

def read_csv_robust(path, headers):
    df = pd.read_csv(path)
    if df.shape[1] <= 1 or (headers and not set(df.columns).intersection(headers)):
        df_alt = pd.read_csv(path, sep=";", decimal=",")
        if df_alt.shape[1] > df.shape[1]:
            df = df_alt
    if headers and not set(df.columns).intersection(headers):
        df_noheader = pd.read_csv(path, header=None)
        if df_noheader.shape[1] == len(headers):
            df = df_noheader
            df.columns = headers
        else:
            df_noheader_alt = pd.read_csv(path, sep=";", decimal=",", header=None)
            if df_noheader_alt.shape[1] == len(headers):
                df = df_noheader_alt
                df.columns = headers
    return df

def clean_dataframe(df, headers):
    if headers:
        cols = [c for c in df.columns if c in headers]
        if cols:
            df = df[cols]
    df = df.drop(columns=[c for c in ("parts {1=akoustiko,2=optiko,3=mousiki}", "Subject ID") if c in df.columns], errors="ignore")
    df = df.replace({"CN": 0, "DYS": 1, "cn": 0, "dys": 1})
    return df

def resolve_label_col(df, headers):
    for col in df.columns:
        if col.lower() == "class":
            return col
    if headers:
        for h in headers:
            if h.lower() == "class" and h in df.columns:
                return h
    return df.columns[-1]

def prepare_train_test(train_df, test_df, headers, seed):
    train_df = clean_dataframe(train_df, headers)
    if test_df is not None:
        test_df = clean_dataframe(test_df, headers)
        nan_cols = set(train_df.columns[train_df.isna().any()]) | set(test_df.columns[test_df.isna().any()])
        if nan_cols:
            train_df = train_df.drop(columns=nan_cols, errors="ignore")
            test_df = test_df.drop(columns=nan_cols, errors="ignore")
        label_col = resolve_label_col(train_df, headers)
        if label_col not in test_df.columns:
            label_col = resolve_label_col(test_df, headers)
        common_cols = [c for c in train_df.columns if c in test_df.columns]
        train_df = train_df[common_cols]
        test_df = test_df[common_cols]
        y_train = train_df[label_col]
        X_train = train_df.drop(columns=[label_col])
        y_test = test_df[label_col]
        X_test = test_df.drop(columns=[label_col])
    else:
        nan_cols = train_df.columns[train_df.isna().any()]
        if len(nan_cols) > 0:
            train_df = train_df.drop(columns=nan_cols)
        label_col = resolve_label_col(train_df, headers)
        y = train_df[label_col]
        X = train_df.drop(columns=[label_col])
        if len(y) < 2:
            X_train, X_test, y_train, y_test = X, X.copy(), y, y.copy()
        else:
            stratify = None
            if y.nunique() > 1:
                counts = y.value_counts()
                if (counts >= 2).all():
                    stratify = y
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=seed, stratify=stratify
            )
    X_train = np.asarray(X_train, dtype=np.float32)
    X_test = np.asarray(X_test, dtype=np.float32)
    y_train = np.asarray(y_train, dtype=np.int32)
    y_test = np.asarray(y_test, dtype=np.int32)
    return X_train, X_test, y_train, y_test

def confmatrix_of_RandomForest(trainingdata_filepath, testdata_filepath=None, criterion="entropy", n_runs=1, seed=RANDOM_SEED, headers=None):
    if headers is None:
        headers = HEADERS
    train_df = read_csv_robust(trainingdata_filepath, headers)
    test_df = read_csv_robust(testdata_filepath, headers) if testdata_filepath and os.path.exists(testdata_filepath) else None
    X_train, X_test, y_train, y_test = prepare_train_test(train_df, test_df, headers, seed)
    if n_runs < 1:
        n_runs = 1
    conf_sum = None
    acc_sum = 0.0
    for i in range(n_runs):
        model = RandomForestClassifier(
            criterion=criterion,
            max_depth=8,
            random_state=seed + i,
            n_estimators=100
        )
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        acc = float(np.mean(preds == y_test))
        acc_sum += acc
        conf = confusion_matrix(y_test, preds, labels=[0, 1])
        if conf_sum is None:
            conf_sum = conf.astype(np.float64)
        else:
            conf_sum += conf
    conf_mean = conf_sum / n_runs if conf_sum is not None else None
    accuracy = acc_sum / n_runs
    return conf_mean, accuracy

def main():
    test_path = None
    candidate_test = os.path.join(os.path.dirname(DATASET_PATH), "testfinal.csv")
    if os.path.exists(candidate_test):
        test_path = candidate_test
    _, accuracy = confmatrix_of_RandomForest(DATASET_PATH, test_path, criterion="entropy", n_runs=N_RUNS, seed=RANDOM_SEED, headers=HEADERS)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# Reduced redundant computation by running a single deterministic training pass instead of multiple stochastic repeats.
# Eliminated unnecessary file writes and removed unused columns early to lower I/O and memory overhead.
# Converted data once to compact float32/int32 NumPy arrays to minimize data movement and memory footprint.
# Reused a single prediction pass to compute accuracy and confusion matrix, avoiding duplicate inference.
# Added robust CSV parsing with delimiter fallback and fixed seeds for reproducible, stable results.