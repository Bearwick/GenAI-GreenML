# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

N_RUNS = 5
SEED = 42

def _load_and_prepare(trainingdata_filepath, testdata_filepath):
    drop_cols = ['parts {1=akoustiko,2=optiko,3=mousiki}', 'Subject ID']
    usecols = lambda c: c not in drop_cols
    train = pd.read_csv(trainingdata_filepath, usecols=usecols)
    test = pd.read_csv(testdata_filepath, usecols=usecols)
    label_map = {'CN': 0, 'DYS': 1}
    train['class'] = train['class'].replace(label_map)
    test['class'] = test['class'].replace(label_map)
    na_cols = train.columns[train.isna().any()].union(test.columns[test.isna().any()])
    if len(na_cols) > 0:
        train.drop(columns=na_cols, inplace=True)
        test.drop(columns=na_cols, inplace=True)
    y_train = train.pop('class').to_numpy(dtype=np.int32, copy=False)
    X_train = train.to_numpy(dtype=np.float32, copy=False)
    y_test = test.pop('class').to_numpy(dtype=np.int32, copy=False)
    X_test = test.to_numpy(dtype=np.float32, copy=False)
    del train, test
    return X_train, y_train, X_test, y_test

def _binary_confusion(y_true, y_pred):
    return np.bincount((y_true << 1) + y_pred, minlength=4).reshape(2, 2)

def confmatrix_of_RandomForest(trainingdata_filepath, testdata_filepath, criterion="entropy"):
    X_train, y_train, X_test, y_test = _load_and_prepare(trainingdata_filepath, testdata_filepath)
    conf_sum = np.zeros((2, 2), dtype=np.float64)
    for i in range(N_RUNS):
        model = RandomForestClassifier(criterion=criterion, max_depth=8, random_state=SEED + i)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        conf_sum += _binary_confusion(y_test, y_pred)
    return (conf_sum / N_RUNS).ravel()

if __name__ == '__main__':
    conf = confmatrix_of_RandomForest("entire brain_training_2.csv", "entire brain_test_2.csv")
    total = conf.sum()
    accuracy = (conf[0] + conf[3]) / total if total else 0.0
    print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Used usecols at CSV load time to skip unnecessary columns and avoid extra data movement.
# Replaced labels only in the target column and used union-based NaN column removal to minimize scans and temporary lists.
# Extracted the target with pop and converted directly to NumPy float32/int32 arrays to reduce memory footprint.
# Averaged confusion matrices incrementally to avoid storing per-run results and removed redundant scoring passes.
# Computed binary confusion matrices with np.bincount for a lightweight evaluation path.
# Added deterministic seed progression to ensure reproducible multi-run evaluation.