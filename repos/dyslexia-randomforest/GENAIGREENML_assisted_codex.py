# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

SEED = 42
np.random.seed(SEED)

DATASET_HEADERS = "AF3_Shannon,AF3_Energy0_4,AF3_Energy4_8,AF3_Energy8_10,AF3_Energy10_12,AF3_Energy13_19,AF3_Energy20_30,AF3_Energy30_60,F3_Shannon,F3_Energy0_4,F3_Energy4_8,F3_Energy8_10,F3_Energy10_12,F3_Energy13_19,F3_Energy20_30,F3_Energy30_60,F7_Energy0_4,F7_Energy4_8,F7_Energy8_10,F7_Energy10_12,F7_Energy13_19,F7_Energy20_30,F7_Energy30_60,FC5_Shannon,FC5_Energy0_4,FC5_Energy4_8,FC5_Energy8_10,FC5_Energy10_12,FC5_Energy13_19,FC5_Energy20_30,FC5_Energy30_60,O1_Shannon,O1_Energy0_4,O1_Energy4_8,O1_Energy8_10,O1_Energy10_12,O1_Energy13_19,O1_Energy20_30,O1_Energy30_60,P7_Shannon,P7_Energy0_4,P7_Energy4_8,P7_Energy8_10,P7_Energy10_12,P7_Energy13_19,P7_Energy20_30,P7_Energy30_60,T7_Shannon,T7_Energy0_4,T7_Energy4_8,T7_Energy8_10,T7_Energy10_12,T7_Energy13_19,T7_Energy20_30,T7_Energy30_60,AF4_Shannon,AF4_Energy0_4,AF4_Energy4_8,AF4_Energy8_10,AF4_Energy10_12,AF4_Energy13_19,AF4_Energy20_30,AF4_Energy30_60,F4_Shannon,F4_Energy0_4,F4_Energy4_8,F4_Energy8_10,F4_Energy10_12,F4_Energy13_19,F4_Energy20_30,F4_Energy30_60,F8_Shannon,F8_Energy0_4,F8_Energy4_8,F8_Energy8_10,F8_Energy10_12,F8_Energy13_19,F8_Energy20_30,F8_Energy30_60,FC6_Shannon,FC6_Energy0_4,FC6_Energy4_8,FC6_Energy8_10,FC6_Energy10_12,FC6_Energy13_19,FC6_Energy20_30,FC6_Energy30_60,O2_Shannon,O2_Energy0_4,O2_Energy4_8,O2_Energy8_10,O2_Energy10_12,O2_Energy13_19,O2_Energy20_30,O2_Energy30_60,T8_Shannon,T8_Energy0_4,T8_Energy4_8,T8_Energy8_10,T8_Energy10_12,T8_Energy13_19,T8_Energy20_30,T8_Energy30_60,P8_Shannon,P8_Energy0_4,P8_Energy4_8,P8_Energy8_10,P8_Energy10_12,P8_Energy13_19,P8_Energy20_30,P8_Energy30_60,class"
DATASET_HEADERS_LIST = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]

def _looks_wrong(df):
    if df.shape[1] <= 1:
        return True
    overlap = len(set(df.columns) & set(DATASET_HEADERS_LIST))
    return overlap == 0

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        return pd.read_csv(path, sep=";", decimal=",")
    if _looks_wrong(df):
        try:
            df_alt = pd.read_csv(path, sep=";", decimal=",")
            if not _looks_wrong(df_alt):
                df = df_alt
        except Exception:
            pass
    return df

def preprocess(df):
    df = df.drop(columns=["parts {1=akoustiko,2=optiko,3=mousiki}", "Subject ID"], errors="ignore")
    df.replace({"CN": 0, "DYS": 1}, inplace=True)
    return df

def _ensure_class_column(df):
    if "class" in df.columns:
        return df
    for col in df.columns:
        if str(col).lower() == "class":
            return df.rename(columns={col: "class"})
    return df

LAST_ACCURACY = None

def confmatrix_of_RandomForest(trainingdata_filepath, testdata_filepath, criterion="entropy"):
    global LAST_ACCURACY
    train = read_csv_robust(trainingdata_filepath)
    test = read_csv_robust(testdata_filepath)
    train = preprocess(train)
    test = preprocess(test)
    train = _ensure_class_column(train)
    test = _ensure_class_column(test)
    train_nan_cols = train.columns[train.isna().any()]
    test_nan_cols = test.columns[test.isna().any()]
    exclude = set(train_nan_cols).union(set(test_nan_cols))
    if exclude:
        train = train.drop(columns=list(exclude))
        test = test.drop(columns=list(exclude))
    common_cols = [c for c in train.columns if c in test.columns]
    if "class" not in common_cols:
        raise ValueError("Class column not found in both datasets.")
    ordered_features = [c for c in DATASET_HEADERS_LIST if c in common_cols and c != "class"]
    if not ordered_features:
        ordered_features = [c for c in train.columns if c in common_cols and c != "class"]
    train = train[ordered_features + ["class"]]
    test = test[ordered_features + ["class"]]
    X_train = train[ordered_features].to_numpy(dtype=np.float32, copy=False)
    y_train = train["class"].to_numpy(dtype=np.int32, copy=False)
    X_test = test[ordered_features].to_numpy(dtype=np.float32, copy=False)
    y_test = test["class"].to_numpy(dtype=np.int32, copy=False)
    y_test_int = y_test.astype(np.int64, copy=False)
    n_test = y_test_int.shape[0]
    n_runs = 5
    conf_sum = np.zeros(4, dtype=np.float64)
    acc_sum = 0.0
    for i in range(n_runs):
        model = RandomForestClassifier(criterion=criterion, max_depth=8, random_state=SEED + i)
        model.fit(X_train, y_train)
        y_pred_int = model.predict(X_test).astype(np.int64, copy=False)
        idx = (y_test_int << 1) + y_pred_int
        conf = np.bincount(idx, minlength=4)
        conf_sum += conf
        acc_sum += (conf[0] + conf[3]) / n_test
    LAST_ACCURACY = acc_sum / n_runs
    return conf_sum / n_runs

if __name__ == "__main__":
    confmatrix_of_RandomForest("entire brain_training_2.csv", "entire brain_test_2.csv")
    accuracy = LAST_ACCURACY if LAST_ACCURACY is not None else 0.0
    print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Implemented robust CSV parsing with schema-based validation to avoid misreads and retries.
# - Dropped irrelevant and NaN columns early and aligned features using header schema to minimize data movement.
# - Converted data once to NumPy arrays and derived accuracy from confusion counts to avoid redundant predictions.
# - Aggregated confusion matrices incrementally to reduce intermediate storage and memory overhead.
# - Set fixed seeds and deterministic random states for reproducible results.