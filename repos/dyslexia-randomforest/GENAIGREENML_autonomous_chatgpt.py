# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import sys
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _find_dataset_path() -> str:
    # Prefer common filenames; fall back to first CSV in working directory.
    candidates = [
        "data.csv",
        "dataset.csv",
        "train.csv",
        "eeg.csv",
        "EEG.csv",
        "Dataset.csv",
        "Data.csv",
    ]
    for c in candidates:
        if os.path.isfile(c):
            return c
    csvs = [f for f in os.listdir(".") if os.path.isfile(f) and f.lower().endswith(".csv")]
    if csvs:
        csvs.sort()
        return csvs[0]
    raise FileNotFoundError("No CSV dataset found in the current directory.")


def main() -> None:
    path = _find_dataset_path()
    df = pd.read_csv(path)

    if "class" not in df.columns:
        raise ValueError("Expected target column named 'class'.")

    y = df["class"]
    X = df.drop(columns=["class"])

    # Ensure purely numeric feature matrix; coerce non-numeric to NaN for imputation.
    X = X.apply(pd.to_numeric, errors="coerce")

    # Stratify when classification labels are suitable; otherwise fall back safely.
    stratify = None
    try:
        uniq = pd.unique(y)
        if len(uniq) > 1:
            stratify = y
    except Exception:
        stratify = None

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=stratify,
    )

    pipeline = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("var", VarianceThreshold(threshold=0.0)),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
            ("clf", LogisticRegression(solver="liblinear", max_iter=300, n_jobs=1)),
        ]
    )

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Chose LogisticRegression (linear model) for CPU efficiency and strong baseline performance on tabular EEG features.
# - Used a single-pass preprocessing pipeline: median imputation + variance filter + standardization to keep training stable and lightweight.
# - Avoided deep learning, embeddings, and ensemble-heavy methods to reduce compute and energy use.
# - Kept deterministic split (random_state) for reproducibility; solver='liblinear' is efficient for small/medium datasets on CPU.
# - n_jobs=1 avoids oversubscription that can waste energy on small workloads.