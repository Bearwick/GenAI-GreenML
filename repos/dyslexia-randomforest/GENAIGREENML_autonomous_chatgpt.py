# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings
from typing import Tuple, Optional

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")


DATASET_PATH = "trainingfinal.csv"


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _drop_unnamed(df: pd.DataFrame) -> pd.DataFrame:
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _read_csv_robust(path: str) -> pd.DataFrame:
    # First attempt: default CSV parsing
    try:
        df1 = pd.read_csv(path)
    except Exception:
        df1 = pd.DataFrame()

    # Second attempt: semicolon separator + comma decimal
    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
    except Exception:
        df2 = pd.DataFrame()

    # Pick the more plausible parse (more columns, non-empty)
    candidates = [df for df in [df1, df2] if isinstance(df, pd.DataFrame) and not df.empty]
    if not candidates:
        return pd.DataFrame()

    def score_df(d: pd.DataFrame) -> Tuple[int, int]:
        ncols = int(d.shape[1])
        nonnull = int(d.notna().sum().sum())
        return (ncols, nonnull)

    df = sorted(candidates, key=score_df, reverse=True)[0]
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)
    return df


def _coerce_numeric_inplace(df: pd.DataFrame, cols) -> None:
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")


def _select_target(df: pd.DataFrame, preferred: Optional[str] = None) -> str:
    cols = list(df.columns)
    if preferred is not None and preferred in cols:
        return preferred

    # Heuristic: if a column named like 'class' exists (case-insensitive), prefer it
    for c in cols:
        if str(c).strip().lower() in ("class", "target", "label", "y"):
            return c

    # Otherwise pick a non-constant numeric column with few unique values (classification-friendly)
    numeric_cols = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() > 0:
            numeric_cols.append(c)

    best = None
    best_score = None
    for c in numeric_cols:
        s = pd.to_numeric(df[c], errors="coerce")
        s = s.replace([np.inf, -np.inf], np.nan).dropna()
        if s.empty:
            continue
        nunique = int(s.nunique(dropna=True))
        if nunique <= 1:
            continue
        # Prefer smaller nunique but not too small; and avoid almost-unique continuous columns
        score = (min(nunique, 50), -float(s.var()) if np.isfinite(s.var()) else 0.0)
        if best is None or score < best_score:
            best = c
            best_score = score

    if best is not None:
        return best

    # Fallback: any column (will later fall back to dummy)
    return cols[-1]


def _build_preprocessor(X: pd.DataFrame) -> Tuple[ColumnTransformer, list, list]:
    # Determine column types based on pandas dtypes (robust to unknown schema)
    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False)),  # sparse-friendly; cheap on CPU
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    return preprocessor, numeric_features, categorical_features


def _safe_bounded_regression_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    # Stable proxy in [0,1]: 1 / (1 + normalized MAE)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = float(np.mean(np.abs(y_true - y_pred))) if y_true.size else 0.0
    denom = float(np.std(y_true)) + 1e-12
    nmae = mae / denom if np.isfinite(denom) else mae
    score = 1.0 / (1.0 + max(0.0, nmae))
    return float(np.clip(score, 0.0, 1.0))


def main():
    df = _read_csv_robust(DATASET_PATH)
    assert isinstance(df, pd.DataFrame) and not df.empty, "Dataset is empty or could not be read."

    df = df.copy()
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Choose target robustly, prefer provided header if present
    target_col = _select_target(df, preferred="class" if "class" in df.columns else None)

    # Ensure target exists
    if target_col not in df.columns:
        target_col = df.columns[-1]

    # Separate features/target
    y_raw = df[target_col]
    X_raw = df.drop(columns=[target_col]) if df.shape[1] > 1 else df.copy()

    # If no features, create a constant feature to allow pipeline to run
    if X_raw.shape[1] == 0:
        X_raw = pd.DataFrame({"__constant__": np.ones(len(df), dtype=float)})

    # Coerce likely numeric columns in X and y (without assuming schema)
    _coerce_numeric_inplace(X_raw, [c for c in X_raw.columns if X_raw[c].dtype == object])
    y_num = pd.to_numeric(y_raw, errors="coerce")

    # Decide classification vs regression
    # Classification if y has few discrete values (including non-numeric) and at least 2 classes
    y_for_class = y_raw.copy()
    # Try to use numeric if it looks like class labels 0/1/2..., else keep as object
    if y_num.notna().mean() > 0.9:
        y_for_class = y_num

    # Clean infinite
    if isinstance(y_for_class, pd.Series):
        y_for_class = y_for_class.replace([np.inf, -np.inf], np.nan)

    # Drop rows with missing target; keep X aligned
    mask = pd.notna(y_for_class)
    X = X_raw.loc[mask].reset_index(drop=True)
    y = pd.Series(y_for_class.loc[mask]).reset_index(drop=True)

    assert len(X) > 0 and len(y) > 0, "No data left after removing missing targets."

    # Re-infer dtypes after coercion
    preprocessor, num_cols, cat_cols = _build_preprocessor(X)

    # Determine if classification is viable
    y_unique = pd.Series(y).dropna().unique()
    n_unique = len(y_unique)

    is_classification = False
    if n_unique >= 2:
        # Heuristic: classification if small number of unique values relative to samples
        if n_unique <= max(20, int(0.05 * len(y))):
            is_classification = True

    # Train/test split
    # Use stratify when classification and feasible
    stratify = y if is_classification and n_unique >= 2 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=stratify,
    )

    assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

    if is_classification:
        # If y is numeric but not integer-like, cast to string labels to avoid weirdness
        if pd.api.types.is_numeric_dtype(y_train):
            # Keep as int if all close to integer values
            yt = pd.to_numeric(y_train, errors="coerce")
            if yt.notna().all() and np.all(np.isclose(yt.values, np.round(yt.values))):
                y_train_use = np.round(yt.values).astype(int)
                y_test_use = np.round(pd.to_numeric(y_test, errors="coerce").values).astype(int)
            else:
                y_train_use = y_train.astype(str).values
                y_test_use = y_test.astype(str).values
        else:
            y_train_use = y_train.astype(str).values
            y_test_use = y_test.astype(str).values

        # If class collapses in train set, fallback to dummy
        if len(np.unique(y_train_use)) < 2:
            clf = DummyClassifier(strategy="most_frequent")
        else:
            clf = LogisticRegression(
                solver="liblinear",  # CPU-friendly for small/medium data
                max_iter=200,
                C=1.0,
            )

        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train_use)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test_use, y_pred))
    else:
        # Regression fallback
        y_train_num = pd.to_numeric(y_train, errors="coerce").replace([np.inf, -np.inf], np.nan)
        y_test_num = pd.to_numeric(y_test, errors="coerce").replace([np.inf, -np.inf], np.nan)

        # If too many NaNs, use dummy regressor
        if y_train_num.notna().sum() < max(2, int(0.5 * len(y_train_num))):
            reg = DummyRegressor(strategy="mean")
            model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
            model.fit(X_train, np.nan_to_num(y_train_num.values, nan=float(np.nanmean(y_train_num.values)) if np.isfinite(np.nanmean(y_train_num.values)) else 0.0))
            y_pred = model.predict(X_test)
            y_true = np.nan_to_num(y_test_num.values, nan=float(np.nanmean(y_train_num.values)) if np.isfinite(np.nanmean(y_train_num.values)) else 0.0)
            accuracy = _safe_bounded_regression_score(y_true, y_pred)
        else:
            y_train_filled = y_train_num.fillna(float(y_train_num.median()))
            y_test_filled = y_test_num.fillna(float(y_train_num.median()))
            reg = Ridge(alpha=1.0, random_state=42)
            model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
            model.fit(X_train, y_train_filled.values)
            y_pred = model.predict(X_test)
            accuracy = _safe_bounded_regression_score(y_test_filled.values, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) and a single preprocessing pass via sklearn Pipeline/ColumnTransformer for CPU efficiency.
# - Robust CSV parsing: tries default read_csv, then retries with sep=';' and decimal=','; selects the more plausible parse.
# - Defensive schema handling: normalizes column names, drops 'Unnamed:*', selects target from available columns with preference for 'class' (case-insensitive).
# - Minimizes compute/memory: median/most_frequent imputation + sparse one-hot; StandardScaler(with_mean=False) avoids densifying sparse matrices.
# - Handles missing/inf safely: coercion with errors='coerce', drops missing targets, imputes features; avoids numeric stats on object dtype.
# - If classification target is invalid (<2 classes) falls back to regression; regression "accuracy" is a bounded proxy in [0,1]: 1/(1+normalized_MAE) for stability.