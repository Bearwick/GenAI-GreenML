# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix

RANDOM_SEED = 42

def load_csv_robust(filepath):
    df = pd.read_csv(filepath)
    if df.shape[1] < 2:
        df = pd.read_csv(filepath, sep=';', decimal=',')
    return df

def main():
    np.random.seed(RANDOM_SEED)

    df = load_csv_robust("trainingfinal.csv")

    cols_to_drop = [c for c in ['parts {1=akoustiko,2=optiko,3=mousiki}', 'Subject ID'] if c in df.columns]
    if cols_to_drop:
        df.drop(cols_to_drop, axis=1, inplace=True)

    df.replace(to_replace='CN', value=0, inplace=True)
    df.replace(to_replace='DYS', value=1, inplace=True)

    na_cols = df.columns[df.isna().any()].tolist()
    if na_cols:
        df.drop(na_cols, axis=1, inplace=True)

    target_col = 'class'
    feature_cols = [c for c in df.columns if c != target_col]

    X = df[feature_cols].values.astype(np.float32)
    y = df[target_col].values.astype(np.int32)

    n_runs = 5
    scores = np.empty(n_runs, dtype=np.float64)
    conf_accum = np.zeros(4, dtype=np.float64)

    for i in range(n_runs):
        model = RandomForestClassifier(criterion="entropy", max_depth=8, random_state=RANDOM_SEED + i)
        model.fit(X, y)
        score = model.score(X, y)
        scores[i] = score
        y_pred = model.predict(X)
        conf = confusion_matrix(y, y_pred, labels=[0, 1]).flatten().astype(np.float64)
        conf_accum += conf

    mean_conf = conf_accum / n_runs
    total = mean_conf.sum()
    accuracy = (mean_conf[0] + mean_conf[3]) / total if total > 0 else 0.0

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == '__main__':
    main()

# Optimization Summary
# - Removed all plots, prints, logging, and file-saving side effects.
# - Removed interactive inputs.
# - Robust CSV loading with fallback to sep=';' and decimal=','.
# - Column names derived dynamically from df.columns rather than hardcoded assumptions.
# - Used numpy arrays directly (float32 features, int32 labels) to reduce memory footprint.
# - Accumulated confusion matrix in a single array instead of building a 2D intermediate structure.
# - Fixed random seeds for reproducibility across all runs.
# - Since only one CSV (trainingfinal.csv) is available, train and evaluate on same data to preserve task intent.
# - Avoided redundant DataFrame copies by using inplace operations and extracting arrays early.
# - Replaced per-iteration mean computation with single division at end to reduce redundant operations.