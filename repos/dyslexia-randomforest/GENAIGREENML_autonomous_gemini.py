# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

def load_data(path):
    # Robust loading logic
    try:
        df = pd.read_csv(path)
        # Check if parsing was successful (more than one column)
        if df.shape[1] <= 1:
            df = pd.read_csv(path, sep=';', decimal=',')
    except Exception:
        # Final fallback for unusual formatting
        df = pd.read_csv(path, sep=None, engine='python')

    # Normalize column names: strip, single spaces, remove Unnamed
    df.columns = [str(c).strip() for c in df.columns]
    df.columns = [" ".join(str(c).split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def solve():
    dataset_path = 'trainingfinal.csv'
    try:
        df = load_data(dataset_path)
    except Exception:
        # If file is missing or unreadable, exit silently to meet requirements
        return

    if df.empty:
        return

    # Identify target
    # Prefer 'class' as defined in headers, fallback to last column
    target_col = 'class'
    if target_col not in df.columns:
        target_col = df.columns[-1]

    # Clean numeric data: coerce errors to NaN and drop/impute
    # We exclude the target if it's already categorical
    X = df.drop(columns=[target_col])
    y = df[target_col]

    # Convert features to numeric where possible
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')

    # Drop columns that are entirely NaN after coercion
    X = X.dropna(axis=1, how='all')

    # Handle target: ensure it's discrete for classification or fallback
    y_coerced = pd.to_numeric(y, errors='coerce')
    if y_coerced.notnull().all():
        y = y_coerced

    # Handle classification vs regression fallback
    # If target has many unique values relative to size, it might be regression
    # but the task implies classification ('class' label).
    is_classification = True
    unique_targets = y.nunique()
    if unique_targets < 2:
        # Trivial case: 100% accuracy or 0 if no data
        accuracy = 1.000000 if not y.empty else 0.000000
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if unique_targets > 1 else None
    )

    # Pipeline: Simple, low-CPU components
    # Logistic Regression is highly energy efficient compared to ensembles
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(
            max_iter=1000, 
            random_state=42, 
            solver='lbfgs',
            n_jobs=1 # Minimize parallel overhead for energy efficiency
        ))
    ])

    # Fit and Predict
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# Optimization Summary
# 1. Model Choice: Logistic Regression was selected over Random Forest for better energy efficiency. 
#    Linear models have O(p) prediction complexity vs O(trees * depth) for RF.
# 2. Solver: 'lbfgs' is efficient for small-to-medium datasets on CPU.
# 3. Data Loading: Implemented a robust multi-pass CSV parser to handle potential encoding or delimiter variations.
# 4. Preprocessing: Used StandardScaler to ensure fast convergence of the gradient-based solver.
# 5. Imputation: 'median' strategy used to handle outliers in EEG data without the computational cost of iterative imputation.
# 6. Resource Usage: n_jobs=1 used to prevent thread-spawning overhead, which can be less energy-efficient on small tasks.
# 7. Robustness: Included fallback logic for target selection and numeric coercion to ensure end-to-end execution.
# 8. Dimensionality: Dropped empty/NaN columns post-coercion to reduce the feature matrix size before training.