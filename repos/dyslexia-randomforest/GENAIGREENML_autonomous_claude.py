# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("dataset.csv")

# Separate features and target
X = df.drop(columns=["class"]).values
y = df["class"].values

# Preprocessing: StandardScaler for normalization (lightweight, no fitting overhead)
scaler = StandardScaler()

# Use StratifiedKFold for robust evaluation on small-scale data
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

accuracies = []

for train_idx, test_idx in skf.split(X, y):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Fit scaler on train, transform both
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # ExtraTreesClassifier: fast, CPU-efficient, handles high-dimensional EEG features well
    # n_estimators=300 balances accuracy and compute cost for tabular EEG band-power features
    # max_features="sqrt" reduces per-tree computation
    # min_samples_leaf=2 adds slight regularization
    model = ExtraTreesClassifier(
        n_estimators=300,
        max_features="sqrt",
        min_samples_leaf=2,
        n_jobs=-1,
        random_state=42,
        class_weight="balanced",
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(y_test, y_pred))

accuracy = np.mean(accuracies)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. ExtraTreesClassifier chosen over Random Forest for faster training (random splits, no search for best threshold).
# 2. StandardScaler used for consistent feature scaling across EEG band-power and Shannon entropy features.
# 3. StratifiedKFold (5-fold) ensures reliable evaluation without data leakage, suitable for small datasets.
# 4. max_features="sqrt" reduces per-tree computation from O(n_features) to O(sqrt(n_features)).
# 5. n_estimators=300 provides good ensemble diversity while remaining CPU-friendly.
# 6. class_weight="balanced" handles potential class imbalance in EEG emotion/state classification.
# 7. No deep learning used â€” tree ensembles are optimal for tabular EEG spectral features.
# 8. n_jobs=-1 uses all CPU cores for parallel tree construction, minimizing wall-clock time.