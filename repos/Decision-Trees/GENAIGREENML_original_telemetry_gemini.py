# Generated by generate_llm_code.py
# LLM: gemini
# Mode: original_telemetry

import csv
import random
import math

def read_data(csv_path):
    examples = []
    with open(csv_path, 'r') as csv_file:
        csv_reader = csv.DictReader(csv_file)
        for example in csv_reader:
            for k, v in example.items():
                if v == '':
                    example[k] = None
                else:
                    try:
                        example[k] = float(v)
                    except ValueError:
                         example[k] = v
            examples.append(example)
    return examples

def train_test_split(examples, test_perc):
    test_size = round(test_perc * len(examples))    
    shuffled = random.sample(examples, len(examples))
    return shuffled[test_size:], shuffled[:test_size]

class TreeNodeInterface():
    def classify(self, example): 
        raise NotImplementedError

class DecisionNode(TreeNodeInterface):
    def __init__(self, test_attr_name, test_attr_threshold, child_lt, child_ge, child_miss):
        self.test_attr_name = test_attr_name  
        self.test_attr_threshold = test_attr_threshold 
        self.child_ge = child_ge
        self.child_lt = child_lt
        self.child_miss = child_miss

    def classify(self, example):
        test_val = example[self.test_attr_name]
        if test_val is None:
            return self.child_miss.classify(example)
        elif test_val < self.test_attr_threshold:
            return self.child_lt.classify(example)
        else:
            return self.child_ge.classify(example)

class LeafNode(TreeNodeInterface):
    def __init__(self, pred_class, pred_class_count, total_count):
        self.pred_class = pred_class
        self.pred_class_count = pred_class_count
        self.total_count = total_count
        self.prob = pred_class_count / total_count if total_count > 0 else 0

    def classify(self, example):
        return self.pred_class, self.prob

class DecisionTree:
    def __init__(self, examples, id_name, class_name, min_leaf_count=1):
        self.id_name = id_name
        self.class_name = class_name
        self.min_leaf_count = min_leaf_count
        self.root = self.learn_tree(examples)  

    def learn_tree(self, examples):
        attribute_set = set()
        if not examples:
            return None
        example = examples[0]
        for attribute in example:
            if attribute != self.id_name and attribute != self.class_name:
                attribute_set.add(attribute)
        return attributeSplit(attribute_set, examples, self.min_leaf_count, self.class_name)
    
    def classify(self, example):
        return self.root.classify(example) 

def attributeSplit(attribute_set, examples, min_leaf_count, class_name):
    attribute_name, threshold, examples_lt, examples_ge = getBestAttributeAndSplit(attribute_set, examples, class_name)
    
    if not attribute_name or len(examples_ge) <= min_leaf_count or len(examples_lt) <= min_leaf_count:
        predictiveClass, predictiveClassCount = getPredictiveClass(examples, class_name)
        return LeafNode(predictiveClass, predictiveClassCount, len(examples))

    new_attribute_set = attribute_set.copy()
    new_attribute_set.remove(attribute_name)

    child_lt = attributeSplit(new_attribute_set.copy(), examples_lt, min_leaf_count, class_name)
    child_ge = attributeSplit(new_attribute_set.copy(), examples_ge, min_leaf_count, class_name)

    if len(examples_lt) >= len(examples_ge):
        child_miss = child_lt
    else:
        child_miss = child_ge

    return DecisionNode(attribute_name, threshold, child_lt, child_ge, child_miss)

def getBestAttributeAndSplit(attribute_set, examples, class_label):
    maxAttribute = {"name": "", "infogain": 0.0, "threshold": None, "ex_lt": [], "ex_ge": []}
    for attribute in attribute_set:
        infoGainValue, threshold_, lt, ge = getInfoGain(attribute, examples, class_label)
        if infoGainValue > maxAttribute['infogain']:
            maxAttribute = {"name": attribute, "infogain": infoGainValue, "threshold": threshold_, "ex_lt": lt, "ex_ge": ge}
    return (maxAttribute['name'], maxAttribute['threshold'], maxAttribute['ex_lt'], maxAttribute['ex_ge'])

def getInfoGain(attribute, examples, class_label):
    maxInfoGain = 0
    threshold = 0
    lt_split = []
    ge_split = []
    min_, max_, step = getRange(attribute, examples)
    if step == 0:
        return (0, 0, [], [])
    curThreshold = min_ + step
    while curThreshold < max_:
        lt, ge = splitExamplesOnAttribute(attribute, examples, curThreshold)
        if not lt or not ge:
            curThreshold += step
            continue
        gain = infoGain(examples, lt, ge, class_label)
        if gain > maxInfoGain:
            maxInfoGain = gain
            threshold = curThreshold
            lt_split = lt
            ge_split = ge
        curThreshold += step
    return (maxInfoGain, threshold, lt_split, ge_split)

def getRange(attribute, examples):
    min_val = float('inf')
    max_val = float('-inf')
    valid_found = False
    for ex in examples:
        if ex[attribute] is not None:
            val = float(ex[attribute])
            if val < min_val: min_val = val
            if val > max_val: max_val = val
            valid_found = True
    if not valid_found or min_val == max_val:
        return 0.0, 0.0, 0.0
    step = (max_val - min_val) / 15
    return min_val, max_val, step

def splitExamplesOnAttribute(attribute, examples, threshold):
    lt, ge = [], []
    for example in examples:
        if example[attribute] is None:
            continue
        elif example[attribute] >= threshold:
            ge.append(example)
        else:
            lt.append(example)
    return lt, ge

def infoGain(ex0, ex1, ex2, class_label):
    pc_1 = len(ex1) / len(ex0)
    pc_2 = len(ex2) / len(ex0)
    return entropy(ex0, class_label) - ((pc_1 * entropy(ex1, class_label)) + (pc_2 * entropy(ex2, class_label)))

def entropy(examples, class_label):
    if not examples:
        return 0
    class_label_dict = {}
    for example in examples:
        label = example[class_label]
        if label not in class_label_dict:
            class_label_dict[label] = 0
        class_label_dict[label] += 1
    
    ent = 0
    for label in class_label_dict:
        p = class_label_dict[label] / len(examples)
        if p > 0:
            ent -= p * math.log(p, 2)
    return ent

def getPredictiveClass(examples, class_label):
    if not examples:
        return None, 0
    class_counts = {}
    max_label = None
    max_count = -1
    for example in examples:
        label = example[class_label]
        class_counts[label] = class_counts.get(label, 0) + 1
        if class_counts[label] > max_count:
            max_count = class_counts[label]
            max_label = label
    return max_label, max_count

def test_model(model, test_examples):
    if not test_examples:
        return 0.0
    correct = 0
    for example in test_examples:
        actual = example[model.class_name]
        pred, prob = model.classify(example)
        if pred == actual:
            correct += 1
    return correct / len(test_examples)

if __name__ == '__main__':
    path_to_csv = 'town_vax_data.csv'
    id_attr_name = 'town'
    class_attr_name = 'vax_level'
    min_examples = 10
    examples = read_data(path_to_csv)
    if examples:
        train_examples, test_examples = train_test_split(examples, 0.25)
        tree = DecisionTree(train_examples, id_attr_name, class_attr_name, min_examples)
        accuracy = test_model(tree, test_examples)
        print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Fixed majority class counting in getPredictiveClass; original code initialized counts at 0 and resulted in (actual_count - 1).
# 2. Fixed attribute_set mutation in attributeSplit recursion; original code modified the same set object across sibling branches.
# 3. Standardized range initialization in getRange using float infinity to handle arbitrary data ranges.
# 4. Added defensive checks in attributeSplit and getInfoGain to handle cases where no gainful attribute is found or empty splits occur.
# 5. Removed all visualization logic (ASCII tree, confusion matrix, and per-example prints) to adhere to baseline requirements.
# 6. Renamed the 'sum' variable in the entropy function to 'ent' to avoid shadowing the built-in Python function.
# 7. Ensured missing values (None) are handled correctly during training splits and inference classification.