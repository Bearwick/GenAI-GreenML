# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
import random
import math

def load_data(path):
    try:
        df = pd.read_csv(path)
    except:
        df = pd.read_csv(path, sep=';', decimal=',')
    
    for col in df.columns:
        if col not in ['town', 'vax_level']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    return df

class LeafNode:
    def __init__(self, pred_class, pred_class_count, total_count):
        self.pred_class = pred_class
        self.prob = pred_class_count / total_count if total_count > 0 else 0

    def classify(self, example):
        return self.pred_class, self.prob

class DecisionNode:
    def __init__(self, test_attr_name, test_attr_threshold, child_lt, child_ge, child_miss):
        self.test_attr_name = test_attr_name
        self.test_attr_threshold = test_attr_threshold
        self.child_lt = child_lt
        self.child_ge = child_ge
        self.child_miss = child_miss

    def classify(self, example):
        test_val = example.get(self.test_attr_name)
        if pd.isna(test_val):
            return self.child_miss.classify(example)
        return self.child_lt.classify(example) if test_val < self.test_attr_threshold else self.child_ge.classify(example)

def get_entropy(df, class_label):
    if df.empty: return 0
    counts = df[class_label].value_counts()
    entropy = 0
    total = len(df)
    for c in counts:
        p = c / total
        if p > 0:
            entropy -= p * math.log(p, 2)
    return entropy

def get_predictive_class(df, class_label):
    counts = {}
    max_class = ""
    max_count = 0
    for val in df[class_label]:
        if val not in counts:
            counts[val] = 0
        else:
            counts[val] += 1
        if counts[val] > max_count:
            max_count = counts[val]
            max_class = val
    return max_class, max_count

def get_best_split(attribute_set, df, class_label):
    best = {"name": "", "infogain": -1.0, "threshold": 0, "df_lt": pd.DataFrame(), "df_ge": pd.DataFrame()}
    parent_entropy = get_entropy(df, class_label)
    
    for attr in attribute_set:
        valid_df = df[df[attr].notna()]
        if valid_df.empty: continue
        
        min_v, max_v = valid_df[attr].min(), valid_df[attr].max()
        step = (max_v - min_v) / 15
        if step <= 0: continue
        
        curr_t = min_v + step
        while curr_t < max_v:
            lt = valid_df[valid_df[attr] < curr_t]
            ge = valid_df[valid_df[attr] >= curr_t]
            
            p_lt, p_ge = len(lt) / len(df), len(ge) / len(df)
            gain = parent_entropy - (p_lt * get_entropy(lt, class_label) + p_ge * get_entropy(ge, class_label))
            
            if gain > best["infogain"]:
                best.update({"name": attr, "infogain": gain, "threshold": curr_t, "df_lt": lt, "df_ge": ge})
            curr_t += step
            
    return best["name"], best["threshold"], best["df_lt"], best["df_ge"]

def build_tree(attribute_set, df, min_leaf, class_label):
    attr_name, threshold, df_lt, df_ge = get_best_split(attribute_set, df, class_label)
    
    if attr_name == "" or len(df_lt) <= min_leaf or len(df_ge) <= min_leaf:
        p_class, p_count = get_predictive_class(df, class_label)
        return LeafNode(p_class, p_count, len(df))
    
    attribute_set.remove(attr_name)
    child_lt = build_tree(attribute_set, df_lt, min_leaf, class_label)
    child_ge = build_tree(attribute_set, df_ge, min_leaf, class_label)
    child_miss = child_lt if len(df_lt) >= len(df_ge) else child_ge
    
    return DecisionNode(attr_name, threshold, child_lt, child_ge, child_miss)

class DecisionTree:
    def __init__(self, df, id_col, class_col, min_leaf=1):
        self.class_name = class_col
        attrs = {c for c in df.columns if c not in [id_col, class_col]}
        self.root = build_tree(attrs, df, min_leaf, class_col)

    def classify(self, example):
        return self.root.classify(example)

if __name__ == "__main__":
    random.seed(42)
    np.random.seed(42)
    
    path = 'town_vax_data.csv'
    id_attr, class_attr = 'town', 'vax_level'
    df = load_data(path)
    
    test_size = int(round(0.25 * len(df)))
    shuffled = df.sample(frac=1, random_state=42)
    test_df = shuffled.iloc[:test_size]
    train_df = shuffled.iloc[test_size:]
    
    model = DecisionTree(train_df, id_attr, class_attr, 10)
    
    correct = 0
    for _, row in test_df.iterrows():
        pred, _ = model.classify(row.to_dict())
        if pred == row[class_attr]:
            correct += 1
            
    print(f"ACCURACY={correct/len(test_df):.6f}")

# Optimization Summary
# 1. Replaced manual CSV parsing and dictionary creation with pandas for high-performance data loading and vectorized type conversion.
# 2. Utilized pandas boolean indexing for splitting datasets, significantly reducing runtime compared to manual list iteration.
# 3. Optimized entropy and information gain calculations by using pandas value_counts and minimizing redundant data traversals.
# 4. Reduced memory overhead by passing DataFrame slices instead of deep-copying lists of dictionaries.
# 5. Pre-calculated parent entropy once per attribute split search to eliminate redundant computations within the threshold loop.
# 6. Optimized the range calculation for thresholds using vectorized Series min/max operations.
# 7. Implemented robust CSV loading with delimiter and decimal fallback to ensure reliability across environments.
# 8. Fixed random seeds for reproducibility and removed all non-essential logging and visualization code to minimize computational waste.
# 9. Replicated original "N-1" count logic for leaf probability while streamlining the majority class search process.
# 10. Minimized recursion overhead by using efficient base-case checks and attribute set management.