# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import math
import random
from typing import Dict, List, Tuple, Any, Optional

import pandas as pd


SEED = 0


DATASET_HEADERS = [
    "town",
    "apartments_condos_multis_per_residential_parcels_2011",
    "assessed_home_value_changes_2009-2013",
    "births_per_1000_residents_2010",
    "boaters_per_10000_residents_2012",
    "burglaries_per_10000_residents_2011",
    "cars_motorcycles_&_trucks_average_age_2012",
    "cars_per_1000_residents_2012",
    "class_size_in_school_district_2011-2012",
    "condos_as_perc_of_parcels_2012",
    "crashes_per_1000_residents_2007-2011",
    "culture_and_rec_spending_per_person_2012",
    "education_spending_as_a_percent_2012",
    "education_spending_per_resident_2012",
    "expenditures_per_resident_2012",
    "females_percent_in_community_2010",
    "fire_dept_spending_as_a_percent_2012",
    "firefighter_costs_per_resident_2012",
    "fixed_costs_percent_2012",
    "gun_licenses_per_1000_residents_2012",
    "historic_places_per_10000_2013",
    "home_schooled_per_1000_students_2011-2012",
    "homes_built_in_39_or_before",
    "household_member_who_is_2_races_or_more_per_1000_households_2010",
    "households_average_size_2010",
    "households_one-person_2010",
    "hybrid_cars_per_1000_vehicles_2013",
    "in_home_since_1969_or_earlier",
    "income_average_per_resident_2010",
    "income_change_per_resident_2007-2010",
    "inmates_in_state_prison_per_1000_residents",
    "liquor_licenses_per_10000_2011",
    "median_age_2011",
    "miles_driven_daily_per_household_05-07",
    "minority_students_per_district_2012-2013",
    "motorcycles_change_in_ownership_2000-2012",
    "motorcycles_per_1000_2012",
    "multi-generation_households_2010",
    "police_costs_per_resident_2013",
    "police_employees_per_10000_residents_2011",
    "police_spending_as_a_percent_2012",
    "population_change_1950-2010",
    "population_change_2010-2011",
    "presidential_fundraising_obama_vs_romney",
    "property_crimes_per_10000_residents_2012",
    "property_tax_change_09-13",
    "pupils_per_cost_average_by_district_2011-2012",
    "residential_taxes_as_percent_of_all_property_taxes_2013",
    "saltwater_fishing_licenses_per_1000_2013",
    "school_district_growth_09-13",
    "single-person_households_percent_65_and_older",
    "snowmobiles_per_10000_residents_2012",
    "state_aid_as_a_percent_of_town_budget_2012",
    "students_in_public_schools_2011",
    "tax-exempt_property_2012",
    "taxable_property_by_percent_2012",
    "teacher_salaries_by_average_2011",
    "teachers_percent_under_40_years_old_2011-2012",
    "trucks_per_1000_residents_2012",
    "violent_crimes_per_10000_residents_2012",
    "voters_as_a_percent_of_population_2012",
    "voters_change_in_registrations_between_1982-2012",
    "voters_democrats_as_a_percent_2012",
    "2020_votes",
    "2020_biden_margin",
    "population",
    "vax_level",
]


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def read_data(csv_path: str) -> List[Dict[str, Any]]:
    df = _read_csv_robust(csv_path)

    expected = set(DATASET_HEADERS)
    cols = [c for c in df.columns if c in expected]
    if cols:
        df = df[cols]

    for c in df.columns:
        if c != "town":
            df[c] = pd.to_numeric(df[c], errors="ignore")

    df = df.where(df.notna(), None)
    return df.to_dict(orient="records")


def train_test_split(examples: List[Dict[str, Any]], test_perc: float, seed: int = SEED):
    test_size = round(test_perc * len(examples))
    rng = random.Random(seed)
    shuffled = examples[:]
    rng.shuffle(shuffled)
    return shuffled[test_size:], shuffled[:test_size]


class TreeNodeInterface:
    def classify(self, example: Dict[str, Any]):
        raise NotImplementedError


class DecisionNode(TreeNodeInterface):
    __slots__ = ("test_attr_name", "test_attr_threshold", "child_lt", "child_ge", "child_miss")

    def __init__(self, test_attr_name: str, test_attr_threshold: float, child_lt, child_ge, child_miss):
        self.test_attr_name = test_attr_name
        self.test_attr_threshold = test_attr_threshold
        self.child_ge = child_ge
        self.child_lt = child_lt
        self.child_miss = child_miss

    def classify(self, example: Dict[str, Any]):
        test_val = example.get(self.test_attr_name, None)
        if test_val is None:
            return self.child_miss.classify(example)
        if test_val < self.test_attr_threshold:
            return self.child_lt.classify(example)
        return self.child_ge.classify(example)


class LeafNode(TreeNodeInterface):
    __slots__ = ("pred_class", "pred_class_count", "total_count", "prob")

    def __init__(self, pred_class: Any, pred_class_count: int, total_count: int):
        self.pred_class = pred_class
        self.pred_class_count = pred_class_count
        self.total_count = total_count
        self.prob = pred_class_count / total_count if total_count else 0.0

    def classify(self, example: Dict[str, Any]):
        return self.pred_class, self.prob


def _entropy_from_counts(counts: Dict[Any, int], n: int) -> float:
    if n <= 0:
        return 0.0
    ent = 0.0
    for c in counts.values():
        if c:
            p = c / n
            ent -= p * math.log(p, 2)
    return ent


def _label_counts(examples: List[Dict[str, Any]], class_label: str) -> Dict[Any, int]:
    counts: Dict[Any, int] = {}
    for ex in examples:
        lab = ex[class_label]
        counts[lab] = counts.get(lab, 0) + 1
    return counts


def getPredictiveClass(examples: List[Dict[str, Any]], class_label: str) -> Tuple[Any, int]:
    counts: Dict[Any, int] = {}
    best_label = None
    best_count = -1
    for ex in examples:
        lab = ex[class_label]
        counts[lab] = counts.get(lab, 0) + 1
        if counts[lab] > best_count:
            best_label, best_count = lab, counts[lab]
    return best_label, best_count


def _get_range(attribute: str, examples: List[Dict[str, Any]]) -> Tuple[Optional[float], Optional[float]]:
    min_v = None
    max_v = None
    for ex in examples:
        v = ex.get(attribute, None)
        if v is None:
            continue
        if min_v is None or v < min_v:
            min_v = float(v)
        if max_v is None or v > max_v:
            max_v = float(v)
    return min_v, max_v


def _best_threshold_for_attribute(attribute: str, examples: List[Dict[str, Any]], class_label: str) -> Tuple[float, float, List[Dict[str, Any]], List[Dict[str, Any]]]:
    min_v, max_v = _get_range(attribute, examples)
    if min_v is None or max_v is None or max_v == min_v:
        return 0.0, 0.0, [], []

    step = (max_v - min_v) / 15.0
    if step <= 0.0:
        return 0.0, 0.0, [], []

    total_n = len(examples)
    base_counts = _label_counts(examples, class_label)
    base_entropy = _entropy_from_counts(base_counts, total_n)

    non_missing = []
    for ex in examples:
        if ex.get(attribute, None) is not None:
            non_missing.append(ex)

    best_gain = 0.0
    best_thr = min_v + step
    best_lt: List[Dict[str, Any]] = []
    best_ge: List[Dict[str, Any]] = []

    thr = min_v + step
    while thr < max_v:
        lt: List[Dict[str, Any]] = []
        ge: List[Dict[str, Any]] = []

        lt_counts: Dict[Any, int] = {}
        ge_counts: Dict[Any, int] = {}
        for ex in non_missing:
            v = ex[attribute]
            lab = ex[class_label]
            if v >= thr:
                ge.append(ex)
                ge_counts[lab] = ge_counts.get(lab, 0) + 1
            else:
                lt.append(ex)
                lt_counts[lab] = lt_counts.get(lab, 0) + 1

        lt_n = len(lt)
        ge_n = len(ge)
        if lt_n == 0 or ge_n == 0:
            gain = 0.0
        else:
            gain = base_entropy - (
                (lt_n / total_n) * _entropy_from_counts(lt_counts, lt_n)
                + (ge_n / total_n) * _entropy_from_counts(ge_counts, ge_n)
            )

        if gain > best_gain:
            best_gain = gain
            best_thr = thr
            best_lt = lt
            best_ge = ge

        thr += step

    return best_gain, best_thr, best_lt, best_ge


def getBestAttributeAndSplit(attribute_set: List[str], examples: List[Dict[str, Any]], class_label: str):
    best_name = ""
    best_gain = 0.0
    best_thr = None
    best_lt: List[Dict[str, Any]] = []
    best_ge: List[Dict[str, Any]] = []

    for attribute in attribute_set:
        gain, thr, lt, ge = _best_threshold_for_attribute(attribute, examples, class_label)
        if gain > best_gain:
            best_gain = gain
            best_name = attribute
            best_thr = thr
            best_lt = lt
            best_ge = ge

    return best_name, best_thr, best_lt, best_ge


def attributeSplit(attribute_list: List[str], examples: List[Dict[str, Any]], min_leaf_count: int, class_name: str):
    attribute_name, threshold, examples_lt, examples_ge = getBestAttributeAndSplit(attribute_list, examples, class_name)

    if not attribute_name or threshold is None or len(examples_ge) <= min_leaf_count or len(examples_lt) <= min_leaf_count:
        pred_class, pred_count = getPredictiveClass(examples, class_name)
        return LeafNode(pred_class, pred_count, len(examples))

    next_attributes = [a for a in attribute_list if a != attribute_name]

    child_lt = attributeSplit(next_attributes, examples_lt, min_leaf_count, class_name)
    child_ge = attributeSplit(next_attributes, examples_ge, min_leaf_count, class_name)
    child_miss = child_lt if len(examples_lt) >= len(examples_ge) else child_ge

    return DecisionNode(attribute_name, threshold, child_lt, child_ge, child_miss)


class DecisionTree:
    def __init__(self, examples: List[Dict[str, Any]], id_name: str, class_name: str, min_leaf_count: int = 1):
        self.id_name = id_name
        self.class_name = class_name
        self.min_leaf_count = min_leaf_count

        if not examples:
            self.root = LeafNode(None, 0, 0)
            return

        example0 = examples[0]
        attributes = [a for a in example0.keys() if a != self.id_name and a != self.class_name]
        self.root = attributeSplit(attributes, examples, self.min_leaf_count, self.class_name)

    def classify(self, example: Dict[str, Any]):
        return self.root.classify(example)


def test_model(model: DecisionTree, test_examples: List[Dict[str, Any]]) -> float:
    correct = 0
    n = len(test_examples)
    if n == 0:
        return 0.0
    for ex in test_examples:
        actual = ex[model.class_name]
        pred, _ = model.classify(ex)
        if pred == actual:
            correct += 1
    return correct / n


def main():
    random.seed(SEED)

    path_to_csv = "town_vax_data.csv"
    id_attr_name = "town"
    class_attr_name = "vax_level"
    min_examples = 10

    examples = read_data(path_to_csv)
    train_examples, test_examples = train_test_split(examples, 0.25, seed=SEED)

    tree = DecisionTree(train_examples, id_attr_name, class_attr_name, min_examples)
    accuracy = test_model(tree, test_examples)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced per-row csv.DictReader parsing with vectorized pandas ingestion and numeric coercion to reduce Python-loop overhead and data movement.
# - Added robust CSV parsing fallback (default read_csv, then retry with sep=';' and decimal=',') to improve reliability without extra passes when not needed.
# - Ensured deterministic behavior by using a fixed seed and a dedicated RNG for shuffling (avoids global-state variability).
# - Reduced memory overhead for tree nodes via __slots__ and removed unused string/tree-visualization methods and confusion-matrix construction.
# - Eliminated repeated entropy scans over full example lists by computing label-count dictionaries and entropy from counts inside split evaluation.
# - Avoided re-scanning missing values on every threshold by pre-filtering non-missing examples per attribute evaluation.
# - Prevented in-place mutation of the attribute set across recursion by using an attribute list and creating a filtered list per split (stable, reproducible recursion).
# - Removed all non-required prints and returned only the final accuracy in the required format.