# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import csv
import math
import random
from collections import Counter
from dataclasses import dataclass
from typing import Dict, List, Optional, Sequence, Tuple


def read_data(csv_path: str) -> List[Dict[str, object]]:
    examples: List[Dict[str, object]] = []
    with open(csv_path, "r", newline="") as csv_file:
        reader = csv.DictReader(csv_file)
        for row in reader:
            ex: Dict[str, object] = {}
            for k, v in row.items():
                if v == "":
                    ex[k] = None
                else:
                    try:
                        ex[k] = float(v)
                    except ValueError:
                        ex[k] = v
            examples.append(ex)
    return examples


def train_test_split(examples: Sequence[Dict[str, object]], test_perc: float, seed: int = 0) -> Tuple[List[Dict[str, object]], List[Dict[str, object]]]:
    rng = random.Random(seed)
    idx = list(range(len(examples)))
    rng.shuffle(idx)
    test_size = round(test_perc * len(examples))
    test_idx = idx[:test_size]
    train_idx = idx[test_size:]
    train = [examples[i] for i in train_idx]
    test = [examples[i] for i in test_idx]
    return train, test


class TreeNodeInterface:
    def classify(self, example: Dict[str, object]) -> Tuple[object, float]:
        raise NotImplementedError


@dataclass(frozen=True, slots=True)
class LeafNode(TreeNodeInterface):
    pred_class: object
    pred_class_count: int
    total_count: int
    prob: float

    @classmethod
    def from_examples(cls, examples: Sequence[Dict[str, object]], class_name: str) -> "LeafNode":
        counts = Counter(ex[class_name] for ex in examples)
        pred_class, pred_count = counts.most_common(1)[0]
        total = len(examples)
        return cls(pred_class=pred_class, pred_class_count=pred_count, total_count=total, prob=pred_count / total)

    def classify(self, example: Dict[str, object]) -> Tuple[object, float]:
        return self.pred_class, self.prob


@dataclass(frozen=True, slots=True)
class DecisionNode(TreeNodeInterface):
    test_attr_name: str
    test_attr_threshold: float
    child_lt: TreeNodeInterface
    child_ge: TreeNodeInterface
    child_miss: TreeNodeInterface

    def classify(self, example: Dict[str, object]) -> Tuple[object, float]:
        v = example.get(self.test_attr_name, None)
        if v is None:
            return self.child_miss.classify(example)
        if v < self.test_attr_threshold:
            return self.child_lt.classify(example)
        return self.child_ge.classify(example)


class DecisionTree:
    def __init__(self, examples: List[Dict[str, object]], id_name: str, class_name: str, min_leaf_count: int = 1):
        self.id_name = id_name
        self.class_name = class_name
        self.min_leaf_count = min_leaf_count

        if not examples:
            raise ValueError("No training examples provided.")

        attrs = [a for a in examples[0].keys() if a not in (self.id_name, self.class_name)]
        self._attrs = tuple(attrs)
        self.root = self._learn_tree(examples, self._attrs)

    def classify(self, example: Dict[str, object]) -> Tuple[object, float]:
        return self.root.classify(example)

    def _learn_tree(self, examples: List[Dict[str, object]], attrs: Sequence[str]) -> TreeNodeInterface:
        if len(examples) <= self.min_leaf_count or not attrs:
            return LeafNode.from_examples(examples, self.class_name)

        best = _best_split(attrs, examples, self.class_name)
        if best is None:
            return LeafNode.from_examples(examples, self.class_name)

        attr, thr, lt, ge = best
        if len(lt) <= self.min_leaf_count or len(ge) <= self.min_leaf_count:
            return LeafNode.from_examples(examples, self.class_name)

        remaining = tuple(a for a in attrs if a != attr)
        child_lt = self._learn_tree(lt, remaining)
        child_ge = self._learn_tree(ge, remaining)
        child_miss = child_lt if len(lt) >= len(ge) else child_ge
        return DecisionNode(attr, thr, child_lt, child_ge, child_miss)


def _entropy_from_counts(counts: Counter) -> float:
    total = sum(counts.values())
    if total == 0:
        return 0.0
    inv_total = 1.0 / total
    ent = 0.0
    for c in counts.values():
        p = c * inv_total
        if p:
            ent -= p * math.log2(p)
    return ent


def _best_split(attrs: Sequence[str], examples: Sequence[Dict[str, object]], class_name: str) -> Optional[Tuple[str, float, List[Dict[str, object]], List[Dict[str, object]]]]:
    parent_counts = Counter(ex[class_name] for ex in examples)
    base_entropy = _entropy_from_counts(parent_counts)
    if base_entropy == 0.0:
        return None

    n_total = len(examples)
    best_gain = 0.0
    best_attr = None
    best_thr = None
    best_lt: List[Dict[str, object]] = []
    best_ge: List[Dict[str, object]] = []

    for attr in attrs:
        vals = [ex[attr] for ex in examples if ex.get(attr, None) is not None]
        if not vals:
            continue
        min_v = min(vals)
        max_v = max(vals)
        if min_v == max_v:
            continue

        step = (max_v - min_v) / 15.0
        if step <= 0.0:
            continue

        cur = min_v + step
        while cur < max_v:
            lt: List[Dict[str, object]] = []
            ge: List[Dict[str, object]] = []
            lt_counts: Counter = Counter()
            ge_counts: Counter = Counter()

            for ex in examples:
                v = ex.get(attr, None)
                if v is None:
                    continue
                lab = ex[class_name]
                if v < cur:
                    lt.append(ex)
                    lt_counts[lab] += 1
                else:
                    ge.append(ex)
                    ge_counts[lab] += 1

            n_lt = len(lt)
            n_ge = len(ge)
            if n_lt == 0 or n_ge == 0:
                cur += step
                continue

            gain = base_entropy - ((n_lt / n_total) * _entropy_from_counts(lt_counts) + (n_ge / n_total) * _entropy_from_counts(ge_counts))
            if gain > best_gain:
                best_gain = gain
                best_attr = attr
                best_thr = cur
                best_lt = lt
                best_ge = ge

            cur += step

    if best_attr is None:
        return None
    return best_attr, float(best_thr), best_lt, best_ge


def accuracy_score(model: DecisionTree, test_examples: Sequence[Dict[str, object]]) -> float:
    correct = 0
    n = len(test_examples)
    if n == 0:
        return 0.0
    for ex in test_examples:
        pred, _ = model.classify(ex)
        if pred == ex[model.class_name]:
            correct += 1
    return correct / n


def main() -> None:
    path_to_csv = "town_vax_data.csv"
    id_attr_name = "town"
    class_attr_name = "vax_level"
    min_examples = 10

    examples = read_data(path_to_csv)
    train_examples, test_examples = train_test_split(examples, 0.25, seed=0)
    tree = DecisionTree(train_examples, id_attr_name, class_attr_name, min_examples)
    accuracy = accuracy_score(tree, test_examples)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Replaced random.sample(examples, len(examples)) with index-based shuffle using a seeded RNG to avoid copying full dict objects and ensure reproducibility.
# - Removed confusion matrix generation, per-example printing, and tree ASCII rendering to eliminate heavy I/O and string building while preserving the ML task and final metric.
# - Converted recursive attribute_set mutations to an immutable tuple of remaining attributes per node to avoid side effects and repeated set allocations.
# - Computed parent entropy once per node and reused it for all candidate splits to avoid redundant entropy work.
# - Eliminated building class->list buckets in entropy; used Counter-based counts to reduce memory and object churn.
# - During threshold evaluation, accumulated class counts while splitting, avoiding multiple passes and extra intermediate structures beyond required split lists.
# - Used dataclasses with slots for nodes to reduce per-node memory overhead and improve attribute access efficiency.