# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import csv
import math
import random
from typing import Dict, List, Tuple, Any, Optional


SEED = 42


def _try_read_csv_dicts(path: str, delimiter: str = ",", decimal: str = ".") -> List[Dict[str, Any]]:
    examples: List[Dict[str, Any]] = []
    with open(path, "r", newline="") as f:
        reader = csv.DictReader(f, delimiter=delimiter)
        for row in reader:
            ex: Dict[str, Any] = {}
            for k, v in row.items():
                if v is None or v == "":
                    ex[k] = None
                    continue
                vv = v
                if decimal != ".":
                    vv = vv.replace(decimal, ".")
                try:
                    ex[k] = float(vv)
                except ValueError:
                    ex[k] = v
            examples.append(ex)
    return examples


def read_data(csv_path: str, expected_headers: List[str]) -> List[Dict[str, Any]]:
    examples = _try_read_csv_dicts(csv_path, delimiter=",", decimal=".")
    if not examples:
        examples = _try_read_csv_dicts(csv_path, delimiter=";", decimal=",")
        return examples

    keys = set(examples[0].keys())
    expected = set(expected_headers)

    looks_wrong = (
        (len(keys) <= 2)
        or (len(keys & expected) < max(3, min(10, len(expected) // 5)))
        or any((";" in k) for k in keys)
    )
    if looks_wrong:
        examples = _try_read_csv_dicts(csv_path, delimiter=";", decimal=",")
    return examples


def train_test_split(examples: List[Dict[str, Any]], test_perc: float, seed: int = SEED):
    rng = random.Random(seed)
    shuffled = examples[:]
    rng.shuffle(shuffled)
    test_size = round(test_perc * len(shuffled))
    return shuffled[test_size:], shuffled[:test_size]


class TreeNodeInterface:
    def classify(self, example: Dict[str, Any]):
        raise NotImplementedError


class DecisionNode(TreeNodeInterface):
    __slots__ = ("test_attr_name", "test_attr_threshold", "child_lt", "child_ge", "child_miss")

    def __init__(self, test_attr_name: str, test_attr_threshold: float, child_lt, child_ge, child_miss):
        self.test_attr_name = test_attr_name
        self.test_attr_threshold = test_attr_threshold
        self.child_ge = child_ge
        self.child_lt = child_lt
        self.child_miss = child_miss

    def classify(self, example: Dict[str, Any]):
        test_val = example.get(self.test_attr_name)
        if test_val is None:
            return self.child_miss.classify(example)
        if test_val < self.test_attr_threshold:
            return self.child_lt.classify(example)
        return self.child_ge.classify(example)


class LeafNode(TreeNodeInterface):
    __slots__ = ("pred_class", "pred_class_count", "total_count", "prob")

    def __init__(self, pred_class: Any, pred_class_count: int, total_count: int):
        self.pred_class = pred_class
        self.pred_class_count = pred_class_count
        self.total_count = total_count
        self.prob = pred_class_count / total_count if total_count else 0.0

    def classify(self, example: Dict[str, Any]):
        return self.pred_class, self.prob


def entropy_from_counts(counts: Dict[Any, int], total: int) -> float:
    if total <= 0:
        return 0.0
    s = 0.0
    inv_total = 1.0 / total
    for c in counts.values():
        if c:
            p = c * inv_total
            s -= p * math.log(p, 2)
    return s


def majority_class_and_count(examples: List[Dict[str, Any]], class_label: str) -> Tuple[Any, int]:
    counts: Dict[Any, int] = {}
    best_label = None
    best_count = -1
    for ex in examples:
        lab = ex[class_label]
        newc = counts.get(lab, 0) + 1
        counts[lab] = newc
        if newc > best_count:
            best_label, best_count = lab, newc
    return best_label, best_count


def _minmax_nonmissing(examples: List[Dict[str, Any]], attr: str) -> Tuple[Optional[float], Optional[float]]:
    mn = None
    mx = None
    for ex in examples:
        v = ex.get(attr)
        if v is None:
            continue
        if mn is None or v < mn:
            mn = v
        if mx is None or v > mx:
            mx = v
    return mn, mx


def best_attribute_split(
    attribute_set: List[str],
    examples: List[Dict[str, Any]],
    class_label: str,
    base_entropy: float,
    base_counts: Dict[Any, int],
) -> Tuple[str, float, List[Dict[str, Any]], List[Dict[str, Any]]]:
    n_total = len(examples)
    best_attr = attribute_set[0]
    best_gain = -1.0
    best_thr = 0.0
    best_lt: List[Dict[str, Any]] = []
    best_ge: List[Dict[str, Any]] = []

    for attr in attribute_set:
        mn, mx = _minmax_nonmissing(examples, attr)
        if mn is None or mx is None or mn == mx:
            continue

        step = (mx - mn) / 15.0
        if step <= 0.0:
            continue

        thr = mn + step
        while thr < mx:
            lt_counts: Dict[Any, int] = {}
            ge_counts: Dict[Any, int] = {}
            lt_total = 0
            ge_total = 0
            lt_ex: List[Dict[str, Any]] = []
            ge_ex: List[Dict[str, Any]] = []

            for ex in examples:
                v = ex.get(attr)
                if v is None:
                    continue
                if v >= thr:
                    ge_ex.append(ex)
                    ge_total += 1
                    lab = ex[class_label]
                    ge_counts[lab] = ge_counts.get(lab, 0) + 1
                else:
                    lt_ex.append(ex)
                    lt_total += 1
                    lab = ex[class_label]
                    lt_counts[lab] = lt_counts.get(lab, 0) + 1

            if lt_total == 0 or ge_total == 0:
                thr += step
                continue

            pc1 = lt_total / n_total
            pc2 = ge_total / n_total
            gain = base_entropy - (
                pc1 * entropy_from_counts(lt_counts, lt_total) + pc2 * entropy_from_counts(ge_counts, ge_total)
            )

            if gain > best_gain:
                best_gain = gain
                best_attr = attr
                best_thr = thr
                best_lt = lt_ex
                best_ge = ge_ex

            thr += step

    return best_attr, best_thr, best_lt, best_ge


def attribute_split(
    attribute_list: List[str],
    examples: List[Dict[str, Any]],
    min_leaf_count: int,
    class_name: str,
) -> TreeNodeInterface:
    pred_class, pred_count = majority_class_and_count(examples, class_name)

    if len(examples) <= min_leaf_count or not attribute_list:
        return LeafNode(pred_class, pred_count, len(examples))

    base_counts: Dict[Any, int] = {}
    for ex in examples:
        lab = ex[class_name]
        base_counts[lab] = base_counts.get(lab, 0) + 1
    base_entropy = entropy_from_counts(base_counts, len(examples))

    attr_name, threshold, examples_lt, examples_ge = best_attribute_split(
        attribute_list, examples, class_name, base_entropy, base_counts
    )

    if not examples_lt or not examples_ge or len(examples_ge) <= min_leaf_count or len(examples_lt) <= min_leaf_count:
        return LeafNode(pred_class, pred_count, len(examples))

    next_attrs = [a for a in attribute_list if a != attr_name]

    child_lt = attribute_split(next_attrs, examples_lt, min_leaf_count, class_name)
    child_ge = attribute_split(next_attrs, examples_ge, min_leaf_count, class_name)
    child_miss = child_lt if len(examples_lt) >= len(examples_ge) else child_ge

    return DecisionNode(attr_name, threshold, child_lt, child_ge, child_miss)


class DecisionTree:
    def __init__(self, examples: List[Dict[str, Any]], id_name: str, class_name: str, min_leaf_count: int = 1):
        self.id_name = id_name
        self.class_name = class_name
        self.min_leaf_count = min_leaf_count

        if not examples:
            self.root = LeafNode(None, 0, 0)
            return

        example0 = examples[0]
        attributes = [a for a in example0.keys() if a != self.id_name and a != self.class_name]
        self.root = attribute_split(attributes, examples, self.min_leaf_count, self.class_name)

    def classify(self, example: Dict[str, Any]):
        return self.root.classify(example)


def test_model(model: DecisionTree, test_examples: List[Dict[str, Any]], label_ordering: List[str]) -> float:
    correct = 0
    idx = {lab: i for i, lab in enumerate(label_ordering)}
    _almost = 0
    for ex in test_examples:
        actual = ex[model.class_name]
        pred, _prob = model.classify(ex)
        if pred == actual:
            correct += 1
        if abs(idx[pred] - idx[actual]) < 2:
            _almost += 1
    return correct / len(test_examples) if test_examples else 0.0


def main():
    random.seed(SEED)

    path_to_csv = "town_vax_data.csv"
    id_attr_name = "town"
    class_attr_name = "vax_level"
    label_ordering = ["low", "medium", "high", "very high"]
    min_examples = 10

    expected_headers = [
        "town",
        "apartments_condos_multis_per_residential_parcels_2011",
        "assessed_home_value_changes_2009-2013",
        "births_per_1000_residents_2010",
        "boaters_per_10000_residents_2012",
        "burglaries_per_10000_residents_2011",
        "cars_motorcycles_&_trucks_average_age_2012",
        "cars_per_1000_residents_2012",
        "class_size_in_school_district_2011-2012",
        "condos_as_perc_of_parcels_2012",
        "crashes_per_1000_residents_2007-2011",
        "culture_and_rec_spending_per_person_2012",
        "education_spending_as_a_percent_2012",
        "education_spending_per_resident_2012",
        "expenditures_per_resident_2012",
        "females_percent_in_community_2010",
        "fire_dept_spending_as_a_percent_2012",
        "firefighter_costs_per_resident_2012",
        "fixed_costs_percent_2012",
        "gun_licenses_per_1000_residents_2012",
        "historic_places_per_10000_2013",
        "home_schooled_per_1000_students_2011-2012",
        "homes_built_in_39_or_before",
        "household_member_who_is_2_races_or_more_per_1000_households_2010",
        "households_average_size_2010",
        "households_one-person_2010",
        "hybrid_cars_per_1000_vehicles_2013",
        "in_home_since_1969_or_earlier",
        "income_average_per_resident_2010",
        "income_change_per_resident_2007-2010",
        "inmates_in_state_prison_per_1000_residents",
        "liquor_licenses_per_10000_2011",
        "median_age_2011",
        "miles_driven_daily_per_household_05-07",
        "minority_students_per_district_2012-2013",
        "motorcycles_change_in_ownership_2000-2012",
        "motorcycles_per_1000_2012",
        "multi-generation_households_2010",
        "police_costs_per_resident_2013",
        "police_employees_per_10000_residents_2011",
        "police_spending_as_a_percent_2012",
        "population_change_1950-2010",
        "population_change_2010-2011",
        "presidential_fundraising_obama_vs_romney",
        "property_crimes_per_10000_residents_2012",
        "property_tax_change_09-13",
        "pupils_per_cost_average_by_district_2011-2012",
        "residential_taxes_as_percent_of_all_property_taxes_2013",
        "saltwater_fishing_licenses_per_1000_2013",
        "school_district_growth_09-13",
        "single-person_households_percent_65_and_older",
        "snowmobiles_per_10000_residents_2012",
        "state_aid_as_a_percent_of_town_budget_2012",
        "students_in_public_schools_2011",
        "tax-exempt_property_2012",
        "taxable_property_by_percent_2012",
        "teacher_salaries_by_average_2011",
        "teachers_percent_under_40_years_old_2011-2012",
        "trucks_per_1000_residents_2012",
        "violent_crimes_per_10000_residents_2012",
        "voters_as_a_percent_of_population_2012",
        "voters_change_in_registrations_between_1982-2012",
        "voters_democrats_as_a_percent_2012",
        "2020_votes",
        "2020_biden_margin",
        "population",
        "vax_level",
    ]

    examples = read_data(path_to_csv, expected_headers)
    train_examples, test_examples = train_test_split(examples, 0.25, seed=SEED)

    tree = DecisionTree(train_examples, id_attr_name, class_attr_name, min_examples)
    accuracy = test_model(tree, test_examples, label_ordering)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Ensured reproducibility by using a fixed seed and a local RNG for shuffling to keep splits deterministic.
# - Implemented robust CSV parsing with automatic fallback to ';' separator and ',' decimal when the initial parse looks incorrect, avoiding manual edits.
# - Removed all per-example printing, confusion matrix generation, and tree visualization to reduce I/O overhead and runtime while preserving the final accuracy evaluation intent.
# - Reduced redundant computation in entropy/information gain by computing entropy from class-count dictionaries rather than building per-class example lists.
# - Avoided repeated list traversals where possible by computing majority class once per node and reusing it for leaf creation decisions.
# - Reduced memory footprint of tree nodes via __slots__ to avoid per-instance __dict__ overhead.
# - Minimized unnecessary data movement by shuffling a copy of the list in-place rather than creating an additional sampled list.