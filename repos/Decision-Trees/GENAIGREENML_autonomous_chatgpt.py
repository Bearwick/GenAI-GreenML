# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "town_vax_data.csv"
DATASET_HEADERS = [
    "town",
    "apartments_condos_multis_per_residential_parcels_2011",
    "assessed_home_value_changes_2009-2013",
    "births_per_1000_residents_2010",
    "boaters_per_10000_residents_2012",
    "burglaries_per_10000_residents_2011",
    "cars_motorcycles_&_trucks_average_age_2012",
    "cars_per_1000_residents_2012",
    "class_size_in_school_district_2011-2012",
    "condos_as_perc_of_parcels_2012",
    "crashes_per_1000_residents_2007-2011",
    "culture_and_rec_spending_per_person_2012",
    "education_spending_as_a_percent_2012",
    "education_spending_per_resident_2012",
    "expenditures_per_resident_2012",
    "females_percent_in_community_2010",
    "fire_dept_spending_as_a_percent_2012",
    "firefighter_costs_per_resident_2012",
    "fixed_costs_percent_2012",
    "gun_licenses_per_1000_residents_2012",
    "historic_places_per_10000_2013",
    "home_schooled_per_1000_students_2011-2012",
    "homes_built_in_39_or_before",
    "household_member_who_is_2_races_or_more_per_1000_households_2010",
    "households_average_size_2010",
    "households_one-person_2010",
    "hybrid_cars_per_1000_vehicles_2013",
    "in_home_since_1969_or_earlier",
    "income_average_per_resident_2010",
    "income_change_per_resident_2007-2010",
    "inmates_in_state_prison_per_1000_residents",
    "liquor_licenses_per_10000_2011",
    "median_age_2011",
    "miles_driven_daily_per_household_05-07",
    "minority_students_per_district_2012-2013",
    "motorcycles_change_in_ownership_2000-2012",
    "motorcycles_per_1000_2012",
    "multi-generation_households_2010",
    "police_costs_per_resident_2013",
    "police_employees_per_10000_residents_2011",
    "police_spending_as_a_percent_2012",
    "population_change_1950-2010",
    "population_change_2010-2011",
    "presidential_fundraising_obama_vs_romney",
    "property_crimes_per_10000_residents_2012",
    "property_tax_change_09-13",
    "pupils_per_cost_average_by_district_2011-2012",
    "residential_taxes_as_percent_of_all_property_taxes_2013",
    "saltwater_fishing_licenses_per_1000_2013",
    "school_district_growth_09-13",
    "single-person_households_percent_65_and_older",
    "snowmobiles_per_10000_residents_2012",
    "state_aid_as_a_percent_of_town_budget_2012",
    "students_in_public_schools_2011",
    "tax-exempt_property_2012",
    "taxable_property_by_percent_2012",
    "teacher_salaries_by_average_2011",
    "teachers_percent_under_40_years_old_2011-2012",
    "trucks_per_1000_residents_2012",
    "violent_crimes_per_10000_residents_2012",
    "voters_as_a_percent_of_population_2012",
    "voters_change_in_registrations_between_1982-2012",
    "voters_democrats_as_a_percent_2012",
    "2020_votes",
    "2020_biden_margin",
    "population",
    "vax_level",
]


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    df1 = pd.read_csv(path)
    # Heuristic: if only 1 column and it contains many separators, parsing likely wrong
    if df1.shape[1] <= 1:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        if df2.shape[1] > df1.shape[1]:
            return df2
    return df1


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _choose_target(df):
    # Prefer a known target name if present
    candidates = []
    for name in ["vax_level", "target", "label", "y"]:
        if name in df.columns:
            candidates.append(name)

    if candidates:
        return candidates[0]

    # Otherwise pick a reasonable last column if it's not constant and not an ID-like column
    non_id_cols = [c for c in df.columns if c.lower() not in ("town", "id", "index")]
    if non_id_cols:
        tail = non_id_cols[-1]
        if df[tail].nunique(dropna=True) > 1:
            return tail

    # Otherwise: pick any non-constant column, prefer categorical-like (object) with few unique values
    best = None
    best_score = None
    for c in df.columns:
        nun = df[c].nunique(dropna=True)
        if nun <= 1:
            continue
        if df[c].dtype == "object":
            score = (0, nun)  # prefer object first, then fewer uniques
        else:
            score = (1, nun)
        if best is None or score < best_score:
            best, best_score = c, score
    return best


def _safe_train_test_split(X, y, random_state=42):
    if len(X) < 4:
        return X, X, y, y

    # Try stratify when it is classification-like and feasible
    stratify = None
    try:
        if y.dtype == "object" or pd.api.types.is_bool_dtype(y) or pd.api.types.is_integer_dtype(y):
            nun = y.nunique(dropna=True)
            if nun >= 2 and nun <= max(2, int(0.25 * len(y))):
                vc = y.value_counts(dropna=True)
                if vc.min() >= 2:
                    stratify = y
    except Exception:
        stratify = None

    try:
        return train_test_split(X, y, test_size=0.2, random_state=random_state, stratify=stratify)
    except Exception:
        return train_test_split(X, y, test_size=0.2, random_state=random_state)


def _bounded_regression_accuracy(y_true, y_pred):
    # Convert regression goodness to [0,1] robustly without heavy metrics:
    # accuracy = 1 / (1 + normalized_mae), normalized by target IQR (fallback to std or 1).
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = np.nanmean(np.abs(y_true - y_pred))
    q75, q25 = np.nanpercentile(y_true, [75, 25])
    scale = q75 - q25
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = np.nanstd(y_true)
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = 1.0
    nmae = mae / scale
    acc = 1.0 / (1.0 + max(0.0, float(nmae)))
    if not np.isfinite(acc):
        acc = 0.0
    return float(np.clip(acc, 0.0, 1.0))


def main():
    if not os.path.exists(DATASET_PATH):
        # Minimal failure-safe path while still producing output
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # If header row was misread, try to recover by assigning provided headers when counts match
    if len(df.columns) == len(DATASET_HEADERS):
        # Only override if many names look like generic/unhelpful
        generic_count = sum(1 for c in df.columns if re.fullmatch(r"V?\d+", str(c).strip(), flags=re.IGNORECASE) is not None)
        if generic_count >= max(3, len(df.columns) // 4):
            df.columns = list(DATASET_HEADERS)

    # Drop fully empty rows
    df = df.dropna(axis=0, how="all")
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _choose_target(df)
    if target_col is None or target_col not in df.columns:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # Remove duplicate columns to avoid transformer issues
    if X.columns.duplicated().any():
        X = X.loc[:, ~X.columns.duplicated()].copy()

    # Identify column types
    # Coerce likely-numeric object columns to numeric when they look numeric-like
    X_work = X.copy()
    for c in X_work.columns:
        if X_work[c].dtype == "object":
            s = X_work[c].astype(str).str.replace(",", ".", regex=False).str.strip()
            # If a majority look numeric, coerce
            sample = s.head(min(len(s), 200))
            numeric_like = sample.str.fullmatch(r"[-+]?(\d+(\.\d*)?|\.\d+)([eE][-+]?\d+)?").mean()
            if pd.notna(numeric_like) and float(numeric_like) >= 0.8:
                X_work[c] = pd.to_numeric(s, errors="coerce")

    # Build column lists after coercion
    numeric_features = [c for c in X_work.columns if pd.api.types.is_numeric_dtype(X_work[c])]
    categorical_features = [c for c in X_work.columns if c not in numeric_features]

    # Prepare y and determine task type
    is_classification = False
    y = y_raw.copy()

    # If y is object, treat as classification
    if y.dtype == "object" or pd.api.types.is_bool_dtype(y):
        is_classification = True
    else:
        # Numeric target: classification if small number of unique integer-like labels
        y_num = pd.to_numeric(y, errors="coerce")
        nun = y_num.nunique(dropna=True)
        if nun >= 2 and nun <= 20:
            # if values are close to integers -> classification
            vals = y_num.dropna().values
            if vals.size > 0 and np.nanmean(np.abs(vals - np.round(vals))) < 1e-6:
                is_classification = True
                y = y_num.round().astype("Int64")
        if not is_classification:
            y = y_num

    # Drop rows with missing target
    valid_mask = ~pd.isna(y)
    X_work = X_work.loc[valid_mask].reset_index(drop=True)
    y = y.loc[valid_mask].reset_index(drop=True)

    assert len(X_work) > 0

    # If classification but too few classes, fallback to regression on numeric coercion if possible
    if is_classification:
        y_str = y.astype(str)
        classes = pd.Series(y_str).nunique(dropna=True)
        if classes < 2:
            is_classification = False
            y = pd.to_numeric(y_raw.loc[valid_mask].reset_index(drop=True), errors="coerce")
            # If still unusable, trivial baseline
            if y.isna().all():
                accuracy = 0.0
                print(f"ACCURACY={accuracy:.6f}")
                return
            y = y.fillna(y.median() if np.isfinite(y.median()) else 0.0)

    # Preprocess pipelines
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False)),  # with_mean=False can be cheaper/safer with sparse unions
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    X_train, X_test, y_train, y_test = _safe_train_test_split(X_work, y, random_state=42)
    assert len(X_train) > 0 and len(X_test) > 0

    if is_classification:
        # Use lightweight linear model for multiclass; saga handles sparse, but lbfgs is often faster for small data.
        # Choose solver based on sparsity risk (categoricals -> sparse).
        use_sparse = len(categorical_features) > 0
        clf = LogisticRegression(
            max_iter=200,
            solver="saga" if use_sparse else "lbfgs",
            n_jobs=1,
            multi_class="auto",
            C=1.0,
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train.astype(str) if y_train.dtype == "object" else y_train)
        preds = model.predict(X_test)
        # Ensure compatible types for accuracy
        y_true = y_test.astype(str) if y_test.dtype == "object" else y_test
        accuracy = float(accuracy_score(y_true, preds))
    else:
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        accuracy = _bounded_regression_accuracy(y_test, preds)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses CPU-friendly linear models (LogisticRegression/Ridge) to minimize compute and energy versus ensembles/deep nets.
# - Preprocessing is a single sklearn Pipeline + ColumnTransformer to avoid redundant passes and ensure reproducibility.
# - Robust CSV loading retries with ';' separator and ',' decimals to reduce failure cases without manual edits.
# - Column names are normalized and 'Unnamed:' columns dropped to prevent wasted features and parsing artifacts.
# - Object columns that look numeric are coerced to numeric to reduce one-hot dimensionality (saves memory/CPU).
# - Sparse one-hot encoding with handle_unknown='ignore' keeps memory low and avoids crashes on unseen categories.
# - For regression fallback, prints ACCURACY as a bounded proxy in [0,1]: 1/(1+normalized_MAE) normalized by IQR (or std fallback).