# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import sys
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression


def _find_target_column(df: pd.DataFrame) -> str:
    candidates = ["vax_level", "2020_votes", "2020_biden_margin"]
    for c in candidates:
        if c in df.columns:
            return c
    raise ValueError("No known target column found. Expected one of: vax_level, 2020_votes, 2020_biden_margin.")


def _load_dataset() -> pd.DataFrame:
    # Lightweight IO: try common local filenames, else read from stdin to support pipeline execution
    candidates = [
        "data.csv",
        "dataset.csv",
        "train.csv",
        "input.csv",
        "town_data.csv",
    ]
    for path in candidates:
        if os.path.exists(path) and os.path.isfile(path):
            return pd.read_csv(path)

    if not sys.stdin.isatty():
        return pd.read_csv(sys.stdin)

    raise FileNotFoundError(
        "Dataset file not found. Place CSV as one of: "
        + ", ".join(candidates)
        + " or pipe CSV via stdin."
    )


def _prepare_target(y: pd.Series):
    if y.dtype.kind in "bifc":
        y_num = pd.to_numeric(y, errors="coerce")
        y_non_na = y_num.dropna()
        unique_vals = np.unique(y_non_na.values)
        if unique_vals.size >= 2:
            if unique_vals.size > 20:
                med = float(np.nanmedian(y_num.values))
                y_bin = (y_num >= med).astype(int)
                return y_bin.fillna(y_bin.mode(dropna=True).iloc[0]).astype(int)
            else:
                y_filled = y_num.fillna(y_non_na.median() if y_non_na.size else 0)
                unique_vals2 = np.unique(y_filled.values)
                if unique_vals2.size == 2 and set(unique_vals2.tolist()) <= {0.0, 1.0}:
                    return y_filled.astype(int)
                # Small multi-class numeric target: cast to int labels (assumes discrete)
                if np.allclose(y_filled.values, np.round(y_filled.values)):
                    return np.round(y_filled.values).astype(int)
                # Otherwise, binarize around median for classification
                med = float(np.nanmedian(y_filled.values))
                return (y_filled >= med).astype(int)
        return y_num.fillna(0).astype(int)

    y_cat = y.astype("string").fillna("MISSING").str.strip()
    return y_cat


def main():
    df = _load_dataset()

    target_col = _find_target_column(df)
    y_raw = df[target_col]
    X = df.drop(columns=[target_col])

    y = _prepare_target(y_raw)

    numeric_cols = X.select_dtypes(include=["number", "bool"]).columns.tolist()
    categorical_cols = [c for c in X.columns if c not in numeric_cols]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    clf = LogisticRegression(
        solver="saga",
        max_iter=500,
        n_jobs=1,
        C=1.0,
        tol=1e-3,
        class_weight="balanced",
        random_state=42,
    )

    model = Pipeline(
        steps=[
            ("preprocess", preprocessor),
            ("clf", clf),
        ]
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y if pd.Series(y).nunique() > 1 else None,
    )

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Uses LogisticRegression (linear model) for strong baseline accuracy with low compute and memory footprint.
# - Solver 'saga' supports sparse one-hot features efficiently on CPU; OneHotEncoder emits sparse matrix.
# - SimpleImputer with median/most_frequent avoids costly iterative imputation; stable and fast.
# - ColumnTransformer + Pipeline ensures reproducible preprocessing without data leakage.
# - No deep learning, embeddings, or feature scaling to reduce compute; logistic regression handles varied feature scales reasonably.
# - max_iter and tol tuned to converge with fewer iterations; n_jobs=1 avoids extra CPU overhead and nondeterminism.