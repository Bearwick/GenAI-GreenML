# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    new_cols = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        new_cols.append(c2)
    return new_cols


def _read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(dfx):
        if dfx is None or dfx.shape[0] == 0 or dfx.shape[1] <= 1:
            return True
        # Heuristic: if most rows have NaN in many columns, parsing might be off; keep lightweight
        na_ratio = float(dfx.isna().mean().mean())
        return na_ratio > 0.85

    if _looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            # last resort: python engine with sep inference disabled
            df = pd.read_csv(path, engine="python")

    return df


def _drop_unnamed(df):
    cols = []
    for c in df.columns:
        if isinstance(c, str) and c.strip().lower().startswith("unnamed:"):
            continue
        cols.append(c)
    return df[cols]


def _coerce_numeric_inplace(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _choose_target(df, preferred_targets):
    # Prefer known target if present
    for t in preferred_targets:
        if t in df.columns:
            return t

    # Otherwise choose best non-constant numeric column
    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    best = None
    best_var = -1.0
    for c in num_cols:
        s = df[c]
        s2 = s.replace([np.inf, -np.inf], np.nan).dropna()
        if s2.size < 10:
            continue
        v = float(s2.nunique(dropna=True))
        if v > best_var:
            best_var = v
            best = c
    return best


def _infer_task(y):
    # classification if object/category/bool or low-cardinality integer-like
    if y is None:
        return "regression"
    if pd.api.types.is_bool_dtype(y) or pd.api.types.is_categorical_dtype(y) or pd.api.types.is_object_dtype(y):
        return "classification"
    # numeric: treat as classification if few unique values relative to rows
    y2 = pd.to_numeric(y, errors="coerce").replace([np.inf, -np.inf], np.nan).dropna()
    if y2.size == 0:
        return "regression"
    nun = int(y2.nunique())
    if nun <= 20 and nun <= max(2, int(0.05 * y2.size)):
        return "classification"
    return "regression"


def _bounded_regression_score(y_true, y_pred):
    # Bounded [0,1] proxy derived from normalized MAE: score = 1 - MAE/(MAE + IQR)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = float(np.mean(np.abs(y_true - y_pred))) if y_true.size else 0.0
    q75, q25 = np.nanpercentile(y_true, 75), np.nanpercentile(y_true, 25)
    iqr = float(q75 - q25)
    scale = iqr if np.isfinite(iqr) and iqr > 1e-12 else float(np.nanstd(y_true))
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = 1.0
    score = 1.0 - (mae / (mae + scale))
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    dataset_path = "town_vax_data.csv"

    df = _read_csv_robust(dataset_path)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Drop completely empty rows
    df = df.dropna(how="all")
    assert df.shape[0] > 0 and df.shape[1] > 0

    # Use provided headers as soft guidance (do not hard-fail if mismatch)
    preferred_target_candidates = ["vax_level", "vax level", "vax_level ", "vaxlevel"]

    target_col = _choose_target(df, preferred_target_candidates)
    if target_col is None:
        # Fallback: create a trivial target from first column to keep end-to-end; will be handled as classification
        target_col = df.columns[-1]

    # Feature columns: all except target
    feature_cols = [c for c in df.columns if c != target_col]
    if len(feature_cols) == 0:
        # fallback: use target as featureless problem; we'll make a dummy feature column
        df["_dummy_feature"] = 0
        feature_cols = ["_dummy_feature"]

    # Coerce numeric-looking object columns to numeric when possible (lightweight heuristic)
    obj_cols = [c for c in feature_cols if pd.api.types.is_object_dtype(df[c])]
    for c in obj_cols:
        s_num = pd.to_numeric(df[c], errors="coerce")
        # If most values convert, keep numeric to avoid large one-hot
        if float(s_num.notna().mean()) >= 0.9:
            df[c] = s_num

    # Ensure target is clean for task inference
    y_raw = df[target_col]

    task = _infer_task(y_raw)

    # Build preprocessing lists
    X = df[feature_cols].copy()

    # Replace inf
    for c in X.columns:
        if pd.api.types.is_numeric_dtype(X[c]):
            X[c] = X[c].replace([np.inf, -np.inf], np.nan)

    # Determine columns by dtype
    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Prepare y
    if task == "classification":
        y = y_raw.astype("string").fillna("MISSING")
        # If too many unique labels, fallback to regression on numeric-coerced target
        nun = int(y.nunique(dropna=False))
        if nun < 2:
            task = "regression"
        elif nun > max(50, int(0.5 * len(y))):
            # likely identifier-like; prefer regression fallback if numeric possible
            y_num = pd.to_numeric(y_raw, errors="coerce").replace([np.inf, -np.inf], np.nan)
            if int(y_num.dropna().nunique()) >= 2:
                task = "regression"
                y = y_num
            else:
                # keep classification but will behave like trivial baseline
                pass

    if task == "regression":
        y = pd.to_numeric(y_raw, errors="coerce").replace([np.inf, -np.inf], np.nan)
        # If target is almost entirely missing, fallback to a constant
        if float(y.notna().mean()) < 0.5:
            y = y.fillna(0.0)
        else:
            y = y.fillna(y.median())

    # Split (stratify only if classification and feasible)
    stratify = None
    if task == "classification":
        try:
            if int(pd.Series(y).nunique()) >= 2:
                stratify = y
        except Exception:
            stratify = None

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=stratify,
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    if task == "classification":
        # Lightweight linear classifier; saga handles sparse one-hot efficiently on CPU
        clf = LogisticRegression(
            solver="saga",
            max_iter=300,
            n_jobs=1,
            C=1.0,
            tol=1e-3,
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_score(np.asarray(y_test, dtype=float), np.asarray(y_pred, dtype=float))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used robust CSV parsing with a lightweight fallback (semicolon separator, comma decimals) to avoid expensive retries.
# - Column normalization/dropping 'Unnamed' reduces wasted preprocessing and prevents schema brittleness.
# - Prefer simple linear models (LogisticRegression / Ridge) for CPU efficiency and good baselines on tabular data.
# - OneHotEncoder(handle_unknown='ignore') + sparse_output keeps memory/compute low for categorical features.
# - StandardScaler applied only to numeric columns; minimal feature engineering to reduce overhead.
# - Defensive target selection: prefer provided 'vax_level'; otherwise pick a non-constant numeric column to keep pipeline runnable.
# - If regression is used, reported ACCURACY is a bounded [0,1] proxy: 1 - MAE/(MAE + IQR) for stability across scales.