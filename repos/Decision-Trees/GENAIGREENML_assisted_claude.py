# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import csv
import random
import math

def read_data(csv_path):
    examples = []
    with open(csv_path, 'r') as csv_file:
        csv_reader = csv.DictReader(csv_file)
        for example in csv_reader:
            for k, v in example.items():
                if v == '':
                    example[k] = None
                else:
                    try:
                        example[k] = float(v)
                    except ValueError:
                        example[k] = v
            examples.append(example)
    return examples

def train_test_split(examples, test_perc):
    test_size = round(test_perc * len(examples))
    shuffled = random.sample(examples, len(examples))
    return shuffled[test_size:], shuffled[:test_size]

class LeafNode:
    __slots__ = ('pred_class', 'pred_class_count', 'total_count', 'prob')

    def __init__(self, pred_class, pred_class_count, total_count):
        self.pred_class = pred_class
        self.pred_class_count = pred_class_count
        self.total_count = total_count
        self.prob = pred_class_count / total_count

    def classify(self, example):
        return self.pred_class, self.prob

class DecisionNode:
    __slots__ = ('test_attr_name', 'test_attr_threshold', 'child_lt', 'child_ge', 'child_miss')

    def __init__(self, test_attr_name, test_attr_threshold, child_lt, child_ge, child_miss):
        self.test_attr_name = test_attr_name
        self.test_attr_threshold = test_attr_threshold
        self.child_ge = child_ge
        self.child_lt = child_lt
        self.child_miss = child_miss

    def classify(self, example):
        test_val = example[self.test_attr_name]
        if test_val is None:
            return self.child_miss.classify(example)
        elif test_val < self.test_attr_threshold:
            return self.child_lt.classify(example)
        else:
            return self.child_ge.classify(example)

def entropy(examples, class_label):
    counts = {}
    for ex in examples:
        c = ex[class_label]
        counts[c] = counts.get(c, 0) + 1
    n = len(examples)
    s = 0.0
    log2 = math.log2
    for count in counts.values():
        p = count / n
        if p > 0:
            s -= p * log2(p)
    return s

def get_predictive_class(examples, class_label):
    counts = {}
    for ex in examples:
        c = ex[class_label]
        counts[c] = counts.get(c, 0) + 1
    best = max(counts, key=counts.get)
    return best, counts[best]

def get_sorted_unique_values(attribute, examples):
    vals = set()
    for ex in examples:
        v = ex[attribute]
        if v is not None:
            vals.add(v)
    return sorted(vals)

def find_best_split(attribute_set, examples, class_label):
    n = len(examples)
    parent_entropy = entropy(examples, class_label)
    best_gain = 0.0
    best_attr = None
    best_threshold = None
    best_lt = None
    best_ge = None

    for attribute in attribute_set:
        sorted_vals = get_sorted_unique_values(attribute, examples)
        if len(sorted_vals) < 2:
            continue

        valid = [ex for ex in examples if ex[attribute] is not None]
        valid.sort(key=lambda ex: ex[attribute])
        n_valid = len(valid)
        if n_valid == 0:
            continue