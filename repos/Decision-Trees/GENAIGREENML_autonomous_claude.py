# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("dataset.csv")

# Target variable is 'vax_level' - appears to be a classification target
target_col = "vax_level"

# Separate features and target
y = df[target_col].copy()
X = df.drop(columns=[target_col])

# Drop non-numeric identifier columns (town names)
if "town" in X.columns:
    X = X.drop(columns=["town"])

# Drop columns that are leaky or not useful predictors
# Keep all remaining numeric features

# Convert all columns to numeric, coercing errors
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors="coerce")

# Encode target if it's categorical
if y.dtype == object or isinstance(y.iloc[0], str):
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y = le.fit_transform(y)
else:
    y = y.values

# Build a lightweight, energy-efficient pipeline
# SimpleImputer handles missing values; StandardScaler normalizes features
# GradientBoosting with limited depth/estimators is efficient and strong for tabular data
pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler()),
    ("clf", GradientBoostingClassifier(
        n_estimators=200,
        max_depth=4,
        learning_rate=0.1,
        subsample=0.8,
        min_samples_leaf=5,
        random_state=42
    ))
])

# Evaluate with stratified 5-fold cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring="accuracy")

# Train final model on full data and report accuracy as mean CV accuracy
accuracy = cv_scores.mean()

# Also fit on full data for completeness
pipeline.fit(X, y)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Used GradientBoostingClassifier - strong for small tabular datasets, CPU-efficient
# 2. Limited n_estimators=200 and max_depth=4 to avoid overfitting and reduce computation
# 3. subsample=0.8 reduces training cost per iteration (stochastic gradient boosting)
# 4. SimpleImputer with median strategy handles missing values robustly without extra computation
# 5. StandardScaler ensures numeric stability; lightweight O(n) operation
# 6. Stratified 5-fold CV provides reliable accuracy estimate for small datasets
# 7. No deep learning, no GPU required - entire pipeline runs efficiently on CPU
# 8. Dropped non-numeric 'town' column to avoid unnecessary encoding overhead
# 9. min_samples_leaf=5 prevents overly specific splits, improving generalization