# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

df = pd.read_csv('data.csv')
df = df.dropna(subset=['vax_level'])
X = df.drop(columns=['vax_level', 'town'], errors='ignore')
y = df['vax_level']

for col in X.select_dtypes(include=['object']).columns:
    cleaned = X[col].astype(str).str.replace(r'[%\$,]', '', regex=True)
    numeric_s = pd.to_numeric(cleaned, errors='coerce')
    if numeric_s.isna().mean() < 0.5:
        X[col] = numeric_s
    else:
        X[col] = pd.factorize(X[col])[0]

y_encoded = LabelEncoder().fit_transform(y.astype(str))
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42))
])

pipeline.fit(X_train, y_train)
accuracy = accuracy_score(y_test, pipeline.predict(X_test))
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Selected a shallow RandomForestClassifier (50 estimators, depth 5) to balance accuracy with low power consumption.
# 2. Implemented a vectorized cleaning routine to convert numeric strings (with %, $, or commas) into floats without duplicating data.
# 3. Used pd.factorize for categorical features to keep the feature space low-dimensional and avoid the overhead of One-Hot Encoding.
# 4. Employed SimpleImputer with a median strategy to handle missing values efficiently on CPU.
# 5. Integrated all steps into a single Pipeline to minimize memory usage and ensure a reproducible, lightweight deployment.
# 6. Restricted model complexity to prevent overfitting on small-scale data and reduce total training time.
# 7. Avoided deep learning and complex embeddings to ensure the solution remains compatible with basic hardware environments.
# 8. Removed all non-essential output, logging, and visualization to streamline execution and focus on computational efficiency.