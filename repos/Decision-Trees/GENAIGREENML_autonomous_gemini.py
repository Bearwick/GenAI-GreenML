# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

def load_data(path):
    # Robust loading: try default, then common variations
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError
    except:
        df = pd.read_csv(path, sep=';', decimal=',')
    
    # Normalize column names
    df.columns = [str(c).strip().replace('\n', ' ') for c in df.columns]
    df.columns = [" ".join(c.split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def solve():
    data_path = 'town_vax_data.csv'
    try:
        df = load_data(data_path)
    except Exception:
        # Fallback for empty or missing file to ensure script runs
        print("ACCURACY=0.000000")
        return

    if df.empty:
        print("ACCURACY=0.000000")
        return

    # Identify Target
    target_col = 'vax_level'
    if target_col not in df.columns:
        # Fallback: find any categorical or then numeric column
        potential_targets = [c for c in df.columns if 'vax' in c.lower()]
        if potential_targets:
            target_col = potential_targets[0]
        else:
            target_col = df.columns[-1]

    # Drop ID-like or high cardinality string columns
    drop_cols = ['town']
    features = [c for c in df.columns if c != target_col and c not in drop_cols]
    
    X = df[features].copy()
    y = df[target_col].copy()

    # Pre-processing: Force numeric where possible
    for col in X.columns:
        if X[col].dtype == 'object':
            converted = pd.to_numeric(X[col], errors='coerce')
            if converted.notnull().sum() > (len(X) * 0.5):
                X[col] = converted

    # Split features by type
    num_features = X.select_dtypes(include=[np.number]).columns.tolist()
    cat_features = X.select_dtypes(exclude=[np.number]).columns.tolist()

    # Pipelines for different feature types
    num_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])

    cat_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', num_transformer, num_features),
            ('cat', cat_transformer, cat_features)
        ])

    # Model Selection: Decision Tree (Lightweight and aligned with project context)
    # Shallow depth prevents overfitting and minimizes compute cycles
    model_type = 'classification'
    if y.dtype == 'object' or len(np.unique(y.dropna())) < 15:
        clf = DecisionTreeClassifier(max_depth=5, min_samples_leaf=5, random_state=42)
    else:
        # Fallback for continuous target if vax_level was numeric
        from sklearn.tree import DecisionTreeRegressor
        clf = DecisionTreeRegressor(max_depth=5, min_samples_leaf=5, random_state=42)
        model_type = 'regression'

    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('classifier', clf)])

    # Handle missing values in target
    valid_idx = y.notnull()
    X = X[valid_idx]
    y = y[valid_idx]

    if len(X) < 10:
        print("ACCURACY=0.000000")
        return

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    try:
        pipeline.fit(X_train, y_train)
        preds = pipeline.predict(X_test)
        
        if model_type == 'classification':
            accuracy = accuracy_score(y_test, preds)
        else:
            # R2 proxy for regression fallback, clipped to [0,1]
            from sklearn.metrics import r2_score
            r2 = r2_score(y_test, preds)
            accuracy = max(0, min(1, r2))
            
        print(f"ACCURACY={accuracy:.6f}")
    except:
        print("ACCURACY=0.000000")

if __name__ == "__main__":
    solve()

# Optimization Summary:
# 1. Used DecisionTreeClassifier: CPU-efficient, non-parametric, and handles non-linearities without heavy compute.
# 2. Pipeline Architecture: Scikit-learn Pipelines ensure minimal data leakage and reproducible preprocessing.
# 3. Memory Efficiency: Used sparse-output=False only for small-scale tree inputs; avoided deep learning/embeddings.
# 4. Energy Efficiency: Restricted tree depth (max_depth=5) to reduce traversal time and prevent overfitting.
# 5. Robustness: Implemented multi-stage CSV parsing and automatic feature typing to ensure end-to-end execution.
# 6. Preprocessing: SimpleImputer and StandardScaler provide a lightweight baseline for stable model convergence.
# 7. Regression Fallback: In cases where the target 'vax_level' might be encoded numerically, the script switches to regression and maps R2 to an accuracy-like metric.