# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")

path = "town_vax_data.csv"
try:
    df = pd.read_csv(path)
except Exception:
    df = pd.read_csv(path, sep=";", decimal=",")

if df.shape[1] == 1:
    try:
        df_try = pd.read_csv(path, sep=";", decimal=",")
        if df_try.shape[1] > 1:
            df = df_try
    except Exception:
        pass

def clean_col(c):
    return re.sub(r"\s+", " ", str(c).strip())

df.columns = [clean_col(c) for c in df.columns]
df = df.loc[:, ~df.columns.str.contains(r"^Unnamed", case=False, regex=True)]
new_cols = []
counts = {}
for i, c in enumerate(df.columns):
    if c == "" or c.lower() == "nan":
        c = f"col_{i}"
    if c in counts:
        counts[c] += 1
        c = f"{c}_{counts[c]}"
    else:
        counts[c] = 0
    new_cols.append(c)
df.columns = new_cols

df = df.dropna(axis=0, how="all")
df = df.dropna(axis=1, how="all")

assert df.shape[0] > 0 and df.shape[1] > 0

provided_headers_str = """town,apartments_condos_multis_per_residential_parcels_2011,assessed_home_value_changes_2009-2013,births_per_1000_residents_2010,boaters_per_10000_residents_2012,burglaries_per_10000_residents_2011,cars_motorcycles_&_trucks_average_age_2012,cars_per_1000_residents_2012,class_size_in_school_district_2011-2012,condos_as_perc_of_parcels_2012,crashes_per_1000_residents_2007-2011,culture_and_rec_spending_per_person_2012,education_spending_as_a_percent_2012,education_spending_per_resident_2012,expenditures_per_resident_2012,females_percent_in_community_2010,fire_dept_spending_as_a_percent_2012,firefighter_costs_per_resident_2012,fixed_costs_percent_2012,gun_licenses_per_1000_residents_2012,historic_places_per_10000_2013,home_schooled_per_1000_students_2011-2012,homes_built_in_39_or_before,household_member_who_is_2_races_or_more_per_1000_households_2010,households_average_size_2010,households_one-person_2010,hybrid_cars_per_1000_vehicles_2013,in_home_since_1969_or_earlier,income_average_per_resident_2010,income_change_per_resident_2007-2010,inmates_in_state_prison_per_1000_residents,liquor_licenses_per_10000_2011,median_age_2011,miles_driven_daily_per_household_05-07,minority_students_per_district_2012-2013,motorcycles_change_in_ownership_2000-2012,motorcycles_per_1000_2012,multi-generation_households_2010,police_costs_per_resident_2013,police_employees_per_10000_residents_2011,police_spending_as_a_percent_2012,population_change_1950-2010,population_change_2010-2011,presidential_fundraising_obama_vs_romney,property_crimes_per_10000_residents_2012,property_tax_change_09-13,pupils_per_cost_average_by_district_2011-2012,residential_taxes_as_percent_of_all_property_taxes_2013,saltwater_fishing_licenses_per_1000_2013,school_district_growth_09-13,single-person_households_percent_65_and_older,snowmobiles_per_10000_residents_2012,state_aid_as_a_percent_of_town_budget_2012,students_in_public_schools_2011,tax-exempt_property_2012,taxable_property_by_percent_2012,teacher_salaries_by_average_2011,teachers_percent_under_40_years_old_2011-2012,trucks_per_1000_residents_2012,violent_crimes_per_10000_residents_2012,voters_as_a_percent_of_population_2012,voters_change_in_registrations_between_1982-2012,voters_democrats_as_a_percent_2012,2020_votes,2020_biden_margin,population,vax_level"""
provided_headers = [clean_col(c) for c in provided_headers_str.split(",") if str(c).strip() != ""]
has_expected = any(c in df.columns for c in provided_headers)

def non_constant(s):
    return s.dropna().nunique() > 1

target = None
keywords = ["target", "label", "class", "outcome", "y", "vax", "level"]
for col in df.columns:
    col_low = col.lower()
    if any(k in col_low for k in keywords):
        if non_constant(df[col]):
            target = col
            break

if target is None:
    if has_expected:
        obj_cols = [c for c in df.columns if df[c].dtype == object or str(df[c].dtype).startswith("category")]
        low_card = []
        for c in obj_cols:
            nun = df[c].nunique(dropna=True)
            if nun > 1:
                low_card.append((nun, c))
        if low_card:
            low_card.sort()
            target = low_card[0][1]
    if target is None:
        numeric_candidate = None
        best_var = -np.inf
        for c in df.columns:
            s_num = pd.to_numeric(df[c], errors="coerce")
            if s_num.notna().sum() > 0 and s_num.nunique(dropna=True) > 1:
                var = s_num.var()
                if np.isfinite(var) and var > best_var:
                    best_var = var
                    numeric_candidate = c
        if numeric_candidate is not None:
            target = numeric_candidate
    if target is None:
        for c in df.columns:
            if non_constant(df[c]):
                target = c
                break
    if target is None:
        target = df.columns[0]

y_raw = df[target]
feature_cols = [c for c in df.columns if c != target]
if len(feature_cols) == 0:
    df["dummy_feature"] = 0
    feature_cols = ["dummy_feature"]

for c in feature_cols:
    s = df[c]
    if not pd.api.types.is_numeric_dtype(s):
        s_num = pd.to_numeric(s, errors="coerce")
        non_na = s_num.notna().sum()
        if non_na >= 0.5 * len(s):
            df[c] = s_num

y = y_raw
if pd.api.types.is_numeric_dtype(y):
    y = pd.to_numeric(y, errors="coerce")
    y = y.replace([np.inf, -np.inf], np.nan)
else:
    y_num = pd.to_numeric(y, errors="coerce")
    if y_num.notna().sum() >= 0.5 * len(y):
        y = y_num.replace([np.inf, -np.inf], np.nan)
    else:
        y = y.astype(str).str.strip()
        y = y.replace({"": np.nan, "nan": np.nan, "NaN": np.nan})

mask = ~pd.isna(y)
df = df.loc[mask]
y = y.loc[mask]

assert len(df) > 0

all_nan_cols = [c for c in feature_cols if c in df.columns and df[c].isna().all()]
if all_nan_cols:
    df = df.drop(columns=all_nan_cols)
    feature_cols = [c for c in feature_cols if c not in all_nan_cols]

if len(feature_cols) == 0:
    df["dummy_feature"] = 0
    feature_cols = ["dummy_feature"]

numeric_features = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c])]
categorical_features = [c for c in feature_cols if c not in numeric_features]

if numeric_features:
    df[numeric_features] = df[numeric_features].replace([np.inf, -np.inf], np.nan)

X = df[feature_cols]

if pd.api.types.is_numeric_dtype(y):
    nunique = y.nunique(dropna=True)
    if nunique <= max(20, int(0.05 * len(y))):
        task = "classification"
    else:
        task = "regression"
else:
    task = "classification"

if task == "classification":
    if pd.Series(y).nunique(dropna=True) < 2:
        task = "dummy"
if task == "regression":
    if pd.Series(y).nunique(dropna=True) < 2:
        task = "regression_dummy"

n_samples = len(df)
test_size = 0.2 if n_samples >= 5 else 0.5
if n_samples < 2:
    X_train, X_test, y_train, y_test = X, X, y, y
else:
    stratify = None
    if task in ["classification", "dummy"]:
        value_counts = pd.Series(y).value_counts()
        if len(value_counts) > 1 and value_counts.min() >= 2:
            stratify = y
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )

assert len(X_train) > 0 and len(X_test) > 0

if task == "classification" and pd.Series(y_train).nunique(dropna=True) < 2:
    task = "dummy"
if task == "regression" and pd.Series(y_train).nunique(dropna=True) < 2:
    task = "regression_dummy"

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False))
])
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])
transformers = []
if numeric_features:
    transformers.append(("num", numeric_transformer, numeric_features))
if categorical_features:
    transformers.append(("cat", categorical_transformer, categorical_features))
if transformers:
    preprocessor = ColumnTransformer(transformers=transformers, sparse_threshold=0.3)
else:
    preprocessor = "passthrough"

if task == "classification":
    model = LogisticRegression(max_iter=200, solver="liblinear")
elif task == "dummy":
    model = DummyClassifier(strategy="most_frequent")
elif task == "regression":
    model = Ridge(alpha=1.0)
else:
    model = DummyRegressor(strategy="mean")

clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

if task in ["classification", "dummy"]:
    accuracy = accuracy_score(y_test, y_pred) if len(y_test) > 0 else 0.0
else:
    if len(y_test) > 1:
        r2 = r2_score(y_test, y_pred)
        accuracy = (r2 + 1.0) / 2.0
    else:
        accuracy = 0.0
    if not np.isfinite(accuracy):
        accuracy = 0.0
    accuracy = max(0.0, min(1.0, accuracy))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear/logistic models and dummy baselines to keep CPU usage low.
# - ColumnTransformer with simple imputation, scaling, and one-hot encoding ensures reproducible preprocessing.
# - Robust parsing, column cleaning, and heuristic target selection keep the pipeline resilient to schema issues.
# - For regression, ACCURACY maps R2 to [0,1] via (r2+1)/2 with clipping for stability.