# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")

DATASET_PATH = "town_vax_data.csv"
DATASET_HEADERS_RAW = (
    "town,apartments_condos_multis_per_residential_parcels_2011,"
    "assessed_home_value_changes_2009-2013,births_per_1000_residents_2010,"
    "boaters_per_10000_residents_2012,burglaries_per_10000_residents_2011,"
    "cars_motorcycles_&_trucks_average_age_2012,cars_per_1000_residents_2012,"
    "class_size_in_school_district_2011-2012,condos_as_perc_of_parcels_2012,"
    "crashes_per_1000_residents_2007-2011,culture_and_rec_spending_per_person_2012,"
    "education_spending_as_a_percent_2012,education_spending_per_resident_2012,"
    "expenditures_per_resident_2012,females_percent_in_community_2010,"
    "fire_dept_spending_as_a_percent_2012,firefighter_costs_per_resident_2012,"
    "fixed_costs_percent_2012,gun_licenses_per_1000_residents_2012,"
    "historic_places_per_10000_2013,home_schooled_per_1000_students_2011-2012,"
    "homes_built_in_39_or_before,household_member_who_is_2_races_or_more_per_1000_households_2010,"
    "households_average_size_2010,households_one-person_2010,"
    "hybrid_cars_per_1000_vehicles_2013,in_home_since_1969_or_earlier,"
    "income_average_per_resident_2010,income_change_per_resident_2007-2010,"
    "inmates_in_state_prison_per_1000_residents,liquor_licenses_per_10000_2011,"
    "median_age_2011,miles_driven_daily_per_household_05-07,minority_students_per_district_2012-2013,"
    "motorcycles_change_in_ownership_2000-2012,motorcycles_per_1000_2012,"
    "multi-generation_households_2010,police_costs_per_resident_2013,"
    "police_employees_per_10000_residents_2011,police_spending_as_a_percent_2012,"
    "population_change_1950-2010,population_change_2010-2011,"
    "presidential_fundraising_obama_vs_romney,property_crimes_per_10000_residents_2012,"
    "property_tax_change_09-13,pupils_per_cost_average_by_district_2011-2012,"
    "residential_taxes_as_percent_of_all_property_taxes_2013,saltwater_fishing_licenses_per_1000_2013,"
    "school_district_growth_09-13,single-person_households_percent_65_and_older,"
    "snowmobiles_per_10000_residents_2012,state_aid_as_a_percent_of_town_budget_2012,"
    "students_in_public_schools_2011,tax-exempt_property_2012,taxable_property_by_percent_2012,"
    "teacher_salaries_by_average_2011,teachers_percent_under_40_years_old_2011-2012,"
    "trucks_per_1000_residents_2012,violent_crimes_per_10000_residents_2012,"
    "voters_as_a_percent_of_population_2012,voters_change_in_registrations_between_1982-2012,"
    "voters_democrats_as_a_percent_2012,2020_votes,2020_biden_margin,population,vax_level"
)
DATASET_HEADERS = [h.strip() for h in DATASET_HEADERS_RAW.split(",") if h.strip()]

def load_csv(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    if df.shape[1] <= 1:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > df.shape[1]:
                df = df_alt
        except Exception:
            pass
    return df

def normalize_columns(df):
    cols = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
    cols = [c if c != '' else f'col_{i}' for i, c in enumerate(cols)]
    seen = {}
    new_cols = []
    for c in cols:
        if c not in seen:
            seen[c] = 0
            new_cols.append(c)
        else:
            seen[c] += 1
            new_cols.append(f"{c}_{seen[c]}")
    df.columns = new_cols
    df = df.loc[:, ~df.columns.str.match(r'^Unnamed')]
    df = df.loc[:, df.columns != '']
    return df

def select_target(df, headers_norm):
    cols = list(df.columns)
    if not cols:
        return None
    lower_map = {c.lower(): c for c in cols}
    candidate_names = ['vax_level', 'target', 'label', 'class', 'y', 'outcome']
    for cand in candidate_names:
        if cand in lower_map:
            return lower_map[cand]
    for c in cols:
        if 'vax' in c.lower():
            return c
    for hdr in reversed(headers_norm):
        if hdr in cols:
            return hdr
    return cols[-1]

def choose_fallback_target(df, current):
    if current is None:
        return None
    if current in df.columns and df[current].nunique(dropna=True) >= 2:
        return current
    numeric_candidates = []
    for col in df.columns:
        if col == current:
            continue
        converted = pd.to_numeric(df[col], errors='coerce')
        if converted.nunique(dropna=True) > 1:
            numeric_candidates.append(col)
    if numeric_candidates:
        return numeric_candidates[0]
    for col in df.columns:
        if col == current:
            continue
        if df[col].nunique(dropna=True) > 1:
            return col
    return current

df = load_csv(DATASET_PATH)
df = normalize_columns(df)
if df.shape[1] == 0:
    df['dummy'] = 0

headers_norm = [re.sub(r'\s+', ' ', str(h).strip()) for h in DATASET_HEADERS]
target = select_target(df, headers_norm)
if target is None:
    df['target'] = 0
    target = 'target'
target = choose_fallback_target(df, target)

df.replace([np.inf, -np.inf], np.nan, inplace=True)

y_is_numeric = False
if target in df.columns:
    if pd.api.types.is_numeric_dtype(df[target]):
        y_is_numeric = True
    else:
        converted = pd.to_numeric(df[target], errors='coerce')
        if converted.notna().mean() >= 0.8:
            df[target] = converted
            y_is_numeric = True

df = df.loc[~df[target].isna()].copy()
assert df.shape[0] > 0

y = df[target]
if y_is_numeric:
    unique_count = y.nunique(dropna=True)
    ratio = unique_count / max(1, len(y))
    is_classification = unique_count <= 20 and ratio <= 0.2
else:
    is_classification = True

use_dummy_classifier = False
use_dummy_regressor = False
if is_classification:
    if y.nunique(dropna=True) < 2:
        use_dummy_classifier = True
else:
    if y.nunique(dropna=True) < 2:
        use_dummy_regressor = True

feature_cols = [c for c in df.columns if c != target]
if len(feature_cols) == 0:
    df['constant_feature'] = 1.0
    feature_cols = ['constant_feature']

numeric_features = []
categorical_features = []
for col in feature_cols:
    series = df[col]
    if pd.api.types.is_numeric_dtype(series):
        numeric_features.append(col)
    else:
        converted = pd.to_numeric(series, errors='coerce')
        non_na_ratio = converted.notna().mean()
        if non_na_ratio >= 0.8:
            df[col] = converted
            numeric_features.append(col)
        else:
            categorical_features.append(col)

for col in categorical_features:
    df[col] = df[col].astype(str)

if not numeric_features and not categorical_features:
    df['constant_feature'] = 1.0
    numeric_features = ['constant_feature']

transformers = []
if numeric_features:
    num_pipe = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler(with_mean=False))
    ])
    transformers.append(('num', num_pipe, numeric_features))
if categorical_features:
    cat_pipe = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    transformers.append(('cat', cat_pipe, categorical_features))

preprocessor = ColumnTransformer(transformers=transformers)

if is_classification:
    if use_dummy_classifier:
        model = DummyClassifier(strategy='most_frequent')
    else:
        n_classes = y.nunique(dropna=True)
        if n_classes <= 2:
            model = LogisticRegression(max_iter=200, solver='liblinear')
        else:
            model = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='auto')
else:
    if use_dummy_regressor:
        model = DummyRegressor(strategy='mean')
    else:
        model = Ridge(alpha=1.0)

clf = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])

X = df[feature_cols]
y = df[target]
if is_classification and not y_is_numeric:
    y = y.astype(str)

if len(X) < 2:
    X = pd.concat([X, X], ignore_index=True)
    y = pd.concat([y, y], ignore_index=True)

n_samples = len(X)
test_size = 0.2 if n_samples >= 5 else 0.5
stratify = y if is_classification and y.nunique(dropna=True) >= 2 else None
try:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )
except Exception:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=None
    )

assert len(X_train) > 0 and len(X_test) > 0

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if is_classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    if len(y_test) == 0:
        accuracy = 0.0
    else:
        y_true = np.array(y_test)
        y_pred_arr = np.array(y_pred)
        if np.nanvar(y_true) == 0:
            accuracy = 1.0 if np.allclose(y_true, y_pred_arr) else 0.0
        else:
            r2 = r2_score(y_true, y_pred_arr)
            if not np.isfinite(r2):
                r2 = 0.0
            accuracy = max(0.0, min(1.0, r2))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# - Lightweight linear/logistic or dummy models minimize CPU and energy use while providing a solid baseline.
# - Simple, reproducible preprocessing with imputation, scaling, and one-hot encoding avoids heavy feature engineering.
# - Robust parsing, target fallback, and clipped R2 accuracy proxy keep the pipeline stable on edge cases.