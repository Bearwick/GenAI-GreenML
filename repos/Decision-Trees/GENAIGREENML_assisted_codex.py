# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import math
import random
import pandas as pd

DATASET_PATH = "town_vax_data.csv"
DATASET_HEADERS = (
    "town,apartments_condos_multis_per_residential_parcels_2011,"
    "assessed_home_value_changes_2009-2013,births_per_1000_residents_2010,"
    "boaters_per_10000_residents_2012,burglaries_per_10000_residents_2011,"
    "cars_motorcycles_&_trucks_average_age_2012,cars_per_1000_residents_2012,"
    "class_size_in_school_district_2011-2012,condos_as_perc_of_parcels_2012,"
    "crashes_per_1000_residents_2007-2011,culture_and_rec_spending_per_person_2012,"
    "education_spending_as_a_percent_2012,education_spending_per_resident_2012,"
    "expenditures_per_resident_2012,females_percent_in_community_2010,"
    "fire_dept_spending_as_a_percent_2012,firefighter_costs_per_resident_2012,"
    "fixed_costs_percent_2012,gun_licenses_per_1000_residents_2012,"
    "historic_places_per_10000_2013,home_schooled_per_1000_students_2011-2012,"
    "homes_built_in_39_or_before,household_member_who_is_2_races_or_more_per_1000_households_2010,"
    "households_average_size_2010,households_one-person_2010,hybrid_cars_per_1000_vehicles_2013,"
    "in_home_since_1969_or_earlier,income_average_per_resident_2010,income_change_per_resident_2007-2010,"
    "inmates_in_state_prison_per_1000_residents,liquor_licenses_per_10000_2011,median_age_2011,"
    "miles_driven_daily_per_household_05-07,minority_students_per_district_2012-2013,"
    "motorcycles_change_in_ownership_2000-2012,motorcycles_per_1000_2012,"
    "multi-generation_households_2010,police_costs_per_resident_2013,police_employees_per_10000_residents_2011,"
    "police_spending_as_a_percent_2012,population_change_1950-2010,population_change_2010-2011,"
    "presidential_fundraising_obama_vs_romney,property_crimes_per_10000_residents_2012,property_tax_change_09-13,"
    "pupils_per_cost_average_by_district_2011-2012,residential_taxes_as_percent_of_all_property_taxes_2013,"
    "saltwater_fishing_licenses_per_1000_2013,school_district_growth_09-13,"
    "single-person_households_percent_65_and_older,snowmobiles_per_10000_residents_2012,"
    "state_aid_as_a_percent_of_town_budget_2012,students_in_public_schools_2011,tax-exempt_property_2012,"
    "taxable_property_by_percent_2012,teacher_salaries_by_average_2011,"
    "teachers_percent_under_40_years_old_2011-2012,trucks_per_1000_residents_2012,"
    "violent_crimes_per_10000_residents_2012,voters_as_a_percent_of_population_2012,"
    "voters_change_in_registrations_between_1982-2012,voters_democrats_as_a_percent_2012,"
    "2020_votes,2020_biden_margin,population,vax_level"
)
EXPECTED_HEADERS = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]

SEED = 42
TEST_PERC = 0.25
MIN_LEAF_COUNT = 10

def read_csv_robust(path, expected_headers):
    dtype_map = {}
    if expected_headers:
        if "town" in expected_headers:
            dtype_map["town"] = str
        if "vax_level" in expected_headers:
            dtype_map["vax_level"] = str

    def _read(sep=",", decimal="."):
        return pd.read_csv(path, sep=sep, decimal=decimal, dtype=dtype_map)

    try:
        df = _read()
    except Exception:
        df = _read(sep=";", decimal=",")

    def looks_wrong(frame):
        if frame.shape[1] <= 1:
            return True
        if expected_headers:
            overlap = sum(1 for c in frame.columns if c in expected_headers)
            if overlap == 0:
                return True
        return False

    if looks_wrong(df):
        df = _read(sep=";", decimal=",")

    df.columns = [str(c).strip() for c in df.columns]
    if expected_headers and len(df.columns) == len(expected_headers):
        overlap = sum(1 for c in df.columns if c in expected_headers)
        if overlap < len(expected_headers) // 2:
            df.columns = expected_headers
    return df

def prepare_examples(df, expected_headers):
    cols = list(df.columns)
    if expected_headers:
        if "town" in cols:
            id_col = "town"
        elif expected_headers[0] in cols:
            id_col = expected_headers[0]
        else:
            id_col = cols[0]
        if "vax_level" in cols:
            class_col = "vax_level"
        elif expected_headers[-1] in cols:
            class_col = expected_headers[-1]
        else:
            class_col = cols[-1]
    else:
        id_col = cols[0]
        class_col = cols[-1]

    feature_cols = [c for c in cols if c not in (id_col, class_col)]
    if feature_cols:
        df[feature_cols] = df[feature_cols].apply(pd.to_numeric, errors="coerce").astype(float)
    df = df.where(pd.notnull(df), None)
    examples = df.to_dict(orient="records")
    return examples, id_col, class_col

class TreeNodeInterface:
    __slots__ = ()
    def classify(self, example):
        raise NotImplementedError

class DecisionNode(TreeNodeInterface):
    __slots__ = ("test_attr_name", "test_attr_threshold", "child_lt", "child_ge", "child_miss")
    def __init__(self, test_attr_name, test_attr_threshold, child_lt, child_ge, child_miss):
        self.test_attr_name = test_attr_name
        self.test_attr_threshold = test_attr_threshold
        self.child_ge = child_ge
        self.child_lt = child_lt
        self.child_miss = child_miss
    def classify(self, example):
        test_val = example[self.test_attr_name]
        if test_val is None:
            return self.child_miss.classify(example)
        if test_val < self.test_attr_threshold:
            return self.child_lt.classify(example)
        return self.child_ge.classify(example)

class LeafNode(TreeNodeInterface):
    __slots__ = ("pred_class", "pred_class_count", "total_count", "prob")
    def __init__(self, pred_class, pred_class_count, total_count):
        self.pred_class = pred_class
        self.pred_class_count = pred_class_count
        self.total_count = total_count
        self.prob = pred_class_count / total_count
    def classify(self, example):
        return self.pred_class, self.prob

class DecisionTree:
    __slots__ = ("id_name", "class_name", "min_leaf_count", "root")
    def __init__(self, examples, id_name, class_name, min_leaf_count=1):
        self.id_name = id_name
        self.class_name = class_name
        self.min_leaf_count = min_leaf_count
        self.root = self.learn_tree(examples)
    def learn_tree(self, examples):
        example = examples[0]
        attribute_set = {attr for attr in example if attr != self.id_name and attr != self.class_name}
        return attributeSplit(attribute_set, examples, self.min_leaf_count, self.class_name)
    def classify(self, example):
        return self.root.classify(example)

def attributeSplit(attribute_set, examples, min_leaf_count, class_name):
    if not attribute_set:
        predictiveClass, predictiveClassCount = getPredictiveClass(examples, class_name)
        return LeafNode(predictiveClass, predictiveClassCount, len(examples))
    attribute_name, threshold, examples_lt, examples_ge = getBestAttributeAndSplit(attribute_set, examples, class_name)
    if len(examples_ge) <= min_leaf_count or len(examples_lt) <= min_leaf_count:
        predictiveClass, predictiveClassCount = getPredictiveClass(examples, class_name)
        return LeafNode(predictiveClass, predictiveClassCount, len(examples))
    attribute_set.remove(attribute_name)
    child_lt = attributeSplit(attribute_set, examples_lt, min_leaf_count, class_name)
    child_ge = attributeSplit(attribute_set, examples_ge, min_leaf_count, class_name)
    child_miss = child_lt if len(examples_lt) >= len(examples_ge) else child_ge
    return DecisionNode(attribute_name, threshold, child_lt, child_ge, child_miss)

def getBestAttributeAndSplit(attribute_set, examples, class_label):
    best_name = ""
    best_gain = 0.0
    best_threshold = None
    best_lt = []
    best_ge = []
    for attribute in sorted(attribute_set):
        gain, threshold_, lt, ge = getInfoGain(attribute, examples, class_label)
        if gain > best_gain:
            best_gain = gain
            best_name = attribute
            best_threshold = threshold_
            best_lt = lt
            best_ge = ge
    return best_name, best_threshold, best_lt, best_ge

def getInfoGain(attribute, examples, class_label):
    maxInfoGain = 0.0
    threshold = 0.0
    lt_split = []
    ge_split = []
    min_, max_, step = getRange(attribute, examples)
    base_entropy = entropy(examples, class_label)
    total_len = len(examples)
    curThreshold = min_ + step
    split_func = splitExamplesOnAttribute
    info_func = infoGain
    while curThreshold < max_:
        lt, ge = split_func(attribute, examples, curThreshold)
        infogain = info_func(base_entropy, lt, ge, total_len, class_label)
        if infogain > maxInfoGain:
            maxInfoGain = infogain
            threshold = curThreshold
            lt_split = lt
            ge_split = ge
        curThreshold += step
    return maxInfoGain, threshold, lt_split, ge_split

def getRange(attribute, examples):
    min_ = 1000000.0
    max_ = -1000000.0
    for ex in examples:
        val = ex[attribute]
        if val is None:
            continue
        if val < min_:
            min_ = val
        if val > max_:
            max_ = val
    step = (max_ - min_) / 15
    return min_, max_, step

def splitExamplesOnAttribute(attribute, examples, threshold):
    lt, ge = [], []
    append_lt = lt.append
    append_ge = ge.append
    for example in examples:
        val = example[attribute]
        if val is None:
            continue
        if val >= threshold:
            append_ge(example)
        else:
            append_lt(example)
    return lt, ge

def infoGain(base_entropy, ex1, ex2, total_len, class_label):
    pc_1 = len(ex1) / total_len
    pc_2 = len(ex2) / total_len
    ent1 = entropy(ex1, class_label) if pc_1 else 0.0
    ent2 = entropy(ex2, class_label) if pc_2 else 0.0
    return base_entropy - (pc_1 * ent1 + pc_2 * ent2)

def entropy(examples, class_label):
    class_counts = {}
    for example in examples:
        label = example[class_label]
        class_counts[label] = class_counts.get(label, 0) + 1
    total = len(examples)
    log = math.log
    ent = 0.0
    for count in class_counts.values():
        p = count / total
        if p:
            ent -= p * log(p, 2)
    return ent

def getPredictiveClass(examples, class_label):
    classDict = {}
    max_label = ""
    max_count = 0
    for example in examples:
        class_name = example[class_label]
        if class_name not in classDict:
            classDict[class_name] = 0
        else:
            classDict[class_name] += 1
        if classDict[class_name] > max_count:
            max_label = class_name
            max_count = classDict[class_name]
    return max_label, max_count

def train_test_split(examples, test_perc, rng):
    test_size = round(test_perc * len(examples))
    shuffled = rng.sample(examples, len(examples))
    return shuffled[test_size:], shuffled[:test_size]

def evaluate_accuracy(model, test_examples):
    total = len(test_examples)
    if total == 0:
        return 0.0
    correct = 0
    class_name = model.class_name
    classify = model.classify
    for example in test_examples:
        pred, _ = classify(example)
        if pred == example[class_name]:
            correct += 1
    return correct / total

def main():
    df = read_csv_robust(DATASET_PATH, EXPECTED_HEADERS)
    examples, id_col, class_col = prepare_examples(df, EXPECTED_HEADERS)
    rng = random.Random(SEED)
    train_examples, test_examples = train_test_split(examples, TEST_PERC, rng)
    tree = DecisionTree(train_examples, id_col, class_col, MIN_LEAF_COUNT)
    accuracy = evaluate_accuracy(tree, test_examples)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced per-cell CSV parsing with vectorized pandas reading and numeric conversion to reduce Python overhead.
# - Added robust parsing fallback and deterministic seeding to ensure stable, reproducible splits.
# - Precomputed base entropy and used compact label counting to avoid redundant calculations and memory use.
# - Applied __slots__ to tree node classes to reduce per-node memory footprint.
# - Simplified evaluation to compute only required accuracy without extra data processing.