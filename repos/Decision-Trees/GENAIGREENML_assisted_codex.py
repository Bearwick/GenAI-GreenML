# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import random
import math
from numbers import Number

DATASET_HEADERS = "town,apartments_condos_multis_per_residential_parcels_2011,assessed_home_value_changes_2009-2013,births_per_1000_residents_2010,boaters_per_10000_residents_2012,burglaries_per_10000_residents_2011,cars_motorcycles_&_trucks_average_age_2012,cars_per_1000_residents_2012,class_size_in_school_district_2011-2012,condos_as_perc_of_parcels_2012,crashes_per_1000_residents_2007-2011,culture_and_rec_spending_per_person_2012,education_spending_as_a_percent_2012,education_spending_per_resident_2012,expenditures_per_resident_2012,females_percent_in_community_2010,fire_dept_spending_as_a_percent_2012,firefighter_costs_per_resident_2012,fixed_costs_percent_2012,gun_licenses_per_1000_residents_2012,historic_places_per_10000_2013,home_schooled_per_1000_students_2011-2012,homes_built_in_39_or_before,household_member_who_is_2_races_or_more_per_1000_households_2010,households_average_size_2010,households_one-person_2010,hybrid_cars_per_1000_vehicles_2013,in_home_since_1969_or_earlier,income_average_per_resident_2010,income_change_per_resident_2007-2010,inmates_in_state_prison_per_1000_residents,liquor_licenses_per_10000_2011,median_age_2011,miles_driven_daily_per_household_05-07,minority_students_per_district_2012-2013,motorcycles_change_in_ownership_2000-2012,motorcycles_per_1000_2012,multi-generation_households_2010,police_costs_per_resident_2013,police_employees_per_10000_residents_2011,police_spending_as_a_percent_2012,population_change_1950-2010,population_change_2010-2011,presidential_fundraising_obama_vs_romney,property_crimes_per_10000_residents_2012,property_tax_change_09-13,pupils_per_cost_average_by_district_2011-2012,residential_taxes_as_percent_of_all_property_taxes_2013,saltwater_fishing_licenses_per_1000_2013,school_district_growth_09-13,single-person_households_percent_65_and_older,snowmobiles_per_10000_residents_2012,state_aid_as_a_percent_of_town_budget_2012,students_in_public_schools_2011,tax-exempt_property_2012,taxable_property_by_percent_2012,teacher_salaries_by_average_2011,teachers_percent_under_40_years_old_2011-2012,trucks_per_1000_residents_2012,violent_crimes_per_10000_residents_2012,voters_as_a_percent_of_population_2012,voters_change_in_registrations_between_1982-2012,voters_democrats_as_a_percent_2012,2020_votes,2020_biden_margin,population,vax_level"
EXPECTED_HEADERS = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]


def _looks_wrong(df, expected_headers):
    if df.shape[1] <= 1:
        return True
    if expected_headers:
        matches = sum(1 for h in expected_headers if h in df.columns)
        if matches < max(1, len(expected_headers) // 2):
            return True
    return False


def _read_csv_with_fallback(path, expected_headers):
    df = pd.read_csv(path, dtype=str, keep_default_na=False)
    used_fallback = False
    if _looks_wrong(df, expected_headers):
        df = pd.read_csv(path, sep=";", decimal=",", dtype=str, keep_default_na=False)
        used_fallback = True
    return df, used_fallback


def _convert_value(v, decimal_comma=False):
    if v is None:
        return None
    if isinstance(v, str):
        if v == "":
            return None
        try:
            return float(v)
        except ValueError:
            if decimal_comma and "," in v:
                try:
                    return float(v.replace(",", "."))
                except ValueError:
                    return v
            return v
    if isinstance(v, Number):
        return float(v)
    return v


def read_data(csv_path, expected_headers):
    df, used_fallback = _read_csv_with_fallback(csv_path, expected_headers)
    records = df.to_dict(orient="records")
    for rec in records:
        for k, v in rec.items():
            rec[k] = _convert_value(v, used_fallback)
    return records, df.columns.tolist()


def train_test_split(examples, test_perc, seed=0):
    test_size = round(test_perc * len(examples))
    rng = random.Random(seed)
    shuffled = rng.sample(examples, len(examples))
    return shuffled[test_size:], shuffled[:test_size]


class DecisionNode:
    __slots__ = ("test_attr_name", "test_attr_threshold", "child_lt", "child_ge", "child_miss")

    def __init__(self, test_attr_name, test_attr_threshold, child_lt, child_ge, child_miss):
        self.test_attr_name = test_attr_name
        self.test_attr_threshold = test_attr_threshold
        self.child_lt = child_lt
        self.child_ge = child_ge
        self.child_miss = child_miss

    def classify(self, example):
        test_val = example[self.test_attr_name]
        if test_val is None:
            return self.child_miss.classify(example)
        if test_val < self.test_attr_threshold:
            return self.child_lt.classify(example)
        return self.child_ge.classify(example)


class LeafNode:
    __slots__ = ("pred_class", "pred_class_count", "total_count", "prob")

    def __init__(self, pred_class, pred_class_count, total_count):
        self.pred_class = pred_class
        self.pred_class_count = pred_class_count
        self.total_count = total_count
        self.prob = pred_class_count / total_count if total_count else 0.0

    def classify(self, example):
        return self.pred_class, self.prob


class DecisionTree:
    __slots__ = ("id_name", "class_name", "min_leaf_count", "root")

    def __init__(self, examples, id_name, class_name, min_leaf_count=1):
        self.id_name = id_name
        self.class_name = class_name
        self.min_leaf_count = min_leaf_count
        if examples:
            attributes = [a for a in examples[0] if a != id_name and a != class_name]
            self.root = attribute_split(attributes, examples, min_leaf_count, class_name)
        else:
            self.root = LeafNode("", 0, 0)

    def classify(self, example):
        return self.root.classify(example)


def attribute_split(attributes, examples, min_leaf_count, class_name):
    if not attributes:
        pred_class, pred_count = get_predictive_class(examples, class_name)
        return LeafNode(pred_class, pred_count, len(examples))
    attribute_name, threshold, examples_lt, examples_ge = get_best_attribute_and_split(attributes, examples, class_name)
    if len(examples_ge) <= min_leaf_count or len(examples_lt) <= min_leaf_count:
        pred_class, pred_count = get_predictive_class(examples, class_name)
        return LeafNode(pred_class, pred_count, len(examples))
    if attribute_name not in attributes:
        pred_class, pred_count = get_predictive_class(examples, class_name)
        return LeafNode(pred_class, pred_count, len(examples))
    attributes.remove(attribute_name)
    child_lt = attribute_split(attributes, examples_lt, min_leaf_count, class_name)
    child_ge = attribute_split(attributes, examples_ge, min_leaf_count, class_name)
    child_miss = child_lt if len(examples_lt) >= len(examples_ge) else child_ge
    return DecisionNode(attribute_name, threshold, child_lt, child_ge, child_miss)


def get_best_attribute_and_split(attributes, examples, class_label):
    best_name = ""
    best_gain = 0.0
    best_threshold = None
    best_lt = []
    best_ge = []
    for attribute in attributes:
        info_gain, threshold, lt, ge = get_info_gain(attribute, examples, class_label)
        if info_gain > best_gain:
            best_gain = info_gain
            best_name = attribute
            best_threshold = threshold
            best_lt = lt
            best_ge = ge
    return best_name, best_threshold, best_lt, best_ge


def get_info_gain(attribute, examples, class_label):
    min_val, max_val, step = get_range(attribute, examples)
    if step <= 0:
        return 0.0, 0, [], []
    base_entropy = entropy(examples, class_label)
    best_gain = 0.0
    best_threshold = 0
    best_lt = []
    best_ge = []
    threshold = min_val + step
    total_count = len(examples)
    entropy_func = entropy
    split_func = split_examples_on_attribute
    while threshold < max_val:
        lt, ge = split_func(attribute, examples, threshold)
        pc1 = len(lt) / total_count
        pc2 = len(ge) / total_count
        gain = base_entropy - (pc1 * entropy_func(lt, class_label) + pc2 * entropy_func(ge, class_label))
        if gain > best_gain:
            best_gain = gain
            best_threshold = threshold
            best_lt = lt
            best_ge = ge
        threshold += step
    return best_gain, best_threshold, best_lt, best_ge


def get_range(attribute, examples):
    min_val = None
    max_val = None
    for ex in examples:
        val = ex[attribute]
        if val is None:
            continue
        if min_val is None or val < min_val:
            min_val = val
        if max_val is None or val > max_val:
            max_val = val
    if min_val is None or max_val is None:
        return 0.0, 0.0, 0.0
    return min_val, max_val, (max_val - min_val) / 15


def split_examples_on_attribute(attribute, examples, threshold):
    lt = []
    ge = []
    lt_append = lt.append
    ge_append = ge.append
    for example in examples:
        val = example[attribute]
        if val is None:
            continue
        if val >= threshold:
            ge_append(example)
        else:
            lt_append(example)
    return lt, ge


def entropy(examples, class_label):
    total = len(examples)
    if total == 0:
        return 0.0
    counts = {}
    for example in examples:
        label = example[class_label]
        counts[label] = counts.get(label, 0) + 1
    ent = 0.0
    log2 = math.log2
    for count in counts.values():
        p = count / total
        if p:
            ent -= p * log2(p)
    return ent


def get_predictive_class(examples, class_label):
    class_dict = {}
    max_pair = ("", 0)
    for example in examples:
        class_name = example[class_label]
        if class_name not in class_dict:
            class_dict[class_name] = 0
        else:
            class_dict[class_name] += 1
        if class_dict[class_name] > max_pair[1]:
            max_pair = (class_name, class_dict[class_name])
    return max_pair


def evaluate(model, test_examples):
    if not test_examples:
        return 0.0
    correct = 0
    classify = model.classify
    class_name = model.class_name
    for example in test_examples:
        if classify(example)[0] == example[class_name]:
            correct += 1
    return correct / len(test_examples)


def main():
    path_to_csv = "town_vax_data.csv"
    examples, columns = read_data(path_to_csv, EXPECTED_HEADERS)
    if not examples:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return
    if EXPECTED_HEADERS and EXPECTED_HEADERS[0] in columns:
        id_attr_name = EXPECTED_HEADERS[0]
    else:
        id_attr_name = columns[0]
    if EXPECTED_HEADERS and EXPECTED_HEADERS[-1] in columns:
        class_attr_name = EXPECTED_HEADERS[-1]
    else:
        class_attr_name = columns[-1]
    train_examples, test_examples = train_test_split(examples, 0.25, seed=0)
    tree = DecisionTree(train_examples, id_attr_name, class_attr_name, min_leaf_count=10)
    accuracy = evaluate(tree, test_examples)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used pandas CSV parsing with delimiter/decimal fallback to reduce manual parsing overhead and improve robustness.
# - Converted data in place with a shared conversion routine to avoid redundant data movement.
# - Replaced list-based entropy computation with count-based aggregation to lower memory use.
# - Cached base entropy and loop invariants during information gain calculation to reduce repeated computation.
# - Added __slots__ to tree nodes to shrink memory footprint.
# - Employed a deterministic local RNG for reproducible, low-overhead train/test splitting.