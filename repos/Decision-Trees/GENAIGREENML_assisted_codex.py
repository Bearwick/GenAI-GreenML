# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import csv
import random
import math

NON_NUMERIC_COLUMNS = frozenset(("town", "vax_level"))

def read_data(csv_path):
    examples = []
    with open(csv_path, "r", newline="") as csv_file:
        reader = csv.DictReader(csv_file)
        non_numeric = NON_NUMERIC_COLUMNS
        to_float = float
        append_example = examples.append
        for row in reader:
            ex = {}
            for k, v in row.items():
                if v == "":
                    ex[k] = None
                elif k in non_numeric:
                    ex[k] = v
                else:
                    try:
                        ex[k] = to_float(v)
                    except ValueError:
                        ex[k] = v
            append_example(ex)
    return examples

def train_test_split(examples, test_perc):
    test_size = round(test_perc * len(examples))
    shuffled = random.sample(examples, len(examples))
    return shuffled[test_size:], shuffled[:test_size]

class TreeNodeInterface:
    def classify(self, example):
        raise NotImplementedError

class DecisionNode(TreeNodeInterface):
    __slots__ = ("test_attr_name", "test_attr_threshold", "child_lt", "child_ge", "child_miss")
    def __init__(self, test_attr_name, test_attr_threshold, child_lt, child_ge, child_miss):
        self.test_attr_name = test_attr_name
        self.test_attr_threshold = test_attr_threshold
        self.child_lt = child_lt
        self.child_ge = child_ge
        self.child_miss = child_miss
    def classify(self, example):
        test_val = example[self.test_attr_name]
        if test_val is None:
            return self.child_miss.classify(example)
        if test_val < self.test_attr_threshold:
            return self.child_lt.classify(example)
        return self.child_ge.classify(example)
    def __str__(self):
        return "test: {} < {:.4f}".format(self.test_attr_name, self.test_attr_threshold)

class LeafNode(TreeNodeInterface):
    __slots__ = ("pred_class", "pred_class_count", "total_count", "prob")
    def __init__(self, pred_class, pred_class_count, total_count):
        self.pred_class = pred_class
        self.pred_class_count = pred_class_count
        self.total_count = total_count
        self.prob = pred_class_count / total_count
    def classify(self, example):
        return self.pred_class, self.prob
    def __str__(self):
        return "leaf {} {}/{}={:.2f}".format(self.pred_class, self.pred_class_count, self.total_count, self.prob)

class DecisionTree:
    __slots__ = ("id_name", "class_name", "min_leaf_count", "root")
    def __init__(self, examples, id_name, class_name, min_leaf_count=1):
        self.id_name = id_name
        self.class_name = class_name
        self.min_leaf_count = min_leaf_count
        self.root = self.learn_tree(examples)
    def learn_tree(self, examples):
        attribute_set = {attr for attr in examples[0] if attr != self.id_name and attr != self.class_name}
        return attributeSplit(attribute_set, examples, self.min_leaf_count, self.class_name)
    def classify(self, example):
        return self.root.classify(example)
    def __str__(self):
        ln_bef, ln, ln_aft = self._ascii_tree(self.root)
        return "\n".join(ln_bef + [ln] + ln_aft)
    def _ascii_tree(self, node):
        indent = 6
        if isinstance(node, LeafNode):
            return [""], "leaf {} {}/{}={:.2f}".format(node.pred_class, node.pred_class_count, node.total_count, node.prob), [""]
        child_ln_bef, child_ln, child_ln_aft = self._ascii_tree(node.child_ge)
        lines_before = [" " * indent * 2 + " " + " " * indent + line for line in child_ln_bef]
        lines_before.append(" " * indent * 2 + "\u250c" + " >={}----".format(node.test_attr_threshold) + child_ln)
        lines_before.extend([" " * indent * 2 + "|" + " " * indent + line for line in child_ln_aft])
        line_mid = node.test_attr_name
        child_ln_bef, child_ln, child_ln_aft = self._ascii_tree(node.child_lt)
        lines_after = [" " * indent * 2 + "|" + " " * indent + line for line in child_ln_bef]
        lines_after.append(" " * indent * 2 + "\u2514" + "- <{}----".format(node.test_attr_threshold) + child_ln)
        lines_after.extend([" " * indent * 2 + " " + " " * indent + line for line in child_ln_aft])
        return lines_before, line_mid, lines_after

def attributeSplit(attribute_set, examples, min_leaf_count, class_name):
    attribute_name, threshold, examples_lt, examples_ge = getBestAttributeAndSplit(attribute_set, examples, class_name)
    len_lt = len(examples_lt)
    len_ge = len(examples_ge)
    if len_ge <= min_leaf_count or len_lt <= min_leaf_count:
        predictive_class, predictive_count = getPredictiveClass(examples, class_name)
        return LeafNode(predictive_class, predictive_count, len(examples))
    attribute_set.remove(attribute_name)
    child_lt = attributeSplit(attribute_set, examples_lt, min_leaf_count, class_name)
    child_ge = attributeSplit(attribute_set, examples_ge, min_leaf_count, class_name)
    child_miss = child_lt if len_lt >= len_ge else child_ge
    return DecisionNode(attribute_name, threshold, child_lt, child_ge, child_miss)

def getBestAttributeAndSplit(attribute_set, examples, class_label):
    best_name = ""
    best_gain = 0.0
    best_threshold = None
    best_lt = []
    best_ge = []
    get_gain = getInfoGain
    for attribute in attribute_set:
        info_gain, threshold, lt, ge = get_gain(attribute, examples, class_label)
        if info_gain > best_gain:
            best_gain = info_gain
            best_name = attribute
            best_threshold = threshold
            best_lt = lt
            best_ge = ge
    return best_name, best_threshold, best_lt, best_ge

def getInfoGain(attribute, examples, class_label):
    max_info_gain = 0.0
    threshold = 0
    lt_split = []
    ge_split = []
    min_val, max_val, step = getRange(attribute, examples)
    if step <= 0:
        return max_info_gain, threshold, lt_split, ge_split
    base_entropy = entropy(examples, class_label)
    if base_entropy == 0:
        return max_info_gain, threshold, lt_split, ge_split
    cur_threshold = min_val + step
    n = len(examples)
    ent = entropy
    split = splitExamplesOnAttribute
    while cur_threshold < max_val:
        lt, ge = split(attribute, examples, cur_threshold)
        if lt or ge:
            info_gain = base_entropy - ((len(lt) / n) * ent(lt, class_label) + (len(ge) / n) * ent(ge, class_label))
            if info_gain > max_info_gain:
                max_info_gain = info_gain
                threshold = cur_threshold
                lt_split = lt
                ge_split = ge
        cur_threshold += step
    return max_info_gain, threshold, lt_split, ge_split

def getRange(attribute, examples):
    min_val = 1000000.0
    max_val = -1000000.0
    for ex in examples:
        val = ex[attribute]
        if val is None:
            continue
        if val < min_val:
            min_val = val
        if val > max_val:
            max_val = val
    step = (max_val - min_val) / 15.0
    return min_val, max_val, step

def splitExamplesOnAttribute(attribute, examples, threshold):
    lt = []
    ge = []
    for example in examples:
        val = example[attribute]
        if val is None:
            continue
        if val >= threshold:
            ge.append(example)
        else:
            lt.append(example)
    return lt, ge

def infoGain(ex0, ex1, ex2, class_label, base_entropy=None):
    total = len(ex0)
    if total == 0:
        return 0.0
    if base_entropy is None:
        base_entropy = entropy(ex0, class_label)
    return base_entropy - ((len(ex1) / total) * entropy(ex1, class_label) + (len(ex2) / total) * entropy(ex2, class_label))

def entropy(examples, class_label):
    total = len(examples)
    if total == 0:
        return 0.0
    counts = {}
    for example in examples:
        label = example[class_label]
        counts[label] = counts.get(label, 0) + 1
    ent = 0.0
    log = math.log
    for count in counts.values():
        p = count / total
        if p:
            ent -= p * log(p, 2)
    return ent

def getPredictiveClass(examples, class_label):
    class_dict = {}
    max_class = ("", 0)
    for example in examples:
        class_name = example[class_label]
        if class_name not in class_dict:
            class_dict[class_name] = 0
        else:
            class_dict[class_name] += 1
        if class_dict[class_name] > max_class[1]:
            max_class = (class_name, class_dict[class_name])
    return max_class

def test_model(model, test_examples, label_ordering):
    correct = 0
    almost = 0
    test_act_pred = {}
    label_index = {lab: i for i, lab in enumerate(label_ordering)}
    classify = model.classify
    class_name = model.class_name
    for example in test_examples:
        actual = example[class_name]
        pred, _ = classify(example)
        if pred == actual:
            correct += 1
        if abs(label_index[pred] - label_index[actual]) < 2:
            almost += 1
        key = (actual, pred)
        test_act_pred[key] = test_act_pred.get(key, 0) + 1
    total = len(test_examples)
    acc = correct / total if total else 0.0
    near_acc = almost / total if total else 0.0
    return acc, near_acc, test_act_pred

def confusion4x4(labels, vals):
    n = sum(vals.values())
    abbr = ["".join(w[0] for w in lab.split()) for lab in labels]
    s = ""
    s += " actual ___________________________________  \n"
    for ab, labp in zip(abbr, labels):
        row = [vals.get((labp, laba), 0) / n for laba in labels]
        s += "       |        |        |        |        | \n"
        s += "  {:^4s} | {:5.2f}  | {:5.2f}  | {:5.2f}  | {:5.2f}  | \n".format(ab, *row)
        s += "       |________|________|________|________| \n"
    s += "          {:^4s}     {:^4s}     {:^4s}     {:^4s} \n".format(*abbr)
    s += "                     predicted \n"
    return s

if __name__ == "__main__":
    path_to_csv = "town_vax_data.csv"
    id_attr_name = "town"
    class_attr_name = "vax_level"
    label_ordering = ["low", "medium", "high", "very high"]
    min_examples = 10
    examples = read_data(path_to_csv)
    train_examples, test_examples = train_test_split(examples, 0.25)
    tree = DecisionTree(train_examples, id_attr_name, class_attr_name, min_examples)
    accuracy, _, _ = test_model(tree, test_examples, label_ordering)
    print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# - Avoided exception-heavy conversion for known non-numeric fields during CSV parsing.
# - Replaced list-based entropy aggregation with lightweight class counts to cut memory use.
# - Cached base entropy per attribute to reduce redundant computations when scanning thresholds.
# - Added __slots__ to tree node classes to lower per-node memory overhead.
# - Precomputed label indices in evaluation to avoid repeated list searches.