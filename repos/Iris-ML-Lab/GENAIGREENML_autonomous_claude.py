# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("dataset.csv")

# Separate features and target
X = df[["sepal_length", "sepal_width", "petal_length", "petal_width"]].values
y_raw = df["species"].values

# Encode target labels
le = LabelEncoder()
y = le.fit_transform(y_raw)

# Standardize features for faster convergence and better performance
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Use stratified k-fold cross-validation for robust accuracy estimation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
accuracies = []

for train_idx, test_idx in skf.split(X, y):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Logistic Regression: lightweight, interpretable, CPU-friendly
    # solver='lbfgs' is efficient for small datasets with multiclass
    model = LogisticRegression(
        solver="lbfgs",
        max_iter=200,
        multi_class="multinomial",
        random_state=42,
        C=1.0,
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(y_test, y_pred))

# Report mean cross-validated accuracy
accuracy = np.mean(accuracies)

# Final model trained on all data for completeness
model_final = LogisticRegression(
    solver="lbfgs",
    max_iter=200,
    multi_class="multinomial",
    random_state=42,
    C=1.0,
)
model_final.fit(X, y)
y_pred_full = model_final.predict(X)
accuracy_full = accuracy_score(y, y_pred_full)

# Use CV accuracy as the reported metric (more honest than train accuracy)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Logistic Regression chosen: extremely lightweight, fast on CPU, ideal for small tabular datasets like Iris.
# 2. StandardScaler used: improves convergence speed for gradient-based solvers with negligible cost.
# 3. No deep learning or heavy models: Iris has ~150 samples; complex models would waste energy and risk overfitting.
# 4. StratifiedKFold (5-fold): provides reliable accuracy estimate while keeping computation minimal.
# 5. solver='lbfgs': efficient quasi-Newton method, well-suited for small-scale multinomial classification.
# 6. No unnecessary embeddings, feature engineering, or hyperparameter search to minimize compute.