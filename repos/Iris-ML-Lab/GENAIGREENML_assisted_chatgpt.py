# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import numpy as np


RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)


def _load_iris_csv(path: str, expected_headers: str):
    expected_cols = [h.strip() for h in expected_headers.split(",") if h.strip()]

    def _read(delimiter, decimal=None):
        arr = np.genfromtxt(
            path,
            delimiter=delimiter,
            dtype=str,
            encoding="utf-8",
            autostrip=True,
        )
        if arr.ndim == 1:
            arr = arr.reshape(1, -1)
        if arr.size == 0:
            raise ValueError("Empty dataset.")
        header = [h.strip() for h in arr[0].tolist()]
        body = arr[1:]
        if body.ndim == 1:
            body = body.reshape(1, -1)
        if body.shape[1] != len(header):
            raise ValueError("Column mismatch.")
        if len(header) != len(expected_cols):
            raise ValueError("Unexpected header width.")
        if not all(h in set(header) for h in expected_cols):
            raise ValueError("Unexpected header names.")
        col_idx = {name: header.index(name) for name in expected_cols if name in header}

        feature_names = expected_cols[:-1]
        label_name = expected_cols[-1]

        X_str = body[:, [col_idx[n] for n in feature_names]]
        y_str = body[:, col_idx[label_name]]

        if decimal is not None:
            X_str = np.char.replace(X_str, decimal, ".")

        X = X_str.astype(np.float32, copy=False)
        y = y_str.astype(str, copy=False)
        return X, y

    try:
        X, y = _read(delimiter=",", decimal=None)
    except Exception:
        X, y = _read(delimiter=";", decimal=",")

    return X, y


def _encode_labels(y: np.ndarray):
    mapping = {"setosa": 0, "versicolor": 1, "virginica": 2}
    y_clean = np.char.strip(y.astype(str, copy=False))
    try:
        y_enc = np.vectorize(mapping.__getitem__, otypes=[np.int32])(y_clean)
    except Exception as e:
        unknown = set(np.unique(y_clean)) - set(mapping.keys())
        raise ValueError(f"Unknown label(s) encountered: {sorted(unknown)}") from e
    return y_enc


def _nearest_centroid_accuracy_petal(X_all: np.ndarray, y_int: np.ndarray) -> float:
    X = X_all[:, 2:4].astype(np.float32, copy=False)

    centroids = np.empty((3, 2), dtype=np.float32)
    for k in range(3):
        mask = y_int == k
        centroids[k] = X[mask].mean(axis=0)

    d0 = X - centroids[0]
    d1 = X - centroids[1]
    d2 = X - centroids[2]
    dist2 = np.stack(
        (
            d0[:, 0] * d0[:, 0] + d0[:, 1] * d0[:, 1],
            d1[:, 0] * d1[:, 0] + d1[:, 1] * d1[:, 1],
            d2[:, 0] * d2[:, 0] + d2[:, 1] * d2[:, 1],
        ),
        axis=1,
    )

    y_pred = np.argmin(dist2, axis=1).astype(np.int32, copy=False)
    return float(np.mean(y_pred == y_int))


def main():
    X, y = _load_iris_csv("iris.csv", "sepal_length,sepal_width,petal_length,petal_width,species")
    y_int = _encode_labels(y)
    accuracy = _nearest_centroid_accuracy_petal(X, y_int)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed matplotlib usage and all plotting/saving to eliminate heavy rendering overhead and file I/O.
# - Replaced scipy.cdist with manual squared-Euclidean distance computation to avoid expensive sqrt and external dependency overhead.
# - Loaded only once and derived schema from the file header while validating against DATASET_HEADERS intent; avoids hardcoded column positions beyond the specified header order.
# - Implemented robust CSV parsing fallback ("," then ";" with decimal ",") to reduce failure retries and ensure reliable ingestion.
# - Used float32 for features/centroids to lower memory footprint and improve cache efficiency while preserving classifier behavior.
# - Avoided redundant recomputation by computing centroids once and vectorizing distance calculations across all samples.
# - Ensured reproducibility by setting a fixed NumPy seed (even though no randomness is used in the classifier) for stable execution environments.