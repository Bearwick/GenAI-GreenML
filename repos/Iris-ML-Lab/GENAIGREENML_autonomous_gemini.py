# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, r2_score

def robust_load(path):
    """Loads CSV with robust separator and decimal handling."""
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError
    except Exception:
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            # Final fallback for empty or unreadable files
            return pd.DataFrame()
    return df

def clean_and_prepare():
    """Main execution flow for energy-efficient ML."""
    df = robust_load('iris.csv')
    
    if df.empty:
        # In case of empty dataset, we cannot proceed meaningfully
        return

    # Clean column names: strip whitespace, normalize internal spaces, drop Unnamed
    df.columns = [str(c).strip() for c in df.columns]
    df.columns = [" ".join(c.split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed', case=False)]

    # Schema Inference
    # Target selection: prefer known names or the last column
    target_keywords = ['species', 'target', 'class', 'label', 'type']
    target_col = None
    for kw in target_keywords:
        matches = [c for c in df.columns if kw in c.lower()]
        if matches:
            target_col = matches[0]
            break
    
    if target_col is None:
        target_col = df.columns[-1]

    feature_cols = [c for c in df.columns if c != target_col]
    
    if not feature_cols:
        # Trivial case: no features available
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Extract X and y
    X = df[list(feature_cols)].copy()
    y = df[target_col].copy()

    # Coerce features to numeric, non-numeric become NaN
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')

    # Drop rows where target is missing or all features are missing
    valid_mask = y.notna() & X.notna().any(axis=1)
    X = X[valid_mask]
    y = y[valid_mask]

    if len(X) < 2:
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Determine if Classification or Regression
    # Iris is classification, but we provide a fallback for robustness
    is_numeric_target = np.issubdtype(y.dtype, np.number)
    unique_targets = y.nunique()

    if not is_numeric_target or unique_targets < 15:
        # Classification Path
        le = LabelEncoder()
        y_proc = le.fit_transform(y.astype(str))
        num_classes = len(np.unique(y_proc))

        if num_classes < 2:
            print(f"ACCURACY={1.000000:.6f}")
            return

        X_train, X_test, y_train, y_test = train_test_split(
            X, y_proc, test_size=0.2, random_state=42, stratify=y_proc
        )

        # Efficient Logistic Regression
        model = Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler()),
            ('clf', LogisticRegression(max_iter=500, solver='lbfgs', multi_class='auto', random_state=42))
        ])
        
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        score = accuracy_score(y_test, preds)
    else:
        # Regression Fallback (Energy-efficient Ridge)
        from sklearn.linear_model import Ridge
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        model = Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler()),
            ('reg', Ridge(alpha=1.0))
        ])
        
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        # Scale R2 to [0,1] proxy for 'accuracy'
        r2 = r2_score(y_test, preds)
        score = max(0, min(1, r2))

    print(f"ACCURACY={score:.6f}")

if __name__ == "__main__":
    clean_and_prepare()

# Optimization Summary:
# 1. Model Selection: Used Logistic Regression (classification) and Ridge (regression) as they are 
#    mathematically simple, converge quickly on CPU, and have extremely low energy consumption.
# 2. Preprocessing: Leveraged sklearn Pipelines to minimize redundant computations and ensure 
#    data is only transformed once during inference.
# 3. Memory Efficiency: Used in-place column normalization and explicit numeric coercion 
#    to prevent memory bloat from object-type overhead in pandas.
# 4. CPU-Friendliness: Avoided compute-heavy ensembles (RandomForest/XGBoost) or 
#    Deep Learning, reducing the CO2 footprint of the training cycle.
# 5. Robustness: Implemented multi-stage CSV parsing and schema-agnostic target selection 
#    to ensure the pipeline never hard-fails, maintaining reproducibility across environments.
# 6. Proxy Metric: For regression fallbacks, R-squared is used as a bounded accuracy proxy [0,1].