# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
import sys

def load_and_preprocess(filepath):
    # Robust CSV parsing
    try:
        df = pd.read_csv(filepath)
        if len(df.columns) <= 1:
            raise ValueError
    except Exception:
        df = pd.read_csv(filepath, sep=';', decimal=',')

    # Normalize column names
    df.columns = [str(c).strip() for c in df.columns]
    df.columns = [" ".join(c.split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

    if df.empty:
        return None, None

    # Derive target and features
    # Priority: column named 'species', then any column containing 'target'/'class', else the last column
    potential_targets = [c for c in df.columns if 'species' in c.lower() or 'class' in c.lower() or 'target' in c.lower()]
    if potential_targets:
        target_col = potential_targets[0]
    else:
        target_col = df.columns[-1]

    feature_cols = [c for c in df.columns if c != target_col]

    # Clean numeric data
    for col in feature_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Drop rows with missing target or features
    df = df.dropna(subset=[target_col])
    df = df.dropna(subset=feature_cols, how='all')
    df = df.fillna(0) # Simple imputation for remaining NaNs in features

    if df.empty or len(df[target_col].unique()) < 1:
        return None, None

    X = df[feature_cols]
    y = df[target_col]

    return X, y

def run_pipeline():
    # Dataset path provided in task
    dataset_path = 'iris.csv'
    
    X, y = load_and_preprocess(dataset_path)

    if X is None or y is None or len(X) < 5:
        # Trivial fallback for invalid data
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Encode target if categorical
    if y.dtype == 'object' or len(np.unique(y)) < len(y) * 0.1:
        le = LabelEncoder()
        y_encoded = le.fit_transform(y.astype(str))
        is_regression = False
    else:
        # Fallback to regression if target looks continuous
        y_encoded = y
        is_regression = True

    # Check for single class
    if not is_regression and len(np.unique(y_encoded)) < 2:
        print(f"ACCURACY={1.000000:.6f}")
        return

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42
    )

    if is_regression:
        # Simple Linear Regression for regression fallback
        from sklearn.linear_model import LinearRegression
        model = Pipeline([
            ('scaler', StandardScaler()),
            ('regressor', LinearRegression())
        ])
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        # R^2 proxy clipped to [0,1]
        from sklearn.metrics import r2_score
        score = max(0, min(1, r2_score(y_test, preds)))
        print(f"ACCURACY={score:.6f}")
    else:
        # Logistic Regression: efficient and CPU-friendly for small multi-class tasks
        model = Pipeline([
            ('scaler', StandardScaler()),
            ('classifier', LogisticRegression(max_iter=1000, multi_class='auto', solver='lbfgs', random_state=42))
        ])
        model.fit(X_train, y_train)
        accuracy = accuracy_score(y_test, model.predict(X_test))
        print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary:
# 1. Model Selection: Logistic Regression chosen for its low computational complexity (O(n_features * n_samples)) compared to ensembles or deep learning.
# 2. Hardware Efficiency: Designed to run purely on CPU with minimal memory overhead, using scikit-learn's optimized C-based solvers.
# 3. Robust Data Handling: Implemented multi-stage CSV parsing and automatic schema inference to prevent failures on malformed input.
# 4. Energy-Efficient Preprocessing: Used sklearn Pipeline to avoid redundant data transformations and maintain a small memory footprint.
# 5. Minimal Feature Engineering: Relied on StandardScaler for fast convergence of the linear solver without expensive high-dimensional projections.
# 6. Defensive Programming: Included safe fallbacks for regression/classification and handling of small or degenerate datasets.
# 7. Metrics: Used a bounded R2-score proxy for regression cases to satisfy the unique [0,1] accuracy output requirement.