# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset using sklearn's internal loader to minimize disk I/O
data = load_iris()
X = data.data
y = data.target

# Standardize features to ensure faster convergence of the optimization algorithm
# This reduces the number of iterations needed, saving CPU cycles
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data with a fixed seed for reproducibility without additional computational overhead
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Use Logistic Regression with the L-BFGS solver
# It is computationally efficient for small datasets and handles multinomial loss directly
# This avoids the overhead of complex ensembles or deep learning layers
model = LogisticRegression(
    multi_class='multinomial', 
    solver='lbfgs', 
    max_iter=200, 
    tol=1e-4
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Model Selection: Logistic Regression was chosen over Random Forests or Neural Networks
#    as it has minimal memory footprint and O(n_features) prediction complexity.
# 2. Solver Efficiency: The 'lbfgs' solver is an optimization of the BFGS algorithm 
#    that uses a limited amount of computer memory, making it ideal for CPU-bound tasks.
# 3. Convergence Speed: Pre-scaling features via StandardScaler ensures the gradient 
#    descent-like optimization converges in fewer steps, directly reducing energy consumption.
# 4. Data Handling: Used NumPy/Scikit-learn built-in structures which are implemented 
#    in C/Cython, ensuring high execution speed compared to pure Python loops.
# 5. Low Overhead: Avoided deep learning frameworks (PyTorch/TensorFlow) which carry 
#    heavy initialization costs and library overhead unnecessary for 4-feature datasets.