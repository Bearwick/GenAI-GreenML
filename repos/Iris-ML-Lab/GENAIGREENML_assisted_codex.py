# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd

np.random.seed(0)

DATASET_PATH = "iris.csv"
DATASET_HEADERS = "sepal_length,sepal_width,petal_length,petal_width,species"


def read_csv_robust(path, headers_str):
    headers = [h.strip() for h in headers_str.split(",") if h.strip()]

    def parse(cfg):
        return pd.read_csv(path, **cfg)

    cfg = {}
    df = parse(cfg)
    if df.shape[1] < len(headers):
        cfg = {"sep": ";", "decimal": ","}
        df = parse(cfg)
    if df.shape[1] == len(headers) + 1 and str(df.columns[0]).lower().startswith("unnamed"):
        df = df.iloc[:, 1:]

    def numeric_like(x):
        try:
            float(str(x))
            return True
        except ValueError:
            return False

    if df.shape[1] == len(headers) and all(numeric_like(c) for c in df.columns):
        df = parse({**cfg, "header": None})
        df.columns = headers
    elif df.shape[1] == len(headers) and not set(headers).issubset(df.columns):
        df = df.copy()
        df.columns = headers
    return df


def encode_labels(labels):
    if np.issubdtype(labels.dtype, np.number):
        unique = np.unique(labels)
        if np.array_equal(unique, np.arange(unique.size)):
            return labels.astype(int)
        return pd.factorize(labels)[0]
    series = pd.Series(labels).astype(str).str.strip().str.lower()
    label_map = {"setosa": 0, "versicolor": 1, "virginica": 2}
    mapped = series.map(label_map)
    if mapped.isna().any():
        numeric = pd.to_numeric(series, errors="coerce")
        if numeric.isna().any():
            return pd.factorize(series)[0]
        return numeric.astype(int).to_numpy()
    return mapped.astype(int).to_numpy()


def main():
    headers = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]
    df = read_csv_robust(DATASET_PATH, DATASET_HEADERS)

    if all(h in df.columns for h in headers):
        df = df[headers]
        label_col = headers[-1]
        feature_cols = headers[:-1]
    else:
        label_col = df.columns[-1]
        feature_cols = list(df.columns[:-1])

    if all(h in df.columns for h in headers):
        petal_cols = headers[-3:-1]
    else:
        petal_cols = feature_cols[-2:]

    petal_features = df[petal_cols].to_numpy(dtype=np.float64, copy=False)
    labels_raw = df[label_col].to_numpy()
    labels_int = encode_labels(labels_raw)

    n_classes = int(labels_int.max()) + 1
    centroids = np.empty((n_classes, petal_features.shape[1]), dtype=np.float64)
    for i in range(n_classes):
        centroids[i] = petal_features[labels_int == i].mean(axis=0)

    sample = np.array([3.1, 1.2], dtype=np.float64)
    centroid_sq = np.sum(centroids ** 2, axis=1)
    sample_sq = np.sum(sample ** 2)
    dist_sq_sample = sample_sq + centroid_sq - 2 * centroids.dot(sample)
    dist_sq_sample = np.maximum(dist_sq_sample, 0)
    _ = int(np.argmin(dist_sq_sample))

    feature_sq = np.sum(petal_features ** 2, axis=1, keepdims=True)
    dist_sq = feature_sq + centroid_sq - 2 * petal_features.dot(centroids.T)
    dist_sq = np.maximum(dist_sq, 0)
    y_pred = np.argmin(dist_sq, axis=1)
    accuracy = np.mean(y_pred == labels_int)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed visualization and file output to eliminate unnecessary computation and disk I/O.
# - Replaced SciPy distance calculations with vectorized NumPy formulas to reduce dependencies and overhead.
# - Used squared-distance algebra to avoid square roots and large intermediate tensors.
# - Selected only required columns after robust CSV parsing to minimize data movement and memory use.
# - Set a fixed random seed to ensure deterministic execution.