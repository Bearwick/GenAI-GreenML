# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "iris.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _safe_read_csv(path):
    df = None
    # Try default parsing
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    # If default parsing failed or likely wrong (e.g., single column with separators), retry robustly
    if df is None or df.empty or (df.shape[1] == 1 and isinstance(df.columns[0], str) and ("," in df.columns[0] or ";" in df.columns[0])):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df = None

    if df is None:
        # Final fallback: attempt python engine with auto delimiter inference
        df = pd.read_csv(path, sep=None, engine="python")

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)
    return df


def _choose_target(df):
    # Prefer common supervised target patterns
    preferred = ["target", "label", "class", "species", "y"]
    col_map = {c.lower(): c for c in df.columns}
    for p in preferred:
        if p in col_map:
            return col_map[p]

    # Prefer last non-constant object/categorical column
    for c in reversed(df.columns.tolist()):
        s = df[c]
        if s.dtype == "object" or str(s.dtype).startswith("category"):
            if s.nunique(dropna=True) >= 2:
                return c

    # Otherwise pick a numeric non-constant column
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    for c in reversed(numeric_cols):
        if df[c].nunique(dropna=True) >= 2:
            return c

    # As a last resort, pick the last column
    return df.columns[-1]


def _coerce_numeric_columns(df, numeric_cols):
    for c in numeric_cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _bounded_regression_score(y_true, y_pred):
    # Convert regression quality to a stable [0,1] proxy:
    # score = 1 / (1 + RMSE / (std(y_true)+eps))
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2))) if y_true.size else 0.0
    denom = float(np.std(y_true)) + 1e-12
    return float(1.0 / (1.0 + (rmse / denom)))


def main():
    df = _safe_read_csv(DATASET_PATH)

    # Defensive: ensure non-empty after load
    assert df is not None and df.shape[0] > 0 and df.shape[1] > 0

    # Normalize column names again after any potential modifications
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    target_col = _choose_target(df)

    # Build feature set from available columns (robust to missing expected headers)
    feature_cols = [c for c in df.columns if c != target_col]
    if not feature_cols:
        # If only one column exists, create a trivial constant feature
        df["_const_feature_"] = 1.0
        feature_cols = ["_const_feature_"]

    X = df[feature_cols].copy()
    y = df[target_col].copy()

    # Identify numeric vs categorical based on current dtypes; coerce numeric candidates safely
    # Attempt to coerce columns that look numeric by name or by sampling
    numeric_candidates = []
    for c in X.columns:
        if pd.api.types.is_numeric_dtype(X[c]):
            numeric_candidates.append(c)
        else:
            # Try coercion; if many values become numeric, treat as numeric
            coerced = pd.to_numeric(X[c], errors="coerce")
            non_na_ratio = float(np.mean(~coerced.isna())) if len(coerced) else 0.0
            if non_na_ratio >= 0.6:
                X[c] = coerced
                numeric_candidates.append(c)

    numeric_features = [c for c in X.columns if c in numeric_candidates and pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    # Replace inf with NaN (safe for later imputers)
    if numeric_features:
        X[numeric_features] = X[numeric_features].replace([np.inf, -np.inf], np.nan)

    # Decide classification vs regression robustly
    is_classification = False
    y_is_numeric = pd.api.types.is_numeric_dtype(y)
    if not y_is_numeric:
        is_classification = True
    else:
        # If numeric but low number of unique values, treat as classification
        nunique = int(pd.Series(y).nunique(dropna=True))
        if 2 <= nunique <= max(20, int(0.1 * len(y))):
            is_classification = True

    # Handle target cleaning
    if is_classification:
        y = y.astype("object")
        # Drop rows where y is missing
        mask = ~pd.isna(y)
        X = X.loc[mask].copy()
        y = y.loc[mask].copy()
    else:
        y = pd.to_numeric(y, errors="coerce")
        mask = ~pd.isna(y)
        X = X.loc[mask].copy()
        y = y.loc[mask].copy()

    assert X.shape[0] > 1 and len(y) == X.shape[0]

    # Train/test split
    stratify = None
    if is_classification:
        # If too few per class, stratify can fail; guard it
        vc = pd.Series(y).value_counts(dropna=True)
        if (vc.min() >= 2) and (vc.shape[0] >= 2):
            stratify = y

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
    )

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    # If classification but only one class in training, fallback safely
    if is_classification and pd.Series(y_train).nunique(dropna=True) < 2:
        # Trivial baseline: always predict the only class
        only_class = pd.Series(y_train).dropna().iloc[0] if len(pd.Series(y_train).dropna()) else None
        y_pred = np.array([only_class] * len(y_test), dtype=object)
        accuracy = float(accuracy_score(y_test, y_pred)) if len(y_test) else 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Preprocessing pipeline
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if is_classification:
        # CPU-friendly linear model; small dataset -> deterministic, fast, robust
        clf = LogisticRegression(
            max_iter=200,
            solver="lbfgs",
            n_jobs=1,
            multi_class="auto",
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight, CPU-friendly linear models (LogisticRegression/Ridge) instead of ensembles/deep learning to minimize compute and energy.
# - ColumnTransformer+Pipeline ensures single-pass, reproducible preprocessing and avoids redundant conversions.
# - Robust CSV loader retries with alternative delimiter/decimal to prevent costly manual debugging and reruns.
# - Numeric coercion is guarded and minimal; inf/NaN handled via SimpleImputer to keep computation stable and efficient.
# - Sparse one-hot encoding for categoricals reduces memory and CPU compared to dense encoding.
# - If regression fallback is used, converts RMSE to a bounded [0,1] proxy score: 1/(1+RMSE/std), printed as ACCURACY for a stable metric.