# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "iris.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _read_csv_robust(path):
    df = pd.read_csv(path)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Heuristic: if only one column, likely wrong separator -> retry with ';' and decimal ','
    if df.shape[1] <= 1:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        df2.columns = _normalize_columns(df2.columns)
        df2 = _drop_unnamed(df2)
        if df2.shape[1] > df.shape[1]:
            df = df2

    # If still odd, attempt a last fallback with python engine and sep inference (lightweight)
    if df.shape[1] <= 1:
        df3 = pd.read_csv(path, sep=None, engine="python")
        df3.columns = _normalize_columns(df3.columns)
        df3 = _drop_unnamed(df3)
        if df3.shape[1] > df.shape[1]:
            df = df3

    return df


def _is_probably_numeric_series(s):
    if pd.api.types.is_numeric_dtype(s):
        return True
    if pd.api.types.is_bool_dtype(s):
        return True
    if pd.api.types.is_datetime64_any_dtype(s):
        return False
    # Try coercion ratio
    coerced = pd.to_numeric(s, errors="coerce")
    non_na = coerced.notna().mean() if len(coerced) else 0.0
    return non_na >= 0.8


def _choose_target(df, preferred_name=None):
    cols = list(df.columns)
    if preferred_name is not None and preferred_name in cols:
        return preferred_name

    # Prefer last column if it looks like a label (few unique values)
    if cols:
        last = cols[-1]
        nunique = df[last].nunique(dropna=True)
        if 2 <= nunique <= max(10, int(0.2 * len(df))):
            return last

    # Prefer non-numeric categorical with few unique values
    candidate = None
    best_card = None
    for c in cols:
        nunique = df[c].nunique(dropna=True)
        if nunique < 2:
            continue
        if not _is_probably_numeric_series(df[c]):
            if best_card is None or nunique < best_card:
                candidate = c
                best_card = nunique
    if candidate is not None:
        return candidate

    # Fallback: choose a non-constant numeric column (regression)
    for c in cols:
        if _is_probably_numeric_series(df[c]):
            coerced = pd.to_numeric(df[c], errors="coerce")
            if coerced.nunique(dropna=True) >= 2:
                return c

    # Absolute fallback
    return cols[-1] if cols else None


def _bounded_regression_score(y_true, y_pred):
    # 1 / (1 + MSE) is stable in [0,1], avoids negative values and is cheap
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mse = np.mean((y_true - y_pred) ** 2) if y_true.size else np.inf
    if not np.isfinite(mse):
        return 0.0
    return float(1.0 / (1.0 + mse))


def main():
    if not os.path.exists(DATASET_PATH):
        # Minimal stdout requirement: still must print ACCURACY line; return 0 baseline
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _read_csv_robust(DATASET_PATH)

    # Normalize column names again defensively
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Strip whitespace in object columns (cheap, helps label consistency)
    for c in df.columns:
        if pd.api.types.is_object_dtype(df[c]):
            df[c] = df[c].astype(str).str.strip()

    # Choose target
    target_col = _choose_target(df, preferred_name="species" if "species" in df.columns else None)
    if target_col is None or target_col not in df.columns:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Separate X/y
    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # If no features, create a constant feature to allow pipeline to run
    if X.shape[1] == 0:
        X = pd.DataFrame({"__bias__": np.ones(len(df), dtype=float)})

    # Identify numeric/categorical features robustly
    numeric_features = []
    categorical_features = []
    for c in X.columns:
        if _is_probably_numeric_series(X[c]):
            numeric_features.append(c)
        else:
            categorical_features.append(c)

    # Coerce numeric columns safely
    for c in numeric_features:
        X[c] = pd.to_numeric(X[c], errors="coerce")
        # Replace inf with NaN for imputation
        X[c] = X[c].replace([np.inf, -np.inf], np.nan)

    assert len(df) > 0, "Dataset is empty after loading."

    # Decide task type
    y_is_numeric = _is_probably_numeric_series(y_raw)
    if y_is_numeric:
        y = pd.to_numeric(y_raw, errors="coerce").replace([np.inf, -np.inf], np.nan)
    else:
        y = y_raw

    # Drop rows where y is missing (keeps pipeline simpler/cheaper than custom missing-label handling)
    valid_mask = y.notna() if hasattr(y, "notna") else ~pd.isna(y)
    X = X.loc[valid_mask].reset_index(drop=True)
    y = y.loc[valid_mask].reset_index(drop=True)

    assert len(X) > 1, "Not enough samples after dropping missing labels."

    # Determine classification viability
    is_classification = not y_is_numeric
    if is_classification:
        n_classes = pd.Series(y).nunique(dropna=True)
        if n_classes < 2:
            is_classification = False  # fallback to regression-like scoring on coerced numeric codes
            y = pd.factorize(y)[0].astype(float)

    # Preprocessing
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Split
    stratify = y if is_classification and pd.Series(y).nunique() >= 2 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.2,
        random_state=RANDOM_STATE,
        stratify=stratify
    )
    assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

    if is_classification:
        # Small, CPU-friendly baseline
        model = LogisticRegression(
            solver="lbfgs",
            max_iter=200,
            n_jobs=1,
            multi_class="auto"
        )
        clf = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Regression fallback (lightweight) + bounded score in [0,1]
        y_train_num = pd.to_numeric(pd.Series(y_train), errors="coerce").astype(float)
        y_test_num = pd.to_numeric(pd.Series(y_test), errors="coerce").astype(float)

        # If coercion creates NaNs, align by dropping
        train_mask = y_train_num.notna().values
        test_mask = y_test_num.notna().values
        X_train2 = X_train.loc[train_mask].reset_index(drop=True)
        y_train2 = y_train_num.loc[train_mask].reset_index(drop=True)
        X_test2 = X_test.loc[test_mask].reset_index(drop=True)
        y_test2 = y_test_num.loc[test_mask].reset_index(drop=True)

        if len(X_train2) < 2 or len(X_test2) < 1:
            accuracy = 0.0
            print(f"ACCURACY={accuracy:.6f}")
            return

        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        pipe = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", reg),
        ])
        pipe.fit(X_train2, y_train2)
        y_pred = pipe.predict(X_test2)
        accuracy = _bounded_regression_score(y_test2, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight, CPU-friendly models (LogisticRegression / Ridge) instead of heavy ensembles or deep learning.
# - ColumnTransformer+Pipeline ensures single-pass preprocessing, avoiding redundant computation and improving reproducibility.
# - Robust CSV parsing retries common delimiter/decimal variants and normalizes headers to prevent schema brittleness.
# - Numeric coercion with safe NaN/inf handling avoids costly errors and keeps preprocessing deterministic.
# - Minimal feature engineering: median/most_frequent imputation, scaling for linear models, and sparse one-hot encoding for categoricals.
# - Fixed random_state and simple train/test split for reproducible, low-overhead evaluation.
# - Regression fallback uses a bounded proxy score ACCURACY=1/(1+MSE) in [0,1] to remain stable and comparable.