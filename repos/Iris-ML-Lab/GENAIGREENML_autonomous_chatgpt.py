# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge


DATASET_PATH = "iris.csv"
DATASET_HEADERS = ["sepal_length", "sepal_width", "petal_length", "petal_width", "species"]


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _read_csv_robust(path):
    # Attempt 1: default CSV parsing
    df1 = pd.read_csv(path)
    df1.columns = _normalize_columns(df1.columns)
    df1 = _drop_unnamed(df1)

    # Heuristic check for bad parsing: single column containing separators
    bad_parse = False
    if df1.shape[1] == 1:
        sample = df1.iloc[:20, 0].astype(str)
        if sample.str.contains(";").mean() > 0.2 or sample.str.contains(",").mean() > 0.6:
            bad_parse = True

    # Attempt 2: European style
    if bad_parse:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        df2.columns = _normalize_columns(df2.columns)
        df2 = _drop_unnamed(df2)
        # Pick the parse with more columns / more non-null cells
        score1 = df1.notna().sum().sum()
        score2 = df2.notna().sum().sum()
        if df2.shape[1] > df1.shape[1] or score2 > score1:
            return df2

    return df1


def _select_target_and_features(df, expected_headers):
    cols_lower = {c.lower(): c for c in df.columns}
    # Prefer expected target if present
    target = None
    if "species" in cols_lower:
        target = cols_lower["species"]

    # If not present, choose a reasonable fallback target
    if target is None:
        # Prefer a non-constant non-numeric/object column; else numeric non-constant
        candidates = [c for c in df.columns if df[c].nunique(dropna=True) > 1]
        obj_candidates = [c for c in candidates if df[c].dtype == "object"]
        if obj_candidates:
            target = obj_candidates[-1]
        else:
            num_candidates = []
            for c in candidates:
                s = pd.to_numeric(df[c], errors="coerce")
                if s.notna().sum() > 0 and s.nunique(dropna=True) > 1:
                    num_candidates.append(c)
            target = num_candidates[-1] if num_candidates else (df.columns[-1] if len(df.columns) else None)

    # Features: prefer expected numeric feature names if present; else all except target
    preferred = [h for h in expected_headers if h in df.columns and h != target]
    if preferred:
        features = preferred
    else:
        features = [c for c in df.columns if c != target]

    return target, features


def _is_classification_target(y):
    # Classification if non-numeric or small number of unique values
    if y.dtype == "object":
        return True
    y_num = pd.to_numeric(y, errors="coerce")
    # If mostly numeric and many unique values -> regression
    if y_num.notna().mean() > 0.9:
        nunq = y_num.nunique(dropna=True)
        return nunq <= 20
    return True


def _bounded_regression_score(y_true, y_pred):
    # Stable proxy in [0,1]: 1/(1+MAE/scale), with scale based on IQR or std fallback
    yt = pd.to_numeric(pd.Series(y_true), errors="coerce").to_numpy()
    yp = pd.to_numeric(pd.Series(y_pred), errors="coerce").to_numpy()
    mask = np.isfinite(yt) & np.isfinite(yp)
    if mask.sum() == 0:
        return 0.0
    yt = yt[mask]
    yp = yp[mask]
    mae = np.mean(np.abs(yt - yp))
    q75, q25 = np.percentile(yt, [75, 25])
    scale = q75 - q25
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = np.std(yt)
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = max(1.0, np.mean(np.abs(yt)) + 1e-12)
    return float(1.0 / (1.0 + (mae / (scale + 1e-12))))


def main():
    if not os.path.exists(DATASET_PATH):
        # Minimal no-stdout behavior; create empty accuracy
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # If headers were read as one row of data, try to fix by re-reading with header=None
    if df.shape[1] == 1 and isinstance(df.iloc[0, 0], str) and ("," in df.iloc[0, 0] or ";" in df.iloc[0, 0]):
        try:
            df_alt = pd.read_csv(DATASET_PATH, header=None)
            df_alt.columns = [f"col_{i}" for i in range(df_alt.shape[1])]
            df_alt.columns = _normalize_columns(df_alt.columns)
            df_alt = _drop_unnamed(df_alt)
            if df_alt.shape[1] > df.shape[1]:
                df = df_alt
        except Exception:
            pass

    assert df is not None and df.shape[0] > 0 and df.shape[1] > 0

    target_col, feature_cols = _select_target_and_features(df, DATASET_HEADERS)
    if target_col is None or target_col not in df.columns:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Ensure features subset exists
    feature_cols = [c for c in feature_cols if c in df.columns and c != target_col]
    if len(feature_cols) == 0:
        # fallback: use any other column(s)
        feature_cols = [c for c in df.columns if c != target_col][: min(4, max(1, len(df.columns) - 1))]

    X = df[feature_cols].copy()
    y = df[target_col].copy()

    # Determine task type
    is_clf = _is_classification_target(y)

    # Pre-cast numeric columns safely (avoid later object->numeric issues)
    # We'll let ColumnTransformer handle imputing/scaling, but detect numeric columns robustly
    numeric_cols = []
    categorical_cols = []
    for c in X.columns:
        s_num = pd.to_numeric(X[c], errors="coerce")
        # numeric if enough numeric values or dtype already numeric
        if pd.api.types.is_numeric_dtype(X[c]) or (s_num.notna().mean() >= 0.8 and s_num.nunique(dropna=True) > 0):
            X[c] = s_num
            numeric_cols.append(c)
        else:
            X[c] = X[c].astype("object")
            categorical_cols.append(c)

    # Clean target for modeling
    if is_clf:
        y_clean = y.astype("object").fillna("MISSING")
        n_classes = y_clean.nunique(dropna=True)
        if n_classes < 2:
            # fallback to regression proxy on numeric conversion
            is_clf = False
            y_clean = pd.to_numeric(y, errors="coerce")
    else:
        y_clean = pd.to_numeric(y, errors="coerce")

    # Drop rows where regression target is NaN
    if not is_clf:
        mask = np.isfinite(y_clean.to_numpy(dtype=float, na_value=np.nan))
        X = X.loc[mask].copy()
        y_clean = y_clean.loc[mask].copy()

    assert X.shape[0] > 1

    # Split
    stratify = y_clean if is_clf and y_clean.nunique() >= 2 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_clean, test_size=0.2, random_state=42, stratify=stratify
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    # Preprocessing
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Model choice: lightweight linear models
    if is_clf:
        # Use a small, CPU-friendly solver with bounded iterations
        model = LogisticRegression(
            max_iter=200,
            solver="lbfgs",
            multi_class="auto",
            n_jobs=1,
        )
        clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        model = Ridge(alpha=1.0, random_state=42)
        reg = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        accuracy = _bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Robust CSV ingestion: default read_csv first, then fallback to sep=';' and decimal=',' to avoid expensive retries and failures.
# - Schema-agnostic: normalize column names, drop 'Unnamed' columns, and select target/features defensively to run end-to-end on imperfect inputs.
# - CPU/energy efficient baseline: LogisticRegression (lbfgs, n_jobs=1, low max_iter) for classification; Ridge for regression fallback.
# - Minimal preprocessing via ColumnTransformer+Pipeline to avoid redundant passes and ensure reproducibility.
# - Lightweight feature handling: median/most_frequent imputation, StandardScaler only for numeric, OneHotEncoder only for categoricals.
# - Regression fallback score: bounded [0,1] proxy based on MAE normalized by robust scale (IQR/std) to keep a stable "ACCURACY" value.