# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _find_dataset_path() -> str:
    # Prefer explicit env var when provided; otherwise try common local filenames.
    env_path = os.environ.get("DATASET_PATH", "").strip()
    if env_path and os.path.exists(env_path):
        return env_path

    candidates = (
        "data.csv",
        "dataset.csv",
        "iris.csv",
        "Iris.csv",
        "train.csv",
        "iris.data",
    )
    for p in candidates:
        if os.path.exists(p):
            return p

    # Last resort: search current directory for a CSV containing the required headers.
    required = {"sepal_length", "sepal_width", "petal_length", "petal_width", "species"}
    for fname in os.listdir("."):
        if not fname.lower().endswith(".csv"):
            continue
        try:
            df_head = pd.read_csv(fname, nrows=0)
            cols = set(map(str.lower, df_head.columns.astype(str)))
            if required.issubset(cols):
                return fname
        except Exception:
            pass

    raise FileNotFoundError(
        "Dataset file not found. Set DATASET_PATH or place a CSV with Iris headers in the working directory."
    )


def main() -> None:
    path = _find_dataset_path()
    df = pd.read_csv(path)

    # Normalize column names for robustness (energy-cheap string ops on small data).
    df.columns = [c.strip().lower() for c in df.columns]

    feature_cols = ["sepal_length", "sepal_width", "petal_length", "petal_width"]
    target_col = "species"

    X = df[feature_cols].copy()
    y_raw = df[target_col].astype(str).copy()

    # Encode labels with a lightweight encoder (no one-hot to keep memory small).
    le = LabelEncoder()
    y = le.fit_transform(y_raw)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Simple pipeline: imputation + scaling + linear classifier.
    # LogisticRegression with lbfgs is efficient on small dense tabular datasets.
    pipeline = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
            ("clf", LogisticRegression(max_iter=200, solver="lbfgs", multi_class="auto")),
        ]
    )

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Used a linear model (LogisticRegression) suited for small tabular data; avoids deep learning to reduce compute/energy.
# - Kept preprocessing minimal and reproducible with a scikit-learn Pipeline (imputation + standardization).
# - Median imputation is lightweight and robust; StandardScaler helps linear optimization converge quickly.
# - CPU-friendly defaults: small max_iter, efficient solver (lbfgs), no ensembling, no feature expansion.
# - Avoided model saving, interactive inputs, plots, and unnecessary logging to reduce I/O and runtime overhead.