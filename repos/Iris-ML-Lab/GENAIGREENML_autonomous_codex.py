# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, r2_score

PATH = "iris.csv"
DATASET_HEADERS = ["sepal_length", "sepal_width", "petal_length", "petal_width", "species"]

def read_dataset(path):
    df = pd.DataFrame()
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.DataFrame()
    if df.empty or df.shape[1] == 1:
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df.empty or df2.shape[1] > df.shape[1]:
                df = df2
        except Exception:
            pass
    return df

def normalize_columns(df):
    def norm_col(c):
        if not isinstance(c, str):
            c = str(c)
        c = c.strip()
        c = " ".join(c.split())
        return c
    df = df.copy()
    df.columns = [norm_col(c) for c in df.columns]
    keep_cols = [c for c in df.columns if c and not c.lower().startswith("unnamed")]
    df = df[keep_cols]
    return df

def is_number_like(x):
    try:
        float(x)
        return True
    except Exception:
        return False

def choose_target_column(df, headers=None):
    if df is None or df.empty:
        return None
    if headers:
        for h in headers:
            if h in df.columns and h.lower() in ["target", "label", "class", "species", "outcome", "y"]:
                return h
    keywords = ["target", "label", "class", "species", "outcome", "y"]
    for col in df.columns:
        col_lower = col.lower()
        if col_lower in keywords or any(k in col_lower for k in keywords):
            return col
    for col in df.columns:
        if df[col].dtype == object:
            if df[col].nunique(dropna=True) > 1:
                return col
    for col in df.columns:
        col_num = pd.to_numeric(df[col], errors="coerce")
        if col_num.nunique(dropna=True) > 1:
            return col
    return df.columns[-1]

def infer_classification(y):
    if y.dtype == object or y.dtype.name == "category":
        return True
    y_num = pd.to_numeric(y, errors="coerce")
    non_nan = y_num.notna().sum()
    if non_nan == 0:
        return False
    unique = y_num.nunique(dropna=True)
    if unique <= max(2, min(20, int(0.05 * len(y_num)) + 1)):
        if np.all(np.isclose(y_num.dropna(), np.round(y_num.dropna()))):
            return True
    return False

df = read_dataset(PATH)
df = normalize_columns(df)

if DATASET_HEADERS and len(df.columns) == len(DATASET_HEADERS):
    intersection = set([c.lower() for c in df.columns]).intersection(set([h.lower() for h in DATASET_HEADERS]))
    if len(intersection) < max(1, len(DATASET_HEADERS) // 2):
        if all(is_number_like(c) for c in df.columns):
            try:
                df2 = pd.read_csv(PATH, header=None, names=DATASET_HEADERS)
                if df2.shape[0] >= df.shape[0]:
                    df = df2
            except Exception:
                pass
    df = normalize_columns(df)

assert df.shape[0] > 0 and df.shape[1] > 0

target_col = choose_target_column(df, DATASET_HEADERS)
if target_col is None:
    target_col = df.columns[-1]

df = df.replace([np.inf, -np.inf], np.nan)

y_raw = df[target_col]
is_classification = infer_classification(y_raw)

if is_classification:
    mask = y_raw.notna()
    df = df.loc[mask]
    y = y_raw.loc[mask].astype(str)
else:
    y_num = pd.to_numeric(y_raw, errors="coerce")
    mask = y_num.notna()
    df = df.loc[mask]
    y = y_num.loc[mask]

features = [c for c in df.columns if c != target_col]
X = df[features].copy() if features else pd.DataFrame(index=df.index)
X = X.replace([np.inf, -np.inf], np.nan)

numeric_cols = []
categorical_cols = []
drop_cols = []

for col in X.columns:
    series = X[col]
    if pd.api.types.is_bool_dtype(series):
        X[col] = series.astype(float)
        if series.notna().sum() > 0:
            numeric_cols.append(col)
        else:
            drop_cols.append(col)
        continue
    coerced = pd.to_numeric(series, errors="coerce")
    non_nan_ratio = coerced.notna().mean()
    if pd.api.types.is_numeric_dtype(series) or non_nan_ratio >= 0.8:
        X[col] = coerced
        if coerced.notna().sum() > 0:
            numeric_cols.append(col)
        else:
            drop_cols.append(col)
    else:
        if series.notna().sum() > 0:
            categorical_cols.append(col)
        else:
            drop_cols.append(col)

if drop_cols:
    X = X.drop(columns=drop_cols)
    numeric_cols = [c for c in numeric_cols if c in X.columns]
    categorical_cols = [c for c in categorical_cols if c in X.columns]

if X.shape[1] == 0:
    X = pd.DataFrame({"dummy": np.zeros(len(df))}, index=df.index)
    numeric_cols = ["dummy"]
    categorical_cols = []

if is_classification:
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    if len(le.classes_) < 2:
        is_classification = False
        y_numeric = pd.to_numeric(pd.Series(y), errors="coerce")
        if y_numeric.notna().sum() == 0:
            y_numeric = pd.Series(np.zeros(len(y)), index=df.index)
        y = y_numeric.values
    else:
        y = y_encoded
else:
    y_numeric = pd.to_numeric(pd.Series(y), errors="coerce")
    if y_numeric.notna().sum() == 0:
        y_numeric = pd.Series(np.zeros(len(df)), index=df.index)
    y = y_numeric.values

assert len(X) > 0
assert len(X) == len(y)

n_samples = len(X)
test_size = 0.2 if n_samples >= 5 else 1

stratify = None
if is_classification:
    unique_classes = np.unique(y)
    if len(unique_classes) > 1:
        counts = pd.Series(y).value_counts()
        if counts.min() >= 2 and n_samples >= 4:
            stratify = y

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=stratify)

assert len(X_train) > 0 and len(X_test) > 0

if is_classification and len(np.unique(y_train)) < 2:
    is_classification = False
    y_train = pd.to_numeric(pd.Series(y_train), errors="coerce").fillna(0.0).values
    y_test = pd.to_numeric(pd.Series(y_test), errors="coerce").fillna(0.0).values

numeric_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="median"))])
categorical_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="most_frequent")),
                                          ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=False))])

transformers = []
if numeric_cols:
    transformers.append(("num", numeric_transformer, numeric_cols))
if categorical_cols:
    transformers.append(("cat", categorical_transformer, categorical_cols))
if not transformers:
    X_train = pd.DataFrame({"dummy": np.zeros(len(X_train))}, index=X_train.index)
    X_test = pd.DataFrame({"dummy": np.zeros(len(X_test))}, index=X_test.index)
    numeric_cols = ["dummy"]
    transformers = [("num", numeric_transformer, numeric_cols)]

preprocessor = ColumnTransformer(transformers=transformers, remainder="drop")

if is_classification:
    model = LogisticRegression(max_iter=200, solver="liblinear")
else:
    model = LinearRegression()

pipeline = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

if is_classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    if len(np.unique(y_test)) > 1:
        r2 = r2_score(y_test, y_pred)
    else:
        r2 = 0.0
    if not np.isfinite(r2):
        r2 = 0.0
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# - Selected a lightweight linear/logistic model with simple imputations and one-hot encoding to stay CPU-friendly.
# - Limited preprocessing to essential steps and mapped regression quality to a bounded (RÂ²+1)/2 score for a stable accuracy proxy.