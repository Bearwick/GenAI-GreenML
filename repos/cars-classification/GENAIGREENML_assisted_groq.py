# Generated by generate_llm_code_2.py
# LLM: groq
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix

# Load and preprocess data
dataset = pd.read_csv('cars.csv')
dataset.columns = ['mpg', 'cylinders', 'cubicinches', 'hp',
                   'weightlbs', 'time-to-60', 'year', 'brand']

# Convert numeric columns to float and drop rows with missing values
numeric_cols = ['cubicinches', 'weightlbs']
dataset[numeric_cols] = dataset[numeric_cols].apply(pd.to_numeric, errors='coerce')
dataset.dropna(inplace=True)

# Feature matrix and target vector
X = dataset[['mpg', 'cylinders', 'cubicinches', 'hp', 'weightlbs']].astype(np.float32).values
y = dataset['brand'].values

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=0)

# Train SVM classifier
classifier = SVC()
classifier.fit(X_train, y_train)

# Evaluate accuracy
accuracy = classifier.score(X_test, y_test)
print(f"ACCURACY={accuracy:.6f}")

# Predict and compute confusion matrix
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

# ----------------------------------------------------
# Applied Optimizations:
# 1. Removed unnecessary imports (seaborn, matplotlib) and visualizations to save memory and runtime.
# 2. Eliminated redundant data cleaning steps; only one dropna() after converting numeric columns.
# 3. Converted feature array to float32 to reduce memory footprint.
# 4. Directly selected required columns with pandas slicing, avoiding intermediate copies.
# 5. Kept the split and training deterministic with a fixed random_state for reproducibility.
# 6. Avoided unused variables and prints, focusing only on essential outputs.
# ----------------------------------------------------