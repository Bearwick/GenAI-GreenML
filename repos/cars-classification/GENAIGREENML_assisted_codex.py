# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

COLUMN_NAMES = ["mpg", "cylinders", "cubicinches", "hp", "weightlbs", "time-to-60", "year", "brand"]
FEATURE_COLS = ["mpg", "cylinders", "cubicinches", "hp", "weightlbs"]

def load_data(path):
    df = pd.read_csv(path)
    df.columns = COLUMN_NAMES
    df["cubicinches"] = pd.to_numeric(df["cubicinches"], errors="coerce")
    df["weightlbs"] = pd.to_numeric(df["weightlbs"], errors="coerce")
    df.dropna(inplace=True)
    X = df[FEATURE_COLS].to_numpy()
    y = df["brand"].to_numpy()
    return X, y

def train_and_evaluate(X, y):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=0
    )
    model = SVC()
    model.fit(X_train, y_train)
    return model.score(X_test, y_test)

def main():
    X, y = load_data("cars.csv")
    accuracy = train_and_evaluate(X, y)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Removed unused visualization imports and steps to reduce overhead.
# - Consolidated data cleaning into a single pass and removed redundant variables.
# - Selected only required feature columns and converted directly to NumPy arrays to limit data movement.
# - Used model.score to avoid redundant prediction computations while obtaining accuracy.