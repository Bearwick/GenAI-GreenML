# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c = str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        normed.append(c)
    return normed


def _read_csv_robust(path):
    df = None
    last_err = None
    for kwargs in (
        dict(),
        dict(sep=";", decimal=","),
    ):
        try:
            df_try = pd.read_csv(path, **kwargs)
            if isinstance(df_try, pd.DataFrame) and df_try.shape[0] > 0:
                if df_try.shape[1] >= 2:
                    df = df_try
                    break
        except Exception as e:
            last_err = e
            continue
    if df is None:
        if last_err is not None:
            raise last_err
        raise RuntimeError("Failed to read CSV.")
    return df


def _pick_target(df, preferred_headers=None):
    preferred_headers = preferred_headers or []
    cols = list(df.columns)

    lowered = {c.lower(): c for c in cols}
    for p in preferred_headers:
        if p.lower() in lowered:
            return lowered[p.lower()]

    obj_cols = [c for c in cols if df[c].dtype == "object" or str(df[c].dtype).startswith("string")]
    best_obj = None
    best_classes = -1
    for c in obj_cols:
        nun = df[c].nunique(dropna=True)
        if 2 <= nun <= 50 and nun > best_classes:
            best_classes = nun
            best_obj = c
    if best_obj is not None:
        return best_obj

    num_cols = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        nun = s.nunique(dropna=True)
        if nun >= 2:
            num_cols.append((c, nun))
    if num_cols:
        num_cols.sort(key=lambda x: x[1], reverse=True)
        return num_cols[0][0]

    return cols[-1] if cols else None


def _safe_accuracy_proxy_from_r2(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    if y_true.size == 0:
        return 0.0
    y_mean = float(np.mean(y_true))
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    ss_tot = float(np.sum((y_true - y_mean) ** 2))
    if ss_tot <= 0.0:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    acc = 0.5 + 0.5 * r2
    return float(np.clip(acc, 0.0, 1.0))


def main():
    dataset_headers = ["mpg", "cylinders", "cubicinches", "hp", "weightlbs", "time-to-60", "year", "brand"]

    csv_path = "cars.csv"
    if not os.path.exists(csv_path):
        candidates = [f for f in os.listdir(".") if f.lower().endswith(".csv")]
        if candidates:
            csv_path = candidates[0]

    df = _read_csv_robust(csv_path)

    df.columns = _normalize_columns(df.columns)
    df = df.loc[:, [c for c in df.columns if not str(c).strip().lower().startswith("unnamed:")]]

    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _pick_target(df, preferred_headers=["brand"])
    if target_col is None or target_col not in df.columns:
        target_col = df.columns[-1]

    feature_cols = [c for c in df.columns if c != target_col]
    if not feature_cols:
        feature_cols = [target_col]
        target_col = df.columns[-1]

    X = df[feature_cols].copy()
    y_raw = df[target_col].copy()

    for c in X.columns:
        if X[c].dtype == "object" or str(X[c].dtype).startswith("string"):
            coerced = pd.to_numeric(X[c], errors="coerce")
            non_null_ratio = float(coerced.notna().mean()) if len(coerced) else 0.0
            if non_null_ratio >= 0.8:
                X[c] = coerced

    if y_raw.dtype == "object" or str(y_raw.dtype).startswith("string"):
        y_num_try = pd.to_numeric(y_raw, errors="coerce")
        if float(y_num_try.notna().mean()) >= 0.98:
            y_raw = y_num_try

    X.replace([np.inf, -np.inf], np.nan, inplace=True)
    if isinstance(y_raw, pd.Series):
        y_raw = y_raw.replace([np.inf, -np.inf], np.nan)

    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    is_classification = False
    y = y_raw

    if pd.api.types.is_numeric_dtype(y):
        y_num = pd.to_numeric(y, errors="coerce")
        nun = int(y_num.nunique(dropna=True))
        is_classification = nun <= 20 and nun >= 2
        if is_classification:
            y = y_num.astype(int, errors="ignore")
        else:
            y = y_num
    else:
        nun = int(pd.Series(y).nunique(dropna=True))
        is_classification = nun >= 2
        y = y.astype(str)

    valid_mask = pd.Series(True, index=df.index)
    if isinstance(y, pd.Series):
        valid_mask &= y.notna()
    X2 = X.loc[valid_mask].copy()
    y2 = y.loc[valid_mask].copy() if isinstance(y, pd.Series) else y

    assert X2.shape[0] > 0

    stratify = None
    if is_classification:
        y_classes = pd.Series(y2).astype(str)
        if int(y_classes.nunique(dropna=True)) >= 2:
            stratify = y_classes
            y2 = y_classes

    X_train, X_test, y_train, y_test = train_test_split(
        X2,
        y2,
        test_size=0.25,
        random_state=42,
        stratify=stratify,
    )

    assert len(X_train) > 0 and len(X_test) > 0

    if is_classification and int(pd.Series(y_train).nunique(dropna=True)) >= 2:
        clf = LogisticRegression(
            max_iter=200,
            solver="lbfgs",
            n_jobs=1,
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        y_train_num = pd.to_numeric(y_train, errors="coerce")
        y_test_num = pd.to_numeric(y_test, errors="coerce")
        train_mask = y_train_num.notna()
        test_mask = y_test_num.notna()
        X_train_r = X_train.loc[train_mask]
        y_train_r = y_train_num.loc[train_mask]
        X_test_r = X_test.loc[test_mask]
        y_test_r = y_test_num.loc[test_mask]

        if len(X_train_r) >= 2 and len(X_test_r) >= 1:
            model.fit(X_train_r, y_train_r)
            y_pred = model.predict(X_test_r)
            accuracy = _safe_accuracy_proxy_from_r2(y_test_r.values, y_pred)
        else:
            accuracy = 0.0

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses robust CSV parsing with a lightweight fallback (sep=';' and decimal=',') to avoid manual intervention.
# - Normalizes and cleans column names; drops "Unnamed:" columns to prevent accidental feature leakage/noise.
# - Picks target defensively (prefers 'brand' if present; otherwise chooses a reasonable column) and keeps running even if schema differs.
# - CPU-friendly preprocessing via ColumnTransformer: median imputation + StandardScaler for numeric, most_frequent + OneHot for categoricals.
# - Prefers LogisticRegression (cheap convex optimization) for classification; falls back to Ridge regression (closed-form/efficient) when target isn't usable for classification.
# - Regression is converted to a bounded [0,1] "accuracy" proxy: ACC=clip(0.5+0.5*R2,0,1) to keep output stable and comparable.
# - Avoids heavy models (no SVM/ensembles/deep learning), minimizes stdout, fixes random_state for reproducibility, and keeps n_jobs=1 to be energy-aware.