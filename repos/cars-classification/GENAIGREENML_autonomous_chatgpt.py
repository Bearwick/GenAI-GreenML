# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression


def _find_dataset_path() -> str:
    # Minimal I/O scanning: look for a single likely dataset file in current working directory.
    candidates = []
    for fn in os.listdir("."):
        if not os.path.isfile(fn):
            continue
        lower = fn.lower()
        if lower.endswith((".csv", ".tsv", ".txt", ".data")) and any(k in lower for k in ("auto", "mpg", "car", "cars", "dataset", "data")):
            candidates.append(fn)
    if not candidates:
        # Fallback: pick any single csv if present
        for fn in os.listdir("."):
            if os.path.isfile(fn) and fn.lower().endswith(".csv"):
                candidates.append(fn)
    if not candidates:
        raise FileNotFoundError("No dataset file found in current directory.")
    candidates.sort(key=lambda x: (len(x), x))
    return candidates[0]


def _read_table(path: str) -> pd.DataFrame:
    # Robust parsing for small datasets; prefer pandas C engine when possible (fast/low overhead).
    ext = os.path.splitext(path)[1].lower()
    if ext == ".tsv":
        df = pd.read_csv(path, sep="\t")
    elif ext in (".txt", ".data"):
        # Try comma first, then whitespace; avoid expensive sniffer.
        try:
            df = pd.read_csv(path)
        except Exception:
            df = pd.read_csv(path, delim_whitespace=True)
    else:
        df = pd.read_csv(path)
    return df


def _canonicalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    # Normalize column names to match expected headers regardless of minor formatting differences.
    rename_map = {}
    for c in df.columns:
        key = re.sub(r"[^a-z0-9]+", "", str(c).strip().lower())
        rename_map[c] = key
    df = df.rename(columns=rename_map)

    # Map known variants to canonical names.
    variants = {
        "mpg": "mpg",
        "cylinders": "cylinders",
        "cubicinches": "cubicinches",
        "cubicinches": "cubicinches",
        "displacement": "cubicinches",
        "hp": "hp",
        "horsepower": "hp",
        "weightlbs": "weightlbs",
        "weight": "weightlbs",
        "timet060": "timet060",
        "timetto60": "timet060",
        "timeto60": "timet060",
        "time060": "timet060",
        "acceleration": "timet060",
        "year": "year",
        "brand": "brand",
        "origin": "brand",
        "make": "brand",
        "carname": "brand",
        "name": "brand",
    }

    new_cols = {}
    for c in df.columns:
        if c in variants:
            new_cols[c] = variants[c]
    df = df.rename(columns=new_cols)

    # Unify time-to-60 into expected key "timet060"
    if "timet060" not in df.columns:
        # If original header matches dataset header exactly, it may be "time-to-60"
        for c in list(df.columns):
            if c.replace("-", "").replace("_", "") == "timet060":
                df = df.rename(columns={c: "timet060"})
                break

    return df


def _ensure_expected_columns(df: pd.DataFrame) -> pd.DataFrame:
    expected = ["mpg", "cylinders", "cubicinches", "hp", "weightlbs", "timet060", "year", "brand"]
    missing = [c for c in expected if c not in df.columns]
    if missing:
        raise ValueError(f"Missing expected columns: {missing}")
    df = df[expected].copy()
    return df


def _coerce_types(df: pd.DataFrame) -> pd.DataFrame:
    # Convert numeric columns with coercion; keep brand as string/categorical.
    num_cols = ["mpg", "cylinders", "cubicinches", "hp", "weightlbs", "timet060", "year"]
    for c in num_cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    df["brand"] = df["brand"].astype(str).replace({"nan": np.nan})
    return df


def main() -> None:
    dataset_path = _find_dataset_path()
    df = _read_table(dataset_path)
    df = _canonicalize_columns(df)
    df = _ensure_expected_columns(df)
    df = _coerce_types(df)

    # Target choice: predict brand from vehicle attributes (brand is categorical label).
    y = df["brand"]
    X = df.drop(columns=["brand"])

    # Remove rows with missing target; impute missing features in pipeline.
    valid = y.notna()
    X = X.loc[valid].reset_index(drop=True)
    y = y.loc[valid].reset_index(drop=True)

    # Lightweight preprocessing: median imputation + standardization for numeric features.
    numeric_features = list(X.columns)
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
        ],
        remainder="drop",
        sparse_threshold=0.0,
    )

    # Energy-efficient model: multinomial logistic regression (linear, fast on CPU).
    clf = LogisticRegression(
        solver="lbfgs",
        max_iter=300,
        n_jobs=1,
        multi_class="auto",
    )

    model = Pipeline(
        steps=[
            ("preprocess", preprocessor),
            ("clf", clf),
        ]
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if y.nunique() > 1 else None
    )

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Uses a linear classifier (multinomial LogisticRegression) for strong baseline accuracy with low CPU cost.
# - Employs a minimal preprocessing pipeline: median imputation + standard scaling for stable optimization.
# - Avoids deep learning, large embeddings, and extensive feature engineering to reduce compute and energy use.
# - Keeps transformations inside a single sklearn Pipeline/ColumnTransformer for reproducibility and efficiency.
# - Limits parallelism (n_jobs=1) to avoid unnecessary CPU spikes on small datasets.