# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _read_csv_robust(path):
    # Try default CSV parsing first; if it yields a single wide column or too few columns, retry with ';' and decimal ','
    df1 = pd.read_csv(path)
    if df1.shape[1] <= 1:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        return df2
    return df1


def _drop_unnamed(df):
    bad = [c for c in df.columns if str(c).strip().lower().startswith("unnamed")]
    if bad:
        df = df.drop(columns=bad, errors="ignore")
    return df


def _coerce_numeric_like_columns(df):
    # Attempt to convert object columns to numeric where feasible, without breaking real categoricals
    df2 = df.copy()
    for c in df2.columns:
        if df2[c].dtype == "object":
            s = df2[c].astype(str).str.strip()

            # Clean common numeric noise: commas, trailing dots, units/spaces
            s_clean = (
                s.str.replace(",", ".", regex=False)
                .str.replace(r"[^0-9eE\.\-\+]+", "", regex=True)
            )
            num = pd.to_numeric(s_clean, errors="coerce")

            # Only adopt numeric conversion if it meaningfully converts values (avoids turning true categoricals into NaNs)
            non_na_ratio = float(num.notna().mean()) if len(num) else 0.0
            if non_na_ratio >= 0.7:
                df2[c] = num
    return df2


def _pick_target_and_task(df, headers_hint=None):
    cols = list(df.columns)

    # Prefer known target(s) from headers_hint if present
    preferred_targets = []
    if headers_hint:
        # Common dataset goal: classify "brand"; fallback to "mpg" if needed
        for name in ["brand", "mpg"]:
            if name in headers_hint:
                preferred_targets.append(name)

    # Map preferred targets to actual df columns by normalized matching
    norm_map = {re.sub(r"\s+", " ", str(c).strip().lower()): c for c in cols}
    target_col = None
    for pt in preferred_targets:
        pt_norm = pt.strip().lower()
        if pt_norm in norm_map:
            target_col = norm_map[pt_norm]
            break

    # If still not found, pick a reasonable target:
    if target_col is None:
        # Prefer an object column with few unique values (classification-like)
        obj_cols = [c for c in cols if df[c].dtype == "object"]
        obj_cols_sorted = sorted(
            obj_cols,
            key=lambda c: (df[c].nunique(dropna=True), -df[c].notna().mean()),
        )
        for c in obj_cols_sorted:
            nun = df[c].nunique(dropna=True)
            if 2 <= nun <= max(2, min(20, int(0.2 * len(df) + 1))):
                target_col = c
                break

    if target_col is None:
        # Else pick a numeric non-constant column
        num_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]
        for c in num_cols:
            s = df[c]
            nun = s.nunique(dropna=True)
            if nun >= 2:
                target_col = c
                break

    if target_col is None:
        # Last resort: first column
        target_col = cols[0]

    y = df[target_col]

    # Determine task
    is_classification = False
    if y.dtype == "object":
        is_classification = True
    else:
        nun = y.nunique(dropna=True)
        # If numeric but low cardinality, treat as classification
        if 2 <= nun <= 20:
            is_classification = True

    return target_col, is_classification


def _make_preprocessor(X):
    num_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    cat_features = [c for c in X.columns if c not in num_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, num_features),
            ("cat", categorical_transformer, cat_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )
    return preprocessor


def _accuracy_from_regression(y_true, y_pred):
    # Stable bounded score in [0,1] derived from normalized MAE:
    # acc = 1 - MAE / (mean(|y - mean(y)|) + eps). Clipped to [0,1].
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = np.nanmean(np.abs(y_true - y_pred))
    denom = np.nanmean(np.abs(y_true - np.nanmean(y_true))) + 1e-12
    score = 1.0 - (mae / denom)
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    dataset_path = "cars.csv"
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(dataset_path)

    df = _read_csv_robust(dataset_path)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Coerce numeric-like object columns safely
    df = _coerce_numeric_like_columns(df)

    # Basic cleanup: replace inf with NaN
    df = df.replace([np.inf, -np.inf], np.nan)

    # Defensive: ensure non-empty
    df = df.dropna(axis=0, how="all")
    assert df.shape[0] > 0 and df.shape[1] > 0

    headers_hint = [h.strip() for h in "mpg, cylinders, cubicinches, hp, weightlbs, time-to-60, year, brand".split(",")]
    target_col, is_classification = _pick_target_and_task(df, headers_hint=headers_hint)

    # Build features/target with fallback if target missing or unusable
    if target_col not in df.columns:
        target_col = df.columns[-1]

    y = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # If no features remain, create a dummy constant feature so pipeline runs
    if X.shape[1] == 0:
        X = pd.DataFrame({"_bias": np.ones(len(df), dtype=float)})

    # Drop rows where target is missing
    mask = y.notna()
    X = X.loc[mask].reset_index(drop=True)
    y = y.loc[mask].reset_index(drop=True)

    assert len(X) > 0

    # Classification sanity: require at least 2 classes
    if is_classification:
        if y.dtype != "object":
            # For numeric classification, keep as integers if close; else as string bins
            y_num = pd.to_numeric(y, errors="coerce")
            if y_num.notna().all():
                y = y_num.astype(int)
            else:
                y = y.astype(str)

        n_classes = y.nunique(dropna=True)
        if n_classes < 2:
            is_classification = False

    preprocessor = _make_preprocessor(X)

    if is_classification:
        # Lightweight linear classifier; saga handles sparse OHE efficiently
        model = LogisticRegression(
            max_iter=300,
            solver="saga",
            n_jobs=1,
            C=1.0,
            tol=1e-3,
        )
    else:
        # Lightweight linear regressor
        model = Ridge(alpha=1.0, random_state=42)

    clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

    # Split
    if is_classification:
        stratify = y if y.nunique(dropna=True) >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

    assert len(X_train) > 0 and len(X_test) > 0

    clf.fit(X_train, y_train)

    if is_classification:
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        y_test_num = pd.to_numeric(y_test, errors="coerce").astype(float)
        y_pred = clf.predict(X_test)
        accuracy = _accuracy_from_regression(y_test_num.values, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used a simple train/test split and a single lightweight linear model (LogisticRegression or Ridge) for CPU-friendly training/inference.
# - OneHotEncoder(handle_unknown="ignore") + sparse matrices minimize memory and compute for categorical features (e.g., brand).
# - ColumnTransformer+Pipeline ensures reproducible preprocessing with no redundant passes over data.
# - Robust CSV parsing fallback (default then ';' with decimal ',') and defensive schema handling avoid hard failures and rework.
# - Numeric coercion is conservative (only converts object columns when most values parse) to prevent costly/incorrect transformations.
# - Regression fallback uses a bounded [0,1] normalized-MAE score as "ACCURACY" to keep evaluation stable without heavy metrics.