# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

df = pd.read_csv('data.csv')
df.columns = df.columns.str.strip()

num_cols = ['mpg', 'cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60', 'year']
for col in num_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce')
    df[col] = df[col].fillna(df[col].median())

df['brand'] = df['brand'].astype(str).str.strip()

X = df[num_cols]
y = LabelEncoder().fit_transform(df['brand'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

clf = RandomForestClassifier(n_estimators=50, max_depth=7, random_state=42)
clf.fit(X_train, y_train)

accuracy = accuracy_score(y_test, clf.predict(X_test))
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Utilized a lightweight RandomForestClassifier with constrained estimators (50) and depth (7) to minimize CPU cycles and energy consumption.
# 2. Implemented an efficient preprocessing pipeline using vectorized pandas operations for numeric coercion and median imputation.
# 3. Employed StandardScaler to normalize features, ensuring model stability and faster convergence without significant computational cost.
# 4. Avoided complex architectures or deep learning, making the solution ideal for low-power CPU environments.
# 5. Minimized memory footprint by selecting only necessary features and performing in-place cleaning where possible.