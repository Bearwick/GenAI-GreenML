# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

DATASET_PATH = "cars.csv"
COLUMN_NAMES = [
    "mpg",
    "cylinders",
    "cubicinches",
    "hp",
    "weightlbs",
    "time-to-60",
    "year",
    "brand",
]


def load_and_prepare_data(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)

    if list(df.columns) != COLUMN_NAMES:
        df.columns = COLUMN_NAMES

    numeric_cols = ["cubicinches", "weightlbs"]
    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors="coerce")
    df = df.dropna()

    return df


def train_and_evaluate(df: pd.DataFrame, test_size: float = 0.25, random_state: int = 0) -> float:
    feature_cols = ["mpg", "cylinders", "cubicinches", "hp", "weightlbs"]
    X = df[feature_cols]
    y = df["brand"]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )

    model = SVC()
    model.fit(X_train, y_train)

    accuracy = model.score(X_test, y_test)
    return float(accuracy)


def main() -> None:
    df = load_and_prepare_data(DATASET_PATH)
    accuracy = train_and_evaluate(df)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Removed unused imports (numpy, seaborn, matplotlib) to reduce import time and memory footprint.
# - Eliminated dataframe inspection calls, head/info/columns assignments, plots, and extra variables to avoid redundant work.
# - Consolidated cleaning steps: single numeric coercion for required columns and a single dropna to reduce passes over data.
# - Avoided .values materialization; kept features as a DataFrame to reduce copying and unnecessary data movement.
# - Removed duplicate predictions, confusion matrix computation, and unused results to minimize compute overhead.
# - Encapsulated logic into small functions for clarity and to avoid global side effects while keeping reproducibility (fixed random_state).