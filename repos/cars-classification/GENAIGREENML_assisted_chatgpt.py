# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC


RANDOM_SEED = 0
DATASET_HEADERS = ["mpg", "cylinders", "cubicinches", "hp", "weightlbs", "time-to-60", "year", "brand"]


def read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] < len(DATASET_HEADERS):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def load_and_prepare_dataset(path: str) -> tuple[np.ndarray, np.ndarray]:
    df = read_csv_robust(path)

    if list(df.columns) != DATASET_HEADERS and df.shape[1] >= len(DATASET_HEADERS):
        df = df.iloc[:, : len(DATASET_HEADERS)].copy()
        df.columns = DATASET_HEADERS
    elif list(df.columns) == DATASET_HEADERS:
        df = df.copy()
    else:
        df = df.copy()

    df = df.dropna()

    for col in ("cubicinches", "weightlbs"):
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    df = df.dropna()

    X = df.iloc[:, 0:5].to_numpy(copy=False)
    y = df.iloc[:, 7].to_numpy(copy=False)
    return X, y


def train_and_evaluate(X: np.ndarray, y: np.ndarray) -> float:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=RANDOM_SEED
    )
    classifier = SVC()
    classifier.fit(X_train, y_train)
    accuracy = classifier.score(X_test, y_test)
    return float(accuracy)


def main() -> None:
    X, y = load_and_prepare_dataset("cars.csv")
    accuracy = train_and_evaluate(X, y)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused imports (seaborn/matplotlib) and all plotting to cut runtime and avoid heavy dependencies.
# - Eliminated unused intermediate variables and redundant predictions/confusion-matrix computation to reduce extra work.
# - Used a robust CSV loader with a lightweight fallback (sep=';' and decimal=',') to prevent costly downstream failures.
# - Avoided unnecessary copies by using to_numpy(copy=False) for feature/label extraction, reducing memory movement.
# - Consolidated preprocessing into a single function and limited numeric coercion to required columns only.
# - Ensured reproducibility by fixing the train/test split seed.