# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC


RANDOM_SEED = 0


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 2:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _standardize_columns(df: pd.DataFrame, expected_headers) -> pd.DataFrame:
    df_cols = [str(c).strip() for c in df.columns]
    df.columns = df_cols

    expected_set = set(expected_headers)
    if expected_set.issubset(df.columns):
        df = df[list(expected_headers)]
        return df

    if len(df.columns) == len(expected_headers):
        df.columns = list(expected_headers)
        return df

    rename_map = {}
    normalized_expected = {str(c).strip().lower(): c for c in expected_headers}
    for c in df.columns:
        key = str(c).strip().lower()
        if key in normalized_expected:
            rename_map[c] = normalized_expected[key]
    if rename_map:
        df = df.rename(columns=rename_map)

    if expected_set.issubset(df.columns):
        df = df[list(expected_headers)]
        return df

    return df


def load_and_prepare_dataset(path: str, dataset_headers) -> pd.DataFrame:
    df = _read_csv_robust(path)
    df = _standardize_columns(df, dataset_headers)

    if "cubicinches" in df.columns:
        df["cubicinches"] = pd.to_numeric(df["cubicinches"], errors="coerce")
    if "weightlbs" in df.columns:
        df["weightlbs"] = pd.to_numeric(df["weightlbs"], errors="coerce")

    needed = [c for c in ["mpg", "cylinders", "cubicinches", "hp", "weightlbs", "brand"] if c in df.columns]
    df = df.dropna(subset=needed)

    return df


def train_and_evaluate(df: pd.DataFrame) -> float:
    feature_cols = ["mpg", "cylinders", "cubicinches", "hp", "weightlbs"]
    for col in feature_cols + ["brand"]:
        if col not in df.columns:
            raise ValueError(f"Missing required column: {col}")

    X = df[feature_cols].to_numpy(dtype=np.float32, copy=False)
    y = df["brand"].to_numpy(copy=False)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=RANDOM_SEED, stratify=y if len(np.unique(y)) > 1 else None
    )

    clf = SVC()
    clf.fit(X_train, y_train)
    accuracy = float(clf.score(X_test, y_test))
    return accuracy


def main():
    dataset_path = "cars.csv"
    dataset_headers = ["mpg", "cylinders", "cubicinches", "hp", "weightlbs", "time-to-60", "year", "brand"]

    df = load_and_prepare_dataset(dataset_path, dataset_headers)
    accuracy = train_and_evaluate(df)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused imports (seaborn/matplotlib) and all plotting to cut runtime and avoid extra dependency overhead.
# - Eliminated redundant computations and variables (head/info/columns/value_counts, repeated predict calls, confusion matrix) while keeping the same ML task and evaluation intent.
# - Implemented robust CSV parsing with a lightweight fallback (sep=';' and decimal=',') only when initial parsing likely fails, avoiding unnecessary retries.
# - Standardized/validated schema by deriving from provided headers and df.columns, renaming only when safe to prevent wrong column assumptions.
# - Used dropna with a targeted subset of required columns to reduce data movement and avoid full-frame NA scans.
# - Converted features to NumPy float32 with copy=False to reduce memory footprint and avoid extra copies while preserving functionality.
# - Ensured reproducibility via fixed random_state and optional stratification for stable splits when multiple classes exist.