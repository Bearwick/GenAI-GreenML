# Generated by generate_llm_code_2.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

# Load dataset with only necessary processing
dataset = pd.read_csv('cars.csv')

# Standardize column names as per original source logic
dataset.columns = ['mpg', 'cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60', 'year', 'brand']

# Vectorized numeric conversion and single-pass cleaning
# This replaces multiple redundant dropna() calls and individual column assignments
cols_to_fix = ['cubicinches', 'weightlbs']
for col in cols_to_fix:
    dataset[col] = pd.to_numeric(dataset[col], errors='coerce')

dataset.dropna(inplace=True)

# Feature selection using direct slicing to minimize memory overhead
X = dataset.iloc[:, 0:5].values
y = dataset.iloc[:, 7].values

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Initialize and fit classifier
# Preserving SVC as per original requirements
classifier = SVC()
classifier.fit(X_train, y_train)

# Calculate accuracy directly to avoid redundant inference passes
accuracy = classifier.score(X_test, y_test)
print(f"ACCURACY={accuracy:.6f}")

"""
OPTIMIZATIONS APPLIED:
1. Removed unused visualization libraries (seaborn, matplotlib) to reduce memory footprint and import overhead.
2. Eliminated redundant diagnostic calls (dataset.head, info, columns) that consume CPU cycles without affecting output.
3. Streamlined preprocessing by merging numeric conversion and performing a single dropna() operation instead of multiple passes.
4. Removed redundant model prediction calls (original code called predict twice and score once).
5. In-place data cleaning used where possible to reduce intermediate data copies in memory.
6. Removed unused confusion matrix computation as it was not printed or used for subsequent logic.
"""