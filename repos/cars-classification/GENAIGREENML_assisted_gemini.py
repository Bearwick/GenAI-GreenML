# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

try:
    df = pd.read_csv('cars.csv')
    if df.shape[1] < 8:
        raise ValueError
except:
    df = pd.read_csv('cars.csv', sep=';', decimal=',')

df = df.dropna()
df.columns = ['mpg', 'cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60', 'year', 'brand']

for col in ['cubicinches', 'weightlbs']:
    df[col] = pd.to_numeric(df[col], errors='coerce')

df = df.dropna()

X = df.iloc[:, 0:5].values.astype(np.float32)
y = df.iloc[:, 7].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

classifier = SVC()
classifier.fit(X_train, y_train)
accuracy = classifier.score(X_test, y_test)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Removed visualization libraries (Seaborn, Matplotlib) and plotting calls to reduce memory and startup time.
# 2. Eliminated redundant computations such as duplicate predict calls and unused exploratory variables.
# 3. Optimized memory footprint by casting feature matrices to float32.
# 4. Streamlined data loading with robust fallback parsing and combined cleaning steps.
# 5. Used the more efficient model.score() method to calculate accuracy in a single step.
# 6. Removed unnecessary intermediate data structures and logging to minimize computational overhead.
# 7. Ensured deterministic behavior by maintaining fixed random seeds in splitting.