# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

dataset = pd.read_csv('cars.csv')
dataset.columns = ['mpg', 'cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60', 'year', 'brand']
dataset['cubicinches'] = pd.to_numeric(dataset['cubicinches'], errors='coerce')
dataset['weightlbs'] = pd.to_numeric(dataset['weightlbs'], errors='coerce')
dataset = dataset.dropna()

X = dataset[['mpg', 'cylinders', 'cubicinches', 'hp', 'weightlbs']].values
y = dataset['brand'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

classifier = SVC()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Removed unused imports (seaborn, matplotlib) to reduce import overhead
# Removed redundant .head(), .info(), .columns, .value_counts() calls that were stored but never used
# Removed all visualization code (barplots) to eliminate unnecessary computation
# Removed redundant first dropna() call; single dropna() after coercion suffices
# Removed duplicate predict call; predict is called only once now
# Removed confusion matrix computation as it was unused beyond assignment
# Used column-name based indexing for clarity
# Used accuracy_score directly instead of classifier.score to reuse y_pred