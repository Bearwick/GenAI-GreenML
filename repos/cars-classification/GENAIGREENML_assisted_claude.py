# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

RANDOM_SEED = 0
np.random.seed(RANDOM_SEED)

try:
    dataset = pd.read_csv('cars.csv')
    if dataset.shape[1] < 2:
        dataset = pd.read_csv('cars.csv', sep=';', decimal=',')
except Exception:
    dataset = pd.read_csv('cars.csv', sep=';', decimal=',')

expected_cols = ['mpg', 'cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60', 'year', 'brand']
if len(dataset.columns) == len(expected_cols):
    dataset.columns = expected_cols

dataset = dataset.dropna()
dataset['cubicinches'] = pd.to_numeric(dataset['cubicinches'], errors='coerce')
dataset['weightlbs'] = pd.to_numeric(dataset['weightlbs'], errors='coerce')
dataset = dataset.dropna()

feature_cols = ['mpg', 'cylinders', 'cubicinches', 'hp', 'weightlbs']
X = dataset[feature_cols].values
y = dataset['brand'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_SEED)

classifier = SVC(random_state=RANDOM_SEED)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Removed unused imports (seaborn, matplotlib) to reduce import overhead.
# - Removed intermediate variables (data, da, dataa, z) that were never used.
# - Removed all visualization calls (barplot) to eliminate unnecessary computation.
# - Removed duplicate predict call; predict is called only once now.
# - Removed confusion_matrix computation since it was unused beyond assignment.
# - Used column names directly instead of iloc for clarity and robustness.
# - Added robust CSV fallback parsing (sep=';', decimal=',') per requirements.
# - Set fixed random seed for reproducibility.
# - Replaced classifier.score with direct accuracy_score on single prediction to avoid redundant inference.