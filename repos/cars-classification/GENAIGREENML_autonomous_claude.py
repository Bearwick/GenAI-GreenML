# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# --- Robust CSV loading ---
try:
    dataset = pd.read_csv('cars.csv')
    if dataset.shape[1] < 2:
        dataset = pd.read_csv('cars.csv', sep=';', decimal=',')
except Exception:
    dataset = pd.read_csv('cars.csv', sep=';', decimal=',')

# --- Column name normalization ---
dataset.columns = [c.strip().replace('  ', ' ') for c in dataset.columns]
dataset = dataset[[c for c in dataset.columns if not c.startswith('Unnamed')]]

# --- Map expected headers ---
expected_headers = ['mpg', 'cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60', 'year', 'brand']
if len(dataset.columns) == len(expected_headers) and not all(h in dataset.columns for h in expected_headers):
    dataset.columns = expected_headers

# --- Coerce numeric columns ---
numeric_candidates = ['mpg', 'cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60', 'year']
for col in numeric_candidates:
    if col in dataset.columns:
        dataset[col] = pd.to_numeric(dataset[col], errors='coerce')

# --- Drop rows with NaN/inf ---
dataset = dataset.replace([np.inf, -np.inf], np.nan)
dataset = dataset.dropna()

# --- Identify target and features ---
target_col = None
if 'brand' in dataset.columns:
    target_col = 'brand'

if target_col is None:
    object_cols = dataset.select_dtypes(include='object').columns.tolist()
    if object_cols:
        target_col = object_cols[0]

if target_col is None:
    for col in dataset.columns:
        if dataset[col].nunique() < 20 and dataset[col].nunique() >= 2:
            target_col = col
            break

if target_col is None:
    target_col = dataset.columns[-1]

feature_cols = [c for c in dataset.columns if c != target_col]

# --- Determine task type ---
is_classification = True
y_series = dataset[target_col]
if y_series.dtype == 'object' or y_series.nunique() < 20:
    is_classification = True
else:
    is_classification = False

# --- Separate features ---
X = dataset[list(feature_cols)].copy()
y = dataset[target_col].copy()

# --- Encode target if classification ---
label_enc = None
if is_classification:
    if y.dtype == 'object':
        label_enc = LabelEncoder()
        y = pd.Series(label_enc.fit_transform(y), index=y.index)
    unique_classes = y.nunique()
    if unique_classes < 2:
        # Fallback: trivial baseline
        accuracy = 1.0
        print(f"ACCURACY={accuracy:.6f}")
        import sys
        sys.exit(0)

# --- Identify numeric and categorical feature columns ---
num_features = X.select_dtypes(include=[np.number]).columns.tolist()
cat_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

# --- Build preprocessing pipeline ---
transformers = []
if num_features:
    transformers.append(('num', StandardScaler(), num_features))
if cat_features:
    transformers.append(('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features))

if not transformers:
    accuracy = 0.0