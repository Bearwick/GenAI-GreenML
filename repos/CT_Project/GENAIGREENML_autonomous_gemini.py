# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

def run_energy_efficient_pipeline(filepath):
    df = pd.read_csv(filepath)
    
    df['Message'] = df['Message'].fillna('')
    df['Category'] = df['Category'].map({'spam': 0, 'ham': 1})
    
    X_train, X_test, y_train, y_test = train_test_split(
        df['Message'], 
        df['Category'], 
        test_size=0.2, 
        random_state=42
    )

    vectorizer = TfidfVectorizer(
        stop_words='english', 
        max_features=2500,
        lowercase=True,
        dtype=np.float32
    )
    
    X_train_tfidf = vectorizer.fit_transform(X_train)
    X_test_tfidf = vectorizer.transform(X_test)

    model = MultinomialNB(alpha=0.1)
    model.fit(X_train_tfidf, y_train)

    predictions = model.predict(X_test_tfidf)
    accuracy = accuracy_score(y_test, predictions)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_energy_efficient_pipeline('mail_data.csv')

"""
JUSTIFICATION FOR DESIGN DECISIONS:

1. Algorithm Choice: Multinomial Naive Bayes (MNB) was selected over Logistic Regression or Deep Learning. 
   MNB is computationally "cheap" as it relies on simple counting and probability calculations rather than 
   iterative gradient-based optimization, significantly reducing CPU cycles and energy consumption.

2. Feature Engineering: TfidfVectorizer with a restricted 'max_features' count (2500) prevents the creation 
   of unnecessarily large sparse matrices. This limits memory usage and speeds up both training and inference.

3. Data Types: Used np.float32 for vectorizer output where possible to reduce the memory footprint 
   compared to default float64, decreasing the energy required for data movement in the cache/RAM.

4. Preprocessing: Built-in scikit-learn stop-word removal and lowercasing are used to avoid heavy 
   external NLP libraries (like spaCy or NLTK) which carry significant overhead in loading models and 
   tokenization.

5. Hardware Efficiency: The entire pipeline is designed for high-throughput on a single CPU core, 
   obviating the need for power-hungry GPUs or high-wattage multi-core synchronization.
"""
