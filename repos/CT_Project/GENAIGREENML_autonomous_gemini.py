# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

def load_and_preprocess(filepath):
    # Robust CSV loading with fallback
    try:
        df = pd.read_csv(filepath)
        if df.shape[1] < 2:
            raise ValueError
    except Exception:
        df = pd.read_csv(filepath, sep=';', decimal=',')

    # Column normalization
    df.columns = [str(c).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    
    # Identify target and feature columns
    cols_lower = [c.lower() for c in df.columns]
    
    # Prioritize 'category' or 'label' for target
    target_idx = 0
    if 'category' in cols_lower:
        target_idx = cols_lower.index('category')
    elif 'label' in cols_lower:
        target_idx = cols_lower.index('label')
    
    # Prioritize 'message' or 'text' for features
    feature_idx = 1
    if 'message' in cols_lower:
        feature_idx = cols_lower.index('message')
    elif 'text' in cols_lower:
        feature_idx = cols_lower.index('text')
    
    target_col = df.columns[target_idx]
    feature_col = df.columns[feature_idx]
    
    # Basic data cleaning
    df = df.dropna(subset=[target_col, feature_col])
    df[feature_col] = df[feature_col].astype(str)
    
    # Encode target if it's categorical
    y = df[target_col]
    if y.dtype == 'object':
        # Mapping ham/spam to 0/1 specifically if detected
        y_mapping = {val: i for i, val in enumerate(sorted(y.unique()))}
        # Force common spam detection logic if keywords exist
        if 'spam' in y_mapping and 'ham' in y_mapping:
            y = y.map({'ham': 0, 'spam': 1})
        else:
            y = pd.factorize(y)[0]
            
    X = df[feature_col]
    return X, y

def run_pipeline():
    DATA_PATH = "mail_data.csv"
    
    try:
        X, y = load_and_preprocess(DATA_PATH)
    except Exception:
        # Trivial fallback if file operations fail
        print(f"ACCURACY=0.000000")
        return

    # Defensive check for sufficient data
    if len(X) < 10 or len(np.unique(y)) < 2:
        # Return a baseline accuracy if classification is impossible
        print(f"ACCURACY=0.000000")
        return

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Energy-efficient pipeline: Tfidf (sparse) + Logistic Regression
    # max_features is limited to reduce memory and compute footprint
    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(stop_words='english', max_features=3000, lowercase=True)),
        ('clf', LogisticRegression(solver='liblinear', C=1.0))
    ])

    # Model training
    pipeline.fit(X_train, y_train)

    # Evaluation
    predictions = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary:
# 1. Model Choice: Logistic Regression with liblinear solver was chosen for its extremely low CPU and memory overhead compared to ensembles or deep learning.
# 2. Feature Engineering: TfidfVectorizer is used with a capped max_features (3000) to ensure the resulting sparse matrix remains small and computations stay efficient.
# 3. Memory Management: The pipeline uses sparse matrix representations natively provided by scikit-learn, minimizing RAM usage during training.
# 4. Data Loading: Implemented a robust parsing strategy to handle various CSV delimiters without crashing, ensuring the pipeline is production-ready for slightly malformed files.
# 5. Preprocessing: Minimalist approach focusing on text normalization and removal of null values to maintain a fast execution path.
# 6. Green Metric: This script runs in sub-second time on standard CPUs, minimizing the carbon footprint of the training cycle.