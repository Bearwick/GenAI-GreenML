# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

raw_data = pd.read_csv('mail_data.csv')
raw_data['Message'] = raw_data['Message'].fillna('')
raw_data['Category'] = raw_data['Category'].map({'spam': 0, 'ham': 1})
raw_data.dropna(subset=['Category'], inplace=True)

X_train, X_test, Y_train, Y_test = train_test_split(
    raw_data['Message'], 
    raw_data['Category'].astype(int), 
    test_size=0.2, 
    random_state=3
)

vectorizer = TfidfVectorizer(min_df=1, stop_words='english', lowercase=True, max_features=3000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

model = MultinomialNB()
model.fit(X_train_vec, Y_train)

y_pred = model.predict(X_test_vec)
accuracy = accuracy_score(Y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

"""
DESIGN JUSTIFICATION:
1. Algorithm: Multinomial Naive Bayes (MNB) is used instead of Logistic Regression or Neural Networks. 
   MNB is exceptionally energy-efficient as it relies on simple frequency counting and probability 
   calculations rather than iterative optimization or gradient descent, minimizing CPU cycles.
2. Feature Extraction: TfidfVectorizer is restricted with 'max_features=3000'. This prevents the 
   feature matrix from becoming unnecessarily sparse and large, reducing memory consumption and 
   speeding up computation.
3. Preprocessing: The pipeline is streamlined to basic lowercasing and stop-word removal. This 
   avoids computationally expensive tasks like lemmatization or dependency parsing which offer 
   diminishing returns for simple spam classification.
4. Hardware: The implementation is designed for CPU execution, avoiding the high carbon footprint 
   associated with powering and cooling GPUs for small-scale tabular/text tasks.
"""
