# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import FunctionTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _read_csv_robust(path: str) -> pd.DataFrame:
    # Try standard parsing first
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.DataFrame()

    def looks_wrong(d: pd.DataFrame) -> bool:
        if d is None or d.empty:
            return True
        # single-column CSV often indicates wrong delimiter
        if d.shape[1] == 1:
            return True
        # too many unnamed columns can indicate parsing issues
        unnamed = sum(str(c).strip().lower().startswith("unnamed") for c in d.columns)
        if unnamed >= max(1, d.shape[1] // 2):
            return True
        return False

    if looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            pass

    return df


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols = []
    for c in df.columns:
        s = str(c)
        s = s.strip()
        s = re.sub(r"\s+", " ", s)
        cols.append(s)
    df = df.copy()
    df.columns = cols

    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _pick_text_and_target(df: pd.DataFrame, preferred_target="Category", preferred_text="Message"):
    cols_lower = {c.lower(): c for c in df.columns}
    target_col = cols_lower.get(preferred_target.lower(), None)
    text_col = cols_lower.get(preferred_text.lower(), None)

    # If preferred columns missing, infer a reasonable pair
    if target_col is None:
        # Prefer low-cardinality object column for classification target
        obj_cols = [c for c in df.columns if df[c].dtype == "object"]
        best = None
        for c in obj_cols:
            nunique = df[c].nunique(dropna=True)
            if 2 <= nunique <= 50:
                if best is None or nunique < best[0]:
                    best = (nunique, c)
        target_col = best[1] if best else None

    if text_col is None:
        # Prefer the longest average string column for text
        obj_cols = [c for c in df.columns if df[c].dtype == "object"]
        best = None
        for c in obj_cols:
            if c == target_col:
                continue
            lengths = df[c].dropna().astype(str).map(len)
            if lengths.empty:
                continue
            mean_len = float(lengths.mean())
            if best is None or mean_len > best[0]:
                best = (mean_len, c)
        text_col = best[1] if best else None

    # Final fallback: use first column as target, second as text if possible
    if target_col is None and df.shape[1] >= 1:
        target_col = df.columns[0]
    if text_col is None:
        if df.shape[1] >= 2:
            text_col = df.columns[1] if df.columns[1] != target_col else df.columns[0]
        else:
            text_col = target_col

    return text_col, target_col


def _safe_accuracy_proxy_from_r2(y_true, y_pred) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    ss_tot = float(np.sum((y_true - float(np.mean(y_true))) ** 2))
    if ss_tot <= 0.0:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    # Map R^2 to [0,1] robustly
    return float(np.clip(0.5 * (r2 + 1.0), 0.0, 1.0))


df = _read_csv_robust("mail_data.csv")
df = _normalize_columns(df)

assert df is not None and df.shape[0] > 0 and df.shape[1] > 0, "Empty dataset after loading."

text_col, target_col = _pick_text_and_target(df, preferred_target="Category", preferred_text="Message")

# Keep only required columns and drop rows with missing target
work = df[[c for c in [text_col, target_col] if c in df.columns]].copy()
if target_col in work.columns:
    work = work[~work[target_col].isna()].copy()

assert work.shape[0] > 0, "No rows available after dropping missing targets."

# Prepare X/y
X = work[[text_col]].copy() if text_col in work.columns else pd.DataFrame({"text": [""] * len(work)})
if text_col not in X.columns:
    X[text_col] = ""
X[text_col] = X[text_col].astype(str).fillna("")

y_raw = work[target_col].copy() if target_col in work.columns else pd.Series([0] * len(work))

# Determine task: classification if y is categorical-like with >=2 classes
is_classification = False
y = y_raw

if y_raw.dtype == "object" or str(y_raw.dtype).startswith("category") or y_raw.nunique(dropna=True) <= 50:
    y_str = y_raw.astype(str).fillna("")
    n_classes = int(pd.Series(y_str).nunique(dropna=True))
    if n_classes >= 2:
        is_classification = True
        y = y_str
    else:
        is_classification = False

if not is_classification:
    # Try numeric regression target
    y_num = pd.to_numeric(y_raw, errors="coerce")
    if y_num.notna().sum() >= 3 and float(y_num.nunique(dropna=True)) > 1:
        y = y_num
    else:
        # Trivial constant baseline target if no usable target
        y = pd.Series(np.zeros(len(work), dtype=float))

# Train/test split
if is_classification:
    # Avoid stratify when classes too small
    stratify = y if y.nunique() >= 2 and y.value_counts().min() >= 2 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=stratify
    )
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

assert len(X_train) > 0 and len(X_test) > 0, "Train/test split produced empty set."

# Lightweight text pipeline
text_selector = FunctionTransformer(lambda d: d[text_col].astype(str).fillna(""), validate=False)

preprocess = ColumnTransformer(
    transformers=[
        (
            "text",
            Pipeline(
                steps=[
                    ("select", text_selector),
                    (
                        "tfidf",
                        TfidfVectorizer(
                            lowercase=True,
                            strip_accents="unicode",
                            stop_words="english",
                            ngram_range=(1, 1),
                            min_df=2,
                            max_df=0.95,
                            sublinear_tf=True,
                        ),
                    ),
                ]
            ),
            [text_col],
        )
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

if is_classification:
    model = LogisticRegression(
        solver="liblinear",
        max_iter=200,
        C=1.0,
    )
    clf = Pipeline(steps=[("prep", preprocess), ("model", model)])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Ridge regression works well with sparse TF-IDF and is CPU-friendly
    reg = Ridge(alpha=1.0, random_state=42)
    rgr = Pipeline(steps=[("prep", preprocess), ("model", reg)])
    # Drop NaNs in y for regression
    tr_mask = pd.Series(y_train).notna().values
    te_mask = pd.Series(y_test).notna().values
    if tr_mask.sum() == 0 or te_mask.sum() == 0:
        accuracy = 0.0
    else:
        rgr.fit(X_train.iloc[tr_mask], np.asarray(y_train)[tr_mask].astype(float))
        y_pred = rgr.predict(X_test.iloc[te_mask])
        accuracy = _safe_accuracy_proxy_from_r2(np.asarray(y_test)[te_mask].astype(float), y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used TF-IDF with unigrams, min_df/max_df pruning, and sublinear TF to reduce feature size and CPU work.
# - Chose LogisticRegression(liblinear) for sparse text classification: strong baseline, fast on CPU, small memory footprint.
# - Implemented a robust CSV loader with fallback delimiter/decimal to avoid wasted reruns due to parsing failures.
# - Normalized/dropped noisy columns and inferred target/text columns defensively to keep the pipeline schema-agnostic.
# - Used sklearn Pipeline/ColumnTransformer for reproducible preprocessing and to avoid recomputing transformations.
# - Regression fallback uses Ridge on sparse TF-IDF; accuracy proxy maps R^2 to [0,1] via (R^2+1)/2 and clipping.