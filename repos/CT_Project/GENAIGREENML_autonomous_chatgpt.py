# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _robust_read_csv(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d):
        if d is None or not hasattr(d, "shape") or d.shape[0] == 0:
            return True
        if d.shape[1] <= 1:
            return True
        return False

    if _looks_wrong(df):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if not _looks_wrong(df2):
                df = df2
        except Exception:
            pass

    if df is None:
        raise RuntimeError("Failed to read dataset")
    return df


def _pick_target_and_task(df):
    cols = list(df.columns)

    # Prefer known header names if present, but never assume they exist
    candidate_targets = []
    for name in ["Category", "category", "target", "label", "Label", "y", "class", "Class"]:
        if name in cols:
            candidate_targets.append(name)
    candidate_targets += [c for c in cols if c not in candidate_targets]

    def nunique_safe(s):
        try:
            return int(s.nunique(dropna=True))
        except Exception:
            return 0

    # Prefer a non-constant target
    chosen = None
    for c in candidate_targets:
        if nunique_safe(df[c]) >= 2:
            chosen = c
            break
    if chosen is None:
        # If everything is constant, just pick the first column; pipeline will fallback safely
        chosen = cols[0] if cols else None

    if chosen is None:
        raise RuntimeError("No columns available")

    y_raw = df[chosen]

    # Determine classification/regression
    # Classification if: (a) object with limited unique classes, or (b) numeric with few discrete values
    y_nonnull = y_raw.dropna()
    nuniq = nunique_safe(y_nonnull)
    is_numeric = pd.api.types.is_numeric_dtype(y_raw)

    if nuniq < 2:
        return chosen, "trivial"

    if not is_numeric:
        # treat as classification unless too many classes
        task = "classification" if nuniq <= 50 else "regression"
        return chosen, task

    # numeric target
    # if looks like discrete labels (small unique count), treat as classification
    task = "classification" if nuniq <= 20 else "regression"
    return chosen, task


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    ss_res = float(np.sum((yt - yp) ** 2))
    yt_mean = float(np.mean(yt))
    ss_tot = float(np.sum((yt - yt_mean) ** 2))
    if ss_tot <= 0.0:
        return 0.0
    r2 = 1.0 - ss_res / ss_tot
    # Map to [0, 1] for stable "accuracy" proxy
    return float(np.clip(0.5 * (r2 + 1.0), 0.0, 1.0))


# Load dataset
data_path = "mail_data.csv"
df = _robust_read_csv(data_path)

# Normalize columns and drop unnamed
df.columns = _normalize_columns(df.columns)
df = df.loc[:, ~pd.Series(df.columns).str.match(r"^Unnamed:\s*\d+$", na=False)].copy()

# Minimal sanitization
assert df.shape[0] > 0 and df.shape[1] > 0

target_col, task = _pick_target_and_task(df)

# Separate features/target
y = df[target_col]
X = df.drop(columns=[target_col])

# If dataset has no feature columns, create a constant feature to allow end-to-end run
if X.shape[1] == 0:
    X = pd.DataFrame({"__constant__": np.ones(len(df), dtype=np.int8)})

# Identify column types
numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
categorical_cols = [c for c in X.columns if c not in numeric_cols]

# Coerce numeric columns safely
for c in numeric_cols:
    X[c] = pd.to_numeric(X[c], errors="coerce").replace([np.inf, -np.inf], np.nan)

# Prepare y for task
if task == "classification":
    # Try known mapping for spam/ham; otherwise factorize
    if not pd.api.types.is_numeric_dtype(y):
        y_str = y.astype(str).str.strip().str.lower()
        if set(y_str.dropna().unique()).issubset({"spam", "ham"}):
            y_enc = y_str.map({"spam": 0, "ham": 1})
        else:
            y_enc = pd.Series(pd.factorize(y)[0], index=y.index)
        y = y_enc
    else:
        y = pd.to_numeric(y, errors="coerce")
    # Drop rows with missing y
    valid = y.notna()
    X = X.loc[valid].copy()
    y = y.loc[valid].astype(int)

elif task == "regression":
    y = pd.to_numeric(y, errors="coerce").replace([np.inf, -np.inf], np.nan)
    valid = y.notna()
    X = X.loc[valid].copy()
    y = y.loc[valid].astype(float)

else:
    # trivial target (constant)
    # Keep as-is; will use trivial baseline accuracy after split
    valid = y.notna()
    X = X.loc[valid].copy()
    y = y.loc[valid].copy()

assert X.shape[0] > 1

# Build preprocessing
numeric_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ]
)

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_cols),
        ("cat", categorical_transformer, categorical_cols),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Split
stratify = None
if task == "classification":
    if pd.Series(y).nunique(dropna=True) >= 2:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=stratify
)

assert X_train.shape[0] > 0 and X_test.shape[0] > 0

# Model selection (lightweight)
if task == "classification":
    # Ensure at least 2 classes in training
    if pd.Series(y_train).nunique(dropna=True) < 2:
        # Trivial baseline: always predict the only class seen
        majority = int(pd.Series(y_train).mode(dropna=True).iloc[0]) if len(y_train) else 0
        y_pred = np.full(shape=(len(y_test),), fill_value=majority, dtype=int)
        accuracy = float(accuracy_score(y_test.astype(int), y_pred.astype(int))) if len(y_test) else 0.0
    else:
        clf = LogisticRegression(
            solver="liblinear",  # CPU-friendly for small/medium sparse problems
            max_iter=200,
            C=1.0,
        )
        model = Pipeline(steps=[("preprocess", preprocess), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))

elif task == "regression":
    reg = Ridge(alpha=1.0, random_state=42)
    model = Pipeline(steps=[("preprocess", preprocess), ("model", reg)])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = _bounded_regression_score(y_test, y_pred)

else:
    # Trivial baseline: constant target accuracy proxy
    if len(y_test) == 0:
        accuracy = 0.0
    else:
        # If constant, any prediction is correct; else fallback to majority match
        if pd.Series(y_test).nunique(dropna=True) <= 1:
            accuracy = 1.0
        else:
            majority = pd.Series(y_train).mode(dropna=True).iloc[0] if len(y_train) else y_test.iloc[0]
            y_pred = np.full(shape=(len(y_test),), fill_value=majority)
            accuracy = float(accuracy_score(pd.Series(y_test).astype(str), pd.Series(y_pred).astype(str)))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Robust CSV parsing: default read_csv with fallback to ';' separator and ',' decimal to avoid costly manual fixes.
# - Column normalization and dropping 'Unnamed:' columns prevents wasted preprocessing on artifact columns.
# - Lightweight preprocessing: SimpleImputer + OneHotEncoder only; no heavy feature engineering to reduce CPU time.
# - Model choice: LogisticRegression(liblinear) for sparse/one-hot data is CPU-friendly; Ridge for regression is efficient.
# - Defensive schema handling: target inferred from available columns; numeric coercion with safe NaN/inf handling.
# - Reproducibility: fixed random_state; sklearn Pipeline/ColumnTransformer avoids repeated transforms and ensures consistency.
# - Regression "accuracy" proxy: bounded mapping of R^2 to [0,1] to keep a stable single metric printout.