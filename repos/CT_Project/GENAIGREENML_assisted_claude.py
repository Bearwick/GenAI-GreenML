# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

raw_mail_data = pd.read_csv('mail_data.csv')

mail_data = raw_mail_data.fillna('')

mail_data["Category"] = (mail_data["Category"] == "ham").astype(np.int8)

X = mail_data['Message']
Y = mail_data['Category']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=3)

feature_extraction = TfidfVectorizer(min_df=1, stop_words='english', lowercase=True)
X_train_features = feature_extraction.fit_transform(X_train)
X_test_features = feature_extraction.transform(X_test)

model = LogisticRegression(solver='liblinear', max_iter=100)
model.fit(X_train_features, Y_train)

prediction_on_test_data = model.predict(X_test_features)
accuracy = accuracy_score(Y_test, prediction_on_test_data)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Replaced .where(pd.notnull(...)) with .fillna('') — simpler, avoids element-wise boolean mask overhead.
# 2. Replaced .map({"spam": 0, "ham": 1}) with boolean comparison (== "ham") — faster vectorized operation.
# 3. Used np.int8 instead of int64 for labels — reduces memory footprint by 8x for label column.
# 4. Removed redundant prediction on training data — saves a full inference pass not needed for test accuracy.
# 5. Removed redundant .astype('int') calls on Y_train/Y_test since labels are already int8.
# 6. Removed the predictive system demo (input_mail) as it is not part of evaluation.
# 7. Used solver='liblinear' for LogisticRegression — lightweight solver well-suited for small sparse datasets.
# 8. Removed duplicate TfidfVectorizer import.
# 9. Removed all prints, plots, and model saving per requirements.