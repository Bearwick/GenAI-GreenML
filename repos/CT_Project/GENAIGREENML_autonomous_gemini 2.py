# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

df = pd.read_csv('mail_data.csv')
df['Message'] = df['Message'].fillna('')
df['Category'] = df['Category'].map({'spam': 0, 'ham': 1}).astype(int)

X_train, X_test, y_train, y_test = train_test_split(
    df['Message'], 
    df['Category'], 
    test_size=0.2, 
    random_state=3
)

vectorizer = TfidfVectorizer(
    stop_words='english', 
    lowercase=True, 
    max_features=3000,
    min_df=2
)

X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

model = MultinomialNB()
model.fit(X_train_vec, y_train)

predictions = model.predict(X_test_vec)
accuracy = accuracy_score(y_test, predictions)

print(f"ACCURACY={accuracy:.6f}")

"""
DESIGN JUSTIFICATION FOR ENERGY EFFICIENCY:
1. Model Choice: Multinomial Naive Bayes was selected over deep learning or iterative linear models (like Logistic Regression) 
   because it relies on simple frequency counting and probability calculations, resulting in near-zero training latency 
   and minimal CPU cycles.
2. Feature Constraint: TfidfVectorizer is limited to the top 3000 features. This prevents the creation of massive, 
   memory-intensive sparse matrices, significantly reducing the RAM footprint.
3. Preprocessing: Built-in stop-word removal and lowercase normalization reduce the dimensionality of the input data 
   early in the pipeline, ensuring subsequent operations process only the most informative tokens.
4. Hardware Efficiency: The solution avoids heavy mathematical libraries (like PyTorch/TensorFlow) in favor of 
   Scikit-Learn, which is optimized for standard CPU execution without the power-draw overhead associated with GPU initialization.
"""