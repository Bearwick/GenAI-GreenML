# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score

DATA_PATH = "mail_data.csv"
EXPECTED_HEADERS = ["Category", "Message"]

def read_csv_fallback(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    if df.shape[1] == 1:
        col0 = str(df.columns[0])
        if (',' in col0) or (';' in col0) or (len(EXPECTED_HEADERS) > 1):
            try:
                df_alt = pd.read_csv(path, sep=';', decimal=',')
                if df_alt.shape[1] > 1:
                    df = df_alt
            except Exception:
                pass
    return df

df = read_csv_fallback(DATA_PATH)
df.columns = [re.sub(r'\s+', ' ', str(c)).strip() for c in df.columns]
df = df.loc[:, ~df.columns.str.contains(r'^Unnamed', case=False, regex=True)]
assert df.shape[0] > 0

target_col = None
lower_map = {c.lower(): c for c in df.columns}
for cand in ["category", "target", "label", "class", "spam", "y"]:
    if cand in lower_map:
        target_col = lower_map[cand]
        break
if target_col is None:
    numeric_candidates = []
    for c in df.columns:
        ser = pd.to_numeric(df[c], errors='coerce')
        if ser.notna().sum() > 0:
            numeric_candidates.append((c, ser.nunique(dropna=True)))
    for c, nunq in numeric_candidates:
        if nunq > 1:
            target_col = c
            break
    if target_col is None and numeric_candidates:
        target_col = numeric_candidates[0][0]
    if target_col is None:
        for c in df.columns:
            if df[c].nunique(dropna=True) > 1:
                target_col = c
                break
    if target_col is None:
        target_col = df.columns[0]

df = df.dropna(subset=[target_col])
if df[target_col].dtype == object:
    df = df[df[target_col].astype(str).str.strip() != ""]
assert df.shape[0] > 0

feature_cols = [c for c in df.columns if c != target_col]
if len(feature_cols) == 0:
    df["dummy_feature"] = 0
    feature_cols = ["dummy_feature"]

X = df[feature_cols].copy()
y = df[target_col].copy()

accuracy = None
skip_training = False

is_classification = False
if y.dtype == object or pd.api.types.is_bool_dtype(y):
    is_classification = True
else:
    y_num = pd.to_numeric(y, errors='coerce')
    if y_num.nunique(dropna=True) <= 20:
        is_classification = True
    else:
        is_classification = False
    y = y_num

if is_classification:
    y = y.astype(str)
    if y.nunique() < 2:
        y_num = pd.to_numeric(y, errors='coerce')
        if y_num.nunique(dropna=True) >= 2:
            is_classification = False
            y = y_num
        else:
            accuracy = 1.0
            skip_training = True
else:
    y = pd.to_numeric(y, errors='coerce')
    valid_idx = y.notna()
    X = X.loc[valid_idx]
    y = y.loc[valid_idx]
    if y.shape[0] == 0:
        accuracy = 0.0
        skip_training = True

if not skip_training:
    n_samples = X.shape[0]
    if n_samples < 2:
        accuracy = 1.0
        skip_training = True

if not skip_training:
    obj_cols = []
    num_cols = []
    for c in feature_cols:
        if pd.api.types.is_numeric_dtype(X[c]):
            num_cols.append(c)
        else:
            converted = pd.to_numeric(X[c], errors='coerce')
            non_nan_ratio = converted.notna().mean()
            if non_nan_ratio > 0.9:
                X[c] = converted
                num_cols.append(c)
            else:
                obj_cols.append(c)

    for c in num_cols:
        X[c] = pd.to_numeric(X[c], errors='coerce')
        X[c] = X[c].replace([np.inf, -np.inf], np.nan)

    text_cols = []
    cat_cols = []
    if obj_cols:
        avg_lens = {c: X[c].astype(str).str.len().mean() for c in obj_cols}
        for c in obj_cols:
            nunique = X[c].nunique(dropna=True)
            if avg_lens[c] > 20 or nunique > 50:
                text_cols.append(c)
            else:
                cat_cols.append(c)
        if len(text_cols) == 0:
            sorted_cols = sorted(avg_lens, key=avg_lens.get, reverse=True)
            if sorted_cols and avg_lens[sorted_cols[0]] > 5:
                text_cols = [sorted_cols[0]]
                cat_cols = [c for c in obj_cols if c not in text_cols]
            else:
                cat_cols = obj_cols.copy()

    text_feature = None
    if text_cols:
        X["combined_text"] = X[text_cols].astype(str).agg(" ".join, axis=1)
        text_feature = "combined_text"

    transformers = []
    if text_feature is not None:
        text_pipeline = Pipeline([
            ("extract", FunctionTransformer(lambda x: x.iloc[:, 0].astype(str), validate=False)),
            ("tfidf", TfidfVectorizer(max_features=5000, stop_words="english"))
        ])
        transformers.append(("text", text_pipeline, [text_feature]))
    if cat_cols:
        cat_pipeline = Pipeline([
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore"))
        ])
        transformers.append(("cat", cat_pipeline, cat_cols))
    if num_cols:
        num_pipeline = Pipeline([
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False))
        ])
        transformers.append(("num", num_pipeline, num_cols))

    if not transformers:
        X["dummy_feature"] = 0
        num_cols = ["dummy_feature"]
        num_pipeline = Pipeline([
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False))
        ])
        transformers = [("num", num_pipeline, num_cols)]

    preprocess = ColumnTransformer(transformers, remainder="drop", sparse_threshold=0.3)

    if is_classification:
        model = LogisticRegression(max_iter=200, solver="liblinear", random_state=42)
    else:
        model = Ridge(alpha=1.0, random_state=42)

    clf = Pipeline([
        ("preprocess", preprocess),
        ("model", model)
    ])

    test_size = 0.2 if n_samples >= 5 else 0.5
    stratify = None
    if is_classification:
        class_counts = y.value_counts()
        if class_counts.min() >= 2 and len(class_counts) > 1:
            stratify = y

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )
    assert len(X_train) > 0 and len(X_test) > 0

    clf.fit(X_train, y_train)

    if is_classification:
        y_pred = clf.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        y_pred = clf.predict(X_test)
        y_test_num = pd.to_numeric(y_test, errors="coerce")
        y_pred_num = np.array(y_pred, dtype=float)
        mse = np.mean((y_test_num - y_pred_num) ** 2)
        var = np.var(y_test_num)
        if var == 0:
            accuracy = 1.0 if mse == 0 else 0.0
        else:
            accuracy = 1.0 / (1.0 + mse / var)

if accuracy is None or not np.isfinite(accuracy):
    accuracy = 0.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used a capped TF-IDF representation with a simple linear model to minimize CPU/memory overhead.
# - Applied a single ColumnTransformer pipeline with lightweight imputers/scalers for reproducible preprocessing.
# - Added safe fallbacks and minimal splitting to keep computation small while still producing a valid score.