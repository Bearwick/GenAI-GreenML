# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

data = pd.read_csv('mail_data.csv').fillna('')
data['Category'] = data['Category'].map({'spam': 0, 'ham': 1}).astype('int8')

x_train_raw, x_test_raw, y_train, y_test = train_test_split(
    data['Message'], data['Category'], test_size=0.2, random_state=3
)

tfidf = TfidfVectorizer(min_df=1, stop_words='english', lowercase=True)
x_train = tfidf.fit_transform(x_train_raw)
x_test = tfidf.transform(x_test_raw)

model = LogisticRegression(solver='liblinear')
model.fit(x_train, y_train)

test_predictions = model.predict(x_test)
accuracy = accuracy_score(y_test, test_predictions)
print(f"ACCURACY={accuracy:.6f}")

input_mail = ["I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times"]
prediction = model.predict(tfidf.transform(input_mail))
print('Ham mail' if prediction[0] == 1 else 'Spam mail')

# OPTIMIZATIONS APPLIED:
# 1. Memory Footprint: Used fillna('') instead of where() for faster null handling and cast labels to int8 to reduce memory overhead.
# 2. Algorithm Optimization: Specified the 'liblinear' solver for LogisticRegression, which is more efficient for small datasets and binary classification.
# 3. Redundancy Reduction: Removed duplicate imports and consolidated preprocessing steps to avoid creating multiple intermediate data copies.
# 4. Computational Overhead: Removed redundant training accuracy calculation and excessive printing to minimize I/O and CPU cycles.
# 5. Data Movement: Streamlined text transformations and avoided unnecessary loops by using vectorized pandas and sklearn operations.
