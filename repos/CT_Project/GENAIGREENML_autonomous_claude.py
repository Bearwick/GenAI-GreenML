# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
import os

# Load dataset - try common paths
data_path = None
for candidate in ["dataset.csv", "data.csv", "train.csv", "spam.csv", "Dataset.csv"]:
    if os.path.exists(candidate):
        data_path = candidate
        break

if data_path is None:
    for root, dirs, files in os.walk("."):
        for f in files:
            if f.endswith(".csv"):
                data_path = os.path.join(root, f)
                break
        if data_path:
            break

# Read dataset with headers Category, Message (likely spam/ham classification)
df = pd.read_csv(data_path)

# Handle potential encoding issues - select only relevant columns
if "Category" in df.columns and "Message" in df.columns:
    df = df[["Category", "Message"]].dropna()
elif df.shape[1] >= 2:
    df.columns = ["Category", "Message"] + list(df.columns[2:])
    df = df[["Category", "Message"]].dropna()

# Encode labels
le = LabelEncoder()
y = le.fit_transform(df["Category"])

# Text preprocessing: convert to string, lowercase
X_text = df["Message"].astype(str).str.lower()

# Split data before fitting vectorizer to prevent data leakage
X_train_text, X_test_text, y_train, y_test = train_test_split(
    X_text, y, test_size=0.2, random_state=42, stratify=y
)

# TF-IDF vectorization - lightweight and effective for text classification
# max_features=10000 limits vocabulary size for efficiency
# sublinear_tf=True applies log normalization which often improves performance
vectorizer = TfidfVectorizer(
    max_features=10000,
    sublinear_tf=True,
    strip_accents="unicode",
    ngram_range=(1, 2),
    min_df=2,
    max_df=0.95,
    dtype=np.float32
)

X_train = vectorizer.fit_transform(X_train_text)
X_test = vectorizer.transform(X_test_text)

# Logistic Regression - energy efficient, fast on CPU, excellent for text classification
# liblinear solver is efficient for small datasets
# C=1.0 provides good default regularization
model = LogisticRegression(
    solver="liblinear",
    C=1.0,
    max_iter=1000,
    random_state=42
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. TfidfVectorizer with max_features=10000 and bigrams: lightweight yet effective text representation
# 2. float32 dtype in TF-IDF to reduce memory footprint by 50% vs float64
# 3. LogisticRegression with liblinear solver: extremely fast on CPU, ideal for binary text classification
# 4. sublinear_tf and ngram_range=(1,2) boost accuracy without significant compute cost
# 5. min_df=2 and max_df=0.95 prune rare/common terms to reduce dimensionality
# 6. No deep learning needed - linear model achieves high accuracy on spam/ham tasks
# 7. Stratified split ensures balanced evaluation across classes