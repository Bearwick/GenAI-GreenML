# Generated by generate_llm_code.py
# LLM: groq
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
import joblib

# Load and preprocess data
df = pd.read_csv('mail_data.csv')
df['Message'] = df['Message'].fillna('')
df['Category'] = df['Category'].map({'spam': 0, 'ham': 1}).astype(int)

X = df['Message']
y = df['Category']

# Train-test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=3, stratify=y
)

# Build pipeline: TF-IDF vectorization + Logistic Regression
clf = Pipeline([
    ('tfidf', TfidfVectorizer(
        stop_words='english',
        lowercase=True,
        min_df=2,
        max_features=5000
    )),
    ('logreg', LogisticRegression(
        solver='liblinear',
        max_iter=1000
    ))
])

# Train model
clf.fit(X_train, y_train)

# Evaluate
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# Save trained pipeline
joblib.dump(clf, 'spam_classifier.joblib')

# Optimizations applied:
# 1. Pipeline eliminates intermediate data structures, reducing memory usage.
# 2. TfidfVectorizer limited to 5000 features and min_df=2 to drop rare terms, cutting computational load.
# 3. LogisticRegression uses the efficient 'liblinear' solver suited for sparse data and small feature sets.
# 4. Stratified train-test split maintains class distribution, improving model reliability.
# 5. Removed all unnecessary I/O, prints, and interactive elements to lower runtime overhead.