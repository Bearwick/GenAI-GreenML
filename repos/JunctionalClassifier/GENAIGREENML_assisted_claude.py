# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import csv
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier

RANDOM_SEED = 42

def load_csv(filename):
    features = []
    labels = []
    try:
        with open(filename, newline='') as csvfile:
            reader = csv.reader(csvfile, delimiter=' ', quotechar='|')
            first = True
            for row in reader:
                cols = row[0].split(",")
                if cols[-1] == '':
                    cols.pop()
                if not first:
                    both = np.asarray(cols, dtype=np.float64)
                    num = both[-1]
                    feat = both[:-1].tolist()
                    if num > 0:
                        num = 1.0
                    elif num < 0:
                        num = -1.0
                    labels.append(num)
                    features.append(feat)
                else:
                    first = False
    except Exception:
        pass
    return features, labels

def train_and_test():
    features, labels = load_csv('14k.csv')
    feat_train, feat_test, lab_train, lab_test = train_test_split(
        features, labels, test_size=0.3, random_state=RANDOM_SEED
    )
    scaler = StandardScaler()
    feat_train = scaler.fit_transform(feat_train)
    feat_test = scaler.transform(feat_test)
    mlp = MLPClassifier(
        hidden_layer_sizes=(30, 30, 30, 30),
        max_iter=1000,
        random_state=RANDOM_SEED
    )
    mlp.fit(feat_train, lab_train)
    y_pred = mlp.predict(feat_test)
    accuracy = metrics.accuracy_score(lab_test, y_pred)
    return accuracy

accuracy = train_and_test()
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Removed unused imports (svm, pickle, copy, unittest) to reduce memory footprint.
# Eliminated global mutable state; load_csv returns features and labels directly.
# Removed predict/loadModel/saveModel/takeInput functions that were not part of train-and-test evaluation.
# Removed unit test class and all print/logging statements.
# Removed pickle save/load side effects.
# Combined scaler.fit and scaler.transform into fit_transform for training data to avoid redundant pass.
# Used numpy slicing (both[-1], both[:-1]) instead of list pop to reduce intermediate operations.
# Set fixed random_state on train_test_split and MLPClassifier for reproducibility.
# Removed all visualization and interactive input code.
# Kept CSV parsing logic identical to preserve original data loading behavior.