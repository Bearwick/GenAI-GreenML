# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import warnings
import os

warnings.filterwarnings("ignore")

def load_dataset(path):
    if not os.path.exists(path):
        return None, None
    try:
        df = pd.read_csv(path, sep=None, engine='python')
    except Exception:
        try:
            df = pd.read_pickle(path)
        except Exception:
            return None, None
    
    if df is not None:
        if isinstance(df, np.ndarray):
            df = pd.DataFrame(df)
        df = df.dropna(axis=1, how='all')
        data = df.values.astype(np.float32)
        if data.shape[1] < 2:
            return None, None
        X = data[:, :-1]
        y = data[:, -1]
        y = np.where(y > 0, 1, np.where(y < 0, -1, 0))
        return X, y
    return None, None

def execute_pipeline():
    seed = 42
    np.random.seed(seed)
    
    X, y = load_dataset('14k.csv')
    if X is None:
        X, y = load_dataset('dict.pickle')
        
    if X is not None:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=seed
        )
        
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        mlp = MLPClassifier(
            hidden_layer_sizes=(30, 30, 30, 30),
            max_iter=1000,
            random_state=seed
        )
        
        mlp.fit(X_train_scaled, y_train)
        y_pred = mlp.predict(X_test_scaled)
        
        accuracy = accuracy_score(y_test, y_pred)
        print(f"ACCURACY={accuracy:.6f}")
    else:
        print(f"ACCURACY={0.000000}")

if __name__ == "__main__":
    execute_pipeline()

# Optimization Summary
# 1. Replaced iterative manual CSV parsing with vectorized pandas read_csv for faster I/O and reduced CPU cycles.
# 2. Implemented float32 precision for data arrays to minimize memory footprint and accelerate computation.
# 3. Optimized preprocessing by using fit_transform() on the training set to reduce redundant passes through the data.
# 4. Replaced loop-based sign-mapping of labels with vectorized numpy.where operations.
# 5. Removed global variable overhead and manual list-to-array conversions, lowering the overall memory footprint.
# 6. Set fixed random seeds for train/test splitting and MLP initialization to ensure reproducible, stable results.
# 7. Eliminated energy-intensive peripheral tasks such as model persistence, interactive inputs, and logging.
# 8. Added robust automated schema derivation to handle various CSV delimiters and trailing column errors without manual intervention.