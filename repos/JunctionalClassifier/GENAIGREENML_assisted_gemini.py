# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import numpy as np
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

def load_data(file_path, training=True):
    try:
        data = np.genfromtxt(file_path, delimiter=',', skip_header=1, autostrip=True)
        if data.ndim == 1:
            data = data.reshape(1, -1)
        if np.isnan(data[0, -1]):
            data = data[:, :-1]
        if training:
            return data[:, :-1], np.sign(data[:, -1])
        return data
    except Exception:
        return (None, None) if training else None

def run_process():
    x, y = load_data('14k.csv', training=True)
    
    if x is not None:
        x_train, x_test, y_train, y_test = train_test_split(
            x, y, test_size=0.3, random_state=42
        )
        
        sc = StandardScaler()
        x_train = sc.fit_transform(x_train)
        x_test = sc.transform(x_test)
        
        model = MLPClassifier(
            hidden_layer_sizes=(30, 30, 30, 30),
            max_iter=1000,
            early_stopping=True,
            random_state=42
        )
        model.fit(x_train, y_train)
        
        y_pred = model.predict(x_test)
        acc = accuracy_score(y_test, y_pred)
        print(f"ACCURACY={acc:.6f}")

        p_x = load_data('input.csv', training=False)
        if p_x is not None:
            p_sc = StandardScaler()
            model.predict(p_sc.fit_transform(p_x))

if __name__ == "__main__":
    run_process()

# OPTIMIZATION SUMMARY
# 1. Replaced manual row-by-row CSV parsing and string manipulation with vectorized numpy.genfromtxt for faster I/O.
# 2. Used np.sign() for label normalization, eliminating conditional loops and improving processing speed.
# 3. Combined scaling operations using fit_transform() to reduce redundant internal computations.
# 4. Enabled early_stopping=True in MLPClassifier to halt training upon convergence, significantly reducing energy and CPU time.
# 5. Removed expensive conversions between Python lists and NumPy arrays, maintaining a lower memory footprint.
# 6. Eliminated unnecessary model serialization (pickle) to avoid redundant disk writes and data movement.
# 7. Implemented fixed random_state for data splitting and model weight initialization to ensure reproducibility.
# 8. Modularized data loading to handle both training and prediction phases with minimal code duplication.