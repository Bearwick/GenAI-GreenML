# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

def load_data(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            df = pd.read_csv(path, sep=';', decimal=',')
        if df.empty:
            return None
        if df.columns[-1].startswith('Unnamed') and df.iloc[:, -1].isnull().all():
            df = df.iloc[:, :-1]
        return df.to_numpy(dtype=np.float64)
    except:
        return None

def main():
    mlp = MLPClassifier(hidden_layer_sizes=(30, 30, 30, 30), max_iter=1000, random_state=42)
    accuracy = 0.0
    
    train_data = load_data('14k.csv')
    if train_data is not None:
        X = train_data[:, :-1]
        y_raw = train_data[:, -1]
        y = np.where(y_raw > 0, 1.0, np.where(y_raw < 0, -1.0, y_raw))
        
        xtr, xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42)
        
        scaler_train = StandardScaler()
        xtr_s = scaler_train.fit_transform(xtr)
        xte_s = scaler_train.transform(xte)
        
        mlp.fit(xtr_s, ytr)
        y_pred = mlp.predict(xte_s)
        accuracy = accuracy_score(yte, y_pred)

    input_data = load_data('input.csv')
    if input_data is not None and hasattr(mlp, 'classes_'):
        scaler_p = StandardScaler()
        p_s = scaler_p.fit_transform(input_data)
        _ = mlp.predict(p_s)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# 1. Replaced manual row-by-row CSV parsing and list appending with vectorized pandas loading, significantly reducing CPU runtime.
# 2. Utilized NumPy's vectorized operations (np.where) for label transformation, eliminating inefficient loops and conditional branching.
# 3. Switched from Python lists to NumPy arrays for internal data handling, reducing memory overhead and improving cache locality.
# 4. Streamlined preprocessing by combining StandardScaler operations with fit_transform(), minimizing redundant data passes.
# 5. Removed global variables to improve memory management and data isolation.
# 6. Eliminated disk I/O overhead by removing pickle-based model serialization that was unnecessary for a single execution pass.
# 7. Set fixed random seeds (random_state) for deterministic training and reproducible energy profiles.
# 8. Implemented robust automated delimiter detection in CSV parsing to handle different schemas without manual intervention.
# 9. Removed all visualization, logging, and side-effect operations to focus strictly on necessary computation.