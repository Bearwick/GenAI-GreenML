# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings("ignore")

# Load dataset
df = pd.read_csv("input.csv")

# Identify target column - assume last column is target
target_col = df.columns[-1]

# Separate features and target
X = df.drop(columns=[target_col])
y = df[target_col]

# Encode target if categorical
if y.dtype == object or y.dtype.name == 'category':
    le_target = LabelEncoder()
    y = le_target.fit_transform(y)

# Identify numeric and categorical columns
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()

# Handle missing values for numeric columns with median (robust to outliers)
if numeric_cols:
    num_imputer = SimpleImputer(strategy='median')
    X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])

# Handle missing values for categorical columns with mode
if categorical_cols:
    cat_imputer = SimpleImputer(strategy='most_frequent')
    X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])

# Encode categorical features using label encoding (lightweight, no dimensionality increase)
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    label_encoders[col] = le

# Remove constant features (zero variance - they add no information)
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.0)
X_selected = selector.fit_transform(X)
selected_features = X.columns[selector.get_support()].tolist()
X = pd.DataFrame(X_selected, columns=selected_features)

# Scale numeric features for models that benefit from it
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train/test split with stratification for balanced evaluation
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Determine number of classes and samples for model selection
n_samples = X_train.shape[0]
n_features = X_train.shape[1]
n_classes = len(np.unique(y))

# Try multiple lightweight models and pick the best one
models = {}

# Logistic Regression - very lightweight, good baseline
models['lr'] = LogisticRegression(
    max_iter=1000,
    solver='lbfgs',
    multi_class='auto',
    random_state=42,
    n_jobs=-1
)

# Random Forest - good for tabular data, energy efficient with limited trees
models['rf'] = RandomForestClassifier(
    n_estimators=100,
    max_depth=min(20, n_features),
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)

# Gradient Boosting - often best for tabular, keep small for efficiency
models['gb'] = GradientBoostingClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42
)

best_accuracy = 0.0
best_model_name = None

for name, model in models