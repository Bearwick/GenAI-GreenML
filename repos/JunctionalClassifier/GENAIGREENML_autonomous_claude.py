# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pickle
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings("ignore")

# Load the pickle file
DATASET_PATH = "dict.pickle"

with open(DATASET_PATH, "rb") as f:
    data = pickle.load(f, encoding="latin1")

# Inspect what we got
# The pickle is likely a dict; try to convert to DataFrame
df = None

if isinstance(data, dict):
    # Could be a dict of arrays/lists or a dict with keys like 'data', 'target', etc.
    # Try common patterns
    if "data" in data and "target" in data:
        features = np.array(data["data"])
        target = np.array(data["target"])
        if "feature_names" in data:
            col_names = list(data["feature_names"])
        else:
            col_names = [f"feat_{i}" for i in range(features.shape[1])]
        df = pd.DataFrame(features, columns=col_names)
        df["target"] = target
    else:
        # Try to build a DataFrame from dict values
        # Check if values are arrays of same length
        lengths = {}
        for k, v in data.items():
            if isinstance(v, (list, np.ndarray)):
                lengths[k] = len(v)
            elif isinstance(v, pd.Series):
                lengths[k] = len(v)

        if lengths:
            # Find the most common length
            from collections import Counter
            common_len = Counter(lengths.values()).most_common(1)[0][0]
            usable = {k: v for k, v in data.items() if k in lengths and lengths[k] == common_len}
            if usable:
                df = pd.DataFrame(usable)
            else:
                # Fallback: try pd.DataFrame(data) directly
                df = pd.DataFrame(data)
        else:
            df = pd.DataFrame(data)
elif isinstance(data, pd.DataFrame):
    df = data
elif isinstance(data, (list, np.ndarray)):
    df = pd.DataFrame(data)
else:
    # Last resort
    df = pd.DataFrame(data)

assert df is not None and len(df) > 0, "Failed to load dataset into DataFrame"

# Normalize column names
df.columns = [str(c).strip() for c in df.columns]
df.columns = [" ".join(c.split()) for c in df.columns]
# Drop unnamed columns
df = df[[c for c in df.columns if not c.lower().startswith("unnamed")]]

# Based on README context: classification of junctions into -1 (remodelling), 0 (mixed), 1 (inactive)
# Target column might be named 'target', 'class', 'label', 'Class', 'Label', 'classification', etc.

target_col = None
target_candidates = ["target", "class", "label", "classification", "Class", "Label", "Target", "y"]
for tc in target_candidates:
    if tc in df.columns:
        target_col = tc
        break

if target_col is None:
    # Try to find a column with few unique values that looks like a target
    # Prefer integer columns with small cardinality
    best_col = None
    best_nunique = float("inf")
    for c in df.columns:
        col = pd.to_numeric(df[c], errors="coerce")
        nunique = col.dropna().nunique()
        if 2 <= nunique <= 20:
            if nunique < best_nunique:
                best_nunique = nunique
                best_col = c
    if best_col is not None:
        target_col = best_col
    else:
        #