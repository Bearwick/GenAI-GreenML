# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd
import pickle
import random
import unittest
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn import metrics

SEED = 42
random.seed(SEED)
np.random.seed(SEED)

MODEL_PARAMS = {"hidden_layer_sizes": (30, 30, 30, 30), "max_iter": 1000, "random_state": SEED}

mlp = MLPClassifier(**MODEL_PARAMS)
features = []
labels = []
pFeatures = []


def _read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            return pd.DataFrame()
    else:
        if df.shape[1] <= 1:
            try:
                df_alt = pd.read_csv(path, sep=";", decimal=",")
                if df_alt.shape[1] > df.shape[1]:
                    df = df_alt
            except Exception:
                pass
    return df


def _align_columns(df):
    headers = globals().get("DATASET_HEADERS")
    if isinstance(headers, (list, tuple)) and headers:
        if all(h in df.columns for h in headers):
            df = df[list(headers)]
    return df


def _clean_dataframe(df):
    if df.empty:
        return df
    df = _align_columns(df)
    df = df.dropna(axis=1, how="all")
    unnamed = [c for c in df.columns if str(c).startswith("Unnamed")]
    if unnamed:
        df = df.drop(columns=unnamed)
    if df.shape[1] > 0:
        last_col = df.columns[-1]
        series = df[last_col]
        if series.isna().all():
            df = df.iloc[:, :-1]
        else:
            if series.astype(str).str.strip().eq("").all():
                df = df.iloc[:, :-1]
    return df


def _ensure_numeric(df):
    if df.empty:
        return df
    if all(pd.api.types.is_numeric_dtype(t) for t in df.dtypes):
        return df
    return df.apply(pd.to_numeric, errors="coerce")


def _load_supervised(path):
    df = _read_csv_robust(path)
    if df.empty:
        return None, None
    df = _clean_dataframe(df)
    if df.shape[1] < 2:
        return None, None
    df = _ensure_numeric(df).dropna(axis=0, how="any")
    if df.empty:
        return None, None
    data = df.to_numpy(dtype=float, copy=False)
    X = data[:, :-1]
    y = data[:, -1]
    y = np.where(y > 0, 1, np.where(y < 0, -1, 0))
    return X, y


def _load_unsupervised(path):
    df = _read_csv_robust(path)
    if df.empty:
        return None
    df = _clean_dataframe(df)
    if df.empty:
        return None
    df = _ensure_numeric(df).dropna(axis=0, how="any")
    if df.empty:
        return None
    return df.to_numpy(dtype=float, copy=False)


def makeCSV(inpt):
    try:
        X, y = _load_supervised(inpt)
    except Exception:
        return
    if X is None:
        return
    features.extend(X.tolist())
    labels.extend(y.tolist())


def trainAndTestModel(data_path="14k.csv", test_size=0.3):
    global mlp
    X, y = _load_supervised(data_path)
    if X is None or len(X) == 0:
        return None
    features.extend(X.tolist())
    labels.extend(y.tolist())
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=SEED)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    mlp = MLPClassifier(**MODEL_PARAMS)
    mlp.fit(X_train, y_train)
    y_pred = mlp.predict(X_test)
    accuracy = metrics.accuracy_score(y_test, y_pred)
    return accuracy


def loadModel(path="dict.pickle"):
    global mlp
    with open(path, "rb") as f:
        mlp = pickle.load(f)
    return mlp


def saveModel(path="dict.pickle"):
    with open(path, "wb") as f:
        pickle.dump(mlp, f)


def wipeVariables():
    global features, labels
    features = []
    labels = []


def takeInput(path="input.csv"):
    try:
        data = _load_unsupervised(path)
    except Exception:
        return
    if data is None:
        return
    pFeatures.extend(data.tolist())


def predict(input_path="input.csv", model_path="dict.pickle"):
    loadModel(model_path)
    data = _load_unsupervised(input_path)
    if data is None:
        return None
    pFeatures.extend(data.tolist())
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    return mlp.predict(data_scaled)


class Testing(unittest.TestCase):
    def setUp(self):
        wipeVariables()
        return super().setUp()

    def tearDown(self):
        wipeVariables()
        return super().tearDown()

    def test_MakeCSV4(self):
        makeCSV("test.csv")
        self.assertEqual(len(features), 4)

    def test_MakeCSV2(self):
        makeCSV("test2.csv")
        self.assertEqual(len(labels), 2)


def main():
    accuracy = trainAndTestModel()
    if accuracy is None:
        accuracy = 0.0
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()
# Optimization Summary
# - Replaced row-wise CSV parsing with pandas vectorization and centralized cleaning to cut Python-level loops and data movement.
# - Added robust CSV parsing fallback and schema alignment to avoid mis-parsing without extra passes.
# - Used NumPy vectorized label normalization to reduce redundant computations.
# - Ensured deterministic training by setting fixed seeds and random_state parameters.
# - Removed automatic prediction/output side effects and eliminated redundant logging for lower runtime overhead.