# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import pickle
import random
import csv
import numpy as np
try:
    import pandas as pd
except Exception:
    pd = None
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
random.seed(RANDOM_STATE)

DATASET_PATH = os.getenv("DATASET_PATH", "dict.pickle")


def get_dataset_headers():
    headers = globals().get("DATASET_HEADERS")
    if headers is None:
        env = os.getenv("DATASET_HEADERS")
        if env:
            headers = [h.strip() for h in env.split(",") if h.strip()]
    if isinstance(headers, (list, tuple)):
        return list(headers)
    return None


def manual_read_csv(path, delimiter, decimal):
    try:
        with open(path, newline="") as f:
            reader = csv.reader(f, delimiter=delimiter)
            rows = [row for row in reader if row]
        if not rows:
            return None
        header = rows[0]
        data_rows = []
        for row in rows[1:]:
            if row and row[-1] == "":
                row = row[:-1]
            if decimal != ".":
                row = [item.replace(decimal, ".") if isinstance(item, str) else item for item in row]
            data_rows.append(row)
        data = np.array(data_rows)
        try:
            data = data.astype(float)
        except Exception:
            pass
        return {"data": data, "columns": header}
    except Exception:
        return None


def read_csv_flexible(path):
    if pd is not None:
        try:
            df = pd.read_csv(path)
            if df.shape[1] <= 1:
                df = pd.read_csv(path, sep=";", decimal=",")
            return df
        except Exception:
            try:
                return pd.read_csv(path, sep=";", decimal=",")
            except Exception:
                return None
    data = manual_read_csv(path, delimiter=",", decimal=".")
    if data is not None:
        arr = np.asarray(data["data"])
        if arr.ndim == 2 and arr.shape[1] <= 1:
            alt = manual_read_csv(path, delimiter=";", decimal=",")
            if alt is not None:
                alt_arr = np.asarray(alt["data"])
                if alt_arr.ndim == 2 and alt_arr.shape[1] > arr.shape[1]:
                    data = alt
    else:
        data = manual_read_csv(path, delimiter=";", decimal=",")
    return data


def load_raw_data(path):
    data = None
    if os.path.isfile(path):
        try:
            with open(path, "rb") as f:
                data = pickle.load(f)
        except Exception:
            data = None
    if data is None:
        data = read_csv_flexible(path)
    return data


def reorder_by_headers(columns):
    headers = get_dataset_headers()
    if headers:
        ordered = [c for c in headers if c in columns]
        if ordered:
            return ordered
    return None


def infer_target_index(columns):
    col_list = list(columns)
    headers = get_dataset_headers()
    if headers:
        for name in reversed(headers):
            if name in col_list:
                return col_list.index(name)
    for name in ("target", "label", "labels", "class", "y"):
        if name in col_list:
            return col_list.index(name)
    return len(col_list) - 1


def array_from_maybe_with_header(data):
    arr = np.asarray(data)
    header = None
    if arr.ndim == 2 and arr.dtype.kind in "OUS":
        try:
            arr = arr.astype(float)
        except Exception:
            header = arr[0].tolist()
            arr = arr[1:]
            try:
                arr = arr.astype(float)
            except Exception:
                pass
    return arr, header


def split_array(arr, columns=None):
    arr = np.asarray(arr)
    if arr.ndim == 1:
        arr = arr.reshape(-1, 1)
    if columns is not None and len(columns) == arr.shape[1]:
        columns = list(columns)
        ordered = reorder_by_headers(columns)
        if ordered:
            indices = [columns.index(c) for c in ordered]
            arr = arr[:, indices]
            columns = ordered
        target_idx = infer_target_index(columns)
        mask = np.ones(arr.shape[1], dtype=bool)
        mask[target_idx] = False
        return arr[:, mask], arr[:, target_idx]
    if arr.shape[1] > 1:
        return arr[:, :-1], arr[:, -1]
    raise ValueError("Unable to split features and labels")


def extract_split(data):
    if isinstance(data, dict):
        for prefix in ("X", "x"):
            train_key = f"{prefix}_train"
            test_key = f"{prefix}_test"
            if train_key in data and test_key in data and "y_train" in data and "y_test" in data:
                return (
                    np.asarray(data[train_key]),
                    np.asarray(data[test_key]),
                    np.asarray(data["y_train"]),
                    np.asarray(data["y_test"]),
                )
        if "train" in data and "test" in data:
            train = data["train"]
            test = data["test"]
            if isinstance(train, dict) and isinstance(test, dict):
                if "X" in train and "y" in train and "X" in test and "y" in test:
                    return (
                        np.asarray(train["X"]),
                        np.asarray(test["X"]),
                        np.asarray(train["y"]),
                        np.asarray(test["y"]),
                    )
                if "x" in train and "y" in train and "x" in test and "y" in test:
                    return (
                        np.asarray(train["x"]),
                        np.asarray(test["x"]),
                        np.asarray(train["y"]),
                        np.asarray(test["y"]),
                    )
    if hasattr(data, "X_train") and hasattr(data, "X_test") and hasattr(data, "y_train") and hasattr(data, "y_test"):
        return np.asarray(data.X_train), np.asarray(data.X_test), np.asarray(data.y_train), np.asarray(data.y_test)
    if isinstance(data, (tuple, list)) and len(data) == 4:
        return np.asarray(data[0]), np.asarray(data[1]), np.asarray(data[2]), np.asarray(data[3])
    return None


def extract_features_labels(data):
    if pd is not None and isinstance(data, pd.DataFrame):
        df = data
        ordered = reorder_by_headers(df.columns)
        if ordered:
            df = df.loc[:, ordered]
        columns = list(df.columns)
        target_idx = infer_target_index(columns)
        target_col = columns[target_idx]
        y = df[target_col].to_numpy()
        X = df.drop(columns=[target_col]).to_numpy()
        return X, y
    if isinstance(data, dict):
        for x_key, y_key in (("data", "target"), ("features", "labels"), ("X", "y"), ("x", "y"), ("inputs", "outputs")):
            if x_key in data and y_key in data:
                return np.asarray(data[x_key]), np.asarray(data[y_key])
        if "data" in data and "columns" in data:
            arr, header = array_from_maybe_with_header(data["data"])
            columns = data.get("columns") or header
            if columns is not None:
                return split_array(arr, columns)
            if arr.ndim == 2:
                return split_array(arr)
        arrays = {}
        for key, value in data.items():
            try:
                arr = np.asarray(value)
            except Exception:
                continue
            if arr.ndim >= 1 and arr.size > 0:
                arrays[key] = arr
        for arr_y in arrays.values():
            if arr_y.ndim == 1:
                for arr_x in arrays.values():
                    if arr_x.ndim == 2 and arr_x.shape[0] == arr_y.shape[0]:
                        return arr_x, arr_y
        for arr in arrays.values():
            if arr.ndim == 2 and arr.shape[1] > 1:
                return arr[:, :-1], arr[:, -1]
    if hasattr(data, "data") and hasattr(data, "target"):
        return np.asarray(data.data), np.asarray(data.target)
    if isinstance(data, (tuple, list)) and len(data) == 2:
        return np.asarray(data[0]), np.asarray(data[1])
    arr, header = array_from_maybe_with_header(data)
    if header is not None:
        return split_array(arr, header)
    return split_array(arr)


def ensure_2d(X):
    X = np.asarray(X)
    if X.ndim == 1:
        X = X.reshape(-1, 1)
    return X


def map_labels(y):
    y = np.asarray(y).ravel()
    if np.issubdtype(y.dtype, np.number) and np.any(y < 0):
        y = np.sign(y).astype(int, copy=False)
    return y


def prepare_train_test(data):
    split = extract_split(data)
    if split is None:
        X, y = extract_features_labels(data)
        X = ensure_2d(X)
        y = map_labels(y)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=RANDOM_STATE, shuffle=True
        )
    else:
        X_train, X_test, y_train, y_test = split
    X_train = ensure_2d(X_train).astype(float, copy=False)
    X_test = ensure_2d(X_test).astype(float, copy=False)
    y_train = map_labels(y_train)
    y_test = map_labels(y_test)
    return X_train, X_test, y_train, y_test


def train_and_evaluate(X_train, X_test, y_train, y_test):
    scaler = StandardScaler(copy=False)
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    model = MLPClassifier(hidden_layer_sizes=(30, 30, 30, 30), max_iter=1000, random_state=RANDOM_STATE)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return accuracy_score(y_test, y_pred)


def main():
    data = load_raw_data(DATASET_PATH)
    if data is None:
        raise ValueError("No data loaded")
    X_train, X_test, y_train, y_test = prepare_train_test(data)
    return train_and_evaluate(X_train, X_test, y_train, y_test)


if __name__ == "__main__":
    try:
        accuracy = main()
    except Exception:
        accuracy = 0.0
    print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Consolidated data loading with pickle/CSV fallback to avoid redundant file reads.
# - Replaced list-based feature accumulation with direct NumPy extraction for lower overhead.
# - Applied in-place scaling (copy=False) to reduce memory usage during preprocessing.
# - Centralized preprocessing and deterministic splitting to prevent repeated conversions.
# - Removed model persistence and unnecessary I/O to eliminate extra computation.