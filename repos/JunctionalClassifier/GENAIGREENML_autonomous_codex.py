# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import re
import pickle
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")

DATASET_PATH = "dict.pickle"

def read_csv_fallback(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] == 1:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > df.shape[1]:
                df = df_alt
        return df
    except Exception:
        try:
            return pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            return None

def object_to_df(obj):
    if isinstance(obj, pd.DataFrame):
        return obj
    if isinstance(obj, pd.Series):
        return obj.to_frame()
    if isinstance(obj, dict):
        data_key = None
        for key in ['data', 'X', 'x', 'features', 'inputs']:
            if key in obj:
                data_key = key
                break
        if data_key is not None:
            data = obj.get(data_key)
            if isinstance(data, pd.DataFrame):
                df = data.copy()
            else:
                try:
                    df = pd.DataFrame(data)
                except Exception:
                    df = None
            if df is None:
                return None
            feature_names = None
            for key in ['feature_names', 'features', 'columns']:
                if key in obj and obj.get(key) is not None:
                    feature_names = obj.get(key)
                    break
            if feature_names is not None and hasattr(feature_names, '__len__') and len(feature_names) == df.shape[1]:
                df.columns = [str(c) for c in feature_names]
            for tkey in ['target', 'y', 'Y', 'labels', 'label']:
                if tkey in obj:
                    tval = obj.get(tkey)
                    try:
                        if len(tval) == len(df):
                            df['target'] = tval
                            break
                    except Exception:
                        pass
            return df
        else:
            try:
                return pd.DataFrame(obj)
            except Exception:
                return None
    if hasattr(obj, 'shape'):
        try:
            return pd.DataFrame(obj)
        except Exception:
            return None
    if isinstance(obj, list):
        try:
            return pd.DataFrame(obj)
        except Exception:
            return None
    return None

df = None
if os.path.exists(DATASET_PATH):
    if DATASET_PATH.lower().endswith(('.csv', '.txt', '.data')):
        df = read_csv_fallback(DATASET_PATH)
    if df is None:
        try:
            obj = pd.read_pickle(DATASET_PATH)
            df = object_to_df(obj)
        except Exception:
            obj = None
            try:
                with open(DATASET_PATH, 'rb') as f:
                    obj = pickle.load(f)
                df = object_to_df(obj)
            except Exception:
                df = None
if df is None:
    df = read_csv_fallback(DATASET_PATH)
if df is None:
    try:
        obj = pd.read_pickle(DATASET_PATH)
        df = object_to_df(obj)
    except Exception:
        df = None
if df is None:
    df = pd.DataFrame()

if isinstance(df, pd.Series):
    df = df.to_frame()
df = df.copy()
df.columns = [re.sub(r'\s+', ' ', str(c)).strip() for c in df.columns]
df = df.loc[:, ~df.columns.str.contains(r'^Unnamed', case=False, regex=True)]
df = df.loc[:, ~df.columns.duplicated()]
if df.shape[1] == 0 and len(df) > 0:
    df = df.reset_index(drop=True)
    df['feature_0'] = df.index

target_col = None
patterns = ['target', 'label', 'class', 'y', 'output', 'response']
lower_cols = [c.lower() for c in df.columns]
for pat in patterns:
    for col, lcol in zip(df.columns, lower_cols):
        if pat == lcol or pat in lcol:
            target_col = col
            break
    if target_col is not None:
        break

def is_numeric_like(series):
    if pd.api.types.is_numeric_dtype(series):
        return True
    coerced = pd.to_numeric(series, errors='coerce')
    return coerced.notna().mean() > 0.8

if target_col is None:
    numeric_candidates = [col for col in df.columns if is_numeric_like(df[col])]
    non_constant = [col for col in numeric_candidates if df[col].nunique(dropna=True) > 1]
    if non_constant:
        target_col = non_constant[-1]
    elif numeric_candidates:
        target_col = numeric_candidates[-1]
    elif len(df.columns) > 0:
        target_col = df.columns[-1]
    else:
        target_col = None

if target_col is None:
    df['target'] = 0
    target_col = 'target'

feature_cols = [c for c in df.columns if c != target_col]
if len(feature_cols) == 0:
    df['const_feat'] = 0
    feature_cols = ['const_feat']

df = df[feature_cols + [target_col]].copy()
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df[feature_cols] = df[feature_cols].replace(r'^\s*$', np.nan, regex=True)

feature_cols = [c for c in feature_cols if not df[c].isna().all()]
if len(feature_cols) == 0:
    df['const_feat'] = 0
    feature_cols = ['const_feat']

df = df.dropna(subset=[target_col])

assert len(df) > 0

X = df[feature_cols].copy()
y_raw = df[target_col].copy()

y_num = pd.to_numeric(y_raw, errors='coerce')
numeric_y = y_num.notna().mean() > 0.9
if not numeric_y:
    problem_type = 'classification'
else:
    unique_vals = y_num.nunique(dropna=True)
    if unique_vals <= 20 or unique_vals <= max(2, int(len(y_num) * 0.1)):
        problem_type = 'classification'
    else:
        problem_type = 'regression'

if problem_type == 'regression':
    y = y_num
    mask = y.notna()
    X = X.loc[mask]
    y = y.loc[mask]
else:
    y = y_raw
    if numeric_y:
        y = y_num

assert len(X) > 0

numeric_features = []
categorical_features = []
for col in X.columns:
    series = X[col]
    if pd.api.types.is_numeric_dtype(series):
        numeric_features.append(col)
    else:
        coerced = pd.to_numeric(series, errors='coerce')
        if coerced.notna().mean() > 0.8:
            X[col] = coerced
            numeric_features.append(col)
        else:
            categorical_features.append(col)

for col in list(numeric_features):
    if X[col].isna().all():
        X = X.drop(columns=[col])
        numeric_features.remove(col)
for col in list(categorical_features):
    if X[col].isna().all():
        X = X.drop(columns=[col])
        categorical_features.remove(col)

if len(numeric_features) + len(categorical_features) == 0:
    X['const_feat'] = 0
    numeric_features = ['const_feat']
    categorical_features = []

test_size = 0.2
if len(X) < 5:
    test_size = 0.5
stratify = None
if problem_type == 'classification' and y.nunique() > 1 and len(y) >= 10:
    stratify = y
if len(X) < 2:
    X_train, X_test, y_train, y_test = X, X, y, y
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )

assert len(X_train) > 0
assert len(X_test) > 0

transformers = []
if len(numeric_features) > 0:
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler(with_mean=False))
    ])
    transformers.append(('num', numeric_transformer, numeric_features))
if len(categorical_features) > 0:
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
    ])
    transformers.append(('cat', categorical_transformer, categorical_features))

preprocessor = ColumnTransformer(transformers=transformers, remainder='drop', sparse_threshold=0.3)

if problem_type == 'classification':
    n_classes = y_train.nunique()
    if n_classes < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear')
elif problem_type == 'regression':
    if y_train.nunique() < 2:
        model = DummyRegressor(strategy='mean')
    else:
        model = Ridge(alpha=1.0)
else:
    model = DummyRegressor(strategy='mean')

clf = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

if problem_type == 'classification':
    if len(y_test) == 0:
        accuracy = 0.0
    else:
        accuracy = accuracy_score(y_test, y_pred)
else:
    if len(y_test) < 2:
        if len(y_test) == 0:
            accuracy = 0.0
        else:
            mae = float(np.mean(np.abs(np.array(y_test) - np.array(y_pred))))
            accuracy = 1.0 / (1.0 + mae)
    else:
        try:
            r2 = r2_score(y_test, y_pred)
        except Exception:
            r2 = 0.0
        accuracy = float(np.clip(r2, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Chose lightweight linear models (LogisticRegression/Ridge) or Dummy baselines to keep CPU usage low.
# - Used a compact preprocessing pipeline (imputation, optional scaling, one-hot encoding) via ColumnTransformer for reproducibility.
# - Implemented robust schema inference and defensive fallbacks to keep the pipeline running end-to-end on unknown data formats.
# - Regression accuracy is a clipped R2 (or MAE-based for tiny splits) to provide a stable [0,1] score.