# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import re
import pickle
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score, mean_absolute_error

warnings.filterwarnings("ignore")

DATASET_PATH = "dict.pickle"

def read_dataset(path):
    df = None
    obj = None
    try:
        obj = pd.read_pickle(path)
    except Exception:
        try:
            with open(path, "rb") as f:
                obj = pickle.load(f)
        except Exception:
            obj = None
    if obj is not None:
        if isinstance(obj, pd.DataFrame):
            df = obj
        elif isinstance(obj, dict):
            if "data" in obj:
                data = obj.get("data")
                if isinstance(data, pd.DataFrame):
                    df = data.copy()
                else:
                    try:
                        df = pd.DataFrame(data)
                    except Exception:
                        df = pd.DataFrame()
                if "feature_names" in obj:
                    try:
                        df.columns = obj["feature_names"]
                    except Exception:
                        pass
                if "target" in obj:
                    try:
                        df["target"] = obj["target"]
                    except Exception:
                        pass
            elif "X" in obj:
                try:
                    df = pd.DataFrame(obj.get("X"))
                except Exception:
                    df = pd.DataFrame()
                if "feature_names" in obj:
                    try:
                        df.columns = obj["feature_names"]
                    except Exception:
                        pass
                if "y" in obj:
                    try:
                        df["target"] = obj["y"]
                    except Exception:
                        pass
            else:
                try:
                    df = pd.DataFrame(obj)
                except Exception:
                    df = pd.DataFrame()
        elif isinstance(obj, (list, tuple)):
            try:
                df = pd.DataFrame(obj)
            except Exception:
                df = pd.DataFrame()
        else:
            try:
                df = pd.DataFrame(obj)
            except Exception:
                df = pd.DataFrame()
    if df is None or df.empty:
        try:
            df = pd.read_csv(path)
            if df.shape[1] == 1:
                sample = str(df.iloc[0, 0]) if df.shape[0] > 0 else ""
                if (";" in df.columns[0]) or (";" in sample):
                    df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            try:
                df = pd.read_csv(path, sep=";", decimal=",")
            except Exception:
                df = pd.DataFrame()
    return df

df = read_dataset(DATASET_PATH)

if df is None:
    df = pd.DataFrame()

if not df.empty:
    clean_cols = []
    for c in df.columns:
        name = re.sub(r"\s+", " ", str(c).strip())
        clean_cols.append(name)
    df.columns = clean_cols
    df = df.loc[:, [c for c in df.columns if not str(c).lower().startswith("unnamed")]]

assert df.shape[0] > 0 and df.shape[1] > 0

df = df.replace([np.inf, -np.inf], np.nan)

def select_target_column(dataframe):
    cols = list(dataframe.columns)
    lower_cols = [str(c).lower() for c in cols]
    preferred = ["target", "label", "class", "y", "outcome", "output"]
    for i, c in enumerate(lower_cols):
        if c in preferred:
            return cols[i]
    for i, c in enumerate(lower_cols):
        for kw in preferred:
            if kw in c:
                return cols[i]
    nunique = dataframe.nunique(dropna=True)
    nunique = nunique[nunique > 1]
    if len(nunique) > 0:
        return nunique.idxmin()
    return cols[-1]

target_col = select_target_column(df)
if target_col not in df.columns:
    target_col = df.columns[-1]

feature_cols = [c for c in df.columns if c != target_col]
if len(feature_cols) == 0:
    df["constant_feature"] = 0
    feature_cols = ["constant_feature"]

y_raw = df[target_col]
y_nonan = y_raw.dropna()
n_unique = y_nonan.nunique()
is_classification = False
if y_raw.dtype == object or pd.api.types.is_categorical_dtype(y_raw):
    if n_unique >= 2:
        is_classification = True
elif pd.api.types.is_bool_dtype(y_raw):
    if n_unique >= 2:
        is_classification = True
elif pd.api.types.is_integer_dtype(y_raw):
    if n_unique >= 2 and n_unique <= max(20, int(0.2 * len(y_nonan)) + 1):
        is_classification = True
elif pd.api.types.is_numeric_dtype(y_raw):
    if n_unique >= 2 and n_unique <= max(10, int(0.1 * len(y_nonan)) + 1):
        is_classification = True

if is_classification and n_unique < 2:
    is_classification = False

if is_classification:
    y = y_raw
    mask = ~pd.isna(y)
    df = df.loc[mask].copy()
    y = y.loc[mask].astype(str)
else:
    y = pd.to_numeric(y_raw, errors="coerce")
    mask = ~pd.isna(y)
    df = df.loc[mask].copy()
    y = y.loc[mask].astype(float)

assert df.shape[0] > 0

feature_cols = [c for c in df.columns if c != target_col]
if len(feature_cols) == 0:
    df["constant_feature"] = 0
    feature_cols = ["constant_feature"]

numeric_cols = []
categorical_cols = []
for col in feature_cols:
    series = df[col]
    if pd.api.types.is_numeric_dtype(series):
        df[col] = pd.to_numeric(series, errors="coerce")
        numeric_cols.append(col)
    else:
        num_series = pd.to_numeric(series, errors="coerce")
        if num_series.notna().mean() >= 0.5:
            df[col] = num_series
            numeric_cols.append(col)
        else:
            categorical_cols.append(col)

if len(numeric_cols) == 0 and len(categorical_cols) == 0:
    df["constant_feature"] = 0
    numeric_cols = ["constant_feature"]

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False))
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=True))
])

transformers = []
if len(numeric_cols) > 0:
    transformers.append(("num", numeric_transformer, numeric_cols))
if len(categorical_cols) > 0:
    transformers.append(("cat", categorical_transformer, categorical_cols))

preprocess = ColumnTransformer(transformers=transformers)

if is_classification:
    model = LogisticRegression(max_iter=200, solver="liblinear")
else:
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])

X = df[feature_cols]

if len(y) < 2:
    X_train = X
    X_test = X
    y_train = y
    y_test = y
else:
    stratify = None
    if is_classification and y.nunique() > 1:
        class_counts = y.value_counts()
        if (class_counts >= 2).all():
            stratify = y
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

assert X_train.shape[0] > 0 and X_test.shape[0] > 0

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if is_classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    try:
        r2 = r2_score(y_test, y_pred)
    except Exception:
        r2 = np.nan
    if not np.isfinite(r2):
        try:
            mae = mean_absolute_error(y_test, y_pred)
        except Exception:
            mae = np.nan
        if np.isfinite(mae):
            accuracy = 1.0 / (1.0 + mae)
        else:
            accuracy = 0.0
    else:
        accuracy = (r2 + 1.0) / 2.0
        if accuracy < 0.0:
            accuracy = 0.0
        if accuracy > 1.0:
            accuracy = 1.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) with simple preprocessing to remain CPU- and energy-efficient.
# - ColumnTransformer with imputation and one-hot encoding ensures reproducible handling of mixed schemas without heavy feature engineering.
# - Regression fallback maps R2 to a bounded [0,1] proxy and uses MAE-based backup when R2 is undefined.