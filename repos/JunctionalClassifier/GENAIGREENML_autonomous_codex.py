# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import re
import pickle
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")

DATASET_PATH = "dict.pickle"

def load_data(path):
    ext = os.path.splitext(path)[1].lower()
    if ext in [".pickle", ".pkl", ".pck"]:
        try:
            obj = pd.read_pickle(path)
        except Exception:
            with open(path, "rb") as f:
                obj = pickle.load(f)
        if isinstance(obj, pd.DataFrame):
            df = obj.copy()
        elif isinstance(obj, dict):
            if "data" in obj:
                data = obj["data"]
                if isinstance(data, pd.DataFrame):
                    df = data.copy()
                else:
                    df = pd.DataFrame(data)
                if "feature_names" in obj and len(obj["feature_names"]) == df.shape[1]:
                    df.columns = list(obj["feature_names"])
                if "target" in obj and len(obj["target"]) == len(df):
                    df["target"] = obj["target"]
            else:
                try:
                    if len(obj) > 0 and all(hasattr(v, "__len__") for v in obj.values()):
                        lengths = [len(v) for v in obj.values()]
                        if len(set(lengths)) == 1:
                            df = pd.DataFrame(obj)
                        else:
                            df = pd.DataFrame(list(obj))
                    else:
                        df = pd.DataFrame(obj)
                except Exception:
                    df = pd.DataFrame(list(obj))
        elif isinstance(obj, (list, tuple, np.ndarray)):
            df = pd.DataFrame(obj)
        else:
            df = pd.DataFrame(obj)
    else:
        try:
            df = pd.read_csv(path)
            if df.shape[1] == 1:
                try:
                    df2 = pd.read_csv(path, sep=";", decimal=",")
                    if df2.shape[1] > 1:
                        df = df2
                except Exception:
                    pass
        except Exception:
            df = pd.read_csv(path, sep=";", decimal=",")
    return df

df = load_data(DATASET_PATH)
assert isinstance(df, pd.DataFrame)
assert df.shape[0] > 0 and df.shape[1] > 0

def normalize_columns(cols):
    norm = []
    for c in cols:
        c_str = str(c).strip()
        c_str = re.sub(r"\s+", " ", c_str)
        norm.append(c_str)
    return norm

df.columns = normalize_columns(df.columns)
seen = {}
new_cols = []
for c in df.columns:
    if c in seen:
        seen[c] += 1
        new_cols.append(f"{c}_{seen[c]}")
    else:
        seen[c] = 0
        new_cols.append(c)
df.columns = new_cols

df = df.loc[:, [c for c in df.columns if not str(c).lower().startswith("unnamed")]]
df = df.dropna(axis=1, how="all")
assert df.shape[1] > 0

def choose_target(df):
    candidates = []
    for c in df.columns:
        lc = str(c).strip().lower()
        if lc in ["target", "label", "class", "y", "outcome", "output", "result", "response"]:
            candidates.append(c)
        elif "label" in lc or "class" in lc or "target" in lc:
            candidates.append(c)
    if candidates:
        return candidates[0]
    numeric_cols = []
    for c in df.columns:
        ser = pd.to_numeric(df[c], errors="coerce")
        if ser.notna().sum() > 0:
            numeric_cols.append(c)
    non_constant = []
    unique_counts = {}
    for c in numeric_cols:
        ser = pd.to_numeric(df[c], errors="coerce")
        uniq = ser.dropna().unique()
        if len(uniq) > 1:
            non_constant.append(c)
            unique_counts[c] = len(uniq)
    if non_constant:
        return sorted(non_constant, key=lambda x: unique_counts.get(x, np.inf))[0]
    return df.columns[0]

target_col = choose_target(df)
if target_col not in df.columns:
    target_col = df.columns[0]

y_raw = df[target_col]
y_num = pd.to_numeric(y_raw, errors="coerce")
if y_num.notna().sum() > 0:
    y_temp = y_num.replace([np.inf, -np.inf], np.nan)
else:
    y_temp = y_raw
mask = y_temp.notna()
df = df.loc[mask].copy()
y_raw = df[target_col]

assert len(df) > 0

y_num = pd.to_numeric(y_raw, errors="coerce")
numeric_target_available = y_num.notna().sum() > 0
n_samples = len(df)

if numeric_target_available:
    uniq_count = len(y_num.dropna().unique())
    classification = uniq_count <= max(20, int(0.2 * n_samples))
else:
    classification = True

if classification:
    y_for_model = y_raw.astype(str)
    le = LabelEncoder()
    y_encoded = le.fit_transform(y_for_model)
    n_classes = len(le.classes_)
else:
    y_encoded = pd.to_numeric(y_raw, errors="coerce").replace([np.inf, -np.inf], np.nan)
    mask = y_encoded.notna()
    df = df.loc[mask].copy()
    y_encoded = y_encoded.loc[mask].values
    n_classes = None
    n_samples = len(df)

X = df.drop(columns=[target_col])
if X.shape[1] == 0:
    X = pd.DataFrame({"constant": np.zeros(len(df))})

numeric_features = []
categorical_features = []
for c in X.columns:
    ser = pd.to_numeric(X[c], errors="coerce")
    ratio = ser.notna().mean() if len(ser) > 0 else 0
    if ratio >= 0.8:
        numeric_features.append(c)
    else:
        categorical_features.append(c)

numeric_features = list(numeric_features)
categorical_features = list(categorical_features)

X_processed = X.copy()
if numeric_features:
    X_processed[numeric_features] = X_processed[numeric_features].apply(pd.to_numeric, errors="coerce")
    X_processed[numeric_features] = X_processed[numeric_features].replace([np.inf, -np.inf], np.nan)
if categorical_features:
    X_processed[categorical_features] = X_processed[categorical_features].astype(str)

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False))
])
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=True))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ],
    remainder="drop"
)

if classification:
    if n_classes is None:
        n_classes = len(np.unique(y_encoded))
    if n_classes < 2:
        model = DummyClassifier(strategy="most_frequent")
    else:
        solver = "liblinear" if n_classes <= 2 else "saga"
        model = LogisticRegression(max_iter=200, solver=solver, n_jobs=1)
else:
    if np.nanstd(y_encoded) == 0:
        model = DummyRegressor(strategy="mean")
    else:
        model = Ridge(alpha=1.0)

clf = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])

y_encoded_arr = np.array(y_encoded)
n_samples = len(X_processed)

if n_samples < 2:
    X_train, X_test, y_train, y_test = X_processed, X_processed, y_encoded_arr, y_encoded_arr
else:
    test_size = 0.2 if n_samples > 5 else 0.5
    if classification and len(np.unique(y_encoded_arr)) > 1 and n_samples >= 5:
        stratify = y_encoded_arr
    else:
        stratify = None
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X_processed, y_encoded_arr, test_size=test_size, random_state=42, stratify=stratify
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X_processed, y_encoded_arr, test_size=test_size, random_state=42, stratify=None
        )

assert len(X_train) > 0 and len(X_test) > 0

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    accuracy = 0.5 * (r2 + 1.0)
    if not np.isfinite(accuracy):
        accuracy = 0.0
    accuracy = float(np.clip(accuracy, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) with simple preprocessing for CPU efficiency.
# - Applied sparse one-hot encoding and basic imputation/scaling to keep memory and compute low.
# - For regression fallback, converted R^2 to a bounded accuracy proxy via (r2+1)/2 with clipping.