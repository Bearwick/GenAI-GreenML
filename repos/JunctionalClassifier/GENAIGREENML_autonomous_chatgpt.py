# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _read_csv_robust(path):
    # Try default parsing first
    try:
        df1 = pd.read_csv(path)
    except Exception:
        df1 = None

    # Fallback parsing
    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
    except Exception:
        df2 = None

    # Choose the parse that "looks" more columnar
    def score_df(d):
        if d is None:
            return -1
        if d.shape[0] == 0:
            return -1
        # Prefer more columns and fewer object-only single-column failures
        score = d.shape[1]
        if d.shape[1] == 1:
            score -= 2
        return score

    df = df1 if score_df(df1) >= score_df(df2) else df2
    if df is None:
        raise RuntimeError("Could not read CSV with either default or fallback parser.")
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)
    return df


def _pick_target_and_features(df):
    cols = list(df.columns)
    if len(cols) == 0:
        return None, []

    # Build numeric candidates by coercion (do not assume dtypes are correct)
    numeric_cols = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        # consider numeric if has at least some finite values
        finite = np.isfinite(s.to_numpy(dtype=float, copy=False, na_value=np.nan))
        if np.nanmean(finite.astype(float)) > 0.5:  # majority numeric-like
            numeric_cols.append(c)

    # Prefer a target column name if present, otherwise pick last numeric non-constant, else last column
    preferred_names = ["target", "label", "y", "class", "outcome"]
    target_col = None
    lower_map = {str(c).strip().lower(): c for c in cols}
    for pn in preferred_names:
        if pn in lower_map:
            target_col = lower_map[pn]
            break

    def non_constant(col):
        s = pd.to_numeric(df[col], errors="coerce")
        s = s.replace([np.inf, -np.inf], np.nan)
        nun = s.nunique(dropna=True)
        return nun >= 2

    if target_col is None:
        # choose last numeric non-constant
        for c in reversed(numeric_cols):
            if non_constant(c):
                target_col = c
                break

    if target_col is None:
        # fallback: last column
        target_col = cols[-1]

    feature_cols = [c for c in cols if c != target_col]
    if len(feature_cols) == 0:
        # fallback: if only one column exists, use it as feature and make a synthetic target (constant)
        feature_cols = [target_col]
        target_col = None
    return target_col, feature_cols


def _make_preprocessor(X):
    numeric_selector = make_column_selector(dtype_include=np.number)
    categorical_selector = make_column_selector(dtype_exclude=np.number)

    numeric_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False)),
        ]
    )

    categorical_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_pipe, numeric_selector),
            ("cat", categorical_pipe, categorical_selector),
        ],
        sparse_threshold=0.3,
        remainder="drop",
    )
    return preprocessor


def _bounded_regression_accuracy(y_true, y_pred):
    # Stable proxy in [0,1]: 1 / (1 + MAE / (IQR + eps))
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    err = np.nanmean(np.abs(y_true - y_pred))
    q75, q25 = np.nanpercentile(y_true, [75, 25])
    scale = float(q75 - q25)
    if not np.isfinite(scale) or scale <= 0:
        scale = float(np.nanstd(y_true))
    if not np.isfinite(scale) or scale <= 0:
        scale = 1.0
    eps = 1e-9
    score = 1.0 / (1.0 + (err / (scale + eps)))
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    # Locate a dataset file robustly
    candidates = []
    for name in ["dataset.csv", "data.csv", "train.csv", "14k.csv", "input.csv"]:
        if os.path.exists(name):
            candidates.append(name)

    if not candidates:
        # pick first csv in directory, if any
        csvs = [f for f in os.listdir(".") if f.lower().endswith(".csv")]
        csvs.sort()
        if csvs:
            candidates.append(csvs[0])

    if not candidates:
        # Cannot proceed without a CSV; produce deterministic trivial result
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    path = candidates[0]
    df = _read_csv_robust(path)
    df = df.copy()
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    assert df.shape[0] > 0, "Dataset is empty after loading."

    target_col, feature_cols = _pick_target_and_features(df)

    if target_col is None:
        # Synthetic constant target when schema is unusable; trivial baseline
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Prepare X, y with defensive coercions
    X = df[feature_cols].copy()
    y_raw = df[target_col].copy()

    # Replace infinities early
    if isinstance(X, pd.DataFrame):
        for c in X.columns:
            if X[c].dtype == object:
                continue
            X[c] = pd.to_numeric(X[c], errors="coerce")
            X[c] = X[c].replace([np.inf, -np.inf], np.nan)

    # Determine task: classification if y is low-cardinality (or non-numeric)
    y_num = pd.to_numeric(y_raw, errors="coerce").replace([np.inf, -np.inf], np.nan)
    y_is_numeric = np.isfinite(y_num.to_numpy(dtype=float, copy=False, na_value=np.nan)).mean() > 0.5

    if y_is_numeric:
        nunique = y_num.nunique(dropna=True)
        # classification if looks like small discrete set (e.g., -1/0/1, 0/1, 1..K)
        is_classification = nunique >= 2 and nunique <= 20
    else:
        is_classification = True

    # Drop rows with missing target
    if y_is_numeric:
        mask = y_num.notna()
        y_for_model = y_num[mask]
    else:
        mask = y_raw.notna()
        y_for_model = y_raw[mask].astype(str)

    X = X.loc[mask].copy()

    assert X.shape[0] > 1, "Not enough samples after dropping missing targets."

    # Split
    stratify = None
    if is_classification:
        # Only stratify if at least 2 classes with enough support
        y_tmp = y_for_model
        if hasattr(y_tmp, "value_counts"):
            vc = y_tmp.value_counts(dropna=True)
            if vc.shape[0] >= 2 and (vc.min() >= 2):
                stratify = y_tmp

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y_for_model,
        test_size=0.25,
        random_state=42,
        stratify=stratify,
    )

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Train/test split failed."

    preprocessor = _make_preprocessor(X_train)

    if is_classification:
        # If only one class in train, fallback to trivial baseline
        if pd.Series(y_train).nunique(dropna=True) < 2:
            # Predict the only class
            only = pd.Series(y_train).iloc[0]
            y_pred = np.array([only] * len(y_test), dtype=object)
            accuracy = float(accuracy_score(y_test, y_pred))
        else:
            # LogisticRegression is lightweight and strong baseline on CPU
            clf = LogisticRegression(
                solver="liblinear",
                max_iter=200,
                random_state=42,
            )
            pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
            pipe.fit(X_train, y_train)
            y_pred = pipe.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Regression fallback: Ridge is fast and stable on sparse/one-hot features
        reg = Ridge(alpha=1.0, random_state=42)
        pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        pipe.fit(X_train, y_train.astype(float))
        y_pred = pipe.predict(X_test)
        accuracy = _bounded_regression_accuracy(y_test.astype(float), y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced heavy ANN/MLP with LogisticRegression (classification) or Ridge (regression): fast convex solvers, CPU-friendly.
# - Used ColumnTransformer + Pipeline to avoid repeated preprocessing and ensure reproducibility.
# - Robust CSV loading with delimiter/decimal fallback to handle common European CSV formats.
# - Defensive schema handling: auto-select target/features, drop 'Unnamed:' columns, normalize column names, coerce numerics safely.
# - Lightweight preprocessing: median/mode imputation + sparse one-hot encoding; StandardScaler(with_mean=False) supports sparse matrices efficiently.
# - Regression "accuracy" printed as bounded proxy in [0,1]: 1/(1+MAE/(IQR+eps)) for stability across unknown target scales.