# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import pickle
import math
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


RANDOM_STATE = 42


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    new_cols = []
    for c in df.columns:
        if not isinstance(c, str):
            c = str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        new_cols.append(c)
    df.columns = new_cols
    drop_cols = [c for c in df.columns if isinstance(c, str) and c.lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _try_read_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    df = _normalize_columns(df)
    # Heuristic: if single wide column, retry with alternative separators/decimal
    if df.shape[1] <= 1:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        df2 = _normalize_columns(df2)
        if df2.shape[1] > df.shape[1]:
            df = df2
    return df


def _load_dataset(dataset_path: str) -> pd.DataFrame:
    ext = os.path.splitext(dataset_path)[1].lower()
    if ext in [".csv", ".txt"]:
        return _try_read_csv(dataset_path)

    if ext in [".pkl", ".pickle"]:
        with open(dataset_path, "rb") as f:
            obj = pickle.load(f)

        # Robustly convert unknown pickle schema to DataFrame
        if isinstance(obj, pd.DataFrame):
            df = obj
        elif isinstance(obj, dict):
            # Common patterns: {"X":..., "y":...}, {"data":...}, or column->values
            keys_lower = {str(k).lower(): k for k in obj.keys()}
            if "df" in keys_lower and isinstance(obj[keys_lower["df"]], pd.DataFrame):
                df = obj[keys_lower["df"]]
            elif ("x" in keys_lower and "y" in keys_lower):
                X = obj[keys_lower["x"]]
                y = obj[keys_lower["y"]]
                X_df = pd.DataFrame(X)
                y_s = pd.Series(y, name="target")
                df = pd.concat([X_df, y_s], axis=1)
            elif ("data" in keys_lower) and isinstance(obj[keys_lower["data"]], (pd.DataFrame, dict, list, np.ndarray)):
                data = obj[keys_lower["data"]]
                if isinstance(data, pd.DataFrame):
                    df = data
                elif isinstance(data, dict):
                    df = pd.DataFrame(data)
                else:
                    df = pd.DataFrame(data)
            else:
                # Assume dict is column->values
                try:
                    df = pd.DataFrame(obj)
                except Exception:
                    # Last resort: wrap as single column
                    df = pd.DataFrame({"data": pd.Series([obj])})
        elif isinstance(obj, (list, tuple, np.ndarray)):
            df = pd.DataFrame(obj)
        else:
            df = pd.DataFrame({"data": pd.Series([obj])})

        df = _normalize_columns(df)
        return df

    # Fallback: attempt CSV parse
    return _try_read_csv(dataset_path)


def _pick_target_and_features(df: pd.DataFrame):
    df = df.copy()
    df = _normalize_columns(df)

    # If a plausible target exists by name, prefer it; else pick last non-constant column
    lower_map = {c.lower(): c for c in df.columns if isinstance(c, str)}
    preferred_names = ["target", "label", "class", "y", "output"]
    target_col = None
    for nm in preferred_names:
        if nm in lower_map:
            target_col = lower_map[nm]
            break

    # Score candidates by: non-constant, fewer NaNs, and "label-like" (few unique) if available
    def col_stats(c):
        s = df[c]
        nunique = s.nunique(dropna=True)
        na = s.isna().mean()
        return nunique, na

    if target_col is None:
        candidates = [c for c in df.columns if df[c].nunique(dropna=True) > 1]
        if not candidates:
            # no usable target; create a trivial one
            df["target"] = 0
            target_col = "target"
        else:
            # Prefer columns with few unique values (likely classification labels), else numeric non-constant
            best = None
            best_score = None
            for c in candidates:
                nunique, na = col_stats(c)
                # Penalize too many uniques (likely continuous) but keep as fallback
                # Lower score is better
                score = (min(nunique, 2000), na, -int(c == df.columns[-1]))
                if best_score is None or score < best_score:
                    best_score = score
                    best = c
            target_col = best

    # Features: everything except target
    feature_cols = [c for c in df.columns if c != target_col]

    # If no features, create a constant feature so pipeline can run
    if len(feature_cols) == 0:
        df["feat0"] = 0.0
        feature_cols = ["feat0"]

    return df, target_col, feature_cols


def _coerce_target(y: pd.Series):
    # Try numeric coercion; keep original if many NaNs appear
    y_num = pd.to_numeric(y, errors="coerce")
    num_valid = y_num.notna().mean()
    if num_valid >= 0.9:
        y_out = y_num
    else:
        y_out = y.astype("object")
    return y_out


def _is_classification_target(y: pd.Series) -> bool:
    # classification if object/category/bool OR small number of unique values
    if y.dtype == bool:
        return True
    if str(y.dtype) in ("object", "category"):
        return True
    y_nonan = y.dropna()
    if y_nonan.empty:
        return False
    nunique = y_nonan.nunique()
    # treat as classification if low cardinality
    if nunique <= 20:
        return True
    return False


def _bounded_regression_score(y_true, y_pred) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    y_mean = float(np.mean(y_true))
    ss_tot = float(np.sum((y_true - y_mean) ** 2))
    if ss_tot <= 1e-12:
        # If y is (near) constant, treat perfect prediction if close
        mae = float(np.mean(np.abs(y_true - y_pred)))
        return float(max(0.0, 1.0 - mae / (abs(y_mean) + 1.0)))
    r2 = 1.0 - ss_res / ss_tot
    # Map R2 to [0,1] safely for "accuracy" proxy
    return float(max(0.0, min(1.0, (r2 + 1.0) / 2.0)))


def main():
    dataset_path = "dict.pickle"
    df = _load_dataset(dataset_path)
    df = _normalize_columns(df)

    # Basic cleaning for fully empty rows/cols
    df = df.dropna(axis=0, how="all")
    df = df.dropna(axis=1, how="all")
    assert df is not None and isinstance(df, pd.DataFrame)
    assert df.shape[0] > 0 and df.shape[1] > 0

    df, target_col, feature_cols = _pick_target_and_features(df)

    X = df[feature_cols].copy()
    y_raw = df[target_col].copy()
    y = _coerce_target(y_raw)

    # Identify column types based on actual dtypes (after attempting numeric coercion per-column)
    Xc = X.copy()
    for c in Xc.columns:
        if Xc[c].dtype == "object" or str(Xc[c].dtype) == "category":
            # try numeric coercion; keep as object if many non-numeric
            xn = pd.to_numeric(Xc[c], errors="coerce")
            if xn.notna().mean() >= 0.9:
                Xc[c] = xn

    numeric_features = [c for c in Xc.columns if pd.api.types.is_numeric_dtype(Xc[c])]
    categorical_features = [c for c in Xc.columns if c not in numeric_features]

    # Replace inf with NaN for numeric columns
    if numeric_features:
        Xc[numeric_features] = Xc[numeric_features].replace([np.inf, -np.inf], np.nan)

    is_clf = _is_classification_target(y)

    # Ensure we have enough samples for split
    n_samples = int(len(Xc))
    if n_samples < 4:
        # Too small to split reliably; do a trivial evaluation on same data
        if is_clf:
            # Predict majority class
            y_nonan = y.dropna()
            if y_nonan.empty or y_nonan.nunique() < 2:
                accuracy = 1.0
            else:
                majority = y_nonan.value_counts().idxmax()
                y_pred = np.array([majority] * len(y))
                accuracy = float(accuracy_score(y.fillna(majority), y_pred))
        else:
            y_num = pd.to_numeric(y, errors="coerce").fillna(0.0).to_numpy()
            y_pred = np.full_like(y_num, float(np.nanmean(y_num)) if np.isfinite(np.nanmean(y_num)) else 0.0)
            accuracy = _bounded_regression_score(y_num, y_pred)
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Preprocessing pipelines
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if is_clf:
        # Prepare y for classification
        y_clf = y.copy()
        if pd.api.types.is_numeric_dtype(y_clf):
            y_clf = y_clf.astype("Int64").astype("object")
        else:
            y_clf = y_clf.astype("object")

        # Handle missing labels by dropping those rows (safer than imputing labels)
        mask = y_clf.notna()
        Xc2 = Xc.loc[mask].copy()
        y_clf2 = y_clf.loc[mask].copy()

        if y_clf2.nunique() < 2:
            # fallback: regression on numeric-coerced target if possible, else trivial
            y_num = pd.to_numeric(y, errors="coerce")
            if y_num.notna().sum() >= 2 and y_num.nunique(dropna=True) > 1:
                is_clf = False
            else:
                majority = y_clf2.value_counts().idxmax() if len(y_clf2) else 0
                y_pred = np.array([majority] * len(y_clf2))
                accuracy = float(accuracy_score(y_clf2, y_pred)) if len(y_clf2) else 1.0
                print(f"ACCURACY={accuracy:.6f}")
                return

        if is_clf:
            # Stratify if possible (small class counts can break stratification)
            strat = y_clf2 if y_clf2.nunique() > 1 and y_clf2.value_counts().min() >= 2 else None
            X_train, X_test, y_train, y_test = train_test_split(
                Xc2, y_clf2, test_size=0.2, random_state=RANDOM_STATE, stratify=strat
            )
            assert len(X_train) > 0 and len(X_test) > 0

            model = LogisticRegression(
                solver="liblinear",
                max_iter=200,
                class_weight="balanced",
                random_state=RANDOM_STATE,
            )

            clf = Pipeline(steps=[
                ("preprocess", preprocessor),
                ("model", model),
            ])

            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

    # Regression path
    y_num = pd.to_numeric(y, errors="coerce")
    mask = y_num.notna()
    Xc2 = Xc.loc[mask].copy()
    y_num2 = y_num.loc[mask].astype(float)

    if len(y_num2) < 4:
        # trivial regression
        y_true = y_num2.to_numpy()
        y_pred = np.full_like(y_true, float(np.nanmean(y_true)) if np.isfinite(np.nanmean(y_true)) else 0.0)
        accuracy = _bounded_regression_score(y_true, y_pred)
        print(f"ACCURACY={accuracy:.6f}")
        return

    X_train, X_test, y_train, y_test = train_test_split(
        Xc2, y_num2, test_size=0.2, random_state=RANDOM_STATE
    )
    assert len(X_train) > 0 and len(X_test) > 0

    reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)

    reg_pipe = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", reg),
    ])

    reg_pipe.fit(X_train, y_train)
    y_pred = reg_pipe.predict(X_test)
    accuracy = _bounded_regression_score(y_test.to_numpy(), np.asarray(y_pred, dtype=float))
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight, CPU-friendly baselines (LogisticRegression with liblinear; Ridge for regression fallback) to minimize compute/energy.
# - ColumnTransformer + Pipeline ensures single-pass, reproducible preprocessing and avoids duplicated work.
# - Robust schema handling: loads pickle/CSV, normalizes headers, drops "Unnamed" columns, auto-selects target/features, coerces numerics defensively.
# - Handles NaN/inf via SimpleImputer; OneHotEncoder is sparse to reduce memory/compute for categorical features.
# - Uses fixed random_state for determinism; minimal feature engineering (standardization + one-hot only).
# - If regression is used, prints ACCURACY as a bounded proxy: clip((R2 + 1)/2) into [0,1] for stable reporting.