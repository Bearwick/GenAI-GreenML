# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import pickle
import warnings
from typing import Tuple, Optional, Dict, Any

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df: pd.DataFrame) -> pd.DataFrame:
    keep = [c for c in df.columns if not re.match(r"^Unnamed:\s*\d+$", str(c).strip())]
    return df[keep]


def _safe_to_dataframe(obj) -> Optional[pd.DataFrame]:
    if obj is None:
        return None
    if isinstance(obj, pd.DataFrame):
        return obj.copy()
    if isinstance(obj, np.ndarray):
        if obj.ndim == 1:
            return pd.DataFrame({"feature_0": obj})
        return pd.DataFrame(obj, columns=[f"feature_{i}" for i in range(obj.shape[1])])
    if isinstance(obj, dict):
        try:
            df = pd.DataFrame(obj)
            return df
        except Exception:
            return None
    if isinstance(obj, (list, tuple)):
        try:
            df = pd.DataFrame(obj)
            return df
        except Exception:
            return None
    return None


def _coerce_numeric_df(df: pd.DataFrame, numeric_cols) -> pd.DataFrame:
    df2 = df.copy()
    for c in numeric_cols:
        df2[c] = pd.to_numeric(df2[c], errors="coerce")
    return df2


def _extract_X_y_from_pickle(obj) -> Tuple[Optional[pd.DataFrame], Optional[pd.Series]]:
    X_df = None
    y_ser = None

    if isinstance(obj, dict):
        lower_keys = {str(k).lower(): k for k in obj.keys()}
        for cand in ["x", "data", "features", "inputs"]:
            if cand in lower_keys:
                X_df = _safe_to_dataframe(obj[lower_keys[cand]])
                break
        for cand in ["y", "target", "labels", "label", "output", "outputs"]:
            if cand in lower_keys:
                y = obj[lower_keys[cand]]
                if isinstance(y, pd.Series):
                    y_ser = y.copy()
                else:
                    try:
                        y_ser = pd.Series(y)
                    except Exception:
                        y_ser = None
                break

        if X_df is None:
            dfs = [v for v in obj.values() if isinstance(v, pd.DataFrame)]
            if len(dfs) == 1:
                X_df = dfs[0].copy()
        if y_ser is None:
            sers = [v for v in obj.values() if isinstance(v, pd.Series)]
            if len(sers) == 1:
                y_ser = sers[0].copy()

    elif isinstance(obj, (list, tuple)) and len(obj) == 2:
        X_df = _safe_to_dataframe(obj[0])
        y = obj[1]
        if isinstance(y, pd.Series):
            y_ser = y.copy()
        else:
            try:
                y_ser = pd.Series(y)
            except Exception:
                y_ser = None

    elif isinstance(obj, pd.DataFrame):
        X_df = obj.copy()

    return X_df, y_ser


def _choose_target_from_df(df: pd.DataFrame) -> Tuple[pd.Series, pd.DataFrame]:
    df2 = df.copy()

    # Prefer common target column names if present
    preferred = ["target", "label", "class", "y", "output"]
    lower_map = {c.lower(): c for c in df2.columns}
    for p in preferred:
        if p in lower_map:
            tcol = lower_map[p]
            y = df2[tcol]
            X = df2.drop(columns=[tcol])
            return y, X

    # Otherwise: choose a non-constant column; prefer non-numeric (often label), else numeric
    non_constant_cols = []
    for c in df2.columns:
        s = df2[c]
        try:
            nun = s.nunique(dropna=True)
        except Exception:
            nun = 0
        if nun >= 2:
            non_constant_cols.append(c)

    if not non_constant_cols:
        # degenerate: create a constant target and use empty features -> will fallback safely later
        y = pd.Series(np.zeros(len(df2), dtype=int), name="target")
        X = df2.copy()
        return y, X

    obj_cols = [c for c in non_constant_cols if df2[c].dtype == "object"]
    if obj_cols:
        tcol = obj_cols[-1]
        y = df2[tcol]
        X = df2.drop(columns=[tcol])
        return y, X

    # Prefer numeric but avoid all-NaN after coercion
    numeric_cols = []
    for c in non_constant_cols:
        if pd.api.types.is_numeric_dtype(df2[c]):
            numeric_cols.append(c)
    if numeric_cols:
        tcol = numeric_cols[-1]
        y = df2[tcol]
        X = df2.drop(columns=[tcol])
        return y, X

    # Fallback: just pick last non-constant
    tcol = non_constant_cols[-1]
    y = df2[tcol]
    X = df2.drop(columns=[tcol])
    return y, X


def _load_dataset(dataset_path: str) -> pd.DataFrame:
    # Primary path: pickle loading
    with open(dataset_path, "rb") as f:
        obj = pickle.load(f)

    X_df, y_ser = _extract_X_y_from_pickle(obj)

    if X_df is not None and y_ser is not None:
        X_df = X_df.copy()
        y_ser = y_ser.copy()
        y_ser.name = "target"
        df = X_df
        df["target"] = y_ser.values
        return df

    if isinstance(obj, pd.DataFrame):
        return obj.copy()

    df_try = _safe_to_dataframe(obj)
    if df_try is not None and isinstance(df_try, pd.DataFrame) and df_try.shape[0] > 0 and df_try.shape[1] > 0:
        return df_try

    # Fallback: try reading as CSV if user passed a wrong extension
    try:
        df = pd.read_csv(dataset_path)
        return df
    except Exception:
        try:
            df = pd.read_csv(dataset_path, sep=";", decimal=",")
            return df
        except Exception as e:
            raise RuntimeError(f"Could not load dataset from {dataset_path}: {e}")


def _robust_read_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        if df2.shape[1] > df.shape[1]:
            df = df2
    return df


def _make_preprocessor(X: pd.DataFrame) -> Tuple[ColumnTransformer, list, list]:
    Xc = X.copy()

    # Normalize obvious numeric strings (lightweight)
    for c in Xc.columns:
        if Xc[c].dtype == "object":
            # Try to detect numeric-like columns by sampling
            s = Xc[c].astype(str).str.replace(",", ".", regex=False)
            sample = s.head(50)
            num = pd.to_numeric(sample, errors="coerce")
            if num.notna().mean() >= 0.8:
                Xc[c] = pd.to_numeric(s, errors="coerce")

    numeric_features = [c for c in Xc.columns if pd.api.types.is_numeric_dtype(Xc[c])]
    categorical_features = [c for c in Xc.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    return preprocessor, numeric_features, categorical_features


def _is_classification_target(y: pd.Series) -> bool:
    y0 = y.dropna()
    if len(y0) == 0:
        return False

    # If object or bool, assume classification
    if y0.dtype == "object" or pd.api.types.is_bool_dtype(y0):
        return True

    # If numeric: treat as classification when low cardinality
    y_num = pd.to_numeric(y0, errors="coerce")
    y_num = y_num.dropna()
    if len(y_num) == 0:
        return True
    unique_vals = np.unique(y_num.values)
    if len(unique_vals) <= 20:
        # Also ensure not "continuous-like"
        if len(unique_vals) <= max(2, int(0.05 * len(y_num))):
            return True
        # Common case: -1/0/1 labels
        if len(unique_vals) <= 10:
            return True
    return False


def _prepare_y_for_classification(y: pd.Series) -> pd.Series:
    y2 = y.copy()
    if y2.dtype == "object":
        y2 = y2.astype(str)
    return y2


def _bounded_regression_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    denom = np.sum((yt - yt.mean()) ** 2)
    if denom <= 0:
        return 0.0
    r2 = 1.0 - (np.sum((yt - yp) ** 2) / denom)
    acc = float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))
    return acc


def main():
    dataset_path = "dict.pickle"
    if not os.path.exists(dataset_path):
        # Robust CSV fallback if user supplied a different file
        for cand in ["data.csv", "dataset.csv", "input.csv", "train.csv"]:
            if os.path.exists(cand):
                dataset_path = cand
                break

    if dataset_path.lower().endswith(".csv"):
        df = _robust_read_csv(dataset_path)
    else:
        df = _load_dataset(dataset_path)

    df = df.copy()
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Ensure non-empty
    assert df.shape[0] > 0 and df.shape[1] > 0

    # Choose/derive target
    if "target" in df.columns:
        y = df["target"]
        X = df.drop(columns=["target"])
    else:
        y, X = _choose_target_from_df(df)

    # Normalize feature column names
    X = X.copy()
    X.columns = _normalize_columns(X.columns)
    X = _drop_unnamed(X)

    # Clean infinities
    X = X.replace([np.inf, -np.inf], np.nan)
    y = y.replace([np.inf, -np.inf], np.nan) if isinstance(y, pd.Series) else y

    # Assert still usable
    assert len(X) > 0

    is_clf = _is_classification_target(y)

    # Drop rows with missing target
    notna_mask = ~pd.isna(y)
    X = X.loc[notna_mask].reset_index(drop=True)
    y = y.loc[notna_mask].reset_index(drop=True)

    assert len(X) > 1

    preprocessor, num_cols, cat_cols = _make_preprocessor(X)

    # Split (use stratify only when safe)
    if is_clf:
        y_clf = _prepare_y_for_classification(y)
        n_classes = y_clf.nunique(dropna=True)
        if n_classes >= 2:
            strat = y_clf if (n_classes <= max(2, int(0.2 * len(y_clf)))) else None
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_clf, test_size=0.2, random_state=RANDOM_STATE, stratify=strat
            )
            assert len(X_train) > 0 and len(X_test) > 0

            # Lightweight linear baseline; saga handles sparse one-hot efficiently on CPU for small-medium data
            model = LogisticRegression(
                solver="saga",
                max_iter=500,
                n_jobs=1,
                C=1.0,
            )

            clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
        else:
            # Fallback to regression proxy if only one class available
            is_clf = False

    if not is_clf:
        # Regression fallback
        y_reg = pd.to_numeric(y, errors="coerce")
        mask = y_reg.notna()
        Xr = X.loc[mask].reset_index(drop=True)
        yr = y_reg.loc[mask].reset_index(drop=True)

        if len(Xr) < 3 or yr.nunique(dropna=True) < 2:
            # Trivial constant predictor score
            accuracy = 0.0
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                Xr, yr, test_size=0.2, random_state=RANDOM_STATE
            )
            assert len(X_train) > 0 and len(X_test) > 0

            reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
            pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
            pipe.fit(X_train, y_train)
            y_pred = pipe.predict(X_test)
            accuracy = _bounded_regression_accuracy(y_test.values, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) for strong CPU baselines with low energy use.
# - Employs a single sklearn Pipeline + ColumnTransformer to avoid repeated preprocessing and ensure reproducibility.
# - OneHotEncoder (sparse) keeps memory/compute low for categoricals; StandardScaler(with_mean=False) is sparse-friendly.
# - Robust schema handling: auto-detect/choose target, normalize headers, drop Unnamed columns, coerce numeric-like strings.
# - Defensive fallbacks: if classification target is degenerate (<2 classes), switch to regression and report bounded R2 proxy in [0,1].