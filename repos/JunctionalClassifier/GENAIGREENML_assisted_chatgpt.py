# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import csv
import pickle
import unittest
from typing import List, Tuple

import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler


def _read_csv_numeric_rows(path: str) -> List[List[float]]:
    rows: List[List[float]] = []
    with open(path, newline="") as csvfile:
        reader = csv.reader(csvfile)
        next(reader, None)
        for row in reader:
            if not row:
                continue
            if row[-1] == "":
                row = row[:-1]
            rows.append([float(x) for x in row])
    return rows


def makeCSV(inpt: str, features: List[List[float]], labels: List[int]) -> None:
    rows = _read_csv_numeric_rows(inpt)
    for r in rows:
        y = r[-1]
        x = r[:-1]
        if y > 0:
            y_i = 1
        elif y < 0:
            y_i = -1
        else:
            y_i = 0
        labels.append(y_i)
        features.append(x)


def takeInput(inpt: str, pFeatures: List[List[float]]) -> None:
    rows = _read_csv_numeric_rows(inpt)
    pFeatures.extend(rows)


def _build_model() -> MLPClassifier:
    return MLPClassifier(hidden_layer_sizes=(30, 30, 30, 30), max_iter=1000, random_state=0)


def trainAndTestModel(csv_path: str = "14k.csv") -> float:
    features: List[List[float]] = []
    labels: List[int] = []
    makeCSV(csv_path, features, labels)

    X = np.asarray(features, dtype=np.float32)
    y = np.asarray(labels, dtype=np.int8)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0, stratify=y if len(np.unique(y)) > 1 else None
    )

    scaler = StandardScaler(copy=False)
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    mlp = _build_model()
    mlp.fit(X_train, y_train)
    y_pred = mlp.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
    print(f"ACCURACY={accuracy:.6f}")
    return accuracy


def loadModel(path: str = "dict.pickle") -> MLPClassifier:
    with open(path, "rb") as f:
        return pickle.load(f)


def predict(model_path: str = "dict.pickle", input_csv: str = "input.csv") -> np.ndarray:
    mlp = loadModel(model_path)

    pFeatures: List[List[float]] = []
    takeInput(input_csv, pFeatures)

    X = np.asarray(pFeatures, dtype=np.float32)
    scaler = StandardScaler(copy=False)
    X = scaler.fit_transform(X)
    return mlp.predict(X)


class Testing(unittest.TestCase):
    def test_MakeCSV4(self):
        features: List[List[float]] = []
        labels: List[int] = []
        makeCSV("test.csv", features, labels)
        self.assertEqual(len(features), 4)

    def test_MakeCSV2(self):
        features: List[List[float]] = []
        labels: List[int] = []
        makeCSV("test2.csv", features, labels)
        self.assertEqual(len(labels), 2)


if __name__ == "__main__":
    trainAndTestModel()

# OPTIMIZATION SUMMARY
# - Removed unused imports (svm, metrics module wrapper, copy) to reduce load time and memory footprint.
# - Replaced global mutable state with local variables and parameter passing to avoid side effects and repeated allocations.
# - Simplified CSV parsing using csv.reader with default delimiter to avoid extra splitting and per-row string work.
# - Consolidated CSV numeric conversion into a single helper, reducing redundant logic and function overhead.
# - Converted feature/label lists to NumPy arrays once (float32/int8) to reduce memory and speed up preprocessing/training.
# - Used StandardScaler(copy=False) to reduce unnecessary data copies and data movement.
# - Set random_state for reproducibility and stable results without extra runs.
# - Removed model saving, interactive behavior, prints, and plotting; kept only required ACCURACY output.