# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


SEED = 1


def set_reproducible(seed: int = SEED) -> None:
    random.seed(seed)
    np.random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)


def read_csv_robust(path: str, expected_min_cols: int) -> pd.DataFrame:
    df = pd.read_csv(path, header=None)
    if df.shape[1] < expected_min_cols:
        df = pd.read_csv(path, header=None, sep=";", decimal=",")
    if df.shape[1] < expected_min_cols:
        raise ValueError(f"CSV parsing failed or unexpected schema: got {df.shape[1]} columns.")
    return df


def main() -> None:
    set_reproducible(SEED)

    dataset_path = "Sonar Data.csv"
    df = read_csv_robust(dataset_path, expected_min_cols=2)

    label_col = df.columns[-1]
    X = df.drop(columns=[label_col])
    y = df[label_col]

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.7,
        stratify=y,
        random_state=SEED,
    )

    model = LogisticRegression(max_iter=100, random_state=SEED)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused exploratory computations (describe/value_counts/groupby) to avoid redundant full-dataset passes.
# - Eliminated training-set prediction/accuracy computation to reduce extra inference work while preserving evaluation intent (test accuracy).
# - Derived the label column from df.columns[-1] to avoid hardcoding schema and prevent schema-related failures.
# - Implemented robust CSV parsing fallback (default read_csv, then retry with sep=';' and decimal=',') to reduce reruns and parsing errors.
# - Set fixed seeds (Python, NumPy, and PYTHONHASHSEED) and provided random_state to ensure reproducible splits and model behavior.
# - Kept data as pandas DataFrame/Series without extra copies or conversions to minimize memory use and data movement.
# - Added max_iter explicitly to keep solver work bounded and avoid potential non-convergence retries.