# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

RANDOM_SEED = 1

try:
    sonar_data = pd.read_csv('Sonar Data.csv', header=None)
    if sonar_data.shape[1] < 2:
        sonar_data = pd.read_csv('Sonar Data.csv', header=None, sep=';', decimal=',')
except Exception:
    sonar_data = pd.read_csv('Sonar Data.csv', header=None, sep=';', decimal=',')

label_col = sonar_data.columns[-1]
X = sonar_data.iloc[:, :-1]
Y = sonar_data[label_col]

X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.7, stratify=Y, random_state=RANDOM_SEED
)

model = LogisticRegression(max_iter=100, solver='lbfgs', random_state=RANDOM_SEED)
model.fit(X_train, Y_train)

accuracy = accuracy_score(Y_test, model.predict(X_test))
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Removed unused .describe() and .groupby().mean() calls that computed statistics never used downstream.
# Removed redundant training prediction and accuracy computation since only test accuracy is needed.
# Removed all print statements and visualizations per requirements.
# Used iloc for feature/label separation to avoid assuming column name.
# Added robust CSV fallback parsing with sep=';' and decimal=','.
# Set explicit random_state on LogisticRegression for reproducibility.
# Eliminated intermediate variables (X_train_prediction, X_test_prediction) where not needed.
# Reduced memory footprint by not storing unnecessary intermediate arrays.