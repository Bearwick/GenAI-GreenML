# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

np.random.seed(1)

try:
    sonar_data = pd.read_csv('Sonar Data.csv', header=None)
    if sonar_data.shape[1] < 2:
        sonar_data = pd.read_csv('Sonar Data.csv', header=None, sep=';', decimal=',')
except Exception:
    sonar_data = pd.read_csv('Sonar Data.csv', header=None, sep=';', decimal=',')

X = sonar_data.iloc[:, :-1]
Y = sonar_data.iloc[:, -1]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.7, stratify=Y, random_state=1)

model = LogisticRegression(max_iter=100, solver='lbfgs', random_state=1)
model.fit(X_train, Y_train)

accuracy = accuracy_score(Y_test, model.predict(X_test))
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Removed unused .describe() and .groupby().mean() calls that consumed CPU without contributing to output.
# Removed redundant training prediction and accuracy computation (only test accuracy is needed for final output).
# Used iloc for column selection to avoid hardcoding column index 60, making it robust to schema variations.
# Added robust CSV fallback parsing (default then sep=';' with decimal=',').
# Set fixed random seeds for reproducibility.
# Specified solver explicitly to avoid warnings and ensure deterministic behavior.
# Removed all prints, plots, and intermediate variables not needed for the final accuracy output.
# Removed .value_counts() call that had no effect on model training or evaluation.