# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import glob
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier

DATASET_HEADERS = "0.0200,0.0371,0.0428,0.0207,0.0954,0.0986,0.1539,0.1601,0.3109,0.2111,0.1609,0.1582,0.2238,0.0645,0.0660,0.2273,0.3100,0.2999,0.5078,0.4797,0.5783,0.5071,0.4328,0.5550,0.6711,0.6415,0.7104,0.8080,0.6791,0.3857,0.1307,0.2604,0.5121,0.7547,0.8537,0.8507,0.6692,0.6097,0.4943,0.2744,0.0510,0.2834,0.2825,0.4256,0.2641,0.1386,0.1051,0.1343,0.0383,0.0324,0.0232,0.0027,0.0065,0.0159,0.0072,0.0167,0.0180,0.0084,0.0090,0.0032,R"
DATASET_HEADERS_LIST = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip() != ""]

def find_csv_file():
    candidates = []
    for pattern in ["*.csv", "*.CSV"]:
        candidates.extend([f for f in glob.glob(pattern) if os.path.isfile(f)])
    candidates = list(dict.fromkeys(candidates))
    if "Sonar Data.csv" in candidates:
        return "Sonar Data.csv"
    if len(candidates) > 0:
        return candidates[0]
    return None

def looks_like_data_header(cols):
    cols_list = [str(c).strip() for c in cols]
    if len(cols_list) == len(DATASET_HEADERS_LIST) and cols_list == DATASET_HEADERS_LIST:
        return True
    if len(cols_list) <= 1:
        return False
    numeric_like = 0
    for c in cols_list:
        if c.lower().startswith("unnamed"):
            continue
        try:
            float(c.replace(",", "."))
            numeric_like += 1
        except Exception:
            pass
    return numeric_like >= len(cols_list) - 1

def robust_read_csv(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
    if df.shape[1] <= 1:
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df2.shape[1] > 1:
                df = df2
        except Exception:
            pass
    if looks_like_data_header(df.columns):
        try:
            df2 = pd.read_csv(path, header=None)
            if df2.shape[1] > 1:
                df = df2
        except Exception:
            pass
    if df.shape[1] <= 1:
        try:
            df2 = pd.read_csv(path, header=None, sep=";", decimal=",")
            if df2.shape[1] > 1:
                df = df2
        except Exception:
            pass
    return df

def normalize_columns(df):
    new_cols = []
    for c in df.columns:
        s = str(c).strip()
        s = " ".join(s.split())
        new_cols.append(s)
    df.columns = new_cols
    cols_to_keep = [c for c in df.columns if not str(c).lower().startswith("unnamed")]
    df = df[cols_to_keep]
    return df

def select_target_column(df):
    n = len(df)
    threshold = max(20, int(0.1 * n) + 1)
    target = None
    for col in df.columns:
        s = df[col]
        nunique = s.nunique(dropna=True)
        if nunique < 2:
            continue
        numeric = pd.to_numeric(s, errors="coerce")
        frac_numeric = numeric.notna().mean()
        if (s.dtype == object or frac_numeric < 0.5) and nunique <= threshold:
            target = col
    if target is None:
        for col in df.columns:
            s = df[col]
            nunique = s.nunique(dropna=True)
            if nunique < 2:
                continue
            if nunique <= threshold:
                target = col
    if target is None:
        for col in list(df.columns)[::-1]:
            if df[col].nunique(dropna=True) >= 2:
                target = col
                break
    if target is None:
        target = df.columns[-1]
    return target

file_path = find_csv_file()
if file_path is None:
    raise FileNotFoundError("No CSV file found")

df = robust_read_csv(file_path)
df = normalize_columns(df)
df = df.replace([np.inf, -np.inf], np.nan)
df = df.dropna(how="all")
df = df.loc[:, df.notna().any()]

assert df.shape[0] > 0 and df.shape[1] > 0

target_col = select_target_column(df)
y_raw = df[target_col]
X_raw = df.drop(columns=[target_col])

if X_raw.shape[1] == 0:
    X_raw = pd.DataFrame({"dummy": np.zeros(len(df))}, index=df.index)

X_processed = pd.DataFrame(index=X_raw.index)
numeric_cols = []
categorical_cols = []
for col in X_raw.columns:
    series = X_raw[col]
    series = series.replace([np.inf, -np.inf], np.nan)
    numeric_series = pd.to_numeric(series, errors="coerce")
    ratio = numeric_series.notna().mean()
    if ratio >= 0.8:
        X_processed[col] = numeric_series
        numeric_cols.append(col)
    else:
        X_processed[col] = series.astype("string")
        categorical_cols.append(col)

X_processed = X_processed.replace([np.inf, -np.inf], np.nan)
cols_all_nan = [c for c in X_processed.columns if X_processed[c].isna().all()]
if len(cols_all_nan) > 0:
    X_processed = X_processed.drop(columns=cols_all_nan)

numeric_cols = [c for c in numeric_cols if c in X_processed.columns and pd.api.types.is_numeric_dtype(X_processed[c])]
categorical_cols = [c for c in categorical_cols if c in X_processed.columns and not pd.api.types.is_numeric_dtype(X_processed[c])]

if X_processed.shape[1] == 0:
    X_processed = pd.DataFrame({"dummy": np.zeros(len(df))}, index=df.index)
    numeric_cols = ["dummy"]
    categorical_cols = []

n_samples = len(df)
low_unique_threshold = max(20, int(0.1 * n_samples) + 1)
n_unique = y_raw.nunique(dropna=True)
if (y_raw.dtype == object) or (n_unique <= low_unique_threshold):
    problem_type = "classification"
else:
    problem_type = "regression"
if n_unique < 2:
    problem_type = "regression"

if problem_type == "classification":
    y_clean = y_raw.replace([np.inf, -np.inf], np.nan)
    valid_idx = y_clean.notna()
    X_processed = X_processed.loc[valid_idx]
    y_clean = y_clean.loc[valid_idx].astype(str)
else:
    y_num = pd.to_numeric(y_raw, errors="coerce")
    if y_num.notna().sum() == 0:
        y_num = pd.Series(pd.factorize(y_raw.astype(str))[0], index=y_raw.index, dtype=float)
    y_num = y_num.replace([np.inf, -np.inf], np.nan)
    valid_idx = y_num.notna()
    X_processed = X_processed.loc[valid_idx]
    y_clean = y_num.loc[valid_idx]

X_processed = X_processed.replace([np.inf, -np.inf], np.nan)
cols_all_nan = [c for c in X_processed.columns if X_processed[c].isna().all()]
if len(cols_all_nan) > 0:
    X_processed = X_processed.drop(columns=cols_all_nan)

numeric_cols = [c for c in numeric_cols if c in X_processed.columns and pd.api.types.is_numeric_dtype(X_processed[c])]
categorical_cols = [c for c in categorical_cols if c in X_processed.columns and not pd.api.types.is_numeric_dtype(X_processed[c])]

if X_processed.shape[1] == 0:
    X_processed = pd.DataFrame({"dummy": np.zeros(len(X_processed))}, index=X_processed.index)
    numeric_cols = ["dummy"]
    categorical_cols = []

assert len(X_processed) > 0 and len(y_clean) > 0

n_samples = len(X_processed)
if n_samples < 2:
    X_train = X_test = X_processed
    y_train = y_test = y_clean
else:
    test_size = 0.2 if n_samples >= 10 else 0.5
    stratify = None
    if problem_type == "classification":
        class_counts = y_clean.value_counts(dropna=True)
        if len(class_counts) >= 2 and class_counts.min() >= 2:
            stratify = y_clean
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y_clean, test_size=test_size, random_state=42, stratify=stratify
    )

assert len(X_train) > 0 and len(X_test) > 0

transformers = []
if len(numeric_cols) > 0:
    num_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="median")),
                                      ("scaler", StandardScaler())])
    transformers.append(("num", num_transformer, numeric_cols))
if len(categorical_cols) > 0:
    cat_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="most_frequent")),
                                      ("onehot", OneHotEncoder(handle_unknown="ignore"))])
    transformers.append(("cat", cat_transformer, categorical_cols))

if len(transformers) > 0:
    preprocessor = ColumnTransformer(transformers=transformers)
else:
    preprocessor = "passthrough"

if problem_type == "classification":
    n_classes = y_train.nunique(dropna=True)
    if n_classes < 2:
        model = DummyClassifier(strategy="most_frequent")
    else:
        solver = "liblinear" if n_classes <= 2 else "lbfgs"
        model = LogisticRegression(max_iter=200, solver=solver)
else:
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if problem_type == "classification":
    accuracy = accuracy_score(y_test, y_pred)
else:
    if len(y_test) > 1 and len(np.unique(y_test)) > 1:
        r2 = r2_score(y_test, y_pred)
    else:
        r2 = 0.0
    accuracy = (r2 + 1.0) / 2.0
    accuracy = max(0.0, min(1.0, accuracy))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Lightweight linear models (LogisticRegression/Ridge) are used for CPU efficiency and small-scale suitability.
# - Preprocessing uses minimal imputation, scaling, and one-hot encoding within a pipeline for reproducibility with low overhead.
# - Robust schema/target inference and safe fallbacks avoid failures without adding complex computations.
# - Regression performance is mapped to a bounded (r2+1)/2 accuracy proxy to keep reporting stable in [0,1].