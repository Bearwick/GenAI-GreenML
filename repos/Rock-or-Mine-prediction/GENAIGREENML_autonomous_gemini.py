# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline

def load_data(path):
    # Robust parsing logic for different CSV formats
    try:
        df = pd.read_csv(path, header=None)
    except Exception:
        try:
            df = pd.read_csv(path, sep=';', decimal=',', header=None)
        except Exception:
            return pd.DataFrame()
    
    # Normalize column names
    df.columns = [str(col).strip() for col in df.columns]
    # Remove unnamed columns if any
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def run_pipeline():
    filepath = 'Sonar Data.csv'
    df = load_data(filepath)
    
    if df.empty:
        # Fallback if file not found or empty to prevent crash
        print("ACCURACY=0.000000")
        return

    # Identify Target: Sonar dataset usually has label in the last column
    target_col = df.columns[-1]
    
    # Prepare features and target
    X = df.iloc[:, :-1].copy()
    y = df[target_col].copy()
    
    # Preprocessing: Ensure numeric features
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    
    # Drop rows with NaN in target or all NaN in features
    df_clean = pd.concat([X, y], axis=1).dropna(subset=[target_col])
    if df_clean.empty:
        print("ACCURACY=0.000000")
        return
        
    X = df_clean.iloc[:, :-1].fillna(0)
    y = df_clean.iloc[:, -1]
    
    # Encode label if categorical
    if y.dtype == 'object' or y.dtype.name == 'category':
        le = LabelEncoder()
        y = le.fit_transform(y.astype(str))
    
    # Verify we have at least two classes for classification
    unique_classes = np.unique(y)
    if len(unique_classes) < 2:
        print("ACCURACY=1.000000") # Trivial case
        return

    # Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Pipeline: Logistic Regression is energy-efficient and effective for this scale
    # Using 'liblinear' solver as it is lightweight for small datasets
    model_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(
            penalty='l2', 
            C=1.0, 
            solver='liblinear', 
            random_state=42,
            max_iter=1000
        ))
    ])
    
    # Fit model
    model_pipeline.fit(X_train, y_train)
    
    # Predict and evaluate
    y_pred = model_pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary:
# 1. Used Logistic Regression: High energy efficiency, low CPU cycles compared to ensembles or deep learning.
# 2. Implemented StandardScaler: Essential for linear model convergence, minimizing training iterations.
# 3. Robust CSV Loading: Implemented fallback separators and decimal handling to ensure end-to-end execution.
# 4. CPU-Friendly: Relies on scikit-learn's optimized C-extensions via liblinear, requiring minimal RAM and no GPU.
# 5. Pipeline Architecture: Used sklearn.pipeline for clean, reproducible, and efficient data flow.
# 6. Defensive Programming: Handled missing files, single-class targets, and NaN values to prevent runtime failure.
# 7. Low Complexity: Minimal feature engineering to keep the carbon footprint of the training phase low.