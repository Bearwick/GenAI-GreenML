# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

possible_files = ["Iris.csv", "iris.csv", "data.csv", "dataset.csv", "train.csv"]
data_path = None
for f in possible_files:
    if os.path.isfile(f):
        data_path = f
        break
if data_path is None:
    for f in os.listdir("."):
        if f.lower().endswith(".csv"):
            data_path = f
            break
if data_path is None:
    raise FileNotFoundError("No CSV dataset found.")

df = pd.read_csv(data_path)
df = df.dropna()

if "Species" not in df.columns:
    raise ValueError("Target column 'Species' not found in dataset.")

X = df.drop(columns=["Species"])
if "Id" in X.columns:
    X = X.drop(columns=["Id"])
X = X.select_dtypes(include=["number"])

y = df["Species"]
le = LabelEncoder()
y_encoded = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

model = GaussianNB()
model.fit(X_train, y_train)
preds = model.predict(X_test)
accuracy = accuracy_score(y_test, preds)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Use GaussianNB to avoid iterative optimization and keep CPU usage low.
# Minimal preprocessing (drop Id and missing rows) for fast, reproducible pipeline.
# Small train/test split and label encoding ensure lightweight, deterministic workflow.