# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

path = "Iris.csv"
try:
    df = pd.read_csv(path)
    if df.shape[1] == 1:
        df = pd.read_csv(path, sep=';', decimal=',')
except Exception:
    df = pd.read_csv(path, sep=';', decimal=',')

df.columns = [re.sub(r'\s+', ' ', str(c)).strip() for c in df.columns]
df = df.loc[:, ~df.columns.str.contains('^Unnamed', case=False, regex=True)]
df = df.dropna(axis=1, how='all')
assert df.shape[0] > 0 and df.shape[1] > 0

preferred_targets = ['species', 'target', 'label', 'class']
dataset_headers = [h.strip() for h in "Id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species".split(',')]
target_col = None
for c in df.columns:
    if c.lower() in preferred_targets:
        target_col = c
        break
if target_col is None:
    for h in reversed(dataset_headers):
        if h in df.columns:
            target_col = h
            break
if target_col is None:
    obj_cols = [c for c in df.columns if df[c].dtype == object]
    if obj_cols:
        nunique = {c: df[c].nunique(dropna=True) for c in obj_cols}
        cand = [c for c in obj_cols if nunique[c] > 1]
        target_col = min(cand, key=lambda c: nunique[c]) if cand else obj_cols[0]
    else:
        num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
        cand = [c for c in num_cols if df[c].nunique(dropna=True) > 1]
        target_col = cand[0] if cand else (num_cols[0] if num_cols else df.columns[-1])

y = df[target_col]
mask = y.notna()
df = df.loc[mask].copy()
y = df[target_col]
X = df.drop(columns=[target_col])

id_like = [c for c in X.columns if c.lower() in ['id', 'index'] or c.lower().endswith('id')]
if id_like and len(id_like) < X.shape[1]:
    X = X.drop(columns=id_like)

if X.shape[1] == 0:
    X = pd.DataFrame({'constant': np.ones(len(df))})

is_classification = False
if y.dtype == object or str(y.dtype).startswith('category'):
    y_numeric_check = pd.to_numeric(y, errors='coerce')
    if y_numeric_check.notna().mean() > 0.8:
        y = y_numeric_check
        is_classification = False
    else:
        is_classification = True
else:
    nunique = y.nunique(dropna=True)
    if nunique <= max(20, int(0.05 * len(y)) + 1):
        is_classification = True
    else:
        is_classification = False

if not is_classification:
    y = pd.to_numeric(y, errors='coerce')
    mask = y.notna()
    X = X.loc[mask]
    y = y.loc[mask]

if is_classification and y.nunique(dropna=True) < 2:
    is_classification = False
    y = pd.to_numeric(y, errors='coerce')
    mask = y.notna()
    X = X.loc[mask]
    y = y.loc[mask]

assert len(X) > 0

numeric_cols = []
categorical_cols = []
for c in X.columns:
    series = X[c]
    if pd.api.types.is_numeric_dtype(series):
        X[c] = pd.to_numeric(series, errors='coerce')
        numeric_cols.append(c)
    else:
        numeric_series = pd.to_numeric(series, errors='coerce')
        if numeric_series.notna().mean() > 0.8:
            X[c] = numeric_series
            numeric_cols.append(c)
        else:
            temp = series.astype(str)
            temp[series.isna()] = np.nan
            X[c] = temp
            categorical_cols.append(c)

for c in numeric_cols:
    X[c] = X[c].replace([np.inf, -np.inf], np.nan)

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_cols),
        ('cat', categorical_transformer, categorical_cols)
    ],
    remainder='drop'
)

stratify = None
if is_classification and y.nunique() > 1:
    counts = y.value_counts()
    if (counts >= 2).all() and len(y) >= 5:
        stratify = y

try:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=stratify
    )
except Exception:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.5, random_state=42, stratify=None
    )

assert len(X_train) > 0 and len(X_test) > 0

if is_classification:
    model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    model = LinearRegression()

pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])
try:
    pipeline.fit(X_train, y_train)
except Exception:
    if is_classification:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = DummyRegressor(strategy='mean')
    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])
    pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

if is_classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    if np.isnan(r2):
        accuracy = 0.0
    else:
        accuracy = max(0.0, min(1.0, r2))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# - Used lightweight linear models with simple preprocessing to keep computation CPU-friendly.
# - Implemented robust schema-agnostic target selection and numeric coercion to avoid manual fixes.
# - Employed ColumnTransformer with minimal imputation/encoding for reproducible, efficient preprocessing.
# - Regression fallback reports a clipped R2 score in [0,1] as a stable accuracy proxy.