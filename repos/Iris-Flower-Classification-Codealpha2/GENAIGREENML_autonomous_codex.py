# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os, glob, re
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score

paths = []
env_path = os.getenv("DATASET_PATH") or os.getenv("CSV_PATH") or os.getenv("DATA_PATH")
if env_path and os.path.isfile(env_path):
    paths.append(env_path)

search_dirs = ["."]
if os.path.isdir("/mnt/data"):
    search_dirs.append("/mnt/data")
if os.path.isdir("/data"):
    search_dirs.append("/data")

for d in search_dirs:
    paths.extend([p for p in glob.glob(os.path.join(d, "*.csv")) if os.path.isfile(p)])
    paths.extend([p for p in glob.glob(os.path.join(d, "*.CSV")) if os.path.isfile(p)])

if not paths:
    raise FileNotFoundError("No CSV file found")

selected_path = None
for cand in ["Iris.csv", "iris.csv"]:
    for p in paths:
        if os.path.basename(p) == cand:
            selected_path = p
            break
    if selected_path is not None:
        break
if selected_path is None:
    selected_path = sorted(paths)[0]

def read_csv_flexible(path):
    try:
        df_try = pd.read_csv(path)
        if df_try.shape[1] <= 1:
            df_alt = pd.read_csv(path, sep=";", decimal=",")
            if df_alt.shape[1] > df_try.shape[1]:
                return df_alt
        return df_try
    except Exception:
        return pd.read_csv(path, sep=";", decimal=",")

df = read_csv_flexible(selected_path)

df.columns = [re.sub(r"\s+", " ", str(c).strip()) for c in df.columns]
df = df.loc[:, [c for c in df.columns if not str(c).lower().startswith("unnamed")]]
if df.columns.duplicated().any():
    new_cols = []
    counts = {}
    for c in df.columns:
        if c in counts:
            counts[c] += 1
            new_cols.append(f"{c}_{counts[c]}")
        else:
            counts[c] = 0
            new_cols.append(c)
    df.columns = new_cols

assert df.shape[0] > 0 and df.shape[1] > 0

def is_mostly_numeric(series):
    numeric = pd.to_numeric(series, errors="coerce")
    return numeric.notna().mean() >= 0.8

target_col = None
preferred_names = ["target", "label", "class", "species", "y"]
lower_map = {str(c).lower(): c for c in df.columns}
for name in preferred_names:
    if name in lower_map:
        target_col = lower_map[name]
        break

if target_col is None:
    numeric_candidates = [c for c in df.columns if is_mostly_numeric(df[c])]
    numeric_non_constant = [c for c in numeric_candidates if df[c].nunique(dropna=True) > 1]
    if numeric_non_constant:
        variances = []
        for c in numeric_non_constant:
            num = pd.to_numeric(df[c], errors="coerce")
            var = num.var()
            variances.append(var if not np.isnan(var) else -np.inf)
        if np.max(variances) == -np.inf:
            target_col = numeric_non_constant[0]
        else:
            target_col = numeric_non_constant[int(np.argmax(variances))]
    elif numeric_candidates:
        target_col = numeric_candidates[-1]
    else:
        non_constant = [c for c in df.columns if df[c].nunique(dropna=True) > 1]
        if non_constant:
            target_col = non_constant[-1]
        else:
            target_col = df.columns[-1]

df = df.replace([np.inf, -np.inf], np.nan)
df = df.dropna(subset=[target_col]).reset_index(drop=True)

id_like_cols = []
for c in df.columns:
    if c == target_col:
        continue
    c_lower = str(c).lower()
    if df[c].nunique(dropna=True) == len(df) and (c_lower == "id" or c_lower.endswith("id") or "index" in c_lower):
        id_like_cols.append(c)
if id_like_cols:
    df = df.drop(columns=id_like_cols)

feature_cols_all = [c for c in df.columns if c != target_col]
all_nan_cols = [c for c in feature_cols_all if df[c].isna().all()]
if all_nan_cols:
    df = df.drop(columns=all_nan_cols)
feature_cols_all = [c for c in df.columns if c != target_col]

if len(feature_cols_all) == 0:
    df["__dummy__"] = 0
    feature_cols_all = ["__dummy__"]

X = df[feature_cols_all].copy()
y_raw = df[target_col].copy()

numeric_features = []
categorical_features = []
for col in feature_cols_all:
    series = X[col]
    numeric_series = pd.to_numeric(series, errors="coerce")
    if numeric_series.notna().mean() >= 0.8:
        X[col] = numeric_series
        numeric_features.append(col)
    else:
        series_obj = series.astype(str)
        series_obj = series_obj.where(series.notna(), np.nan)
        X[col] = series_obj
        categorical_features.append(col)

y_numeric = pd.to_numeric(y_raw, errors="coerce")
numeric_ratio = y_numeric.notna().mean()

if numeric_ratio >= 0.9:
    y_num = y_numeric
    unique_vals = y_num.nunique(dropna=True)
    if unique_vals <= max(20, int(0.05 * len(y_num)) + 1):
        task = "classification"
        y = y_num
    else:
        task = "regression"
        y = y_num
else:
    task = "classification"
    y = y_raw.astype(str)

if task == "classification" and y.nunique(dropna=True) < 2:
    task = "regression"
    y = pd.to_numeric(y_raw, errors="coerce").fillna(0)

if task == "classification":
    if isinstance(y, pd.Series) and y.isna().any():
        mask = y.notna()
        X = X.loc[mask].reset_index(drop=True)
        y = y.loc[mask].reset_index(drop=True)

if task == "regression":
    y = pd.to_numeric(y, errors="coerce")
    mask = y.notna()
    X = X.loc[mask].reset_index(drop=True)
    y = y.loc[mask].reset_index(drop=True)

assert len(X) > 0

if len(X) < 2:
    X = pd.concat([X, X], ignore_index=True)
    y = pd.concat([y, y], ignore_index=True)

stratify = None
if task == "classification":
    try:
        if y.nunique(dropna=True) >= 2 and y.value_counts().min() >= 2:
            stratify = y
    except Exception:
        stratify = None

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False))
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

transformers = []
if len(numeric_features) > 0:
    transformers.append(("num", numeric_transformer, numeric_features))
if len(categorical_features) > 0:
    transformers.append(("cat", categorical_transformer, categorical_features))

if len(transformers) == 0:
    preprocessor = "passthrough"
else:
    preprocessor = ColumnTransformer(transformers=transformers, remainder="drop")

if task == "classification":
    if y_train.nunique(dropna=True) < 2:
        majority_class = y_train.iloc[0]
        y_pred = np.full(len(y_test), majority_class)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        model = LogisticRegression(max_iter=200, solver="lbfgs", multi_class="auto")
        clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
else:
    model = Ridge(alpha=1.0)
    reg = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
    reg.fit(X_train, y_train)
    y_pred = reg.predict(X_test)
    if len(y_test) < 2:
        mae = float(np.mean(np.abs(y_test - y_pred)))
        accuracy = 1.0 / (1.0 + mae)
    else:
        try:
            r2 = r2_score(y_test, y_pred)
            if np.isnan(r2):
                accuracy = 0.0
            else:
                accuracy = (r2 + 1.0) / 2.0
                if accuracy < 0.0:
                    accuracy = 0.0
                if accuracy > 1.0:
                    accuracy = 1.0
        except Exception:
            mae = float(np.mean(np.abs(y_test - y_pred)))
            accuracy = 1.0 / (1.0 + mae)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Lightweight linear models (LogisticRegression/Ridge) were selected for CPU efficiency and small-scale tasks.
# - A reproducible ColumnTransformer pipeline handles imputation, scaling, and one-hot encoding with minimal overhead.
# - Schema-agnostic target selection and numeric coercion improve robustness on unknown data layouts.
# - Regression accuracy maps R2 into [0,1] with MAE fallback for stability on tiny test splits.