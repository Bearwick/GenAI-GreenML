# Generated by generate_llm_code.py
# LLM: groq
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score

def load_and_prepare(path: str):
    df = pd.read_csv(path)
    if "Id" in df.columns:
        df.drop(columns=["Id"], inplace=True)
    df["Species"] = LabelEncoder().fit_transform(df["Species"])
    X = df.drop("Species", axis=1)
    y = df["Species"]
    return X, y

def build_pipeline():
    return Pipeline([
        ("scaler", StandardScaler()),
        ("model", LinearSVC(max_iter=2000, random_state=42))
    ])

def main():
    X, y = load_and_prepare("Iris.csv")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    pipeline = build_pipeline()
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimizations applied:
# 1. Removed plotting libraries and all visualization to save memory and CPU time.
# 2. Replaced SVC with LinearSVC, a lightweight linear SVM implementation, reducing training overhead.
# 3. Eliminated cross-validation step to avoid extra model fittings; only train/test split used.
# 4. Dropped unnecessary imports (matplotlib, seaborn, cross_val_score) to lower memory footprint.
# 5. Simplified data preprocessing: used inplace drop and direct label encoding without additional steps.
# 6. Encapsulated logic into functions for clearer structure and easier reuse.
# 7. Only the final accuracy is printed, minimizing console I/O.