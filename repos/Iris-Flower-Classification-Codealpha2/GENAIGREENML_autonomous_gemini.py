# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer

def run_pipeline(file_path):
    # 1. Robust CSV Loading
    try:
        df = pd.read_csv(file_path)
    except Exception:
        # Fallback for alternative delimiters/decimals often found in European datasets
        df = pd.read_csv(file_path, sep=';', decimal=',')

    if df.empty:
        return 0.0

    # 2. Schema Normalization
    # Strip whitespace and collapse internal whitespace
    df.columns = [" ".join(str(c).strip().split()) for c in df.columns]
    # Drop "Unnamed" columns
    df = df.loc[:, ~df.columns.str.contains('^Unnamed', case=False)]

    # 3. Target and Feature Identification
    # Heuristic: find columns common in classification targets or take the last one
    target_col = None
    potential_targets = ['Species', 'target', 'class', 'label', 'variety', 'type']
    for pt in potential_targets:
        matches = [c for c in df.columns if c.lower() == pt.lower()]
        if matches:
            target_col = matches[0]
            break
    
    if not target_col:
        target_col = df.columns[-1]

    # Exclude ID-like columns from features
    id_indicators = ['id', 'index', 'uuid', 'no.']
    feature_cols = []
    for c in df.columns:
        if c == target_col:
            continue
        is_id = any(ind in c.lower() for ind in id_indicators)
        if not is_id:
            feature_cols.append(c)

    # Ensure we have features; if none, take all except target
    if not feature_cols:
        feature_cols = [c for c in df.columns if c != target_col]

    X = df[feature_cols].copy()
    y = df[target_col].copy()

    # 4. Data Cleaning & Type Coercion
    # Convert features to numeric, invalid to NaN
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')

    # Handle NaNs and Infinite values safely
    X = X.replace([np.inf, -np.inf], np.nan)
    
    # Check if we have any valid data left
    if X.empty:
        return 0.0

    # 5. Modeling Decision: Classification vs Regression
    # If target is numeric and has many unique values, treat as regression
    is_regression = False
    if pd.api.types.is_numeric_dtype(y) and y.nunique() > 10:
        is_regression = True

    # 6. Preprocessing Pipeline
    # Impute missing numeric values with mean (low overhead)
    imputer = SimpleImputer(strategy='mean')
    X_imputed = imputer.fit_transform(X)

    # Standardization for linear models
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_imputed)

    if is_regression:
        from sklearn.linear_model import Ridge
        y_proc = y.fillna(y.mean())
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_proc, test_size=0.2, random_state=42)
        
        # Simple Ridge regression (low CPU complexity)
        model = Ridge()
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        
        # Calculate R^2 as a proxy for accuracy in [0, 1] range
        from sklearn.metrics import r2_score
        score = r2_score(y_test, preds)
        accuracy = max(0, min(score, 1)) # Bound to [0,1]
    else:
        # Encode Target for classification
        le = LabelEncoder()
        y_encoded = le.fit_transform(y.astype(str))
        
        # Check class count
        if len(np.unique(y_encoded)) < 2:
            # Trivial case: single class
            accuracy = 1.0
        else:
            X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)
            
            # Lightweight Logistic Regression (CPU friendly, small memory footprint)
            # multi_class='auto' handles Iris multinomial case efficiently
            model = LogisticRegression(max_iter=500, solver='lbfgs', random_state=42)
            model.fit(X_train, y_train)
            
            preds = model.predict(X_test)
            accuracy = accuracy_score(y_test, preds)

    return accuracy

if __name__ == "__main__":
    try:
        acc = run_pipeline("Iris.csv")
        print(f"ACCURACY={acc:.6f}")
    except Exception:
        # Fallback to ensure the script runs end-to-end as per requirement
        print(f"ACCURACY={0.000000:.6f}")

# OPTIMIZATION SUMMARY
# 1. Model Choice: Logistic Regression was selected as it is computationally efficient (O(n_samples * n_features)), 
#    providing a high-performance-to-energy ratio for small-scale datasets like Iris.
# 2. CPU Efficiency: Used 'lbfgs' solver which is memory-efficient and converges quickly on small-medium datasets without requiring GPU.
# 3. Energy-Efficient Preprocessing: Utilized a simple Mean Imputer and StandardScaler within a linear workflow, 
#    avoiding complex feature engineering or high-dimensional mappings.
# 4. Robustness: Implemented a schema-agnostic column cleaner and automated target identification to prevent 
#    manual intervention or hard-coded failures.
# 5. Fallback Logic: Included a regression path with R^2 score mapping for unexpected data types, ensuring the 
#    "ACCURACY=" output requirement is always met.
# 6. Resource Management: Avoided deep learning and large ensemble methods (like XGBoost/RandomForest) to minimize 
#    the thermal footprint and training time on CPU.