# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        if c2.lower().startswith("unnamed:"):
            out.append(None)
        else:
            out.append(c2)
    return out


def _read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d):
        if d is None or d.empty:
            return True
        if d.shape[1] <= 1:
            return True
        # Heuristic: if most cells in first column contain separator chars, parsing likely wrong
        try:
            s = d.iloc[:, 0].astype(str)
            frac_sep = (s.str.contains(";").mean() + s.str.contains(",").mean()) / 2.0
            return frac_sep > 0.5
        except Exception:
            return False

    if _looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df = pd.read_csv(path)

    df.columns = _normalize_columns(df.columns)
    df = df.loc[:, [c is not None for c in df.columns]]
    df.columns = [c for c in df.columns if c is not None]
    return df


def _choose_target(df, preferred_headers=None):
    cols = list(df.columns)
    preferred_headers = preferred_headers or []

    # Prefer provided headers if available
    for cand in preferred_headers:
        if cand in cols:
            return cand

    # Prefer common target names
    common_targets = ["target", "label", "y", "class", "species"]
    for cand in cols:
        if str(cand).strip().lower() in common_targets:
            return cand

    # Prefer an object/categorical column with >1 unique values
    for cand in cols:
        s = df[cand]
        if s.dtype == "object" or pd.api.types.is_string_dtype(s) or pd.api.types.is_categorical_dtype(s):
            nun = s.astype(str).nunique(dropna=True)
            if nun >= 2 and nun <= max(2, min(50, len(df) // 2 + 1)):
                return cand

    # Otherwise choose a numeric non-constant column
    numeric_scores = []
    for cand in cols:
        s = pd.to_numeric(df[cand], errors="coerce")
        nun = s.nunique(dropna=True)
        if nun >= 2:
            numeric_scores.append((nun, cand))
    if numeric_scores:
        numeric_scores.sort(reverse=True)
        return numeric_scores[0][1]

    # Fallback: last column
    return cols[-1]


def _build_preprocessor(X):
    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )
    return preprocessor, numeric_features, categorical_features


def _safe_accuracy_proxy_r2(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    y_true = np.where(np.isfinite(y_true), y_true, np.nan)
    y_pred = np.where(np.isfinite(y_pred), y_pred, np.nan)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    denom = np.sum((yt - np.mean(yt)) ** 2)
    if denom <= 0:
        return 0.0
    r2 = 1.0 - (np.sum((yt - yp) ** 2) / denom)
    # Bound to [0,1] for a stable "accuracy" proxy
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    dataset_path = "Iris.csv"
    df = _read_csv_robust(dataset_path)

    # Normalize/clean obvious empty columns and whitespace-only strings
    assert df is not None and not df.empty, "Dataset is empty or could not be read."
    for c in df.columns:
        if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_string_dtype(df[c]):
            df[c] = df[c].astype(str).str.strip().replace({"": np.nan, "nan": np.nan, "NaN": np.nan})

    # Choose target defensively
    data_headers = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]
    target_col = _choose_target(df, preferred_headers=["Species"] + data_headers)

    # Drop obvious ID-like columns from features if present
    id_like = set([c for c in df.columns if str(c).strip().lower() in {"id", "index"}])
    # Also drop target
    feature_cols = [c for c in df.columns if c != target_col and c not in id_like]

    # If no features left, keep at least one non-target column
    if not feature_cols:
        feature_cols = [c for c in df.columns if c != target_col][:1]

    X = df[feature_cols].copy()
    y_raw = df[target_col].copy()

    # Coerce numeric columns safely
    for c in X.columns:
        if not (pd.api.types.is_object_dtype(X[c]) or pd.api.types.is_string_dtype(X[c])):
            X[c] = pd.to_numeric(X[c], errors="coerce")
        else:
            # Try numeric coercion if it looks like numeric strings
            coerced = pd.to_numeric(X[c], errors="coerce")
            if coerced.notna().mean() > 0.8:
                X[c] = coerced

    # Decide classification vs regression
    y_obj_like = (y_raw.dtype == "object") or pd.api.types.is_string_dtype(y_raw) or pd.api.types.is_categorical_dtype(y_raw)
    if y_obj_like:
        y = y_raw.astype(str)
        n_classes = y.nunique(dropna=True)
        task = "classification" if n_classes >= 2 else "degenerate"
    else:
        y_num = pd.to_numeric(y_raw, errors="coerce")
        n_unique = y_num.nunique(dropna=True)
        task = "regression" if n_unique >= 2 else "degenerate"
        y = y_num

    # Drop rows with missing target
    valid_mask = pd.notna(y)
    X = X.loc[valid_mask].copy()
    y = y.loc[valid_mask].copy()
    assert len(X) > 0, "No valid rows after dropping missing targets."

    preprocessor, _, _ = _build_preprocessor(X)

    random_state = 42

    if task == "classification":
        # Stratify when possible and safe
        strat = y if y.nunique(dropna=True) >= 2 and (y.value_counts().min() >= 2) else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=random_state, stratify=strat
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

        clf = LogisticRegression(
            max_iter=200,
            solver="lbfgs",
            n_jobs=1,
        )

        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))

    elif task == "regression":
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=random_state
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

        reg = Ridge(alpha=1.0, random_state=random_state)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _safe_accuracy_proxy_r2(y_test, y_pred)

    else:
        # Degenerate: constant or single-class target; use trivial baseline and score perfectly if predictable
        # (Still keep pipeline structure minimal and CPU-friendly.)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=random_state
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."
        if y_train.dtype == "object" or pd.api.types.is_string_dtype(y_train):
            pred = y_train.dropna().iloc[0] if len(y_train.dropna()) else None
            y_pred = np.array([pred] * len(y_test), dtype=object)
            accuracy = float(accuracy_score(y_test, y_pred)) if pred is not None else 0.0
        else:
            pred = float(pd.to_numeric(y_train, errors="coerce").dropna().iloc[0]) if len(pd.to_numeric(y_train, errors="coerce").dropna()) else 0.0
            y_pred = np.full(shape=(len(y_test),), fill_value=pred, dtype=float)
            accuracy = _safe_accuracy_proxy_r2(pd.to_numeric(y_test, errors="coerce"), y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) for CPU-friendly training/inference; avoids ensembles/deep learning.
# - ColumnTransformer+Pipeline ensures single-pass, reproducible preprocessing and prevents repeated transformations.
# - Robust CSV parsing fallback (default then sep=';' and decimal=',') reduces failures across locales without extra dependencies.
# - Defensive schema handling: normalizes headers, drops 'Unnamed:*', selects target/feature columns dynamically, and removes ID-like columns.
# - Safe numeric coercion and SimpleImputer prevent crashes from mixed dtypes/NaNs while keeping computations minimal.
# - If regression fallback is triggered, reports a bounded accuracy proxy in [0,1] computed from R²: accuracy = clip((R²+1)/2,0,1).