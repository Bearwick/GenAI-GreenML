# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression


def _find_dataset_path() -> str:
    candidates = [
        "Iris.csv",
        "iris.csv",
        "data.csv",
        "dataset.csv",
        "train.csv",
        "./Iris.csv",
        "./iris.csv",
        "./data.csv",
        "./dataset.csv",
        "./train.csv",
    ]
    for p in candidates:
        if os.path.exists(p) and os.path.isfile(p):
            return p
    for fname in os.listdir("."):
        if fname.lower().endswith(".csv"):
            return fname
    raise FileNotFoundError("No CSV dataset found in the current directory.")


def main() -> None:
    path = _find_dataset_path()
    df = pd.read_csv(path)

    expected_cols = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]
    missing = [c for c in expected_cols if c not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns: {missing}")

    X = df[["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm"]]
    y = df["Species"]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    numeric_features = X.columns.tolist()

    preprocess = ColumnTransformer(
        transformers=[
            (
                "num",
                Pipeline(
                    steps=[
                        ("imputer", SimpleImputer(strategy="median")),
                        ("scaler", StandardScaler(with_mean=True, with_std=True)),
                    ]
                ),
                numeric_features,
            )
        ],
        remainder="drop",
        n_jobs=1,
    )

    model = LogisticRegression(
        solver="lbfgs",
        max_iter=200,
        n_jobs=1,
        multi_class="auto",
    )

    clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])

    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Chosen model: Multinomial Logistic Regression for a small tabular dataset; fast to train/infer on CPU and avoids deep learning.
# - Preprocessing is a single sklearn Pipeline/ColumnTransformer for reproducibility and to prevent data leakage.
# - Median imputation is lightweight and robust; StandardScaler helps the optimizer converge with fewer iterations.
# - Fixed random_state for deterministic splits and repeatable results.
# - n_jobs=1 avoids excessive parallelism/overhead and improves energy efficiency on small workloads.
# - No plotting, interactive input, model serialization, or extraneous logging to keep runtime minimal.