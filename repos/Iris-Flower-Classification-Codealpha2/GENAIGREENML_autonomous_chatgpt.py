# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge

warnings.filterwarnings("ignore")


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _read_csv_robust(path):
    # Try default parsing first
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    # Heuristic: retry if single-column or suspicious parsing
    def _looks_wrong(d):
        if d is None or d.shape[0] == 0:
            return True
        if d.shape[1] <= 1:
            return True
        # If most column names contain ';', likely wrong separator
        colnames = [str(c) for c in d.columns]
        if len(colnames) > 0 and sum((";" in c) for c in colnames) / max(1, len(colnames)) > 0.5:
            return True
        return False

    if _looks_wrong(df):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if not _looks_wrong(df2):
                df = df2
        except Exception:
            pass

    if df is None:
        raise RuntimeError("Failed to read dataset.")
    return df


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _choose_target(df, preferred_headers=None):
    cols = list(df.columns)

    # Prefer common target names if present
    preferred = []
    if preferred_headers:
        preferred.extend(preferred_headers)
    preferred.extend(["target", "label", "y", "class", "species", "output"])

    lower_map = {str(c).strip().lower(): c for c in cols}
    for p in preferred:
        if p in lower_map:
            return lower_map[p]

    # Otherwise choose a non-constant categorical/object-like column
    for c in cols:
        s = df[c]
        nun = s.nunique(dropna=True)
        if nun >= 2 and (s.dtype == "object" or str(s.dtype).startswith("category") or nun <= 20):
            return c

    # Otherwise choose a non-constant numeric column (regression)
    numeric_candidates = []
    for c in cols:
        s_num = pd.to_numeric(df[c], errors="coerce")
        nun = s_num.nunique(dropna=True)
        if nun >= 2:
            numeric_candidates.append((c, nun))
    if numeric_candidates:
        numeric_candidates.sort(key=lambda x: x[1], reverse=True)
        return numeric_candidates[0][0]

    # Fallback: last column
    return cols[-1] if cols else None


def _is_classification_target(y):
    if y is None or len(y) == 0:
        return False
    if y.dtype == "object" or str(y.dtype).startswith("category"):
        return y.nunique(dropna=True) >= 2
    # Numeric target: treat as classification if small number of unique values
    y_num = pd.to_numeric(y, errors="coerce")
    nun = y_num.nunique(dropna=True)
    if nun < 2:
        return False
    return nun <= 20


def _bounded_regression_score(y_true, y_pred):
    # Stable "accuracy" proxy in [0,1]: 1 / (1 + NRMSE), NRMSE normalized by target std or range.
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    mse = np.mean((yt - yp) ** 2)
    rmse = float(np.sqrt(mse))
    denom = float(np.nanstd(yt))
    if not np.isfinite(denom) or denom == 0.0:
        denom = float(np.nanmax(yt) - np.nanmin(yt))
    if not np.isfinite(denom) or denom == 0.0:
        denom = 1.0
    nrmse = rmse / denom
    score = 1.0 / (1.0 + nrmse)
    score = float(np.clip(score, 0.0, 1.0))
    return score


def main():
    # Locate dataset file
    candidates = [
        "Iris.csv",
        "iris.csv",
        "data.csv",
        "dataset.csv",
        "train.csv",
        "Iris Flower.csv",
    ]
    data_path = None
    for p in candidates:
        if os.path.exists(p):
            data_path = p
            break
    if data_path is None:
        # As a last resort, pick first CSV in current directory
        for fn in os.listdir("."):
            if fn.lower().endswith(".csv"):
                data_path = fn
                break
    if data_path is None:
        raise FileNotFoundError("No CSV dataset found in the current directory.")

    df = _read_csv_robust(data_path)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Drop empty columns
    df = df.dropna(axis=1, how="all")

    # If Id column exists (common in Iris), drop it as it's usually a non-predictive index
    for col in list(df.columns):
        if str(col).strip().lower() in ("id",):
            df = df.drop(columns=[col])

    assert df.shape[0] > 0 and df.shape[1] > 0, "Dataset is empty after basic cleaning."

    # Choose target robustly; prefer provided headers
    dataset_headers = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]
    target_col = _choose_target(df, preferred_headers=["Species", "species", "Label", "label", "Target", "target"])
    if target_col is None or target_col not in df.columns:
        target_col = df.columns[-1]

    y_raw = df[target_col]
    X = df.drop(columns=[target_col])

    # Identify column types using coercion
    numeric_cols = []
    categorical_cols = []
    for c in X.columns:
        s = X[c]
        s_num = pd.to_numeric(s, errors="coerce")
        # If most values can be parsed as numeric, treat as numeric
        non_na = s.notna().sum()
        num_non_na = np.isfinite(s_num.to_numpy(dtype=float, na_value=np.nan)).sum()
        ratio = (num_non_na / non_na) if non_na > 0 else 0.0
        if ratio >= 0.8:
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)

    # Build preprocessing
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Decide classification vs regression and prepare y accordingly
    is_clf = _is_classification_target(y_raw)

    if is_clf:
        # Keep y as categorical codes deterministically
        if y_raw.dtype == "object" or str(y_raw.dtype).startswith("category"):
            y = y_raw.astype("category").cat.codes
        else:
            y = pd.to_numeric(y_raw, errors="coerce")
        # Drop rows with missing target
        mask = np.isfinite(np.asarray(y, dtype=float))
        X = X.loc[mask].reset_index(drop=True)
        y = np.asarray(y, dtype=int)[mask]

        # If after cleaning we don't have at least 2 classes, fallback to regression
        if pd.Series(y).nunique(dropna=True) < 2:
            is_clf = False

    if not is_clf:
        y = pd.to_numeric(y_raw, errors="coerce")
        mask = np.isfinite(y.to_numpy(dtype=float, na_value=np.nan))
        X = X.loc[mask].reset_index(drop=True)
        y = y.loc[mask].to_numpy(dtype=float)

    assert X.shape[0] > 1, "Not enough samples after target cleaning."

    # Split
    if is_clf:
        # Only stratify if each class has at least 2 samples
        y_series = pd.Series(y)
        can_stratify = (y_series.value_counts().min() >= 2) and (y_series.nunique() >= 2)
        X_train, X_test, y_train, y_test = train_test_split(
            X,
            y,
            test_size=0.2,
            random_state=42,
            stratify=y if can_stratify else None,
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X,
            y,
            test_size=0.2,
            random_state=42,
        )

    assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

    # Model selection: lightweight baselines
    if is_clf:
        # Logistic regression is CPU-friendly and strong for small tabular datasets
        model = LogisticRegression(
            solver="lbfgs",
            max_iter=200,
            n_jobs=1,
            multi_class="auto",
        )
        pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Ridge regression as lightweight baseline
        model = Ridge(alpha=1.0, random_state=42)
        pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = _bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used robust CSV parsing with a lightweight fallback (sep=';' and decimal=',') to prevent expensive manual fixes.
# - Normalized and cleaned column names + dropped 'Unnamed:' columns and a likely index-like 'Id' to reduce noise.
# - Employed ColumnTransformer + Pipeline for reproducibility and to avoid redundant preprocessing work.
# - Used SimpleImputer (median/mode) to handle missing values cheaply; avoided heavy feature engineering.
# - Chose CPU-friendly linear baselines: LogisticRegression for classification, Ridge for regression fallback.
# - For regression fallback, converted error into a bounded [0,1] proxy score: 1/(1+NRMSE) for a stable ACCURACY output.