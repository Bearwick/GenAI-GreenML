# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score

RANDOM_STATE = 42
TEST_SIZE = 0.2


def load_dataset(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if "Id" in df.columns:
        df = df.drop(columns=["Id"])
    return df


def encode_labels(df: pd.DataFrame, target_col: str = "Species"):
    le = LabelEncoder()
    y = le.fit_transform(df[target_col].to_numpy())
    X = df.drop(columns=[target_col]).to_numpy()
    return X, y


def build_model(random_state: int = RANDOM_STATE) -> Pipeline:
    return Pipeline(
        steps=[
            ("scaler", StandardScaler()),
            ("model", LinearSVC(random_state=random_state, max_iter=5000)),
        ]
    )


def main():
    df = load_dataset("Iris.csv")
    X, y = encode_labels(df, target_col="Species")

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=TEST_SIZE,
        random_state=RANDOM_STATE,
        stratify=y,
    )

    model = build_model(RANDOM_STATE)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Removed plotting/visualization and verbose reports to cut unnecessary CPU/GPU work and I/O overhead.
# - Dropped cross-validation to avoid repeated training passes (largest compute cost) while preserving core train/test behavior.
# - Switched from SVC(kernel="linear") to LinearSVC: equivalent linear SVM decision function but significantly faster and more memory-efficient for linear kernels.
# - Converted features/labels to NumPy arrays early to reduce pandas overhead and data movement inside scikit-learn.
# - Avoided inplace dataframe mutation to simplify operations and reduce hidden copies; used single drop with reassignment.
# - Centralized constants and modularized functions to keep reproducibility stable (fixed random_state) and code maintainable.