# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score


RANDOM_SEED = 42


def read_csv_robust(path: str, expected_headers) -> pd.DataFrame:
    def _score_columns(cols) -> int:
        cols_set = set(map(str, cols))
        return sum(h in cols_set for h in expected_headers)

    df1 = pd.read_csv(path)
    if _score_columns(df1.columns) >= max(2, len(expected_headers) // 2):
        return df1

    df2 = pd.read_csv(path, sep=";", decimal=",")
    if _score_columns(df2.columns) > _score_columns(df1.columns):
        return df2
    return df1


def prepare_features(df: pd.DataFrame, expected_headers) -> tuple[pd.DataFrame, pd.Series]:
    df = df.copy()

    if "Id" in df.columns:
        df.drop(columns=["Id"], inplace=True)

    if "Species" not in df.columns:
        raise ValueError(f"Missing target column. Available columns: {list(df.columns)}")

    le = LabelEncoder()
    df["Species"] = le.fit_transform(df["Species"].astype(str))

    feature_cols = [c for c in expected_headers if c in df.columns and c != "Species"]
    if not feature_cols:
        feature_cols = [c for c in df.columns if c != "Species"]

    X = df[feature_cols]
    y = df["Species"]
    return X, y


def build_model(random_seed: int) -> Pipeline:
    return Pipeline(
        steps=[
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
            ("model", SVC(kernel="linear")),
        ]
    )


def main() -> None:
    np.random.seed(RANDOM_SEED)

    dataset_headers = [
        "Id",
        "SepalLengthCm",
        "SepalWidthCm",
        "PetalLengthCm",
        "PetalWidthCm",
        "Species",
    ]

    csv_path = "Iris.csv"
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Dataset file not found: {csv_path}")

    df = read_csv_robust(csv_path, dataset_headers)
    X, y = prepare_features(df, dataset_headers)

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=RANDOM_SEED,
        stratify=y,
    )

    pipeline = build_model(RANDOM_SEED)
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed plotting and extra reporting (confusion matrix heatmap, classification report, cross-validation) to eliminate expensive computations and rendering while keeping the core train/test evaluation intent intact.
# - Implemented robust CSV parsing with a fallback delimiter/decimal configuration to avoid repeated manual fixes and reduce failed parse retries.
# - Dropped unused imports and computations to reduce startup overhead and memory footprint.
# - Selected feature columns by intersecting DATASET_HEADERS with df.columns to avoid schema assumptions and unnecessary dataframe copies.
# - Ensured reproducibility by fixing the random seed for numpy and train_test_split, keeping results stable across runs.
# - Avoided redundant data movement by slicing only required columns for X and using in-place column drop where safe.