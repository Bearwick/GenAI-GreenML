# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score


RANDOM_SEED = 42
DATASET_PATH = "Iris.csv"
DATASET_HEADERS = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]


def _read_csv_with_fallback(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    if df.shape[1] <= 1:
        df = pd.read_csv(path, header=None, names=DATASET_HEADERS)
    return df


def _prepare_data(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:
    df = df.copy()

    if "Id" in df.columns:
        df = df.drop(columns=["Id"])

    target_col = "Species" if "Species" in df.columns else df.columns[-1]

    encoder = LabelEncoder()
    y = pd.Series(encoder.fit_transform(df[target_col].astype(str).to_numpy()), index=df.index, name=target_col)

    X = df.drop(columns=[target_col])
    X = X.apply(pd.to_numeric, errors="coerce")
    X = X.fillna(X.median(numeric_only=True))

    return X, y


def main() -> None:
    np.random.seed(RANDOM_SEED)

    df = _read_csv_with_fallback(DATASET_PATH)
    X, y = _prepare_data(df)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y
    )

    pipeline = Pipeline(
        steps=[
            ("scaler", StandardScaler()),
            ("model", SVC(kernel="linear", cache_size=200)),
        ]
    )

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    _ = cross_val_score(pipeline, X, y, cv=5)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed plotting and verbose reporting to avoid extra computation and I/O while keeping the same training/evaluation intent.
# - Added robust CSV parsing fallback (default -> ';'/'decimal=,' -> headerless with provided schema) to reduce reruns and failures.
# - Dropped the Id column only if present and derived the target column from df.columns to avoid schema assumptions and unnecessary data movement.
# - Used vectorized label encoding via NumPy arrays to reduce pandas overhead and intermediate objects.
# - Ensured numeric feature coercion and median imputation in one pass to avoid repeated conversions and prevent runtime errors.
# - Centralized random seed usage for reproducible splits and stable results without adding extra computation.
# - Kept the existing lightweight linear SVC + StandardScaler pipeline to preserve behavior while avoiding heavier model changes.
# - Avoided storing unused large outputs; cross-validation is executed to preserve evaluation intent but its results are not printed.