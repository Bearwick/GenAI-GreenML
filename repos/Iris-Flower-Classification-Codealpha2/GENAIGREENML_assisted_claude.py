# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

np.random.seed(42)

try:
    df = pd.read_csv("Iris.csv")
    if df.shape[1] < 3:
        raise ValueError("Too few columns")
except (ValueError, pd.errors.ParserError):
    df = pd.read_csv("Iris.csv", sep=";", decimal=",")

id_cols = [c for c in df.columns if c.lower() == "id"]
if id_cols:
    df.drop(columns=id_cols, inplace=True)

target_col = [c for c in df.columns if c.lower() == "species"][0]
feature_cols = [c for c in df.columns if c != target_col]

le = LabelEncoder()
y = le.fit_transform(df[target_col].values)
X = df[feature_cols].values.astype(np.float64)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = SVC(kernel="linear", random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Removed unused imports (matplotlib, seaborn) to reduce import overhead.
# Removed cross_val_score since it retrains 5 additional times and is not needed for final accuracy.
# Removed Pipeline wrapper; applied StandardScaler directly to avoid pipeline dispatch overhead.
# Used numpy arrays directly instead of pandas DataFrames for features to reduce memory and overhead.
# Removed all plots, prints, classification_report, and confusion_matrix computation.
# Derived column names dynamically from actual data for robustness.
# Added robust CSV fallback parsing with sep=';' and decimal=','.
# Set fixed random seeds for reproducibility.