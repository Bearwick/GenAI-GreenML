# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

df = pd.read_csv("Iris.csv", usecols=["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"])

X = df.iloc[:, :-1].values
y = pd.factorize(df["Species"])[0]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = SVC(kernel="linear", cache_size=700)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Reduced I/O and memory overhead by using 'usecols' in read_csv to skip the unnecessary 'Id' column.
# 2. Replaced LabelEncoder with pd.factorize for more efficient categorical-to-integer conversion.
# 3. Converted DataFrame to NumPy arrays early to minimize Pandas overhead during mathematical operations.
# 4. Eliminated 'Pipeline' object to reduce call stack depth and object management overhead.
# 5. Removed heavy visualization libraries (Seaborn/Matplotlib) to reduce memory footprint and import time.
# 6. Excised redundant metrics (confusion matrix, classification report) and cross-validation cycles to save CPU energy.
# 7. Optimized SVC memory usage by increasing 'cache_size' and keeping the lightweight linear kernel.
# 8. Eliminated unnecessary intermediate Pandas operations like 'df.drop' by using efficient 'iloc' slicing.