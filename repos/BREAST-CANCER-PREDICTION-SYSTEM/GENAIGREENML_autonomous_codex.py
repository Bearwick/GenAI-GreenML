# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")

data_path = "data.csv"

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    if df.shape[1] == 1:
        try:
            df2 = pd.read_csv(path, sep=';', decimal=',')
            if df2.shape[1] > 1:
                df = df2
        except Exception:
            pass
    return df

df = read_csv_robust(data_path)
df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
df = df.loc[:, ~df.columns.str.match(r'^Unnamed')]

assert df.shape[0] > 0 and df.shape[1] > 0

lower_cols = {c.lower(): c for c in df.columns}
candidate_targets = ['target', 'label', 'class', 'diagnosis', 'outcome', 'y']
target_col = None
for cand in candidate_targets:
    if cand in lower_cols:
        target_col = lower_cols[cand]
        break
if target_col is None:
    numeric_candidates = []
    for c in df.columns:
        series_num = pd.to_numeric(df[c], errors='coerce')
        if series_num.notna().sum() > 0:
            unique = series_num.nunique(dropna=True)
            if unique > 1:
                numeric_candidates.append((unique, c))
    if numeric_candidates:
        numeric_candidates.sort(key=lambda x: x[0], reverse=True)
        target_col = numeric_candidates[0][1]
    else:
        target_col = df.columns[0]

feature_cols = [c for c in df.columns if c != target_col]
id_cols = []
for c in feature_cols:
    lower = c.lower()
    if lower == 'id' or lower.endswith('_id') or lower.startswith('id_') or lower == 'identifier':
        id_cols.append(c)
feature_cols = [c for c in feature_cols if c not in id_cols]

if len(feature_cols) == 0:
    df['__constant'] = 1.0
    feature_cols = ['__constant']

X = df[feature_cols].copy()
y = df[target_col].copy()

y = y.replace([np.inf, -np.inf], np.nan)
y_numeric = pd.to_numeric(y, errors='coerce')
if y_numeric.notna().mean() >= 0.8:
    y = y_numeric
y = y.replace([np.inf, -np.inf], np.nan)
mask = y.notna()
if mask.sum() == 0:
    y = pd.Series(np.zeros(len(df)), index=df.index)
    mask = y.notna()
X = X.loc[mask].copy()
y = y.loc[mask].copy()

assert len(X) > 0

numeric_features = []
categorical_features = []
n_rows = len(X)
for c in X.columns:
    series_num = pd.to_numeric(X[c], errors='coerce')
    non_na = series_num.notna().sum()
    if non_na == 0:
        categorical_features.append(c)
    else:
        ratio = non_na / max(1, n_rows)
        if ratio >= 0.5:
            numeric_features.append(c)
        else:
            categorical_features.append(c)

if numeric_features:
    X[numeric_features] = X[numeric_features].apply(pd.to_numeric, errors='coerce')
    X[numeric_features] = X[numeric_features].replace([np.inf, -np.inf], np.nan)

all_missing_cols = [c for c in X.columns if X[c].isna().all()]
if all_missing_cols:
    X = X.drop(columns=all_missing_cols)
    numeric_features = [c for c in numeric_features if c not in all_missing_cols]
    categorical_features = [c for c in categorical_features if c not in all_missing_cols]

if len(X.columns) == 0:
    X['__constant'] = 1.0
    numeric_features = ['__constant']
    categorical_features = []

unique_vals = y.nunique(dropna=True)
y_is_numeric = y.dtype.kind in 'biufc'
if not y_is_numeric:
    task = 'classification'
else:
    threshold = max(2, min(20, int(0.2 * len(y))))
    if unique_vals <= threshold:
        task = 'classification'
    else:
        task = 'regression'

n_samples = len(X)
if n_samples < 2:
    X_train = X_test = X
    y_train = y_test = y
else:
    test_size = 0.2 if n_samples >= 5 else 0.5
    stratify = y if task == 'classification' and unique_vals > 1 and n_samples >= 3 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )

assert len(X_train) > 0 and len(X_test) > 0

transformers = []
if numeric_features:
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    transformers.append(('cat', categorical_transformer, categorical_features))

if not transformers:
    X_train = X_train.copy()
    X_test = X_test.copy()
    X_train['__constant'] = 1.0
    X_test['__constant'] = 1.0
    numeric_features = ['__constant']
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    transformers = [('num', numeric_transformer, numeric_features)]

preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')

if task == 'classification':
    if y_train.nunique() < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear', random_state=42)
    clf = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
else:
    model = Ridge(alpha=1.0)
    reg = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])
    reg.fit(X_train, y_train)
    y_pred = reg.predict(X_test)
    if len(y_test) < 2 or np.all(y_test == y_test.iloc[0]):
        r2 = 0.0
    else:
        r2 = r2_score(y_test, y_pred)
        if np.isnan(r2):
            r2 = 0.0
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models and a DummyClassifier fallback to keep CPU usage low.
# - Applied simple, reproducible preprocessing with imputation, scaling, and one-hot encoding via ColumnTransformer.
# - Implemented robust schema detection and parsing to avoid hard failures on unknown column layouts.
# - For regression fallback, converted R^2 to a bounded [0,1] accuracy proxy using (r2+1)/2.