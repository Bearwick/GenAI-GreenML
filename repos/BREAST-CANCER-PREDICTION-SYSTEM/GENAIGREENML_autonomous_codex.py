# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import numpy as np
import pandas as pd
import re
import warnings
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, r2_score

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
        return df
    if df.shape[1] == 1:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > 1:
                df = df_alt
        except Exception:
            pass
    return df

def make_onehot(sparse_flag):
    try:
        return OneHotEncoder(handle_unknown='ignore', sparse=sparse_flag)
    except TypeError:
        return OneHotEncoder(handle_unknown='ignore', sparse_output=sparse_flag)

df = read_csv_robust("data.csv")

df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
df = df.loc[:, ~df.columns.str.startswith('Unnamed')]
df = df.dropna(axis=1, how='all')
assert df.shape[0] > 0

expected_headers = ["id","diagnosis","radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean","compactness_mean","concavity_mean","concave_points_mean","symmetry_mean","fractal_dimension_mean","radius_se","texture_se","perimeter_se","area_se","smoothness_se","compactness_se","concavity_se","concave_points_se","symmetry_se","fractal_dimension_se","radius_worst","texture_worst","perimeter_worst","area_worst","smoothness_worst","compactness_worst","concavity_worst","concave_points_worst","symmetry_worst","fractal_dimension_worst"]

candidate_names = ["diagnosis","target","label","y","class","outcome"]
for h in expected_headers:
    h_low = str(h).lower()
    if any(k in h_low for k in ["diagnosis","target","label","class","outcome"]):
        candidate_names.append(h_low)
candidate_names = list(dict.fromkeys(candidate_names))

def select_target(df, candidate_names):
    cols = list(df.columns)
    for name in candidate_names:
        for col in cols:
            if str(col).lower() == name and df[col].nunique(dropna=True) > 1:
                return col
    obj_cols = [c for c in cols if df[c].dtype == object or str(df[c].dtype).startswith('category')]
    for c in obj_cols:
        n = df[c].nunique(dropna=True)
        if 2 <= n <= 20:
            return c
    num_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]
    for c in num_cols:
        n = df[c].nunique(dropna=True)
        if 2 <= n <= 20:
            return c
    if num_cols:
        variances = {}
        for c in num_cols:
            series = pd.to_numeric(df[c], errors='coerce')
            var = series.var(skipna=True)
            if pd.notna(var):
                variances[c] = var
        if variances:
            return max(variances, key=variances.get)
    return cols[0] if cols else None

target_col = select_target(df, candidate_names)
if target_col is None:
    target_col = df.columns[0]

target_series = df[target_col]
mask = target_series.notna()
df = df[mask].copy()
target_series = target_series[mask]
assert df.shape[0] > 0

n_unique = target_series.nunique(dropna=True)
is_object = target_series.dtype == object or str(target_series.dtype).startswith('category')
if is_object:
    problem_type = "classification"
else:
    problem_type = "classification" if 2 <= n_unique <= 20 else "regression"

classification_possible = False
n_classes = 0
if problem_type == "classification":
    y_encoded, uniques = pd.factorize(target_series)
    y = y_encoded
    n_classes = len(uniques)
    if n_classes >= 2:
        classification_possible = True
else:
    y_numeric = pd.to_numeric(target_series, errors='coerce')
    mask = y_numeric.notna()
    df = df[mask].copy()
    y = y_numeric[mask]
    assert df.shape[0] > 0

feature_cols = [c for c in df.columns if c != target_col]
if feature_cols:
    X_raw = df[feature_cols].copy()
else:
    X_raw = pd.DataFrame(index=df.index)

numeric_features = []
categorical_features = []
X = pd.DataFrame(index=X_raw.index)
for col in X_raw.columns:
    col_data = X_raw[col]
    if pd.api.types.is_numeric_dtype(col_data):
        col_numeric = pd.to_numeric(col_data, errors='coerce')
        if col_numeric.nunique(dropna=True) <= 1:
            continue
        X[col] = col_numeric
        numeric_features.append(col)
    else:
        col_numeric = pd.to_numeric(col_data, errors='coerce')
        non_na_ratio = col_numeric.notna().mean()
        if non_na_ratio > 0.7:
            if col_numeric.nunique(dropna=True) <= 1:
                continue
            X[col] = col_numeric
            numeric_features.append(col)
        else:
            col_cat = col_data.astype(str)
            if col_cat.nunique(dropna=True) <= 1:
                continue
            X[col] = col_cat
            categorical_features.append(col)

if numeric_features:
    X[numeric_features] = X[numeric_features].replace([np.inf, -np.inf], np.nan)

features_exist = X.shape[1] > 0
y_array = y.values if isinstance(y, pd.Series) else np.array(y)

n_samples = len(y_array)
if n_samples < 2:
    y_array = np.concatenate([y_array, y_array])
    if features_exist:
        X = pd.concat([X, X], ignore_index=True)
    n_samples = len(y_array)

test_size = 0.2
if n_samples < 5:
    test_size = 0.5

stratify = y_array if (problem_type == "classification" and classification_possible and len(np.unique(y_array)) > 1 and n_samples >= 5) else None
X_for_split = X if features_exist else np.zeros((n_samples, 1))
try:
    X_train, X_test, y_train, y_test = train_test_split(X_for_split, y_array, test_size=test_size, random_state=42, stratify=stratify)
except ValueError:
    X_train, X_test, y_train, y_test = train_test_split(X_for_split, y_array, test_size=test_size, random_state=42, stratify=None)
assert len(y_train) > 0 and len(y_test) > 0

accuracy = 0.0
if features_exist and ((problem_type == "classification" and classification_possible) or (problem_type == "regression")):
    onehot = make_onehot(True) if problem_type == "classification" else make_onehot(False)
    scaler = StandardScaler(with_mean=False)
    transformers = []
    if numeric_features:
        numeric_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="median")), ("scaler", scaler)])
        transformers.append(("num", numeric_transformer, numeric_features))
    if categorical_features:
        categorical_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="most_frequent")), ("onehot", onehot)])
        transformers.append(("cat", categorical_transformer, categorical_features))
    preprocess = ColumnTransformer(transformers=transformers, remainder="drop")
    if problem_type == "classification":
        model = LogisticRegression(max_iter=200, solver="liblinear")
    else:
        model = LinearRegression()
    pipe = Pipeline(steps=[("preprocess", preprocess), ("model", model)])
    pipe.fit(X_train, y_train)
    preds = pipe.predict(X_test)
    if problem_type == "classification":
        accuracy = accuracy_score(y_test, preds)
    else:
        try:
            r2 = r2_score(y_test, preds)
        except Exception:
            r2 = 0.0
        accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))
else:
    if problem_type == "classification":
        values, counts = np.unique(y_train, return_counts=True)
        if len(values) == 0:
            accuracy = 0.0
        else:
            majority = values[np.argmax(counts)]
            y_pred = np.full_like(y_test, majority)
            accuracy = accuracy_score(y_test, y_pred)
    else:
        mean_val = float(np.mean(y_train)) if len(y_train) > 0 else 0.0
        y_pred = np.full_like(y_test, mean_val, dtype=float)
        try:
            r2 = r2_score(y_test, y_pred)
        except Exception:
            r2 = 0.0
        accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# Used lightweight Logistic/Linear Regression with minimal preprocessing for CPU efficiency.
# Applied ColumnTransformer with imputation and optional scaling/one-hot encoding for reproducible pipelines.
# Implemented robust target/feature selection and simple baselines for degenerate cases.
# Regression accuracy proxy maps (R2 + 1) / 2 to keep the reported score in [0,1].