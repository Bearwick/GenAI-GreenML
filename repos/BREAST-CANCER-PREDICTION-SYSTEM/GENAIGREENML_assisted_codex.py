# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd

def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))

def add_bias(X):
    return np.concatenate((np.ones((X.shape[0], 1), dtype=X.dtype), X), axis=1)

def train_test_split(X, y, test_size=0.2, random_state=2):
    rng = np.random.RandomState(random_state)
    n_samples = X.shape[0]
    n_test = int(np.ceil(n_samples * test_size)) if isinstance(test_size, float) else int(test_size)
    indices = rng.permutation(n_samples)
    test_idx = indices[:n_test]
    train_idx = indices[n_test:]
    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]

def gradient_descent(X, y, alpha=0.01, num_iterations=1000):
    m = y.size
    theta = np.zeros(X.shape[1], dtype=X.dtype)
    X_T = X.T
    step = alpha / m
    for _ in range(num_iterations):
        h = sigmoid(X @ theta)
        theta -= step * (X_T @ (h - y))
    return theta

def predict(X, theta):
    X_b = add_bias(X)
    return np.round(sigmoid(X_b @ theta)).astype(int)

df = pd.read_csv("data.csv", usecols=range(1, 32))
df.fillna(0, inplace=True)
df["diagnosis"] = (df["diagnosis"].values == "M").astype(np.int8)

y = df["diagnosis"].to_numpy()
X = df.iloc[:, 1:].to_numpy()
del df

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

X_train_b = add_bias(X_train)
theta = gradient_descent(X_train_b, y_train, alpha=0.01, num_iterations=1000)

test_preds = predict(X_test, theta)
accuracy = np.mean(test_preds == y_test)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# read only required columns to reduce I/O and memory usage
# replaced external utilities with lightweight vectorized operations
# removed cost tracking and unused computations to cut redundant work
# cached transpose and step size inside gradient descent to avoid repeated calculations
# minimized temporary arrays and freed unused dataframe to lower memory footprint