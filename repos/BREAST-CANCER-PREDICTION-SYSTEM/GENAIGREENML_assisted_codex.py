# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

np.random.seed(42)

DATASET_HEADERS = (
    "id,diagnosis,radius_mean,texture_mean,perimeter_mean,area_mean,"
    "smoothness_mean,compactness_mean,concavity_mean,concave_points_mean,"
    "symmetry_mean,fractal_dimension_mean,radius_se,texture_se,perimeter_se,"
    "area_se,smoothness_se,compactness_se,concavity_se,concave_points_se,"
    "symmetry_se,fractal_dimension_se,radius_worst,texture_worst,perimeter_worst,"
    "area_worst,smoothness_worst,compactness_worst,concavity_worst,"
    "concave_points_worst,symmetry_worst,fractal_dimension_worst"
)

EXPECTED_HEADERS = [h.strip() for h in DATASET_HEADERS.split(",")]
EXPECTED_HEADERS_N = [h.lower() for h in EXPECTED_HEADERS]

def looks_correct(df, expected_headers):
    if df.shape[1] == 1:
        return False
    cols_lower = [c.strip().lower() for c in df.columns]
    expected_lower = [h.lower() for h in expected_headers]
    match_count = sum(1 for h in expected_lower if h in cols_lower)
    if match_count >= len(expected_headers) * 0.5:
        return True
    return len(expected_headers) - 1 <= df.shape[1] <= len(expected_headers) + 1

def load_csv(path, expected_headers):
    df = pd.read_csv(path)
    if not looks_correct(df, expected_headers):
        df_alt = pd.read_csv(path, sep=";", decimal=",")
        if looks_correct(df_alt, expected_headers):
            df = df_alt
    return df

def align_columns(df, expected_headers_n):
    stripped = [c.strip().lower() for c in df.columns]
    mapping = {s: original for s, original in zip(stripped, df.columns)}
    if set(expected_headers_n).issubset(set(stripped)):
        df = df[[mapping[h] for h in expected_headers_n]]
        col_map = {h: mapping[h] for h in expected_headers_n}
    else:
        df = df.iloc[:, :len(expected_headers_n)]
        col_map = {c.strip().lower(): c for c in df.columns}
    return df, col_map

def encode_diagnosis(series):
    if series.dtype.kind in "biufc":
        return series.astype(int)
    values = np.unique(series.astype(str))
    mapping = {val: idx for idx, val in enumerate(values)}
    return series.astype(str).map(mapping).astype(int)

def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))

def gradient_descent(X, y, theta, alpha, num_iterations):
    m = y.size
    X_T = X.T
    for _ in range(num_iterations):
        h = sigmoid(X @ theta)
        theta -= (alpha / m) * (X_T @ (h - y))
    return theta

def add_bias(X):
    m, n = X.shape
    Xb = np.empty((m, n + 1), dtype=X.dtype)
    Xb[:, 0] = 1.0
    Xb[:, 1:] = X
    return Xb

def predict(input_data, theta):
    Xb = add_bias(input_data)
    return ((Xb @ theta) > 0).astype(int)

def main():
    df = load_csv("data.csv", EXPECTED_HEADERS)
    df, col_map = align_columns(df, EXPECTED_HEADERS_N)
    df.fillna(0, inplace=True)
    diag_col = col_map.get("diagnosis", df.columns[1])
    df[diag_col] = encode_diagnosis(df[diag_col])
    n_features = len(EXPECTED_HEADERS_N) - 2
    if all(h in col_map for h in EXPECTED_HEADERS_N):
        feature_cols = [col_map[h] for h in EXPECTED_HEADERS_N[2:]]
    else:
        feature_cols = df.columns[2:2 + n_features]
    X = df[feature_cols].to_numpy(dtype=float, copy=False)
    y = df[diag_col].to_numpy(dtype=int, copy=False)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=2
    )
    X_train_b = add_bias(X_train)
    X_test_b = add_bias(X_test)
    theta = np.zeros(X_train_b.shape[1], dtype=X_train_b.dtype)
    theta = gradient_descent(X_train_b, y_train, theta, alpha=0.01, num_iterations=1000)
    train_preds = ((X_train_b @ theta) > 0).astype(int)
    test_preds = ((X_test_b @ theta) > 0).astype(int)
    train_acc = np.mean(train_preds == y_train)
    test_acc = np.mean(test_preds == y_test)
    accuracy = test_acc
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused visualization/logging paths and cost tracking to cut overhead.
# - Added schema-aware CSV parsing with a deterministic fallback to prevent misreads.
# - Preallocated bias columns and reused transposed matrices to reduce allocations.
# - Replaced sigmoid-based classification with logit thresholding for faster inference.
# - Fixed random seed for reproducible behavior across runs.