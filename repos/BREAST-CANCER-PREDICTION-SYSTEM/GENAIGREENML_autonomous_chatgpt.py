# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(d: pd.DataFrame) -> bool:
        if d is None or d.empty:
            return True
        # If only 1 column, it might have been parsed with wrong separator
        if d.shape[1] <= 1:
            return True
        # If many columns are unnamed or a single giant column exists, treat as suspicious
        unnamed = [c for c in d.columns if str(c).strip().lower().startswith("unnamed")]
        if len(unnamed) >= max(1, int(0.5 * d.shape[1])):
            return True
        return False

    if looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            # last resort: try python engine with common separators inference
            df = pd.read_csv(path, sep=None, engine="python")

    return df


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols = []
    for c in df.columns:
        s = str(c)
        s = s.strip()
        s = re.sub(r"\s+", " ", s)
        cols.append(s)
    df.columns = cols
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _coerce_numeric_df(df: pd.DataFrame, cols) -> pd.DataFrame:
    for c in cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _pick_target(df: pd.DataFrame, preferred: str = "diagnosis") -> str:
    if preferred in df.columns:
        # Ensure non-empty target
        if df[preferred].notna().sum() > 0:
            return preferred

    # Prefer a non-constant column: object first for classification, else numeric
    obj_cols = [c for c in df.columns if df[c].dtype == "object"]
    for c in obj_cols:
        nunique = df[c].nunique(dropna=True)
        if nunique >= 2:
            return c

    # Numeric target fallback
    num_cols = []
    for c in df.columns:
        if c == preferred:
            continue
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() > 0:
            num_cols.append(c)

    # Choose non-constant numeric column with most variance
    best_col = None
    best_var = -1.0
    for c in num_cols:
        s = pd.to_numeric(df[c], errors="coerce")
        s = s.replace([np.inf, -np.inf], np.nan).dropna()
        if s.shape[0] < 5:
            continue
        v = float(np.nanvar(s.values))
        if v > best_var and v > 0:
            best_var = v
            best_col = c
    if best_col is not None:
        return best_col

    # Last resort: pick first column
    return df.columns[0]


def _is_classification_target(y: pd.Series) -> bool:
    if y.dtype == "object":
        return True
    # If numeric with small number of unique values, treat as classification
    yn = pd.to_numeric(y, errors="coerce")
    uniq = pd.Series(yn).dropna().unique()
    if len(uniq) >= 2 and len(uniq) <= 20:
        return True
    return False


def _bounded_regression_score(y_true, y_pred) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    y_true = np.where(np.isfinite(y_true), y_true, np.nan)
    y_pred = np.where(np.isfinite(y_pred), y_pred, np.nan)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    # R2-like, bounded to [0,1] via max(0, r2) then clamp
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    ss_tot = float(np.sum((y_true - float(np.mean(y_true))) ** 2))
    if ss_tot <= 1e-12:
        return 0.0
    r2 = 1.0 - ss_res / ss_tot
    score = max(0.0, min(1.0, r2))
    return float(score)


def main():
    path = "data.csv"
    if not os.path.exists(path):
        # Minimal fallback: cannot read; still produce required output
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _read_csv_robust(path)
    df = _normalize_columns(df)

    # Drop fully empty rows
    df = df.dropna(how="all")
    assert df.shape[0] > 0, "Empty dataset after dropping empty rows."

    target_col = _pick_target(df, preferred="diagnosis")
    y_raw = df[target_col]

    X = df.drop(columns=[target_col], errors="ignore")

    # Identify likely ID columns to drop (low utility, can cause leakage or scaling issues)
    drop_id_like = []
    for c in X.columns:
        cname = str(c).strip().lower()
        if cname in {"id", "identifier"} or cname.endswith("_id"):
            drop_id_like.append(c)
    if drop_id_like:
        X = X.drop(columns=drop_id_like, errors="ignore")

    # Determine column types
    # Coerce numerics conservatively: attempt numeric conversion only to detect numeric-like columns
    numeric_candidates = []
    categorical_candidates = []
    for c in X.columns:
        s = X[c]
        if s.dtype == "object":
            # Try numeric parse; if many values become numeric, treat as numeric
            sn = pd.to_numeric(s, errors="coerce")
            ratio_numeric = float(sn.notna().mean()) if len(sn) else 0.0
            if ratio_numeric >= 0.9:
                numeric_candidates.append(c)
                X[c] = sn
            else:
                categorical_candidates.append(c)
        else:
            numeric_candidates.append(c)

    X = _coerce_numeric_df(X, numeric_candidates)

    # Choose task type
    classification = _is_classification_target(y_raw)

    if classification:
        y = y_raw.astype(str).fillna("NA")
        # Ensure at least 2 classes
        if y.nunique(dropna=False) < 2:
            classification = False
        else:
            # Train/test split with stratification if feasible
            stratify = y if y.nunique() >= 2 and y.value_counts().min() >= 2 else None
            X_train, X_test, y_train, y_test = train_test_split(
                X,
                y,
                test_size=0.2,
                random_state=42,
                stratify=stratify,
            )
            assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Train/test split failed."

            numeric_features = [c for c in numeric_candidates if c in X.columns]
            categorical_features = [c for c in categorical_candidates if c in X.columns]

            preprocessor = ColumnTransformer(
                transformers=[
                    ("num", Pipeline(steps=[
                        ("imputer", SimpleImputer(strategy="median")),
                        ("scaler", StandardScaler(with_mean=False)),
                    ]), numeric_features),
                    ("cat", Pipeline(steps=[
                        ("imputer", SimpleImputer(strategy="most_frequent")),
                        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
                    ]), categorical_features),
                ],
                remainder="drop",
                sparse_threshold=0.3,
            )

            # Lightweight linear model for CPU efficiency; small max_iter for energy savings
            clf = LogisticRegression(
                solver="liblinear",
                max_iter=200,
                C=1.0,
            )

            model = Pipeline(steps=[
                ("preprocess", preprocessor),
                ("model", clf),
            ])

            # If something goes wrong, fall back to trivial baseline
            try:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                accuracy = float(accuracy_score(y_test, y_pred))
            except Exception:
                dummy = DummyClassifier(strategy="most_frequent")
                dummy.fit(X_train.fillna(0), y_train)
                y_pred = dummy.predict(X_test.fillna(0))
                accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

    # Regression fallback
    y_num = pd.to_numeric(y_raw, errors="coerce").replace([np.inf, -np.inf], np.nan)
    # Drop rows with missing target
    mask = y_num.notna()
    Xr = X.loc[mask].copy()
    yr = y_num.loc[mask].astype(float)

    # If after filtering too small, do trivial baseline
    if Xr.shape[0] < 10:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Update types after filtering
    numeric_features = []
    categorical_features = []
    for c in Xr.columns:
        if Xr[c].dtype == "object":
            categorical_features.append(c)
        else:
            numeric_features.append(c)

    X_train, X_test, y_train, y_test = train_test_split(
        Xr,
        yr,
        test_size=0.2,
        random_state=42,
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Train/test split failed."

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler(with_mean=False)),
            ]), numeric_features),
            ("cat", Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
            ]), categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    reg = Ridge(alpha=1.0, random_state=42)

    model = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", reg),
    ])

    try:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_score(y_test.values, y_pred)
    except Exception:
        dummy = DummyRegressor(strategy="mean")
        dummy.fit(X_train.select_dtypes(exclude=["object"]).fillna(0), y_train)
        y_pred = dummy.predict(X_test.select_dtypes(exclude=["object"]).fillna(0))
        accuracy = _bounded_regression_score(y_test.values, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used lightweight linear models (LogisticRegression with liblinear; Ridge for regression fallback) for CPU efficiency and low energy use.
# - Implemented a single sklearn Pipeline + ColumnTransformer to avoid redundant preprocessing and ensure reproducibility.
# - Robust CSV parsing: default read_csv then fallback to ';' separator and ',' decimal to handle common locale variants.
# - Defensive schema handling: normalize headers, drop 'Unnamed' columns, select target flexibly if expected target missing.
# - Minimal feature engineering: median/mode imputation + sparse one-hot encoding; StandardScaler(with_mean=False) keeps sparse matrices efficient.
# - Kept stdout minimal as requested; regression fallback reports a bounded [0,1] proxy score derived from clamped R2.