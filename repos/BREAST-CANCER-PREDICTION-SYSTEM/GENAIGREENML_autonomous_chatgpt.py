# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")


def _normalize_columns(cols):
    out = []
    for c in cols:
        c = str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        out.append(c)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:?\s*\d*$", str(c), flags=re.IGNORECASE)]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _read_csv_robust(path):
    df1 = None
    try:
        df1 = pd.read_csv(path)
    except Exception:
        df1 = None

    if df1 is not None:
        if df1.shape[1] >= 2:
            return df1

    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        return df2
    except Exception:
        pass

    if df1 is not None:
        return df1
    raise FileNotFoundError(f"Could not read CSV file: {path}")


def _choose_target(df):
    cols_lower = {str(c).strip().lower(): c for c in df.columns}

    # Prefer explicit diagnosis/target columns if present
    for key in ["diagnosis", "target", "label", "y", "class"]:
        if key in cols_lower:
            return cols_lower[key]

    # Otherwise choose a non-constant numeric column, preferring low cardinality (classification-friendly)
    numeric_candidates = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        non_na = s.dropna()
        if non_na.size == 0:
            continue
        nunique = int(non_na.nunique())
        if nunique <= 1:
            continue
        numeric_candidates.append((nunique, c))

    if numeric_candidates:
        numeric_candidates.sort(key=lambda t: (t[0], str(t[1])))
        return numeric_candidates[0][1]

    # Fallback: first column
    return df.columns[0]


def _safe_accuracy_proxy_r2(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    if y_true.size == 0:
        return 0.0
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    ss_tot = float(np.sum((y_true - float(np.mean(y_true))) ** 2))
    if ss_tot <= 0.0:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    acc = max(0.0, min(1.0, (r2 + 1.0) / 2.0))
    return acc


csv_path = "data.csv"
df = _read_csv_robust(csv_path)

df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)

assert df.shape[0] > 0 and df.shape[1] > 0

# Drop fully empty columns
df = df.dropna(axis=1, how="all")
assert df.shape[1] > 0

target_col = _choose_target(df)

# Build X/y without assuming schema
y_raw = df[target_col].copy()
X = df.drop(columns=[target_col], errors="ignore").copy()

# If X ends up empty, keep at least one column (cannot happen unless df had one column)
if X.shape[1] == 0:
    X = df.copy()
    y_raw = X.iloc[:, 0].copy()
    X = X.drop(columns=[X.columns[0]], errors="ignore")

# Coerce numeric-looking columns safely; keep others as object for one-hot
for c in X.columns:
    if X[c].dtype == object or str(X[c].dtype).startswith("string"):
        coerced = pd.to_numeric(X[c], errors="coerce")
        non_na_ratio = float(coerced.notna().mean()) if len(coerced) else 0.0
        if non_na_ratio >= 0.9:
            X[c] = coerced

# Prepare y: if object, try label-like encoding via pandas factorize
y = y_raw.copy()
if y.dtype == object or str(y.dtype).startswith("string"):
    y = y.astype(str).str.strip()
    y = y.replace({"nan": np.nan, "None": np.nan})
    codes, uniques = pd.factorize(y, sort=True)
    y = pd.Series(codes, index=y_raw.index).astype(float)
else:
    y = pd.to_numeric(y, errors="coerce")

# Drop rows with missing y
mask_y = np.isfinite(np.asarray(y, dtype=float))
X = X.loc[mask_y].copy()
y = y.loc[mask_y].copy()
assert X.shape[0] > 0

# Determine task type
y_vals = np.asarray(y, dtype=float)
unique_vals = np.unique(y_vals)
is_classification = (unique_vals.size >= 2) and (unique_vals.size <= max(20, int(0.05 * y_vals.size)))

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y_vals, test_size=0.2, random_state=42, stratify=y_vals if is_classification and unique_vals.size >= 2 else None
)
assert X_train.shape[0] > 0 and X_test.shape[0] > 0

# Column types
numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
categorical_features = [c for c in X.columns if c not in numeric_features]

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=True, with_std=True)),
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

if is_classification:
    # Use a small, CPU-friendly linear model; liblinear is efficient for small datasets.
    clf = LogisticRegression(
        solver="liblinear",
        max_iter=200,
        random_state=42,
    )
    model = Pipeline(steps=[("preprocess", preprocess), ("model", clf)])

    # If y is degenerate, fallback to dummy
    if np.unique(y_train).size < 2:
        model = Pipeline(steps=[("preprocess", preprocess), ("model", DummyClassifier(strategy="most_frequent"))])

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Lightweight regression with L2 regularization; stable on CPU.
    reg = Ridge(alpha=1.0, random_state=42)
    model = Pipeline(steps=[("preprocess", preprocess), ("model", reg)])

    # If y is constant, fallback to dummy regressor
    if np.unique(y_train).size < 2:
        model = Pipeline(steps=[("preprocess", preprocess), ("model", DummyRegressor(strategy="mean"))])

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = float(_safe_accuracy_proxy_r2(y_test, y_pred))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used robust CSV parsing (default then ;/decimal=, retry) to avoid manual edits and reduce failure retries.
# - Normalized and de-noised column names and dropped "Unnamed:*" to prevent wasted processing on index artifacts.
# - Inferred target defensively; preferred common label names, else selected a non-constant numeric column.
# - Kept CPU/energy footprint low with a linear model (LogisticRegression liblinear) and simple Ridge for regression fallback.
# - Used ColumnTransformer + Pipeline to ensure single-pass preprocessing, reproducibility, and avoid redundant transforms.
# - Minimal feature engineering: median imputation + standardization for numeric, one-hot for categoricals; no heavy models/ensembles.
# - Regression fallback reports a bounded [0,1] "accuracy" proxy from R2: accuracy=(R2+1)/2 clipped to [0,1].