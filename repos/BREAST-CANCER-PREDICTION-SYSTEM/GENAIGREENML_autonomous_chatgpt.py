# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _find_dataset_path() -> str:
    # Keep I/O minimal: check common local filenames only
    candidates = (
        "data.csv",
        "dataset.csv",
        "train.csv",
        "breast_cancer.csv",
        "wdbc.csv",
        "breast-cancer-wisconsin.csv",
        "BreastCancer.csv",
    )
    for name in candidates:
        if os.path.isfile(name):
            return name
    csvs = [f for f in os.listdir(".") if f.lower().endswith(".csv")]
    if len(csvs) == 1:
        return csvs[0]
    raise FileNotFoundError(
        "Dataset CSV not found. Place a single .csv in the working directory or name it one of: "
        + ", ".join(candidates)
    )


def main() -> None:
    path = _find_dataset_path()
    df = pd.read_csv(path)

    if "diagnosis" not in df.columns:
        raise ValueError("Expected target column 'diagnosis' not found.")

    y_raw = df["diagnosis"].astype(str).str.strip()
    y = y_raw.map({"M": 1, "B": 0})
    if y.isna().any():
        uniq = sorted(y_raw.unique().tolist())
        raise ValueError(f"Unexpected labels in 'diagnosis': {uniq}. Expected 'M'/'B'.")

    drop_cols = [c for c in ["id", "diagnosis"] if c in df.columns]
    X = df.drop(columns=drop_cols)

    numeric_cols = X.columns.tolist()
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
        ],
        remainder="drop",
        sparse_threshold=0.0,
    )

    model = LogisticRegression(
        solver="liblinear",
        max_iter=200,
        C=1.0,
        class_weight=None,
        random_state=42,
    )

    clf = Pipeline(
        steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ]
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y.to_numpy(dtype=np.int32),
        test_size=0.2,
        random_state=42,
        stratify=y,
    )

    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Chosen model: LogisticRegression (linear) is lightweight, fast on CPU, and strong for this tabular dataset.
# - Preprocessing: SimpleImputer(median) + StandardScaler in a Pipeline ensures reproducibility and stable convergence.
# - ColumnTransformer restricts work to numeric features only and avoids extra processing.
# - Small hyperparameter footprint (no grid search) to reduce compute/energy; deterministic split via random_state.
# - No deep learning, embeddings, plotting, or model saving to keep runtime and energy use minimal.