# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import pandas as pd

df = pd.read_csv("data.csv")
df.fillna(0, inplace=True)

diagnosis = df['diagnosis'].values
Y = (diagnosis == 'M').astype(np.float64)

X = df.iloc[:, 2:32].values.astype(np.float64)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.2, random_state=2
)

m_train = X_train.shape[0]
m_test = X_test.shape[0]
n_features = X_train.shape[1]

X_train_b = np.empty((m_train, n_features + 1), dtype=np.float64)
X_train_b[:, 0] = 1.0
X_train_b[:, 1:] = X_train

X_test_b = np.empty((m_test, n_features + 1), dtype=np.float64)
X_test_b[:, 0] = 1.0
X_test_b[:, 1:] = X_test

theta = np.zeros(n_features + 1, dtype=np.float64)

alpha = 0.01
num_iterations = 1000
inv_m = 1.0 / m_train

for _ in range(num_iterations):
    h = 1.0 / (1.0 + np.exp(-X_train_b.dot(theta)))
    theta -= alpha * inv_m * X_train_b.T.dot(h - Y_train)

test_preds = (1.0 / (1.0 + np.exp(-X_test_b.dot(theta))) >= 0.5).astype(int)
Y_test_int = Y_test.astype(int)

accuracy = np.mean(test_preds == Y_test_int)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Removed LabelEncoder; used direct vectorized boolean comparison for encoding diagnosis
# 2. Eliminated separate cost computation inside gradient descent loop (was not needed for training)
# 3. Pre-computed inverse of m to avoid repeated division
# 4. Used pre-allocated arrays with np.empty for bias-augmented matrices instead of np.hstack
# 5. Inlined sigmoid to avoid function call overhead in the training loop
# 6. Removed all plotting, printing, model saving, and unused prediction examples
# 7. Removed unnecessary imports (seaborn, matplotlib, LabelEncoder)
# 8. Used float64 dtype explicitly for consistency and avoided redundant type conversions
# 9. Removed costs list accumulation, saving memory allocation each iteration