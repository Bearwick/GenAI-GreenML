# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import pandas as pd

np.random.seed(2)

df = pd.read_csv("data.csv")
if df.shape[1] < 5:
    df = pd.read_csv("data.csv", sep=';', decimal=',')

df.fillna(0, inplace=True)

diagnosis_col = df.columns[1]
df[diagnosis_col] = (df[diagnosis_col] == 'M').astype(int)

feature_cols = df.columns[2:32]
X = df[feature_cols].values.astype(np.float64)
Y = df[diagnosis_col].values.astype(np.float64)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.2, random_state=2
)

X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))
X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))

def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))

def gradient_descent(X, y, theta, alpha, num_iterations):
    m = len(y)
    inv_m = 1.0 / m
    for _ in range(num_iterations):
        h = sigmoid(X.dot(theta))
        theta -= alpha * (inv_m * X.T.dot(h - y))
    return theta

theta = np.zeros(X_train.shape[1])
theta = gradient_descent(X_train, Y_train, theta, 0.01, 1000)

test_preds = (sigmoid(X_test.dot(theta)) >= 0.5).astype(int)
accuracy = np.mean(test_preds == Y_test)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Removed matplotlib/seaborn imports and all plotting to eliminate unused dependencies
# Replaced LabelEncoder with direct boolean comparison for diagnosis encoding, avoiding sklearn overhead
# Removed cost tracking list in gradient_descent since costs were only used for plotting
# Pre-computed inv_m outside the loop to avoid repeated division
# Combined sigmoid threshold and rounding into a single comparison (>= 0.5)
# Removed redundant train accuracy computation since only test accuracy is reported
# Removed example prediction block as it is not part of accuracy evaluation
# Used direct dtype casting to float64 for predictable numerical behavior
# Added robust CSV fallback with sep=';' and decimal=','
# Set fixed random seed for reproducibility