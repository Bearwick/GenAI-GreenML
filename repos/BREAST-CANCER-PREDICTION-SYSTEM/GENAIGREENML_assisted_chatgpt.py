# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split


RANDOM_SEED = 2
DATASET_PATH = "data.csv"
DATASET_HEADERS = [
    "id",
    "diagnosis",
    "radius_mean",
    "texture_mean",
    "perimeter_mean",
    "area_mean",
    "smoothness_mean",
    "compactness_mean",
    "concavity_mean",
    "concave_points_mean",
    "symmetry_mean",
    "fractal_dimension_mean",
    "radius_se",
    "texture_se",
    "perimeter_se",
    "area_se",
    "smoothness_se",
    "compactness_se",
    "concavity_se",
    "concave_points_se",
    "symmetry_se",
    "fractal_dimension_se",
    "radius_worst",
    "texture_worst",
    "perimeter_worst",
    "area_worst",
    "smoothness_worst",
    "compactness_worst",
    "concavity_worst",
    "concave_points_worst",
    "symmetry_worst",
    "fractal_dimension_worst",
]


def _read_csv_robust(path: str, expected_headers: list[str]) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] == 1 or not set(expected_headers).issubset(set(df.columns)):
        df2 = pd.read_csv(path, sep=";", decimal=",")
        if df2.shape[1] > df.shape[1]:
            df = df2
    if "Unnamed: 32" in df.columns:
        df = df.drop(columns=["Unnamed: 32"])
    return df


def _encode_diagnosis(series: pd.Series) -> np.ndarray:
    s = series.astype(str).str.strip().str.upper()
    return (s == "M").astype(np.int8).to_numpy()


def _prepare_features_labels(df: pd.DataFrame, expected_headers: list[str]) -> tuple[np.ndarray, np.ndarray]:
    df = df.copy()
    df = df.fillna(0)

    if "diagnosis" not in df.columns:
        raise ValueError("Missing required column: diagnosis")

    y = _encode_diagnosis(df["diagnosis"])

    feature_names = [c for c in expected_headers[2:] if c in df.columns]
    if not feature_names:
        raise ValueError("No expected feature columns found in dataset.")

    X_df = df[feature_names]
    X = X_df.to_numpy(dtype=np.float64, copy=False)
    return X, y


def _add_bias_column(X: np.ndarray) -> np.ndarray:
    return np.concatenate((np.ones((X.shape[0], 1), dtype=X.dtype), X), axis=1)


def _sigmoid(z: np.ndarray) -> np.ndarray:
    z = np.clip(z, -50.0, 50.0)
    return 1.0 / (1.0 + np.exp(-z))


def _gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, num_iterations: int) -> np.ndarray:
    m, n = X.shape
    theta = np.zeros(n, dtype=np.float64)
    y = y.astype(np.float64, copy=False)

    for _ in range(num_iterations):
        h = _sigmoid(X @ theta)
        theta -= (alpha / m) * (X.T @ (h - y))

    return theta


def _predict_labels(X: np.ndarray, theta: np.ndarray) -> np.ndarray:
    probs = _sigmoid(X @ theta)
    return np.rint(probs).astype(np.int8)


def main() -> None:
    np.random.seed(RANDOM_SEED)

    df = _read_csv_robust(DATASET_PATH, DATASET_HEADERS)
    X, y = _prepare_features_labels(df, DATASET_HEADERS)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_SEED
    )

    X_train_b = _add_bias_column(X_train)
    X_test_b = _add_bias_column(X_test)

    theta = _gradient_descent(X_train_b, y_train, alpha=0.01, num_iterations=1000)

    test_preds = _predict_labels(X_test_b, theta)
    accuracy = float(np.mean(test_preds == y_test))
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused heavy visualization imports and all plotting/logging to cut import time and overhead.
# - Implemented robust CSV parsing with a fallback separator/decimal pass to avoid repeated manual fixes and ensure reliability.
# - Replaced LabelEncoder with a deterministic vectorized encoding (M->1, else->0) to reduce sklearn overhead and data movement.
# - Derived feature columns from DATASET_HEADERS intersected with df.columns to avoid hard-coded positional slicing and improve robustness.
# - Avoided repeated cost computation and storing per-iteration costs (unused) to reduce redundant computation and memory allocations.
# - Used in-place/zero-copy conversions where safe (to_numpy(copy=False), astype(copy=False)) to reduce memory footprint.
# - Vectorized linear algebra with @ and used a numerically safe sigmoid with clipping to prevent overflow without changing outputs materially.
# - Centralized bias addition and prediction into small functions to prevent repeated intermediate structures and keep code modular/reusable.
# - Set fixed random seeds and train_test_split random_state for reproducible, stable results.