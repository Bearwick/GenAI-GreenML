# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split


def _sigmoid(z):
    z = np.clip(z, -500.0, 500.0)
    return 1.0 / (1.0 + np.exp(-z))


def _gradient_descent(X, y, theta, alpha, num_iterations):
    m = y.shape[0]
    for _ in range(num_iterations):
        h = _sigmoid(X @ theta)
        theta -= alpha * ((X.T @ (h - y)) / m)
    return theta


def _predict_proba_with_bias(X, theta):
    Xb = np.concatenate((np.ones((X.shape[0], 1), dtype=X.dtype), X), axis=1)
    return _sigmoid(Xb @ theta)


def predict(X, theta):
    return (_predict_proba_with_bias(X, theta) >= 0.5).astype(np.int64)


def main():
    df = pd.read_csv("data.csv")
    df = df.fillna(0)

    diag = df["diagnosis"].to_numpy()
    y = (diag == "M").astype(np.float64)

    X = df.iloc[:, 2:32].to_numpy(dtype=np.float64, copy=False)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=2, shuffle=True
    )

    X_train_b = np.concatenate(
        (np.ones((X_train.shape[0], 1), dtype=X_train.dtype), X_train), axis=1
    )
    X_test_b = np.concatenate(
        (np.ones((X_test.shape[0], 1), dtype=X_test.dtype), X_test), axis=1
    )

    theta = np.zeros(X_train_b.shape[1], dtype=np.float64)
    theta = _gradient_descent(X_train_b, y_train, theta, alpha=0.01, num_iterations=1000)

    test_probs = _sigmoid(X_test_b @ theta)
    test_preds = (test_probs >= 0.5).astype(np.float64)
    accuracy = float(np.mean(test_preds == y_test))
    print(f"ACCURACY={accuracy:.6f}")

    example = np.array(
        [
            17.14,
            16.4,
            116,
            912.7,
            0.1186,
            0.2276,
            0.2229,
            0.1401,
            0.304,
            0.07413,
            1.046,
            0.976,
            7.276,
            111.4,
            0.008029,
            0.03799,
            0.03732,
            0.02397,
            0.02308,
            0.007444,
            22.25,
            21.4,
            152.4,
            1461,
            0.1545,
            0.3949,
            0.3853,
            0.255,
            0.4066,
            0.1059,
        ],
        dtype=np.float64,
    ).reshape(1, -1)
    _ = predict(example, theta)


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# Removed unused visualization imports to reduce import time and memory overhead.
# Avoided LabelEncoder; encoded diagnosis via direct boolean comparison (diag == "M") for less work and fewer dependencies.
# Used NumPy arrays early (to_numpy with dtype) to reduce pandas overhead and data movement during training.
# Eliminated cost computation and cost history storage in gradient descent to avoid redundant sigmoid/log operations and list growth.
# Replaced np.hstack with np.concatenate and ensured dtypes align to reduce intermediate copies.
# Added sigmoid input clipping for numerical stability, preventing overflow without changing decision behavior at typical ranges.
# Used matrix multiplication operator (@) for clarity and potentially better BLAS dispatch.
# Kept example prediction but removed all prints/side effects except the required accuracy print to reduce I/O energy cost.