# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: assisted

import numpy as np
    import pandas as pd
    from sklearn.model_selection import train_test_split

    def sigmoid(z):
        return 1 / (1 + np.exp(-z))

    def train_logistic_regression(X, y, alpha=0.01, num_iterations=1000):
        m, n = X.shape
        theta = np.zeros(n)
        for _ in range(num_iterations):
            h = 1 / (1 + np.exp(-X.dot(theta)))
            gradient = (X.T @ (h - y)) / m
            theta -= alpha * gradient
        return theta

    def run_pipeline():
        df = pd.read_csv("data.csv").fillna(0)
        
        y = (df.iloc[:, 1] == 'M').astype(int).values
        X = df.iloc[:, 2:32].values

        X_train, X_test, Y_train, Y_test = train_test_split(
            X, y, test_size=0.2, random_state=2
        )

        X_train_b = np.column_stack((np.ones(X_train.shape[0]), X_train))
        X_test_b = np.column_stack((np.ones(X_test.shape[0]), X_test))

        theta = train_logistic_regression(X_train_b, Y_train, alpha=0.01, num_iterations=1000)

        test_preds = (sigmoid(X_test_b.dot(theta)) >= 0.5).astype(int)
        accuracy = np.mean(test_preds == Y_test)

        print(f"Training accuracy: {np.mean((sigmoid(X_train_b.dot(theta)) >= 0.5).astype(int) == Y_train):.4f}")
        print(f"Testing accuracy:  {accuracy:.4f}")
        print(f"ACCURACY={accuracy:.6f}")

        example = np.array([
            17.14,16.4,116,912.7,0.1186,0.2276,0.2229,0.1401,0.304,0.07413,
            1.046,0.976,7.276,111.4,0.008029,0.03799,0.03732,0.02397,
            0.02308,0.007444,22.25,21.4,152.4,1461,0.1545,0.3949,
            0.3853,0.255,0.4066,0.1059
        ]).reshape(1, -1)
        
        ex_b = np.column_stack((np.ones(1), example))
        pred = (sigmoid(ex_b.dot(theta)) >= 0.5).astype(int)
        print("Prediction:", "Malignant" if pred[0] == 1 else "Benign")

    if __name__ == "__main__":
        run_pipeline()

    # OPTIMIZATIONS APPLIED:
    # 1. Minimized Redundant Computation: Removed the cost function calculation inside the gradient descent loop. 
    #    The original code performed log and sum operations 1000 times which were never used for optimization logic.
    # 2. Reduced Resource Overhead: Removed unused imports (seaborn, matplotlib) and suppressed all visualization logic.
    # 3. Streamlined Preprocessing: Replaced LabelEncoder with a direct boolean comparison/cast, reducing dependency overhead.
    # 4. Optimized Data Movement: Used np.column_stack and vectorized matmul (@) which are more memory-efficient than 
    #    repeatedly creating intermediate structures.
    # 5. Memory Footprint: Eliminated the storage of the 'costs' list which previously grew linearly with the number of iterations.
