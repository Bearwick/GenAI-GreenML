# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv("data.csv")
df.fillna(0, inplace=True)

y = (df.iloc[:, 1] == 'M').astype(np.int8).values
X = df.iloc[:, 2:32].values.astype(np.float64)

X_train, X_test, Y_train, Y_test = train_test_split(
    X, y, test_size=0.2, random_state=2
)

X_train = np.column_stack((np.ones(X_train.shape[0]), X_train))
X_test = np.column_stack((np.ones(X_test.shape[0]), X_test))

def train_logistic_regression(X, y, alpha=0.01, iterations=1000):
    m, n = X.shape
    theta = np.zeros(n)
    for _ in range(iterations):
        h = 1 / (1 + np.exp(-(X @ theta)))
        gradient = (X.T @ (h - y)) / m
        theta -= alpha * gradient
    return theta

theta = train_logistic_regression(X_train, Y_train, alpha=0.01, iterations=1000)

train_probs = 1 / (1 + np.exp(-(X_train @ theta)))
test_probs = 1 / (1 + np.exp(-(X_test @ theta)))

train_acc = np.mean((train_probs >= 0.5).astype(int) == Y_train)
test_acc = np.mean((test_probs >= 0.5).astype(int) == Y_test)

print(f"Training accuracy: {train_acc:.4f}")
print(f"Testing accuracy:  {test_acc:.4f}")
print(f"ACCURACY={test_acc:.6f}")

example = np.array([
    17.14,16.4,116,912.7,0.1186,0.2276,0.2229,0.1401,0.304,0.07413,
    1.046,0.976,7.276,111.4,0.008029,0.03799,0.03732,0.02397,
    0.02308,0.007444,22.25,21.4,152.4,1461,0.1545,0.3949,
    0.3853,0.255,0.4066,0.1059
]).reshape(1, -1)

example_with_bias = np.insert(example, 0, 1)
pred_prob = 1 / (1 + np.exp(-(example_with_bias @ theta)))
pred = 1 if pred_prob >= 0.5 else 0

print("Prediction:", "Malignant" if pred == 1 else "Benign")

# OPTIMIZATIONS APPLIED:
# 1. Removed redundant cost calculation: The original code calculated the sigmoid and log-loss every iteration inside the loop, 
#    effectively doubling the computational workload. Removing this reduces complexity by O(Iterations * N_Samples).
# 2. Vectorized math: Replaced .dot() and manual summation with @ operator and optimized NumPy routines for faster execution.
# 3. Efficient Encoding: Replaced sklearn's LabelEncoder with a direct boolean mapping (df.iloc[:, 1] == 'M'), 
#    reducing memory overhead and library dependency.
# 4. Reduced Data Movement: Used column_stack for bias addition to handle memory allocation in one step rather than repeated hstacking.
# 5. Minimized Memory Footprint: Removed unused visualization libraries (Seaborn, Matplotlib) and eliminated unnecessary 
#    intermediate lists (like the 'costs' list) to free up RAM.
# 6. Type optimization: Cast labels to int8 to save space on large label arrays.
