# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv("data.csv")
df.fillna(0, inplace=True)

y = (df.iloc[:, 1] == 'M').astype(int).values
X = df.iloc[:, 2:32].values

X_train, X_test, Y_train, Y_test = train_test_split(
    X, y, test_size=0.2, random_state=2
)

X_train = np.column_stack((np.ones(X_train.shape[0]), X_train))
X_test = np.column_stack((np.ones(X_test.shape[0]), X_test))

def train_logistic_regression(X, y, alpha, num_iterations):
    m, n = X.shape
    theta = np.zeros(n)
    for _ in range(num_iterations):
        z = np.dot(X, theta)
        h = 1 / (1 + np.exp(-np.clip(z, -250, 250)))
        gradient = np.dot(X.T, (h - y)) / m
        theta -= alpha * gradient
    return theta

theta = train_logistic_regression(X_train, Y_train, alpha=0.01, num_iterations=1000)

def predict(data, theta, has_bias=False):
    if not has_bias:
        data = np.column_stack((np.ones(data.shape[0]), data))
    z = np.dot(data, theta)
    h = 1 / (1 + np.exp(-np.clip(z, -250, 250)))
    return (h >= 0.5).astype(int)

test_preds = predict(X_test, theta, has_bias=True)
accuracy = np.mean(test_preds == Y_test)

print(f"ACCURACY={accuracy:.6f}")

example = np.array([
    17.14,16.4,116,912.7,0.1186,0.2276,0.2229,0.1401,0.304,0.07413,
    1.046,0.976,7.276,111.4,0.008029,0.03799,0.03732,0.02397,
    0.02308,0.007444,22.25,21.4,152.4,1461,0.1545,0.3949,
    0.3853,0.255,0.4066,0.1059
]).reshape(1, -1)
_ = predict(example, theta)

"""
OPTIMIZATIONS APPLIED:
1. Minimized Redundant Computation: Removed the 'compute_cost' function and its call inside the training loop, saving O(Iterations * M * N) operations.
2. Efficient Label Encoding: Replaced sklearn's LabelEncoder with a direct vectorized comparison '(df.iloc[:, 1] == 'M').astype(int)', reducing memory overhead and dependency overhead.
3. Reduced Data Movement: Used 'np.column_stack' for single-pass bias addition and avoided creating multiple intermediate lists (like the costs list).
4. Improved Numerical Stability: Integrated 'np.clip' within the sigmoid logic to prevent overflow errors without requiring additional preprocessing layers.
5. Inlined Logic: Vectorized the prediction and gradient calculations to leverage NumPy's BLAS optimizations, reducing Python-level loop overhead.
6. Memory Management: Eliminated unused variables, plots, and intermediate data structures to keep the memory footprint minimal.
"""