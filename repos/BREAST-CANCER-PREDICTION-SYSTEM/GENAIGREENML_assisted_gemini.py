# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv("data.csv")
df.fillna(0, inplace=True)
df['diagnosis'] = (df['diagnosis'] == 'M').astype(np.int8)

X = df.iloc[:, 2:32].values.astype(np.float32)
y = df.iloc[:, 1].values.astype(np.float32)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=2
)

X_train = np.concatenate([np.ones((X_train.shape[0], 1), dtype=np.float32), X_train], axis=1)
X_test = np.concatenate([np.ones((X_test.shape[0], 1), dtype=np.float32), X_test], axis=1)

theta = np.zeros(X_train.shape[1], dtype=np.float32)
alpha = np.float32(0.01)
m_inv = np.float32(1.0 / X_train.shape[0])

for _ in range(1000):
    z = X_train @ theta
    h = 1.0 / (1.0 + np.exp(-z))
    gradient = (X_train.T @ (h - y_train)) * m_inv
    theta -= alpha * gradient

z_test = X_test @ theta
test_preds = 1.0 / (1.0 + np.exp(-z_test)) >= 0.5
accuracy = np.mean(test_preds == y_test)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Reduced Memory Footprint: Converted feature matrices and labels to float32, reducing memory usage by 50% compared to float64.
# 2. Removed Redundant Computation: Eliminated the expensive compute_cost function inside the training loop, saving O(Iterations * Samples) of logarithmic and summation operations.
# 3. Efficient Preprocessing: Replaced the LabelEncoder class with a direct boolean-to-integer mapping for faster label encoding.
# 4. Optimized Linear Algebra: Leveraged the @ operator to utilize NumPy's optimized BLAS/LAPACK backend for matrix multiplications.
# 5. Minimized Data Movement: Avoided repeated array stacking and list appending (like the 'costs' list) to reduce memory allocation overhead.
# 6. Inlined Logical Steps: Combined sigmoid activation and gradient updates into a single vectorized flow to minimize Python function call overhead.
# 7. Simplified Pre-calculation: Pre-calculated the inverse of the sample size (m_inv) to replace division with multiplication within the training loop.