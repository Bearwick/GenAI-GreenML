# Generated by generate_llm_code_2.py
# LLM: gemini
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv("data.csv", usecols=range(1, 32))
df.fillna(0, inplace=True)

y = (df.iloc[:, 0] == 'M').astype(np.float32).values
X = df.iloc[:, 1:].values.astype(np.float32)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

X_train_b = np.column_stack((np.ones(X_train.shape[0], dtype=np.float32), X_train))
X_test_b = np.column_stack((np.ones(X_test.shape[0], dtype=np.float32), X_test))

def train_logistic_regression(X, y, alpha=0.01, iters=1000):
    theta = np.zeros(X.shape[1], dtype=np.float32)
    m = len(y)
    Xt = X.T 
    for _ in range(iters):
        h = 1.0 / (1.0 + np.exp(-(X @ theta)))
        theta -= (alpha / m) * (Xt @ (h - y))
    return theta

theta = train_logistic_regression(X_train_b, y_train)

train_preds = (1.0 / (1.0 + np.exp(-(X_train_b @ theta)))) >= 0.5
test_preds = (1.0 / (1.0 + np.exp(-(X_test_b @ theta)))) >= 0.5

train_acc = np.mean(train_preds == y_train)
test_acc = np.mean(test_preds == y_test)

print(f"Training accuracy: {train_acc:.4f}")
print(f"Testing accuracy:  {test_acc:.4f}")
print(f"ACCURACY={test_acc:.6f}")

def predict(input_data, theta):
    z = theta[0] + (input_data @ theta[1:])
    return (1.0 / (1.0 + np.exp(-z)) >= 0.5).astype(int)

example = np.array([[
    17.14,16.4,116,912.7,0.1186,0.2276,0.2229,0.1401,0.304,0.07413,
    1.046,0.976,7.276,111.4,0.008029,0.03799,0.03732,0.02397,
    0.02308,0.007444,22.25,21.4,152.4,1461,0.1545,0.3949,
    0.3853,0.255,0.4066,0.1059
]], dtype=np.float32)

pred = predict(example, theta)
print("Prediction:", "Malignant" if pred[0] == 1 else "Benign")

"""
OPTIMIZATIONS APPLIED:
1. Reduced Data Movement: Used 'usecols' in pd.read_csv to only load the 31 required columns, skipping the 'id' column entirely.
2. Memory Footprint: Converted feature matrices to float32 instead of the default float64, reducing memory usage by 50%.
3. Simplified Preprocessing: Replaced LabelEncoder with a direct boolean comparison and cast, removing the need to import sklearn.preprocessing.
4. Minimized Redundant Computation: Removed the compute_cost function call from inside the gradient descent loop. Calculating log-loss every iteration is computationally expensive and unnecessary for weight convergence.
5. Algorithmic Efficiency: Transposed the feature matrix (X.T) once outside the loop rather than implicitly or repeatedly within it.
6. Reduced Overhead: Removed unused imports (seaborn, matplotlib) and removed unnecessary helper functions (sigmoid) to reduce stack frames and startup time.
7. Optimized Data Structures: Used np.column_stack instead of hstack to add bias, and used the '@' operator for optimized BLAS matrix multiplication.
"""