# Generated by generate_llm_code.py
# LLM: groq
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from joblib import dump

# Load data
df = pd.read_csv("data.csv")
df.fillna(0, inplace=True)
df["diagnosis"] = df["diagnosis"].map({"M": 1, "B": 0})

# Features and labels
X = df.iloc[:, 2:].values.astype(np.float32)
Y = df["diagnosis"].values.astype(np.float32)

# Train-test split
X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.2, random_state=2
)

# Add bias term
X_train = np.hstack((np.ones((X_train.shape[0], 1), dtype=np.float32), X_train))
X_test = np.hstack((np.ones((X_test.shape[0], 1), dtype=np.float32), X_test))

# Logistic regression from scratch
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def gradient_descent(X, y, theta, alpha, iterations):
    m = y.size
    for _ in range(iterations):
        z = X.dot(theta)
        h = sigmoid(z)
        theta -= alpha * (X.T.dot(h - y) / m)
    return theta

theta = np.zeros(X_train.shape[1], dtype=np.float32)
alpha = 0.01
iterations = 1000

theta = gradient_descent(X_train, Y_train, theta, alpha, iterations)

# Predictions
train_preds = (sigmoid(X_train.dot(theta)) >= 0.5).astype(np.int32)
test_preds = (sigmoid(X_test.dot(theta)) >= 0.5).astype(np.int32)

train_acc = np.mean(train_preds == Y_train)
test_acc = np.mean(test_preds == Y_test)

print(f"ACCURACY={test_acc:.6f}")

# Save model parameters
dump(theta, "logreg_theta.joblib")