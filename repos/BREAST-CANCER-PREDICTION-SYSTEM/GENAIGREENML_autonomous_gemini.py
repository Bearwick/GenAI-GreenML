# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split

data = pd.read_csv("data.csv")
data = data.iloc[:, :32].dropna()

le = LabelEncoder()
y = le.fit_transform(data.iloc[:, 1])
X = data.iloc[:, 2:32].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

clf = LogisticRegression(solver='lbfgs', max_iter=1000, tol=1e-4)
clf.fit(X_train, y_train)

accuracy = clf.score(X_test, y_test)
print(f"ACCURACY={accuracy:.6f}")

"""
JUSTIFICATION FOR ENERGY-EFFICIENT DESIGN:
1. Model Selection: Logistic Regression was chosen over Deep Learning or Ensemble methods because it has a significantly smaller computational footprint (O(n) complexity) and requires fewer FLOPs for both training and inference on small tabular datasets.
2. Solver Optimization: The 'lbfgs' solver is an optimized second-order optimization algorithm that converges faster than standard gradient descent, reducing the total CPU cycles required.
3. Feature Scaling: Implementing StandardScaler ensures the loss surface is well-conditioned, allowing the optimizer to reach convergence in significantly fewer iterations, directly conserving energy.
4. Library Efficiency: Scikit-learn utilizes vectorized operations and optimized C/Fortran backends (BLAS/LAPACK), which are more power-efficient than manual Python loops for matrix manipulations.
5. Resource Constraints: The pipeline is designed for CPU execution with minimal memory overhead, avoiding the high energy cost associated with GPU initialization and data transfer.
"""
