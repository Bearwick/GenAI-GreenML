# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# Robust CSV parsing
def load_dataset(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError("Possible delimiter mismatch")
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    
    # Normalize column names: strip whitespace, collapse internal spaces
    df.columns = [" ".join(str(c).strip().split()) for c in df.columns]
    # Drop "Unnamed" artifacts
    df = df.loc[:, ~df.columns.str.contains('^Unnamed', case=False)]
    return df

def build_and_evaluate():
    # Load
    df = load_dataset('data.csv')
    if df.empty:
        return

    # Identify Target
    # Priority: 1. 'diagnosis' 2. Second column (common in this dataset) 3. Any binary col
    target_col = None
    if 'diagnosis' in df.columns:
        target_col = 'diagnosis'
    else:
        # Heuristic: look for a column with 2 unique values
        for col in df.columns:
            if df[col].nunique() == 2:
                target_col = col
                break
        if not target_col:
            target_col = df.columns[1] # Fallback to second column

    # Drop ID column if exists
    id_cols = ['id', 'uuid', 'index']
    features_to_drop = [target_col]
    for c in df.columns:
        if c.lower() in id_cols:
            features_to_drop.append(c)
    
    # Separate Features and Target
    X = df.drop(columns=[c for c in features_to_drop if c in df.columns])
    y = df[target_col]

    # Handle Target Encoding
    if y.dtype == 'object' or y.nunique() <= 10:
        le = LabelEncoder()
        y = le.fit_transform(y.astype(str))
        is_classification = True
    else:
        # Fallback to regression if target is continuous
        y = pd.to_numeric(y, errors='coerce')
        valid_idx = ~np.isnan(y)
        X = X[valid_idx]
        y = y[valid_idx]
        is_classification = False

    # Force numeric features
    for col in X.columns:
        if X[col].dtype == 'object':
            try:
                X[col] = pd.to_numeric(X[col], errors='coerce')
            except:
                pass
    
    # Keep only numeric features for this lightweight baseline
    X = X.select_dtypes(include=[np.number])

    # Defensive check
    if X.empty or len(y) < 10:
        print("ACCURACY=0.000000")
        return

    # Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if is_classification else None
    )

    # Pipeline: Simple Imputer + Scaler + Lightweight Model
    if is_classification:
        # Logistic Regression is highly energy-efficient and CPU-friendly
        # liblinear is efficient for small/medium datasets
        clf = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000)
    else:
        # If regression fallback occurs, use Ridge (linear with l2)
        from sklearn.linear_model import Ridge
        clf = Ridge(random_state=42)

    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler()),
        ('model', clf)
    ])

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)

    # Metric Calculation
    if is_classification:
        accuracy = accuracy_score(y_test, y_pred)
    else:
        # Convert R^2 to a bounded 0-1 metric for the "ACCURACY" print requirement
        from sklearn.metrics import r2_score
        r2 = r2_score(y_test, y_pred)
        accuracy = max(0, min(1, r2)) # Clamp to [0, 1]

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    build_and_evaluate()

# Optimization Summary:
# 1. Model Choice: Logistic Regression (liblinear) was selected as it is computationally inexpensive, 
#    converges quickly on CPU, and has a very small memory footprint compared to ensembles or DL.
# 2. Preprocessing: Standardized using StandardScaler to ensure fast convergence for the linear solver.
# 3. Data Integrity: Robust CSV loading handles different delimiters and decimal separators automatically.
# 4. Energy Efficiency: Avoided complex feature engineering or hyperparameter grid searches which 
#    exponentially increase CPU cycles. Median imputation and scaling are O(n) operations.
# 5. Robustness: The pipeline filters out non-numeric columns and "Unnamed" artifacts common in 
#    public datasets, and includes a regression fallback to ensure the script never hard-fails.
# 6. Memory: Used sklearn Pipelines to prevent data leakage and manage memory by processing 
#    transformations in-place during the fit/predict cycle.