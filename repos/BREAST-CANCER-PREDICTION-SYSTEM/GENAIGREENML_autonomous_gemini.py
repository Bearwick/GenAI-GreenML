# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Data is loaded from data.csv which is expected to contain the provided headers
df = pd.read_csv('data.csv')

# Dropping the ID column as it carries no predictive power and increases memory footprint
df = df.drop(columns=['id'], errors='ignore')

# Mapping diagnosis to binary integers (M=1, B=0) for efficient numerical processing
df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})

# Separating features and target variable
X = df.drop(columns=['diagnosis'])
y = df['diagnosis']

# Using a stratified split to maintain class balance in small datasets with minimal compute
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Feature scaling is used to ensure rapid convergence of the optimization algorithm
# This reduces the number of CPU cycles required for training
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Logistic Regression is selected for its high energy efficiency and low FLOPs requirements
# It provides a linear decision boundary which is sufficient for this specific dataset size
# LBFGS solver is used as it is memory-efficient and converges quickly on small datasets
model = LogisticRegression(solver='lbfgs', max_iter=500, random_state=42)
model.fit(X_train, y_train)

# Inference is performed on CPU with O(features) time complexity per sample
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Model Selection: Logistic Regression was chosen because it is significantly more 
#    energy-efficient than ensemble methods or neural networks for small tabular datasets, 
#    requiring minimal CPU and memory resources.
# 2. Computational Efficiency: The model uses the LBFGS optimization algorithm, which 
#    minimizes memory usage and provides fast convergence compared to stochastic gradient descent.
# 3. Dimensionality: Non-predictive features like 'id' were removed to reduce the feature 
#    matrix size and prevent unnecessary computations.
# 4. Scaling: StandardScaler was applied to normalize inputs, which directly reduces the 
#    number of iterations required for the solver to converge, saving energy.
# 5. Hardware Compatibility: The entire solution is designed for lightweight execution 
#    on standard CPUs without the need for high-power GPUs or specialized accelerators.
# 6. Minimal Dependencies: The solution relies on standard, highly-optimized libraries 
#    (pandas, scikit-learn) to ensure low overhead during the execution lifecycle.