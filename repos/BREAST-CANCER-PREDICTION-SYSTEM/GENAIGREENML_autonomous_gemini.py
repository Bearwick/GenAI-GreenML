# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

df = pd.read_csv("data.csv")

if 'Unnamed: 32' in df.columns:
    df = df.drop(columns=['Unnamed: 32'])
df = df.drop(columns=['id'])

le = LabelEncoder()
df['diagnosis'] = le.fit_transform(df['diagnosis'])

X = df.drop('diagnosis', axis=1)
y = df['diagnosis']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = LogisticRegression(solver='liblinear', max_iter=1000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

"""
JUSTIFICATION FOR DESIGN DECISIONS:
1. Model Choice: Logistic Regression was selected because it is a linear model with minimal computational complexity (O(n_features * n_samples)). For a 30-feature tabular dataset, it provides high performance without the energy overhead of deep learning or complex ensembles.
2. Energy Efficiency: We use scikit-learn's optimized C-based implementation (liblinear), which is significantly more energy-efficient than manual Python loops.
3. Preprocessing: Feature scaling (StandardScaler) is implemented to ensure the optimization algorithm converges rapidly, minimizing the number of CPU cycles required during the training phase.
4. Resource Management: The pipeline avoids heavy libraries (TensorFlow/Torch) and large-scale embeddings, ensuring the entire process can run on a low-power CPU with minimal memory footprint.
5. Removal of Waste: All non-essential computations, such as visualizations and iterative manual checks, have been eliminated to reduce the carbon footprint of the execution.
"""
