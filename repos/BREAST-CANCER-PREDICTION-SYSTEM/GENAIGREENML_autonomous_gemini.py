# Generated by generate_llm_code_2.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

df = pd.read_csv("data.csv")
df = df.iloc[:, :32]
df.fillna(0, inplace=True)

le = LabelEncoder()
y = le.fit_transform(df.iloc[:, 1])
X = df.iloc[:, 2:32].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=2
)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = LogisticRegression(max_iter=100, solver='lbfgs', tol=1e-4)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

"""
JUSTIFICATION FOR ENERGY-EFFICIENT DESIGN:
1. Algorithm Selection: Logistic Regression was chosen over Deep Learning or Ensemble methods because it is computationally lightweight (O(n) complexity) and requires minimal memory, making it ideal for small-scale tabular datasets.
2. Optimized Optimization: Instead of manual Python-based gradient descent loops which are CPU-intensive, this implementation uses Scikit-learn's LBFGS solver. This utilizes optimized C-extensions to achieve convergence faster and with lower energy consumption.
3. Feature Scaling: Applying StandardScaler ensures the cost function is spherical, allowing the solver to converge in significantly fewer iterations, directly reducing the total CPU cycles required.
4. Data Pipeline: The preprocessing is limited to essential transformations (Label Encoding and Scaling). By avoiding complex feature engineering or high-dimensional embeddings, we minimize the RAM footprint and intermediate data storage requirements.
5. Hardware Efficiency: The solution is designed to run on a single CPU core without the need for high-wattage GPUs or specialized AI accelerators, promoting a lower carbon footprint for model training and inference.
"""