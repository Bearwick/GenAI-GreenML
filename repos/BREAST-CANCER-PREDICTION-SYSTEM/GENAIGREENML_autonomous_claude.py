# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings("ignore")

# ----------------------------
# Robust CSV loading
# ----------------------------
try:
    df = pd.read_csv("data.csv")
    if df.shape[1] < 3:
        raise ValueError("Too few columns")
except Exception:
    try:
        df = pd.read_csv("data.csv", sep=';', decimal=',')
    except Exception:
        df = pd.read_csv("data.csv", sep='\t')

# ----------------------------
# Column name normalization
# ----------------------------
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
df = df[[c for c in df.columns if not c.lower().startswith('unnamed')]]

# ----------------------------
# Identify target and features
# ----------------------------
target_col = None
expected_target = 'diagnosis'
for c in df.columns:
    if c.lower() == expected_target:
        target_col = c
        break

if target_col is None:
    # Fallback: pick first non-numeric or low-cardinality column as target
    for c in df.columns:
        if df[c].dtype == object or df[c].nunique() < 10:
            target_col = c
            break
    if target_col is None:
        target_col = df.columns[-1]

# Encode target if needed
le = LabelEncoder()
if df[target_col].dtype == object:
    df[target_col] = le.fit_transform(df[target_col].astype(str))
else:
    df[target_col] = pd.to_numeric(df[target_col], errors='coerce')

# Drop rows where target is NaN
df = df.dropna(subset=[target_col])

# ----------------------------
# Feature selection
# ----------------------------
drop_cols = [target_col]
# Drop 'id' column if present
for c in df.columns:
    if c.lower() == 'id':
        drop_cols.append(c)

feature_cols = [c for c in df.columns if c not in drop_cols]

# Coerce all feature columns to numeric, drop non-convertible
for c in feature_cols:
    df[c] = pd.to_numeric(df[c], errors='coerce')

# Remove columns that are entirely NaN after coercion
valid_feature_cols = [c for c in feature_cols if df[c].notna().sum() > 0]
feature_cols = valid_feature_cols

# Fill remaining NaN with column median
df[feature_cols] = df[feature_cols].fillna(df[feature_cols].median())

# Replace inf values
df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)
df[feature_cols] = df[feature_cols].fillna(df[feature_cols].median())

# ----------------------------
# Defensive checks
# ----------------------------
assert len(df) > 0, "Dataset is empty after preprocessing"
assert len(feature_cols) > 0, "No valid feature columns"

X = df[list(feature_cols)].values
y = df[target_col].values.astype(int)

n_classes = len(np.unique(y))
assert n_classes >= 2, "Need at least 2 classes for classification"

# ----------------------------
# Train/test split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=2, stratify=y
)

assert len(X_train) > 0 and len(X_test) > 0, "Empty train or test split"

# ----------------------------
# Lightweight pipeline: StandardScaler + LogisticRegression
# ----------------------------
pipeline = Pipeline([