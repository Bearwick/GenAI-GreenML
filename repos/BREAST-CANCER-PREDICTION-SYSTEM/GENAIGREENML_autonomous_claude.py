# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("input.csv")

# Drop id column as it is not a feature
df = df.drop(columns=["id"])

# Encode target: M=1, B=0
le = LabelEncoder()
df["diagnosis"] = le.fit_transform(df["diagnosis"])

# Separate features and target
X = df.drop(columns=["diagnosis"]).values
y = df["diagnosis"].values

# Handle any missing values by median imputation (lightweight)
col_medians = np.nanmedian(X, axis=0)
inds = np.where(np.isnan(X))
X[inds] = np.take(col_medians, inds[1])

# Standardize features for logistic regression convergence
scaler = StandardScaler()

# Use stratified 5-fold cross-validation for robust accuracy estimate
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
fold_accuracies = []

for train_idx, test_idx in skf.split(X, y):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Fit scaler on training fold only to prevent data leakage
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Logistic Regression: lightweight, interpretable, excellent for binary classification
    # saga solver handles L2 well and is efficient; max_iter sufficient for convergence
    model = LogisticRegression(
        C=1.0,
        solver="lbfgs",
        max_iter=1000,
        random_state=42
    )
    model.fit(X_train_scaled, y_train)
    preds = model.predict(X_test_scaled)
    fold_accuracies.append(accuracy_score(y_test, preds))

accuracy = np.mean(fold_accuracies)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Logistic Regression chosen: extremely lightweight, fast on CPU, well-suited for
#    binary classification on a small tabular dataset (~569 samples, 30 features).
# 2. StandardScaler used: improves convergence for logistic regression without overhead.
# 3. No deep learning or ensemble methods needed: dataset is small and linearly separable
#    features (breast cancer Wisconsin) yield high accuracy with simple linear models.
# 4. StratifiedKFold ensures robust evaluation with balanced class distribution per fold.
# 5. Median imputation as a safety net for missing values (zero overhead if none exist).
# 6. lbfgs solver is memory-efficient and fast for small-to-medium datasets with L2 penalty.
# 7. Entire pipeline runs in milliseconds on CPU with minimal memory footprint.