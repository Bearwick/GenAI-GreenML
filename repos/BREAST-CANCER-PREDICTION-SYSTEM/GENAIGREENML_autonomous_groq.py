# Generated by generate_llm_code_2.py
# LLM: groq
# Mode: autonomous

```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Energy‑efficient solution: lightweight logistic regression with batch gradient descent
# • Small dataset → simple linear model is sufficient
# • No deep learning or large embeddings
# • Standardization improves convergence and accuracy
# • Only CPU operations; no GPU or heavy libraries
# • One concise accuracy report

# ------------------------------------------------------------------
# Data loading and preprocessing
# ------------------------------------------------------------------
df = pd.read_csv("data.csv")
df.fillna(0, inplace=True)

label_encoder = LabelEncoder()
df["diagnosis"] = label_encoder.fit_transform(df["diagnosis"])  # M/B → 1/0

# Features exclude id and diagnosis; use all 30 measurements
X = df.drop(columns=["id", "diagnosis"]).values
y = df["diagnosis"].values

# Standardise features for stable optimisation
mean = X.mean(axis=0)
std = X.std(axis=0)
std[std == 0] = 1.0
X = (X - mean) / std

# Train‑test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Add bias term
X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])
X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])

# ------------------------------------------------------------------
# Logistic regression implementation
# ------------------------------------------------------------------
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def cost_function(X, y, theta):
    m = y.size
    h = sigmoid(X @ theta)
    return -(1 / m) * (y @ np.log(h + 1e-9) + (1 - y) @ np.log(1 - h + 1e-9))

def gradient_descent(X, y, theta, lr, iters):
    m = y.size
    for _ in range(iters):
        h = sigmoid(X @ theta)
        gradient = (X.T @ (h - y)) / m
        theta -= lr * gradient
    return theta

theta = np.zeros(X_train.shape[1])
learning_rate = 0.05
iterations = 500

theta = gradient_descent(X_train, y_train, theta, learning_rate, iterations)

# ------------------------------------------------------------------
# Evaluation
# ------------------------------------------------------------------
y_pred = (sigmoid(X_test @ theta) >= 0.5).astype(int)
accuracy = np.mean(y_pred == y_test)

print(f"ACCURACY={accuracy:.6f}")
```