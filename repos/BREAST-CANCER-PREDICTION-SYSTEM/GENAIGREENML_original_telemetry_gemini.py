# Generated by generate_llm_code.py
# LLM: gemini
# Mode: original_telemetry

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

df = pd.read_csv("data.csv")
df.fillna(0, inplace=True)

labelencoder_Y = LabelEncoder()
df['diagnosis'] = labelencoder_Y.fit_transform(df['diagnosis'])

X = df.iloc[:, 2:32].values
Y = df.iloc[:, 1].values

X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.2, random_state=2
)

def sigmoid(z):
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

def compute_cost(X, y, theta):
    m = len(y)
    h = sigmoid(X.dot(theta))
    return (1 / m) * np.sum(
        -y * np.log(h + 1e-9) - (1 - y) * np.log(1 - h + 1e-9)
    )

def gradient_descent(X, y, theta, alpha, num_iterations):
    m = len(y)
    for _ in range(num_iterations):
        h = sigmoid(X.dot(theta))
        gradient = (1 / m) * X.T.dot(h - y)
        theta -= alpha * gradient
    return theta

X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))
X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))

theta = np.zeros(X_train.shape[1])
alpha = 0.01
num_iterations = 1000

theta = gradient_descent(X_train, Y_train, theta, alpha, num_iterations)

test_preds = np.round(sigmoid(X_test.dot(theta))).astype(int)
accuracy = np.mean(test_preds == Y_test)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Removed unused visualization libraries (seaborn, matplotlib) and plotting code.
# 2. Eliminated original print statements and interactive example predictions.
# 3. Cleaned up original comments and consolidated the script flow.
# 4. Fixed potential overflow in the sigmoid function using np.clip to ensure numerical stability.
# 5. Implemented the required accuracy output format for the testing set.
# 6. Retained the original logistic regression logic and hyperparameters as requested.