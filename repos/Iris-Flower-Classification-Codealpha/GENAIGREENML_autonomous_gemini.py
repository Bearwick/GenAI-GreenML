# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

df = pd.read_csv('data.csv')
X = df.drop(columns=['Id', 'Species'])
y = df['Species']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = LogisticRegression(max_iter=500, solver='lbfgs')
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Model Selection: Logistic Regression was chosen as the most energy-efficient model for a small-scale, 4-feature classification task, offering minimal computational overhead.
# 2. Resource Efficiency: The solution is designed to run exclusively on CPU, avoiding the significant energy consumption associated with GPU initialization and data transfer.
# 3. Preprocessing: Standard scaling was implemented to speed up the convergence of the LBFGS optimizer, directly reducing the number of CPU cycles required for training.
# 4. Dimensionality Reduction: The 'Id' column was dropped to minimize the memory footprint and prevent the model from processing non-informative data.
# 5. Reproducibility & Stability: Used stratified splitting and fixed random states to ensure consistent results without needing multiple trial runs, further conserving energy.