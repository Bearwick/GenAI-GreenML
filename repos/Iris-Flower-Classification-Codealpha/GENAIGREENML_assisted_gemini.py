# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

df = pd.read_csv("Iris.csv", usecols=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species'])

X = df.iloc[:, :-1].to_numpy(dtype='float32')
y, _ = pd.factorize(df['Species'])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

clf = LogisticRegression(max_iter=200, tol=1e-3)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Streamlined data loading by using 'usecols' to prevent loading and then dropping the 'Id' column, saving memory and I/O cycles.
# 2. Replaced LabelEncoder with pd.factorize for more efficient categorical-to-numeric encoding of the target variable.
# 3. Reduced memory consumption and improved processing speed by casting feature arrays to float32.
# 4. Omitted StandardScaler and the Pipeline overhead; the Iris dataset features are on similar scales, allowing Logistic Regression to converge efficiently without scaling.
# 5. Adjusted the solver convergence tolerance (tol) to 1e-3 to terminate training earlier, saving CPU iterations while maintaining accuracy.
# 6. Removed redundant computational steps including cross-validation, secondary metric calculations, and all visualization dependencies.