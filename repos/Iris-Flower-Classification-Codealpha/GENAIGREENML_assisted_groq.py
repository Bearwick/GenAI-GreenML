# Generated by generate_llm_code.py
# LLM: groq
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("Iris.csv")

# Drop unused identifier column if present
df.drop(columns=[col for col in ["Id"] if col in df.columns], inplace=True)

# Encode target labels
label_encoder = LabelEncoder()
df["Species"] = label_encoder.fit_transform(df["Species"])

# Features and target
X = df.drop("Species", axis=1)
y = df["Species"]

# Train/test split with fixed random state for reproducibility
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Pipeline: standard scaling + logistic regression
pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("model", LogisticRegression(max_iter=200))
])

# Fit model
pipeline.fit(X_train, y_train)

# Predict and evaluate
y_pred = pipeline.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

# Output accuracy
print(f"ACCURACY={accuracy:.6f}")

# ---- Optimization explanations ----
# 1. Removed matplotlib, seaborn, and visualization code to avoid unnecessary plotting overhead.
# 2. Eliminated classification report and confusion matrix printing to reduce computation and output size.
# 3. Omitted cross-validation to avoid multiple training cycles, thereby saving runtime.
# 4. Used a single, lightweight pipeline instead of separate scaling and model steps, minimizing data movement.
# 5. Dropped the Id column only if it exists, avoiding extra data handling.
# 6. Kept the random_state fixed to guarantee reproducibility while still providing deterministic splits.
# 7. Limited LogisticRegression to a modest 200 iterations (as originally), preventing overâ€‘training and saving time.