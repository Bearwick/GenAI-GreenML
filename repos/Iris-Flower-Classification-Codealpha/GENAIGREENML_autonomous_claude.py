# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset - try common paths for Iris dataset
import os

data_path = None
candidates = ["dataset.csv", "data.csv", "iris.csv", "Iris.csv", "train.csv"]
for c in candidates:
    if os.path.exists(c):
        data_path = c
        break

if data_path is None:
    from sklearn.datasets import load_iris
    iris = load_iris()
    df = pd.DataFrame(iris.data, columns=["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm"])
    df["Species"] = iris.target
else:
    df = pd.read_csv(data_path)

# Drop Id column if present (not a feature)
if "Id" in df.columns:
    df = df.drop(columns=["Id"])

# Separate features and target
feature_cols = ["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm"]
X = df[feature_cols].values
y = df["Species"].values

# Encode target labels if they are strings
le = LabelEncoder()
if y.dtype == object or isinstance(y[0], str):
    y = le.fit_transform(y)

# Train/test split with stratification for reproducibility
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# StandardScaler - lightweight preprocessing, improves convergence for linear models
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Logistic Regression - ideal for small tabular classification tasks
# Lightweight, fast on CPU, excellent for Iris-scale data (150 samples, 4 features)
# Using lbfgs solver which is efficient for small multiclass problems
model = LogisticRegression(
    solver="lbfgs",
    max_iter=200,
    multi_class="multinomial",
    C=1.0,
    random_state=42
)

model.fit(X_train, y_train)

# Evaluate on test set
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Model choice: Logistic Regression is the most energy-efficient classifier
#    for this small-scale Iris dataset (150 samples, 4 numeric features, 3 classes).
#    It trains in milliseconds on CPU with minimal memory usage.
# 2. Preprocessing: StandardScaler normalizes features for stable convergence.
#    No complex feature engineering needed for well-separated Iris classes.
# 3. Solver: LBFGS is memory-efficient and fast for small multinomial problems.
# 4. No deep learning, ensembles, or heavy pipelines - intentionally minimal
#    for maximum energy efficiency while maintaining high accuracy (~96-100%).
# 5. Stratified split ensures balanced class representation in train/test sets.