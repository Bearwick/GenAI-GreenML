# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.dummy import DummyClassifier

warnings.filterwarnings("ignore")


def _normalize_columns(cols):
    norm = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        norm.append(c2)
    return norm


def _read_csv_robust(path):
    # Try default parsing first
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        # Fallback for semicolon-separated and comma decimals
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _pick_target_and_features(df, dataset_headers=None):
    cols = list(df.columns)

    # Prefer a reasonable target using provided headers if present
    target = None
    if dataset_headers:
        candidates = [h for h in dataset_headers if h in cols]
        for cand in candidates[::-1]:
            if cand.lower() not in ("id",):
                target = cand
                break

    if target is None:
        # Prefer a non-numeric column with low-ish unique count for classification
        nonnum = [c for c in cols if df[c].dtype == "object" or str(df[c].dtype).startswith("category")]
        nonnum = [c for c in nonnum if not c.lower().startswith("unnamed")]
        best = None
        best_u = None
        for c in nonnum:
            u = df[c].nunique(dropna=True)
            if u >= 2 and (best is None or u < best_u):
                best, best_u = c, u
        if best is not None:
            target = best
        else:
            # Otherwise pick a numeric column as target (regression fallback)
            num = []
            for c in cols:
                if c.lower() in ("id",) or c.lower().startswith("unnamed"):
                    continue
                s = pd.to_numeric(df[c], errors="coerce")
                if s.notna().sum() > 0 and s.nunique(dropna=True) >= 2:
                    num.append(c)
            target = num[-1] if num else cols[-1]

    # Features: all columns except target, excluding obvious index-like columns
    features = [c for c in cols if c != target and not c.lower().startswith("unnamed")]
    # Drop ID-like columns from features when present
    features = [c for c in features if c.lower() not in ("id", "index")]

    if not features:
        # If no features left, keep at least one column not equal to target (even if ID)
        features = [c for c in cols if c != target][:1]

    return target, features


def _safe_bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    if y_true.size == 0:
        return 0.0
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    y_mean = float(np.mean(y_true))
    ss_tot = float(np.sum((y_true - y_mean) ** 2))
    if ss_tot <= 0.0:
        return 0.0
    r2 = 1.0 - ss_res / ss_tot
    return float(np.clip(r2, 0.0, 1.0))


def main():
    dataset_path = "Iris.csv"
    dataset_headers = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]

    if not os.path.exists(dataset_path):
        # Minimal safe behavior: create empty-like accuracy
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _read_csv_robust(dataset_path)

    # Normalize/clean column names
    df.columns = _normalize_columns(df.columns)
    df = df.loc[:, [c for c in df.columns if not str(c).startswith("Unnamed:")]]

    # Drop completely empty rows
    df = df.dropna(how="all").reset_index(drop=True)

    assert df.shape[0] > 0, "Dataset is empty after basic cleaning."

    target_col, feature_cols = _pick_target_and_features(df, dataset_headers=dataset_headers)

    # Build X/y with defensive copying
    X = df[feature_cols].copy()
    y = df[target_col].copy()

    # Identify numeric vs categorical from X
    numeric_features = []
    categorical_features = []
    for c in X.columns:
        s_num = pd.to_numeric(X[c], errors="coerce")
        # If most values are numeric-coercible, treat as numeric
        valid_ratio = float(s_num.notna().mean()) if len(s_num) else 0.0
        if valid_ratio >= 0.8:
            numeric_features.append(c)
            X[c] = s_num
        else:
            categorical_features.append(c)

    # Decide task type (classification preferred)
    # If y is object/category => classification; else numeric => classification if low unique count
    y_is_object = (y.dtype == "object") or str(y.dtype).startswith("category")
    y_numeric = pd.to_numeric(y, errors="coerce")
    if y_is_object:
        task = "classification"
    else:
        non_na = y_numeric.dropna()
        uniq = int(non_na.nunique()) if non_na.size else 0
        task = "classification" if 2 <= uniq <= max(20, int(0.2 * len(non_na) + 1)) else "regression"

    if task == "classification":
        # Ensure y is clean strings (stable for sklearn)
        y_clean = y.astype(str).replace({"nan": np.nan})
        # If too many missing labels, drop them
        mask = y_clean.notna()
        X = X.loc[mask].reset_index(drop=True)
        y_clean = y_clean.loc[mask].reset_index(drop=True)

        assert len(y_clean) > 0, "No valid target labels."

        n_classes = int(y_clean.nunique(dropna=True))
        if n_classes < 2:
            # Fallback to trivial baseline that still runs end-to-end
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_clean, test_size=0.2, random_state=42, shuffle=True
            )
            assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."
            model = DummyClassifier(strategy="most_frequent")
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

        # Preprocess + lightweight linear classifier
        numeric_transformer = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler(with_mean=True, with_std=True)),
            ]
        )
        categorical_transformer = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
            ]
        )

        preprocessor = ColumnTransformer(
            transformers=[
                ("num", numeric_transformer, numeric_features),
                ("cat", categorical_transformer, categorical_features),
            ],
            remainder="drop",
            sparse_threshold=0.3,
        )

        # LogisticRegression is efficient for small datasets; lbfgs handles multinomial well
        clf = LogisticRegression(
            solver="lbfgs",
            max_iter=200,
            n_jobs=1,
        )

        pipeline = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])

        stratify = y_clean if n_classes >= 2 and len(y_clean) >= n_classes * 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_clean, test_size=0.2, random_state=42, shuffle=True, stratify=stratify
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))

    else:
        # Regression fallback (still prints ACCURACY=...) using bounded R2 proxy
        y_reg = pd.to_numeric(y, errors="coerce")
        mask = y_reg.notna()
        X = X.loc[mask].reset_index(drop=True)
        y_reg = y_reg.loc[mask].reset_index(drop=True)

        assert len(y_reg) > 1, "Not enough numeric target values for regression."

        numeric_transformer = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler(with_mean=True, with_std=True)),
            ]
        )
        categorical_transformer = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
            ]
        )
        preprocessor = ColumnTransformer(
            transformers=[
                ("num", numeric_transformer, numeric_features),
                ("cat", categorical_transformer, categorical_features),
            ],
            remainder="drop",
            sparse_threshold=0.3,
        )

        # RidgeClassifier would be classification; use linear regression-like via SGD? Keep simple:
        # Use RidgeClassifier only if y is discrete; otherwise use LinearRegression.
        # To keep lightweight & stable, we use RidgeClassifier only when unique small; else DummyRegressor proxy.
        from sklearn.linear_model import Ridge
        reg = Ridge(alpha=1.0, random_state=42)

        pipeline = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])

        X_train, X_test, y_train, y_test = train_test_split(
            X, y_reg, test_size=0.2, random_state=42, shuffle=True
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        accuracy = _safe_bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Chose a lightweight linear model (LogisticRegression) suitable for small tabular datasets; avoids heavy ensembles/boosting.
# - Used a single sklearn Pipeline + ColumnTransformer to ensure reproducible preprocessing and prevent redundant computations.
# - Numeric handling: coercion to numeric with errors='coerce', median imputation, and StandardScaler for stable optimization.
# - Categorical handling: most-frequent imputation + OneHotEncoder with sparse output to reduce memory/CPU on wide categories.
# - Robust CSV parsing fallback (default then sep=';' and decimal=',') and column name normalization for schema resilience.
# - Defensive target/feature selection if headers mismatch; drops ID-like columns to reduce noise and computation.
# - Regression fallback uses a bounded R2 proxy clipped to [0,1] to keep ACCURACY consistent and stable.