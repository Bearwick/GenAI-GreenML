# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge

warnings.filterwarnings("ignore")


def _normalize_columns(cols):
    new_cols = []
    for c in cols:
        c = str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        new_cols.append(c)
    return new_cols


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _try_read_csv(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(dfx):
        if dfx is None or dfx.empty:
            return True
        if dfx.shape[1] == 1:
            return True
        return False

    if looks_wrong(df):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if not looks_wrong(df2):
                df = df2
        except Exception:
            pass

    if df is None:
        raise RuntimeError("Failed to read CSV with supported fallbacks.")
    return df


def _pick_target_and_task(df):
    cols = list(df.columns)

    def nunique_non_nan(s):
        return int(pd.Series(s).dropna().nunique())

    preferred_targets = ["Species", "species", "target", "label", "y", "class"]
    for cand in preferred_targets:
        if cand in df.columns:
            y = df[cand]
            # Classification if discrete-ish
            nuniq = nunique_non_nan(y)
            if nuniq >= 2 and nuniq <= max(20, int(0.2 * max(1, len(y)))):
                return cand, "classification"
            # Otherwise fallback to regression
            return cand, "regression"

    # Prefer object/categorical with small number of classes
    obj_cols = [c for c in cols if df[c].dtype == "object" or str(df[c].dtype).startswith("category")]
    best_obj = None
    best_obj_nuniq = None
    for c in obj_cols:
        nuniq = nunique_non_nan(df[c])
        if 2 <= nuniq <= max(20, int(0.2 * max(1, len(df)))):
            if best_obj is None or nuniq < best_obj_nuniq:
                best_obj, best_obj_nuniq = c, nuniq
    if best_obj is not None:
        return best_obj, "classification"

    # Prefer numeric non-constant for regression
    numeric_candidates = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().any():
            nuniq = nunique_non_nan(s)
            if nuniq > 1:
                numeric_candidates.append((c, s.notna().mean(), nuniq))
    if numeric_candidates:
        numeric_candidates.sort(key=lambda t: (t[1], t[2]), reverse=True)
        return numeric_candidates[0][0], "regression"

    # Last resort: pick first column and treat as classification (may collapse)
    return cols[0], "classification"


def _safe_accuracy_proxy_from_regression(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() <= 1:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    ss_tot = float(np.sum((y_true - float(np.mean(y_true))) ** 2))
    if ss_tot <= 0.0:
        r2 = 0.0
    else:
        r2 = 1.0 - (ss_res / ss_tot)
    # Bound into [0,1] for stable "accuracy"
    acc = float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))
    return acc


# --------- Load data ---------
csv_candidates = ["Iris.csv", "iris.csv", "data.csv", "dataset.csv", "train.csv"]
csv_path = None
for p in csv_candidates:
    if os.path.exists(p):
        csv_path = p
        break
if csv_path is None:
    # Try any csv in current directory
    for fn in os.listdir("."):
        if fn.lower().endswith(".csv"):
            csv_path = fn
            break
if csv_path is None:
    raise RuntimeError("No CSV file found in current directory.")

df = _try_read_csv(csv_path)

df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)

# Drop likely ID columns (energy-efficient heuristic)
id_like = []
for c in df.columns:
    cl = c.strip().lower()
    if cl in ("id", "index"):
        id_like.append(c)
    elif cl.endswith(" id") or cl.endswith("_id"):
        id_like.append(c)
if id_like:
    df = df.drop(columns=[c for c in id_like if c in df.columns])

assert df is not None and not df.empty

target_col, task = _pick_target_and_task(df)

# Build X/y with schema robustness
feature_cols = [c for c in df.columns if c != target_col]
if not feature_cols:
    # Minimal fallback: create a constant feature to allow pipeline to run
    df["_const_feature"] = 1.0
    feature_cols = ["_const_feature"]

X = df[feature_cols].copy()
y_raw = df[target_col].copy()

# Coerce numeric columns safely; keep categoricals
numeric_cols = []
categorical_cols = []
for c in X.columns:
    s = X[c]
    if s.dtype == "object" or str(s.dtype).startswith("category"):
        # Try numeric coercion if it looks mostly numeric
        s_num = pd.to_numeric(s, errors="coerce")
        if s_num.notna().mean() >= 0.9:
            X[c] = s_num
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)
    else:
        X[c] = pd.to_numeric(s, errors="coerce")
        numeric_cols.append(c)

# Decide final task based on y properties
if task == "classification":
    y_series = y_raw
    if not (y_series.dtype == "object" or str(y_series.dtype).startswith("category")):
        # still can be numeric classes; keep as-is
        pass
    # If too many unique values, treat as regression
    nuniq_y = int(pd.Series(y_series).dropna().nunique())
    if nuniq_y < 2:
        task = "regression"
    elif nuniq_y > max(50, int(0.3 * max(1, len(y_series)))):
        task = "regression"

if task == "regression":
    y = pd.to_numeric(y_raw, errors="coerce")
else:
    # Keep labels as string/object to avoid assumptions; sklearn can handle
    y = y_raw.astype(str)

# Drop rows with missing y (after coercion for regression)
mask_y = y.notna()
X = X.loc[mask_y].reset_index(drop=True)
y = y.loc[mask_y].reset_index(drop=True)

assert len(X) > 0 and len(y) > 0

# Preprocess
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=True, with_std=True)),
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, [c for c in numeric_cols if c in X.columns]),
        ("cat", categorical_transformer, [c for c in categorical_cols if c in X.columns]),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Split
if task == "classification":
    # Stratify only if viable
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

assert len(X_train) > 0 and len(X_test) > 0

# Model (lightweight)
if task == "classification":
    # Small, CPU-friendly solver; good baseline for Iris-like data
    model = LogisticRegression(max_iter=200, solver="lbfgs", multi_class="auto")
    pipe = Pipeline(steps=[("prep", preprocessor), ("model", model)])
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Ridge is stable and cheap; avoids iterative heavy solvers
    model = Ridge(alpha=1.0, random_state=42)
    pipe = Pipeline(steps=[("prep", preprocessor), ("model", model)])
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    accuracy = _safe_accuracy_proxy_from_regression(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) for CPU efficiency and fast convergence on small datasets.
# - ColumnTransformer + Pipeline avoids repeated preprocessing and improves reproducibility.
# - Robust CSV loading retries with ; separator and , decimal to handle common locale formats without manual edits.
# - Defensive schema handling: normalizes headers, drops Unnamed columns, heuristically drops ID-like columns, and selects a viable target if expected columns are missing.
# - Numeric coercion with errors='coerce' and SimpleImputer prevents failures from mixed types/NaNs while keeping computation minimal.
# - Regression fallback uses bounded proxy ACCURACY in [0,1] derived from R^2: accuracy = clip((R^2+1)/2, 0, 1) for stable reporting.