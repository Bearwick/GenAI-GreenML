# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

RANDOM_STATE = 42

def _load_dataset() -> pd.DataFrame:
    # Prefer local CSV if available; fall back to a lightweight built-in dataset.
    for fname in ("Iris.csv", "iris.csv", "data.csv", "dataset.csv", "train.csv"):
        if os.path.exists(fname):
            return pd.read_csv(fname)
    try:
        from sklearn.datasets import load_iris
        iris = load_iris(as_frame=True)
        df = iris.frame.copy()
        df.insert(0, "Id", range(1, len(df) + 1))
        df.rename(
            columns={
                "sepal length (cm)": "SepalLengthCm",
                "sepal width (cm)": "SepalWidthCm",
                "petal length (cm)": "PetalLengthCm",
                "petal width (cm)": "PetalWidthCm",
                "target": "Species",
            },
            inplace=True,
        )
        df["Species"] = df["Species"].map({i: name for i, name in enumerate(iris.target_names)})
        return df
    except Exception:
        raise FileNotFoundError("No dataset CSV found and sklearn fallback not available.")

def main() -> None:
    df = _load_dataset()

    expected_cols = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]
    missing = [c for c in expected_cols if c not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns: {missing}")

    X = df[["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm"]].copy()
    y_raw = df["Species"].astype(str).copy()

    le = LabelEncoder()
    y = le.fit_transform(y_raw)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
    )

    numeric_features = ["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm"]

    preprocessor = ColumnTransformer(
        transformers=[
            (
                "num",
                Pipeline(
                    steps=[
                        ("imputer", SimpleImputer(strategy="median")),
                        ("scaler", StandardScaler(with_mean=True, with_std=True)),
                    ]
                ),
                numeric_features,
            )
        ],
        remainder="drop",
        sparse_threshold=0.0,
    )

    model = LogisticRegression(
        solver="lbfgs",
        max_iter=200,
        multi_class="auto",
        n_jobs=1,
        random_state=RANDOM_STATE,
    )

    clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Used a small, linear model (multinomial LogisticRegression) suitable for low-dimensional tabular data; avoids deep learning.
# - CPU-efficient pipeline with median imputation + standardization; both are lightweight and improve convergence/stability.
# - Fixed random_state for reproducibility; limited max_iter and kept n_jobs=1 to avoid unnecessary parallel overhead.
# - Avoided saving artifacts/plots/interactive inputs to reduce I/O and runtime overhead.