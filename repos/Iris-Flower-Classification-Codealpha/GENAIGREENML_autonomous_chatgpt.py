# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "Iris.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is not None:
        # Heuristic: if only 1 column, likely wrong separator
        if df.shape[1] <= 1:
            df = None
    if df is None:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _drop_unnamed(df):
    bad = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if bad:
        df = df.drop(columns=bad, errors="ignore")
    return df


def _coerce_numeric_inplace(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _pick_target(df, dataset_headers=None):
    cols = list(df.columns)

    # Prefer a likely target name from provided headers (if present in df)
    preferred_names = []
    if dataset_headers:
        preferred_names.extend([h.strip() for h in dataset_headers.split(",") if h.strip()])

    # Common targets
    preferred_names.extend(["Species", "species", "target", "Target", "label", "Label", "y"])

    for name in preferred_names:
        if name in df.columns:
            y = df[name]
            if y.nunique(dropna=True) >= 2:
                return name

    # Otherwise choose an object/category column with >=2 unique values
    obj_cols = [c for c in cols if df[c].dtype == "object" or str(df[c].dtype).startswith("category")]
    obj_cols = sorted(obj_cols, key=lambda c: df[c].nunique(dropna=True), reverse=True)
    for c in obj_cols:
        if df[c].nunique(dropna=True) >= 2:
            return c

    # Otherwise choose a numeric column with variance (non-constant)
    num_candidates = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        nun = s.nunique(dropna=True)
        if nun >= 2:
            num_candidates.append((nun, c))
    if num_candidates:
        num_candidates.sort(reverse=True)
        return num_candidates[0][1]

    # Fallback: first column
    return cols[0]


def _is_classification_target(y):
    # Treat object/categorical as classification
    if y.dtype == "object" or str(y.dtype).startswith("category"):
        return True
    # Low-cardinality numeric likely classification
    y_num = pd.to_numeric(y, errors="coerce")
    unique_vals = pd.Series(y_num).dropna().unique()
    if unique_vals.size >= 2 and unique_vals.size <= 20:
        # If values are "near-integers", likely labels
        near_int = np.all(np.isclose(unique_vals, np.round(unique_vals), atol=1e-8))
        if near_int:
            return True
    return False


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    r2 = 0.0 if ss_tot == 0 else (1.0 - ss_res / ss_tot)
    # Map to [0,1] for stable "accuracy proxy"
    score = 0.5 * (np.clip(r2, -1.0, 1.0) + 1.0)
    return float(score)


df = _read_csv_robust(DATASET_PATH)
df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)

# Remove fully empty rows/cols early
df = df.dropna(axis=0, how="all").dropna(axis=1, how="all")
assert df.shape[0] > 0 and df.shape[1] > 0

target_col = _pick_target(df, dataset_headers="Id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species")

# Build X/y with defensive feature selection
all_cols = list(df.columns)
feature_cols = [c for c in all_cols if c != target_col]
if len(feature_cols) == 0:
    # If no features, create a constant feature to keep pipeline runnable
    df["_constant_feature"] = 1.0
    feature_cols = ["_constant_feature"]

X = df[feature_cols].copy()
y = df[target_col].copy()

# Identify numeric/categorical columns using coercion trial
numeric_cols = []
categorical_cols = []
for c in feature_cols:
    s_num = pd.to_numeric(X[c], errors="coerce")
    frac_numeric = float(s_num.notna().mean()) if len(s_num) else 0.0
    if frac_numeric >= 0.9:
        X[c] = s_num
        numeric_cols.append(c)
    else:
        categorical_cols.append(c)

# If no numeric columns detected, still allow categoricals
preprocess = ColumnTransformer(
    transformers=[
        ("num", Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]), numeric_cols),
        ("cat", Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]), categorical_cols),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Drop rows with missing target (classification/regression handled below)
mask_y = pd.Series(y).notna()
X = X.loc[mask_y].copy()
y = y.loc[mask_y].copy()
assert len(y) > 0

is_clf = _is_classification_target(y)

# Ensure split is possible; if too small, compute on all data as fallback
do_split = len(y) >= 5

if is_clf:
    # For numeric labels, keep as-is; for objects, keep as strings
    y_series = pd.Series(y)
    n_classes = y_series.nunique(dropna=True)

    if n_classes < 2:
        # Trivial baseline: predict the only class
        accuracy = 1.0
    else:
        if do_split:
            stratify = y_series if n_classes >= 2 and y_series.value_counts().min() >= 2 else None
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_series, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
            )
        else:
            X_train, y_train = X, y_series
            X_test, y_test = X, y_series

        assert len(y_train) > 0 and len(y_test) > 0

        model = LogisticRegression(
            solver="lbfgs",
            max_iter=200,
            n_jobs=1,
            multi_class="auto",
        )

        clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Regression fallback: coerce target to numeric and score with bounded R2 proxy
    y_num = pd.to_numeric(pd.Series(y), errors="coerce")
    mask = y_num.notna()
    Xr = X.loc[mask].copy()
    yr = y_num.loc[mask].copy()
    if len(yr) < 2:
        accuracy = 0.0
    else:
        if do_split and len(yr) >= 5:
            X_train, X_test, y_train, y_test = train_test_split(
                Xr, yr, test_size=0.2, random_state=RANDOM_STATE
            )
        else:
            X_train, y_train = Xr, yr
            X_test, y_test = Xr, yr

        assert len(y_train) > 0 and len(y_test) > 0

        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        reg_pipe = Pipeline(steps=[("preprocess", preprocess), ("model", reg)])
        reg_pipe.fit(X_train, y_train)
        y_pred = reg_pipe.predict(X_test)
        accuracy = _bounded_regression_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Uses lightweight, CPU-friendly models: LogisticRegression for classification; Ridge as regression fallback.
# - ColumnTransformer+Pipeline ensures reproducible, single-pass preprocessing (impute+scale numeric; impute+one-hot categoricals).
# - Robust CSV parsing with separator/decimal fallback; normalized headers and dropped 'Unnamed:' columns to avoid schema issues.
# - Defensive target selection: prefers known target names, otherwise chooses a non-constant column; avoids hard-fail on mismatches.
# - Numeric coercion uses errors='coerce' to handle dirty schemas; imputation avoids dropping many rows and keeps compute small.
# - Regression fallback reports a bounded R2-based proxy mapped to [0,1] to keep ACCURACY stable and comparable.