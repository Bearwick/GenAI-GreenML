# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.dummy import DummyClassifier
from sklearn.metrics import accuracy_score, mean_absolute_error


def read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_bad(d):
        if d is None or d.empty:
            return True
        if d.shape[1] == 1:
            col = d.columns[0]
            if isinstance(col, str) and (';' in col):
                return True
            try:
                val = d.iloc[0, 0]
                if isinstance(val, str) and (';' in val):
                    return True
            except Exception:
                pass
        return False

    if looks_bad(df):
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            df = pd.read_csv(path, sep=None, engine='python')
    return df


def normalize_columns(cols):
    new_cols = []
    for c in cols:
        if isinstance(c, str):
            c_norm = ' '.join(c.strip().split())
        else:
            c_norm = str(c)
        new_cols.append(c_norm)
    return new_cols


def select_target(df):
    cols = [c for c in df.columns if df[c].notna().sum() > 0]
    if not cols:
        return None
    lower_map = {c.lower(): c for c in cols}
    for key in ['species', 'target', 'label', 'class', 'y']:
        if key in lower_map:
            return lower_map[key]
    non_numeric = [c for c in cols if df[c].dtype == object or pd.api.types.is_categorical_dtype(df[c])]
    candidates = [c for c in non_numeric if df[c].nunique(dropna=True) > 1]
    if candidates:
        candidates = sorted(candidates, key=lambda c: df[c].nunique(dropna=True))
        return candidates[0]
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_candidates = [c for c in numeric_cols if df[c].nunique(dropna=True) > 1]
    if numeric_candidates:
        return numeric_candidates[0]
    return cols[-1]


dataset_path = "Iris.csv"
df = read_csv_robust(dataset_path)
if df is None:
    raise ValueError("Dataset could not be loaded.")

df.columns = normalize_columns(df.columns)
df = df.loc[:, [c for c in df.columns if not c.lower().startswith('unnamed')]]

if df.columns.duplicated().any():
    new_cols = []
    counts = {}
    for c in df.columns:
        if c in counts:
            counts[c] += 1
            new_cols.append(f"{c}_{counts[c]}")
        else:
            counts[c] = 0
            new_cols.append(c)
    df.columns = new_cols

df = df.dropna(axis=1, how='all')

for col in df.columns:
    if df[col].dtype == object:
        converted = pd.to_numeric(df[col], errors='coerce')
        if converted.notna().sum() >= max(1, int(0.8 * len(df))):
            df[col] = converted

for col in df.select_dtypes(include=['object']).columns:
    df[col] = df[col].apply(lambda x: x.strip() if isinstance(x, str) else x)

df.replace([np.inf, -np.inf], np.nan, inplace=True)

target = select_target(df)
if target is None:
    raise ValueError("No suitable target column found.")

df = df.loc[df[target].notna()].copy()
df = df.dropna(axis=1, how='all')

assert df.shape[0] > 0

feature_cols = [c for c in df.columns if c != target]

id_like = [c for c in feature_cols if 'id' in c.lower() and df[c].nunique(dropna=True) == len(df)]
if len(id_like) < len(feature_cols):
    feature_cols = [c for c in feature_cols if c not in id_like]

if len(feature_cols) == 0:
    df['__index__'] = np.arange(len(df))
    feature_cols = ['__index__']

X = df[feature_cols]
y = df[target]

n_unique = y.nunique(dropna=True)

if y.dtype == object or pd.api.types.is_categorical_dtype(y) or pd.api.types.is_bool_dtype(y):
    task = 'classification'
elif n_unique <= 20 and n_unique / max(1, len(df)) <= 0.2:
    task = 'classification'
else:
    task = 'regression'

if task == 'regression':
    y_numeric = pd.to_numeric(df[target], errors='coerce')
    mask = y_numeric.notna()
    df = df.loc[mask].copy()
    X = df[feature_cols]
    y = y_numeric.loc[mask]
    n_unique = y.nunique(dropna=True)

assert len(df) > 0

numeric_features = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c])]
categorical_features = [c for c in feature_cols if c not in numeric_features]

transformers = []
if numeric_features:
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    transformers.append(('cat', categorical_transformer, categorical_features))

if transformers:
    preprocessor = ColumnTransformer(transformers, remainder='drop')
else:
    preprocessor = 'passthrough'

if task == 'classification':
    if n_unique < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, n_jobs=1)
else:
    model = LinearRegression(n_jobs=1)

if preprocessor == 'passthrough':
    pipeline = Pipeline(steps=[('model', model)])
else:
    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])

n_samples = len(df)
if n_samples < 2:
    X_train = X.copy()
    X_test = X.copy()
    y_train = y.copy()
    y_test = y.copy()
else:
    test_size = max(1, int(round(0.2 * n_samples)))
    if n_samples - test_size < 1:
        test_size = 1
    stratify = None
    if task == 'classification' and n_unique >= 2:
        class_counts = y.value_counts()
        if class_counts.min() >= 2 and len(class_counts) >= 2:
            stratify = y
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )

assert len(X_train) > 0 and len(X_test) > 0

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    mae = mean_absolute_error(y_test, y_pred)
    y_range = y_test.max() - y_test.min()
    if not np.isfinite(y_range) or y_range == 0:
        accuracy = 1.0
    else:
        accuracy = 1 - mae / y_range
        if accuracy < 0:
            accuracy = 0.0
        if accuracy > 1:
            accuracy = 1.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (Logistic/Linear Regression or Dummy fallback) for CPU-friendly efficiency.
# - Employed a minimal preprocessing pipeline with imputation and scaling/one-hot encoding to ensure robustness.
# - Implemented resilient schema handling, numeric coercion, and safe fallbacks to avoid hard failures on unknown data.
# - Regression uses normalized MAE as a bounded [0,1] accuracy proxy when classification is not applicable.