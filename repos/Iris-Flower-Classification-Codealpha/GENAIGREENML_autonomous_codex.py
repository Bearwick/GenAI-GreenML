# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import glob
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, r2_score

DATASET_HEADERS = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]

def normalize_column_name(name):
    name = str(name).strip()
    name = " ".join(name.split())
    return name

def find_csv_file():
    candidates = []
    for name in ["Iris.csv", "iris.csv", "dataset.csv", "data.csv"]:
        if os.path.exists(name):
            candidates.append(name)
    if not candidates:
        csvs = glob.glob("*.csv")
        if csvs:
            candidates.append(csvs[0])
    if not candidates:
        csvs = glob.glob("/mnt/data/*.csv")
        if csvs:
            candidates.append(csvs[0])
    return candidates[0] if candidates else None

def robust_read_csv(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is None or df.shape[1] <= 1:
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df is None or df2.shape[1] > df.shape[1]:
                df = df2
        except Exception:
            pass
    return df

def is_id_like(series, name):
    name_l = normalize_column_name(name).lower()
    if name_l in ("id", "identifier") or name_l.endswith("id") or name_l.startswith("id"):
        return True
    unique_ratio = series.nunique(dropna=True) / max(1, len(series))
    if unique_ratio > 0.9:
        return True
    return False

def choose_target_column(df, dataset_headers):
    cols = list(df.columns)
    norm_map = {normalize_column_name(c).lower(): c for c in cols}
    target_col = None
    if dataset_headers:
        expected_norm = [normalize_column_name(h).lower() for h in dataset_headers]
        for h in reversed(expected_norm):
            if h in norm_map:
                target_col = norm_map[h]
                break
    if target_col is None:
        candidates = ["target", "label", "class", "species", "outcome", "y"]
        for cand in candidates:
            if cand in norm_map:
                target_col = norm_map[cand]
                break
    if target_col is None:
        for col in cols:
            ser = pd.to_numeric(df[col], errors="coerce")
            if ser.notnull().sum() == 0:
                continue
            if ser.nunique(dropna=True) <= 1:
                continue
            target_col = col
            break
    if target_col is None and cols:
        target_col = cols[-1]
    return target_col

def decide_task(series):
    ser_non_null = series.dropna()
    n_unique = ser_non_null.nunique()
    if n_unique == 0:
        return "regression"
    if series.dtype == object or str(series.dtype).startswith("category"):
        return "classification"
    numeric_ser = pd.to_numeric(series, errors="coerce")
    n_unique_num = numeric_ser.dropna().nunique()
    if n_unique_num <= 20 and (n_unique_num / max(1, len(numeric_ser.dropna()))) <= 0.2:
        return "classification"
    return "regression"

path = find_csv_file()
if path is None:
    raise FileNotFoundError("No CSV file found")

df = robust_read_csv(path)
if df is None:
    raise ValueError("Failed to read CSV")

df.columns = [normalize_column_name(c) for c in df.columns]
df = df.loc[:, [c for c in df.columns if not str(c).startswith("Unnamed")]]
df = df.dropna(axis=1, how="all")
assert df.shape[0] > 0 and df.shape[1] > 0

target_col = choose_target_column(df, DATASET_HEADERS)
if target_col is None:
    target_col = df.columns[-1]

if is_id_like(df[target_col], target_col):
    alternative_cols = [c for c in df.columns if c != target_col and not is_id_like(df[c], c)]
    if alternative_cols:
        target_col = alternative_cols[-1]

feature_cols = [c for c in df.columns if c != target_col]
feature_cols = [c for c in feature_cols if not is_id_like(df[c], c)]
if not feature_cols:
    df = df.copy()
    df["constant"] = 1.0
    feature_cols = ["constant"]

features_df = df[feature_cols].copy()

numeric_cols = []
categorical_cols = []
for col in feature_cols:
    converted = pd.to_numeric(features_df[col], errors="coerce")
    non_null_ratio = converted.notnull().mean()
    if non_null_ratio >= 0.6:
        numeric_cols.append(col)
        features_df[col] = converted
    else:
        categorical_cols.append(col)
        features_df[col] = features_df[col].astype("object")

if numeric_cols:
    features_df[numeric_cols] = features_df[numeric_cols].replace([np.inf, -np.inf], np.nan)

target_series = df[target_col]
task = decide_task(target_series)

if task == "classification":
    y_raw = target_series.replace([np.inf, -np.inf], np.nan)
    mask = y_raw.notna()
    X = features_df.loc[mask, feature_cols]
    y_raw = y_raw[mask]
    le = LabelEncoder()
    y = le.fit_transform(y_raw.astype(str))
else:
    y_num = pd.to_numeric(target_series, errors="coerce")
    y_num = y_num.replace([np.inf, -np.inf], np.nan)
    mask = y_num.notna()
    X = features_df.loc[mask, feature_cols]
    y = y_num[mask]

assert X.shape[0] > 0 and len(y) > 0

stratify = None
if task == "classification":
    unique_classes, class_counts = np.unique(y, return_counts=True)
    if len(unique_classes) > 1 and class_counts.min() >= 2:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=stratify
)

assert X_train.shape[0] > 0 and X_test.shape[0] > 0

baseline_only = False
if task == "classification" and len(np.unique(y_train)) < 2:
    baseline_only = True
    majority_class = np.bincount(y_train).argmax() if len(y_train) > 0 else 0
    y_pred = np.full_like(y_test, majority_class)
    accuracy = accuracy_score(y_test, y_pred) if len(y_test) > 0 else 0.0

if not baseline_only:
    transformers = []
    if numeric_cols:
        numeric_transformer = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler(with_mean=False))
            ]
        )
        transformers.append(("num", numeric_transformer, numeric_cols))
    if categorical_cols:
        categorical_transformer = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore"))
            ]
        )
        transformers.append(("cat", categorical_transformer, categorical_cols))
    if not transformers:
        X_train = X_train.copy()
        X_test = X_test.copy()
        X_train["constant"] = 1.0
        X_test["constant"] = 1.0
        numeric_cols = ["constant"]
        transformers = [
            ("num", Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler(with_mean=False))
            ]), numeric_cols)
        ]
    preprocessor = ColumnTransformer(transformers=transformers, remainder="drop")
    if task == "classification":
        solver = "lbfgs" if len(np.unique(y_train)) > 2 else "liblinear"
        model = LogisticRegression(max_iter=200, solver=solver)
    else:
        model = LinearRegression()
    clf = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    if task == "classification":
        accuracy = accuracy_score(y_test, y_pred)
    else:
        try:
            r2 = r2_score(y_test, y_pred)
        except Exception:
            r2 = 0.0
        accuracy = (r2 + 1.0) / 2.0
        if accuracy < 0.0:
            accuracy = 0.0
        elif accuracy > 1.0:
            accuracy = 1.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Used lightweight linear/logistic models for CPU-friendly training and inference.
# ColumnTransformer with simple imputation and one-hot encoding ensures reproducible preprocessing.
# StandardScaler(with_mean=False) keeps sparse efficiency when categorical features exist.
# Regression fallback maps R2 to [0,1] via (R2+1)/2 to provide a stable ACCURACY proxy.