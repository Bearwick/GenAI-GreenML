# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import glob
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

required_cols = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]

def load_dataset():
    candidates = ["data.csv", "dataset.csv", "Iris.csv", "iris.csv", "train.csv"]
    checked = set()
    for file in candidates + glob.glob("*.csv"):
        if file in checked:
            continue
        checked.add(file)
        if os.path.isfile(file):
            try:
                df = pd.read_csv(file)
            except Exception:
                continue
            if all(col in df.columns for col in required_cols):
                return df
    raise FileNotFoundError("Dataset with required columns not found")

df = load_dataset()

feature_cols = ["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm"]
X = df[feature_cols].apply(pd.to_numeric, errors="coerce")
X = X.fillna(X.median(numeric_only=True))
y = df["Species"].astype(str)

encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Chose GaussianNB for minimal computational overhead on small datasets.
# Applied simple numeric coercion and median imputation to keep preprocessing efficient.
# Avoided complex models and kept the pipeline CPU-friendly and lightweight.