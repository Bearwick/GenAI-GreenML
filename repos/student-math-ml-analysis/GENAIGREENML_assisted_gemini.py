# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

def run_pipeline():
    path = "./data/student-mat.csv"
    try:
        df = pd.read_csv(path)
        if df.shape[1] < 10:
            raise ValueError
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')

    df.columns = [
        'school', 'sex', 'age', 'address', 'family_size', 'parents_status',
        'mother_education', 'father_education', 'mother_job', 'father_job',
        'reason', 'guardian', 'commute_time', 'study_time', 'failures',
        'school_support', 'family_support', 'paid_classes', 'activities',
        'nursery', 'desire_higher_edu', 'internet', 'romantic', 'family_quality',
        'free_time', 'go_out', 'weekday_alcohol_usage', 'weekend_alcohol_usage',
        'health', 'absences', 'period1_score', 'period2_score', 'final_score'
    ]

    x = pd.get_dummies(df, drop_first=True)

    conditions = [
        (df['final_score'] >= 15),
        (df['final_score'] >= 10),
        (df['final_score'] >= 0)
    ]
    choices = ['good', 'fair', 'poor']
    df['final_grade'] = np.select(conditions, choices, default='poor')

    le = LabelEncoder()
    y = le.fit_transform(df['final_grade'])

    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

    svc = SVC()
    svc.fit(x_train, y_train)
    y_svc_predict = svc.predict(x_test)
    accuracy = accuracy_score(y_test, y_svc_predict)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary
# 1. Replaced iterative .loc target assignment with vectorized np.select for energy-efficient preprocessing.
# 2. Removed redundant data exploration steps (info, describe, head) to reduce CPU cycles.
# 3. Eliminated memory-intensive visualizations and plotting library overhead (matplotlib, seaborn).
# 4. Implemented a robust CSV loading mechanism with automated fallback for different delimiters.
# 5. Removed unnecessary dataframe copies and intermediate data structures to minimize memory footprint.
# 6. Simplified the model evaluation pipeline to focus on the final classifier (SVC), reducing redundant training energy.
# 7. Optimized imports to include only necessary components from scikit-learn and pandas.
# 8. Maintained identical predictive behavior by preserving the data leakage present in the original feature set.