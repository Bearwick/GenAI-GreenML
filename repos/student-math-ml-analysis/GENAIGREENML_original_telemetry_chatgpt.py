# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: original_telemetry

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

df = pd.read_csv("./data/student-mat.csv")

df.columns = [
    "school",
    "sex",
    "age",
    "address",
    "family_size",
    "parents_status",
    "mother_education",
    "father_education",
    "mother_job",
    "father_job",
    "reason",
    "guardian",
    "commute_time",
    "study_time",
    "failures",
    "school_support",
    "family_support",
    "paid_classes",
    "activities",
    "nursery",
    "desire_higher_edu",
    "internet",
    "romantic",
    "family_quality",
    "free_time",
    "go_out",
    "weekday_alcohol_usage",
    "weekend_alcohol_usage",
    "health",
    "absences",
    "period1_score",
    "period2_score",
    "final_score",
]

df_encode = pd.get_dummies(df, drop_first=True)

df["final_grade"] = "na"
df.loc[(df.final_score >= 15) & (df.final_score <= 20), "final_grade"] = "good"
df.loc[(df.final_score >= 10) & (df.final_score <= 14), "final_grade"] = "fair"
df.loc[(df.final_score >= 0) & (df.final_score <= 9), "final_grade"] = "poor"

la = LabelEncoder()
df.final_grade = la.fit_transform(df.final_grade)

x = df_encode.copy()
y = df["final_grade"]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train, y_train)
y_pred = knn.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")