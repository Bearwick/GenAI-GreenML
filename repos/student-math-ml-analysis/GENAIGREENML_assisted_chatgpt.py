# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


SEED = 42


def set_reproducible_seed(seed: int = SEED) -> None:
    random.seed(seed)
    np.random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)


DATASET_HEADERS = [
    "school", "sex", "age", "address", "famsize", "Pstatus", "Medu", "Fedu", "Mjob", "Fjob",
    "reason", "guardian", "traveltime", "studytime", "failures", "schoolsup", "famsup", "paid",
    "activities", "nursery", "higher", "internet", "romantic", "famrel", "freetime", "goout",
    "Dalc", "Walc", "health", "absences", "G1", "G2", "G3"
]


def _parse_looks_wrong(df: pd.DataFrame) -> bool:
    if df.empty:
        return True
    if df.shape[1] == 1:
        return True
    if set(DATASET_HEADERS).issubset(set(df.columns)):
        return False
    first_col = str(df.columns[0])
    if ";" in first_col:
        return True
    return False


def read_student_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if _parse_looks_wrong(df):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def align_schema(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    if set(DATASET_HEADERS).issubset(set(df.columns)):
        expected = DATASET_HEADERS
        df = df[expected]
        return df

    if df.shape[1] == len(DATASET_HEADERS) and not set(DATASET_HEADERS).issubset(set(df.columns)):
        df.columns = DATASET_HEADERS
        return df

    return df


def build_features_and_target(df: pd.DataFrame):
    df = df.copy()

    if "G3" not in df.columns:
        raise ValueError("Required target source column 'G3' not found after schema alignment.")

    final_score = pd.to_numeric(df["G3"], errors="coerce")

    final_grade = pd.Series(np.full(len(df), "na", dtype=object), index=df.index)
    final_grade.loc[(final_score >= 15) & (final_score <= 20)] = "good"
    final_grade.loc[(final_score >= 10) & (final_score <= 14)] = "fair"
    final_grade.loc[(final_score >= 0) & (final_score <= 9)] = "poor"

    la = LabelEncoder()
    y = la.fit_transform(final_grade.to_numpy())

    X = pd.get_dummies(df, drop_first=True)

    return X, y


def train_and_evaluate(X: pd.DataFrame, y: np.ndarray, seed: int = SEED) -> float:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed
    )

    model = RandomForestClassifier(random_state=seed)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    return accuracy_score(y_test, y_pred)


def main() -> None:
    set_reproducible_seed(SEED)

    df = read_student_csv("./data/student-mat.csv")
    df = align_schema(df)

    X, y = build_features_and_target(df)
    accuracy = train_and_evaluate(X, y, SEED)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed plotting/visualization and verbose prints to eliminate unnecessary computation and I/O overhead.
# - Avoided redundant intermediate objects (e.g., dropped unused correlation/heatmap computations and repeated dtype prints).
# - Kept a single model training/evaluation path to reduce runtime and energy while preserving the task (grade classification from the same dataset).
# - Implemented robust CSV parsing with a fallback separator/decimal to avoid repeated manual fixes and reruns.
# - Derived schema from DATASET_HEADERS/df.columns and aligned columns defensively to avoid hard assumptions.
# - Minimized data movement by building features/target with only required copies and vectorized grade binning.
# - Ensured reproducibility by fixing random seeds for Python/NumPy and using deterministic random_state in split/model.