# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier


DATASET_PATH = "data/student-mat.csv"
DATASET_HEADERS = [
    "school", "sex", "age", "address", "famsize", "Pstatus", "Medu", "Fedu", "Mjob", "Fjob",
    "reason", "guardian", "traveltime", "studytime", "failures", "schoolsup", "famsup", "paid",
    "activities", "nursery", "higher", "internet", "romantic", "famrel", "freetime", "goout",
    "Dalc", "Walc", "health", "absences", "G1", "G2", "G3",
]


def set_reproducible_seed(seed: int = 42) -> None:
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)


def read_csv_robust(path: str, expected_headers: list[str]) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] == 1 or not set(expected_headers).issubset(set(df.columns)):
        df = pd.read_csv(path, sep=";", decimal=",")
    if not set(expected_headers).issubset(set(df.columns)):
        df.columns = expected_headers[: df.shape[1]]
    return df


def build_final_grade(y_final_score: pd.Series) -> pd.Series:
    bins = [-np.inf, 9, 14, np.inf]
    labels = ["poor", "fair", "good"]
    return pd.cut(y_final_score, bins=bins, labels=labels, include_lowest=True).astype(str)


def prepare_features_and_target(df: pd.DataFrame) -> tuple[pd.DataFrame, np.ndarray]:
    missing = [c for c in DATASET_HEADERS if c not in df.columns]
    if missing:
        raise ValueError(f"Missing expected columns in dataset: {missing}")

    final_grade_str = build_final_grade(df["G3"])
    y = LabelEncoder().fit_transform(final_grade_str)

    X = df.copy()
    X = pd.get_dummies(X, drop_first=True)
    return X, y


def train_and_evaluate(X: pd.DataFrame, y: np.ndarray, seed: int = 42) -> float:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed
    )
    model = DecisionTreeClassifier(
        max_depth=5, min_samples_leaf=17, random_state=seed, criterion="entropy"
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return float(accuracy_score(y_test, y_pred))


def main() -> None:
    set_reproducible_seed(42)
    df = read_csv_robust(DATASET_PATH, DATASET_HEADERS)
    X, y = prepare_features_and_target(df)
    accuracy = train_and_evaluate(X, y, seed=42)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed all exploratory prints, plots, and reports to eliminate unnecessary computation and rendering overhead.
# - Dropped unused model trainings (KNN/RandomForest/SVM) and cross-validation since they did not affect required final output, reducing runtime and energy use.
# - Used a single deterministic DecisionTree configuration (same hyperparameters/seed as original) to preserve task intent while minimizing training cost.
# - Replaced multi-step grade assignment with vectorized pd.cut for fewer passes over data and reduced intermediate writes.
# - Implemented robust CSV parsing with fallback delimiter/decimal handling and schema validation derived from DATASET_HEADERS/df.columns.
# - Reduced redundant data movement by avoiding extra copies and computing only the required target/feature set once.
# - Ensured reproducibility via fixed seeds for Python, NumPy, and deterministic train/test split and model random_state.