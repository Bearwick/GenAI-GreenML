# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
import random
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

SEED = 42
random.seed(SEED)
np.random.seed(SEED)

DATASET_PATH = "data/student-mat.csv"
DATASET_HEADERS = [
    "school", "sex", "age", "address", "famsize", "Pstatus", "Medu", "Fedu",
    "Mjob", "Fjob", "reason", "guardian", "traveltime", "studytime", "failures",
    "schoolsup", "famsup", "paid", "activities", "nursery", "higher", "internet",
    "romantic", "famrel", "freetime", "goout", "Dalc", "Walc", "health", "absences",
    "G1", "G2", "G3"
]

def load_dataset(path, headers):
    df = pd.read_csv(path)
    if df.shape[1] == 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    if df.shape[1] == len(headers) + 1 and str(df.columns[0]).startswith("Unnamed"):
        df = df.drop(columns=df.columns[0])
    if df.shape[1] == len(headers):
        if set(df.columns) == set(headers):
            df = df[headers]
        else:
            df.columns = headers
    return df

def build_target(scores):
    y = np.full(scores.shape, 2, dtype=np.int64)
    y[(scores >= 10) & (scores <= 14)] = 0
    y[(scores >= 15) & (scores <= 20)] = 1
    return y

def evaluate(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    return accuracy_score(y_test, preds)

def main():
    df = load_dataset(DATASET_PATH, DATASET_HEADERS)
    score_col = DATASET_HEADERS[-1] if DATASET_HEADERS[-1] in df.columns else df.columns[-1]
    y = build_target(df[score_col].to_numpy())
    X = pd.get_dummies(df, drop_first=True).to_numpy()
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=SEED
    )
    evaluate(KNeighborsClassifier(n_neighbors=5), X_train, X_test, y_train, y_test)
    evaluate(DecisionTreeClassifier(max_depth=5, min_samples_leaf=17, random_state=SEED, criterion="entropy"), X_train, X_test, y_train, y_test)
    evaluate(RandomForestClassifier(random_state=SEED), X_train, X_test, y_train, y_test)
    accuracy = evaluate(SVC(), X_train, X_test, y_train, y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# Removed all exploratory prints and visualizations to eliminate unnecessary I/O and rendering overhead.
# Added robust CSV loading with delimiter fallback and standardized column alignment using provided headers.
# Vectorized target encoding directly to integer classes to avoid extra encoders and string processing.
# Performed dummy encoding once and converted to NumPy arrays to reduce repeated conversions and data movement.
# Centralized model evaluation in a helper to avoid redundant code paths and keep execution concise.
# Fixed random seeds and model random_state parameters to ensure deterministic, reproducible results.