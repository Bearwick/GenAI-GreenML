# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score

warnings.filterwarnings("ignore")


def _normalize_columns(cols):
    norm = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        if c2.lower().startswith("unnamed:"):
            c2 = None
        norm.append(c2)
    return norm


def _read_csv_robust(path):
    # Try default CSV parsing first; if schema looks wrong, retry with common European settings.
    df1 = pd.read_csv(path)
    if df1.shape[1] <= 2:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        if df2.shape[1] > df1.shape[1]:
            return df2
    return df1


def _safe_to_numeric(series):
    return pd.to_numeric(series, errors="coerce")


def _pick_target(df, preferred="G3"):
    cols_lower = {str(c).strip().lower(): c for c in df.columns}
    if preferred and preferred.lower() in cols_lower:
        cand = cols_lower[preferred.lower()]
        y = _safe_to_numeric(df[cand])
        if y.notna().sum() >= max(10, int(0.5 * len(df))) and y.nunique(dropna=True) > 1:
            return cand

    # Prefer other common grade columns
    for key in ["g3", "g2", "g1"]:
        if key in cols_lower:
            cand = cols_lower[key]
            y = _safe_to_numeric(df[cand])
            if y.notna().sum() >= max(10, int(0.5 * len(df))) and y.nunique(dropna=True) > 1:
                return cand

    # Fallback: choose a non-constant numeric column with best coverage
    best_col = None
    best_score = -1
    for c in df.columns:
        s = _safe_to_numeric(df[c])
        coverage = s.notna().mean()
        nunq = s.nunique(dropna=True)
        if nunq > 1:
            score = coverage + 0.01 * min(nunq, 1000)
            if score > best_score:
                best_score = score
                best_col = c
    return best_col


def _is_classification_target(y_num):
    # If values are mostly small integers with low cardinality, treat as classification.
    y_clean = y_num.dropna()
    if y_clean.empty:
        return False
    nunq = y_clean.nunique()
    if nunq < 2:
        return False
    # integer-like check with tolerance
    frac = np.mean(np.isclose(y_clean.values, np.round(y_clean.values)))
    if frac >= 0.98 and nunq <= 25:
        return True
    return False


def _regression_accuracy_proxy(y_true, y_pred):
    # Stable proxy in [0,1]: 1 / (1 + RMSE / (std + eps))
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    rmse = float(np.sqrt(np.mean((yt - yp) ** 2)))
    std = float(np.std(yt))
    denom = std if std > 1e-12 else 1.0
    score = 1.0 / (1.0 + rmse / denom)
    return float(np.clip(score, 0.0, 1.0))


def main():
    dataset_path = os.path.join("data", "student-mat.csv")
    df = _read_csv_robust(dataset_path)

    # Normalize and drop unusable columns
    df.columns = _normalize_columns(df.columns)
    df = df.loc[:, [c for c in df.columns if c is not None]]
    df = df.copy()

    # Strip whitespace in object columns to reduce cardinality/duplicates
    obj_cols = df.select_dtypes(include=["object"]).columns.tolist()
    for c in obj_cols:
        df[c] = df[c].astype(str).str.strip().replace({"": np.nan, "nan": np.nan, "None": np.nan})

    assert len(df) > 0 and df.shape[1] > 0

    target_col = _pick_target(df, preferred="G3")
    if target_col is None:
        # Trivial fallback: no viable target; create dummy accuracy from always-correct baseline on constant.
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Build y
    y_num = _safe_to_numeric(df[target_col])
    # Drop rows without target
    keep = y_num.notna() & np.isfinite(y_num.values)
    df = df.loc[keep].reset_index(drop=True)
    y_num = y_num.loc[keep].reset_index(drop=True)

    assert len(df) > 0

    # Features: all columns except target
    X = df.drop(columns=[target_col])

    # Coerce numeric-like columns where possible (keep original for categoricals)
    # Decide numeric columns as those that are already numeric or become numeric with decent coverage.
    numeric_cols = []
    categorical_cols = []
    for c in X.columns:
        if pd.api.types.is_numeric_dtype(X[c]):
            numeric_cols.append(c)
        else:
            s_num = _safe_to_numeric(X[c])
            coverage = s_num.notna().mean()
            # If most values are numeric, treat as numeric to reduce one-hot expansion.
            if coverage >= 0.85:
                X[c] = s_num
                numeric_cols.append(c)
            else:
                categorical_cols.append(c)

    # If no features remain (edge case), use trivial baseline
    if len(numeric_cols) == 0 and len(categorical_cols) == 0:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Determine task type
    is_clf = _is_classification_target(y_num)

    # For classification, keep discrete classes (integers); for regression, continuous.
    if is_clf:
        y = np.round(y_num).astype(int)
        # If too many classes or degenerate, fall back to regression
        if pd.Series(y).nunique() < 2:
            is_clf = False
            y = y_num.astype(float)
    else:
        y = y_num.astype(float)

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y if is_clf and pd.Series(y).nunique() >= 2 else None,
    )

    assert len(X_train) > 0 and len(X_test) > 0

    # Preprocess
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Model
    if is_clf:
        model = LogisticRegression(
            solver="lbfgs",
            max_iter=200,
            n_jobs=1,
            multi_class="auto",
        )
    else:
        model = Ridge(alpha=1.0, random_state=42)

    clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)

    if is_clf:
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        accuracy = _regression_accuracy_proxy(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Chose a lightweight baseline: LogisticRegression (classification) or Ridge (regression) for CPU-efficient training/inference.
# - Used ColumnTransformer+Pipeline to avoid redundant preprocessing and ensure reproducible, single-pass transformations.
# - Coerced numeric-like object columns to numeric when coverage is high to reduce one-hot dimensionality (energy/memory savings).
# - OneHotEncoder(handle_unknown="ignore") prevents failures from unseen categories without costly schema alignment.
# - Median/most_frequent imputation is fast, robust, and avoids expensive iterative imputers.
# - Regression fallback uses a bounded [0,1] accuracy proxy: 1/(1+RMSE/std), stable across scales while remaining lightweight.