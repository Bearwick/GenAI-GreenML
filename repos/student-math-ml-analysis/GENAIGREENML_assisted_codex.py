# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
import random
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

DATASET_HEADERS = "school,sex,age,address,famsize,Pstatus,Medu,Fedu,Mjob,Fjob,reason,guardian,traveltime,studytime,failures,schoolsup,famsup,paid,activities,nursery,higher,internet,romantic,famrel,freetime,goout,Dalc,Walc,health,absences,G1,G2,G3"

def load_dataset(path, expected_cols):
    df = pd.read_csv(path)
    if df.shape[1] == 1 or not any(col in df.columns for col in expected_cols):
        df = pd.read_csv(path, sep=";", decimal=",")
    unnamed = [c for c in df.columns if str(c).startswith("Unnamed")]
    if unnamed:
        df = df.drop(columns=unnamed)
    if len(df.columns) == len(expected_cols) and not set(expected_cols).issubset(df.columns):
        df.columns = expected_cols
    return df

def create_final_grade(scores):
    final_grade = np.full(scores.shape, "na", dtype=object)
    final_grade[(scores >= 15) & (scores <= 20)] = "good"
    final_grade[(scores >= 10) & (scores <= 14)] = "fair"
    final_grade[(scores >= 0) & (scores <= 9)] = "poor"
    return final_grade

def prepare_data(df, expected_cols):
    final_score_col = expected_cols[-1] if expected_cols[-1] in df.columns else df.columns[-1]
    scores = pd.to_numeric(df[final_score_col], errors="coerce").to_numpy()
    final_grade = create_final_grade(scores)
    y = pd.factorize(final_grade, sort=True)[0]
    X = pd.get_dummies(df, drop_first=True, dtype=np.uint8)
    return X, y

def compute_accuracy(y_true, y_pred):
    return float(np.mean(y_true == y_pred))

def evaluate_models(X_train, X_test, y_train, y_test):
    models = [
        ("knn", KNeighborsClassifier(n_neighbors=5)),
        ("tree", DecisionTreeClassifier(max_depth=5, min_samples_leaf=17, random_state=42, criterion="entropy")),
        ("rf", RandomForestClassifier(random_state=42, n_jobs=1)),
        ("svc", SVC())
    ]
    accuracy = None
    for name, model in models:
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        acc = compute_accuracy(y_test, preds)
        if name == "svc":
            accuracy = acc
    return accuracy

def main():
    np.random.seed(42)
    random.seed(42)
    expected_cols = [c.strip() for c in DATASET_HEADERS.split(",")]
    df = load_dataset("./data/student-mat.csv", expected_cols)
    X, y = prepare_data(df, expected_cols)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    accuracy = evaluate_models(X_train, X_test, y_train, y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed plotting, verbose logging, and cross-validation to eliminate redundant computation.
# - Vectorized grade creation and avoided extra dataframe copies for lower overhead.
# - Implemented robust CSV parsing with delimiter fallback and numeric coercion.
# - Consolidated model training in a loop and used lightweight accuracy computation.
# - Fixed seeds and limited RandomForest parallelism for reproducible, efficient execution.