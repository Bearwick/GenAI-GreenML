# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, r2_score

DATASET_PATH = "voting_data.csv"

def read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is None or df.shape[1] <= 1:
        try:
            df2 = pd.read_csv(path, sep=';', decimal=',')
            if df is None or df2.shape[1] > df.shape[1]:
                df = df2
        except Exception:
            pass
    if df is None:
        df = pd.DataFrame()
    return df

def normalize_columns(cols):
    normed = []
    for c in cols:
        c = str(c)
        c = re.sub(r'\s+', ' ', c.strip())
        normed.append(c)
    return normed

def choose_target_column(df):
    cols = list(df.columns)
    if not cols:
        return None
    lower_cols = [c.lower() for c in cols]
    preferred = ['label', 'target', 'y', 'class']
    for p in preferred:
        if p in lower_cols:
            return cols[lower_cols.index(p)]
    for p in preferred:
        for c in cols:
            if p in c.lower():
                return c
    numeric_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]
    numeric_nonconst = [c for c in numeric_cols if df[c].nunique(dropna=True) > 1]
    if numeric_nonconst:
        return numeric_nonconst[-1]
    if numeric_cols:
        return numeric_cols[-1]
    return cols[-1]

df = read_csv_robust(DATASET_PATH)
df.columns = normalize_columns(df.columns)
df = df.loc[:, [c for c in df.columns if c and not c.lower().startswith('unnamed')]].copy()
assert df.shape[0] > 0 and df.shape[1] > 0
target_col = choose_target_column(df)
assert target_col is not None
df = df[df[target_col].notna()].copy()
assert len(df) > 0

feature_cols = [c for c in df.columns if c != target_col]
if len(feature_cols) == 0:
    df['__constant__'] = 1
    feature_cols = ['__constant__']

numeric_cols = []
text_cols = []
for c in feature_cols:
    if pd.api.types.is_numeric_dtype(df[c]):
        numeric_cols.append(c)
    else:
        coerced = pd.to_numeric(df[c], errors='coerce')
        if coerced.notna().mean() > 0.7:
            df[c] = coerced
            numeric_cols.append(c)
        else:
            text_cols.append(c)

for c in numeric_cols:
    df[c] = pd.to_numeric(df[c], errors='coerce')
    df[c] = df[c].replace([np.inf, -np.inf], np.nan)

numeric_cols = [c for c in numeric_cols if df[c].notna().any()]

if text_cols:
    df_text = df[text_cols].fillna('').astype(str)
    df['__combined_text__'] = df_text.agg(' '.join, axis=1)

final_feature_cols = []
if text_cols:
    final_feature_cols.append('__combined_text__')
final_feature_cols += numeric_cols
X = df[final_feature_cols]
y_raw = df[target_col]

is_numeric_target = pd.api.types.is_numeric_dtype(y_raw)
n_unique = y_raw.nunique(dropna=True)
if is_numeric_target:
    if n_unique <= max(20, int(np.sqrt(len(y_raw)))):
        task = 'classification'
    else:
        task = 'regression'
else:
    task = 'classification'

if task == 'regression':
    y = pd.to_numeric(y_raw, errors='coerce')
    valid_mask = y.notna()
    X = X.loc[valid_mask].copy()
    y = y.loc[valid_mask]
else:
    y = y_raw.astype(str).str.strip()

assert len(X) > 0 and len(y) > 0

if len(X) < 2:
    X = pd.concat([X, X], ignore_index=True)
    y = pd.concat([y, y], ignore_index=True)

if task == 'classification':
    n_classes = y.nunique(dropna=True)
else:
    n_classes = None

stratify = y if task == 'classification' and n_classes is not None and n_classes >= 2 else None
try:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=stratify
    )
except Exception:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=None
    )
assert len(X_train) > 0 and len(X_test) > 0

text_feature = '__combined_text__' if text_cols else None
numeric_features = [c for c in X.columns if c != text_feature]

transformers = []
if text_feature is not None:
    text_pipe = Pipeline([
        ('selector', FunctionTransformer(lambda x: np.ravel(x).astype(str), validate=False)),
        ('tfidf', TfidfVectorizer(max_features=5000, dtype=np.float32))
    ])
    transformers.append(('text', text_pipe, [text_feature]))
if numeric_features:
    num_pipe = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', MaxAbsScaler())
    ])
    transformers.append(('num', num_pipe, numeric_features))
preprocessor = ColumnTransformer(transformers, sparse_threshold=0.3)

if task == 'classification':
    if n_classes is None or n_classes < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        if text_feature is not None and len(numeric_features) == 0:
            model = MultinomialNB()
        else:
            model = LinearSVC(random_state=42)
else:
    if y.nunique(dropna=True) <= 1:
        model = DummyRegressor(strategy='mean')
    else:
        model = Ridge(alpha=1.0)

model_pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('model', model)
])

model_pipeline.fit(X_train, y_train)
y_pred = model_pipeline.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    if np.isnan(r2):
        accuracy = 0.0
    else:
        accuracy = float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Lightweight TF-IDF with capped features and linear/naive Bayes models keeps CPU usage low.
# - All text fields are concatenated into one column to minimize vectorizers and memory overhead.
# - Numeric data uses sparse-friendly imputation and MaxAbs scaling for efficient preprocessing.
# - Regression fallback maps R2 into [0,1] via (r2+1)/2 as a stable accuracy proxy.