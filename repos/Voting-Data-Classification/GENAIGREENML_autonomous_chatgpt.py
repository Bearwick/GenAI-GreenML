# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "voting_data.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c = "" if c is None else str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        out.append(c)
    return out


def _drop_unnamed(df):
    drop_cols = []
    for c in df.columns:
        if isinstance(c, str) and c.strip().lower().startswith("unnamed:"):
            drop_cols.append(c)
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _read_csv_robust(path):
    # Try default parsing first
    df1 = None
    err1 = None
    try:
        df1 = pd.read_csv(path)
    except Exception as e:
        err1 = e

    def _score_df(df):
        if df is None or not isinstance(df, pd.DataFrame) or df.shape[0] == 0:
            return -1
        # Prefer more columns and fewer unnamed columns
        cols = list(df.columns)
        unnamed = sum(1 for c in cols if isinstance(c, str) and c.strip().lower().startswith("unnamed:"))
        return df.shape[1] * 10 + min(df.shape[0], 1000) - unnamed * 50

    # Fallback parsing: European CSV conventions
    df2 = None
    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
    except Exception:
        df2 = None

    if _score_df(df2) > _score_df(df1):
        df = df2
    else:
        if df1 is None:
            raise RuntimeError(f"Failed to read CSV: {err1}")
        df = df1

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)
    return df


def _coerce_numeric_inplace(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _choose_target(df, dataset_headers=None):
    cols = list(df.columns)

    # Prefer headers-provided target if present
    if dataset_headers:
        for cand in ["label", "target", "y", "class"]:
            if cand in cols:
                return cand

    # Else, choose a categorical-like column with small number of unique values
    best = None
    best_score = -1
    n = len(df)
    for c in cols:
        s = df[c]
        nunique = s.nunique(dropna=True)
        if nunique <= 1:
            continue
        # Prefer non-numeric columns for classification
        is_numeric = pd.api.types.is_numeric_dtype(s)
        # Score: categorical small-cardinality columns are best
        score = 0
        score += 50 if not is_numeric else 10
        score += max(0, 30 - nunique)  # fewer unique preferred
        # penalize extremely high missing
        miss = s.isna().mean()
        score -= int(miss * 30)
        # prefer columns not identical to index-like
        if str(c).strip().lower() in ("id", "index"):
            score -= 10
        if score > best_score:
            best_score = score
            best = c

    if best is not None:
        return best

    # As last resort: first column
    return cols[0] if cols else None


def _choose_text_feature(df, target_col):
    # Prefer a column named "text" if present and not target
    cols = [c for c in df.columns if c != target_col]
    if "text" in cols:
        return "text"

    # Otherwise pick the longest average string length among object columns
    obj_cols = [c for c in cols if df[c].dtype == "object" or pd.api.types.is_string_dtype(df[c])]
    if not obj_cols:
        return None

    best = None
    best_len = -1
    for c in obj_cols:
        s = df[c].astype(str)
        avg_len = s.str.len().replace([np.inf, -np.inf], np.nan).dropna().mean()
        if pd.isna(avg_len):
            avg_len = 0.0
        if avg_len > best_len:
            best_len = avg_len
            best = c
    return best


def _split_features(df, target_col, text_col):
    feature_cols = [c for c in df.columns if c != target_col]
    if text_col and text_col in feature_cols:
        # Keep other features too (numeric/categorical), but avoid duplicates
        pass

    num_cols = []
    cat_cols = []
    for c in feature_cols:
        if text_col is not None and c == text_col:
            continue
        if pd.api.types.is_numeric_dtype(df[c]):
            num_cols.append(c)
        else:
            cat_cols.append(c)
    return feature_cols, num_cols, cat_cols


def _safe_accuracy_proxy_regression(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    denom = np.var(y_true)
    if not np.isfinite(denom) or denom <= 1e-12:
        mae = np.mean(np.abs(y_true - y_pred))
        return float(1.0 / (1.0 + mae))
    r2 = 1.0 - (np.mean((y_true - y_pred) ** 2) / denom)
    r2 = float(np.clip(r2, -1.0, 1.0))
    return float((r2 + 1.0) / 2.0)


def main():
    df = _read_csv_robust(DATASET_PATH)
    assert df is not None and isinstance(df, pd.DataFrame)
    assert df.shape[0] > 0 and df.shape[1] > 0

    dataset_headers = ["text", "label"]
    target_col = None
    if "label" in df.columns:
        target_col = "label"
    else:
        target_col = _choose_target(df, dataset_headers=dataset_headers)

    assert target_col is not None and target_col in df.columns

    # Normalize target
    y_raw = df[target_col]

    text_col = _choose_text_feature(df, target_col=target_col)

    # Coerce numeric columns before building pipelines
    numeric_candidates = [c for c in df.columns if c != target_col and c != text_col]
    numeric_cols = [c for c in numeric_candidates if pd.api.types.is_numeric_dtype(df[c])]
    df = _coerce_numeric_inplace(df, numeric_cols)

    # Build feature sets
    _, num_cols, cat_cols = _split_features(df, target_col=target_col, text_col=text_col)

    # Prepare X/y with minimal cleaning; let imputers handle NaNs
    X = df.drop(columns=[target_col], errors="ignore")

    # Determine task: classification if y has >=2 classes and not too many unique for small dataset
    y = y_raw.copy()
    # If numeric but small unique, still treat as classification; else regression
    y_nunique = y.nunique(dropna=True)

    # If y is object, classification
    is_y_numeric = pd.api.types.is_numeric_dtype(y)
    classification = True
    if is_y_numeric and y_nunique > max(20, int(0.2 * len(df))):
        classification = False

    # Handle degenerate target
    if y_nunique < 2:
        classification = False

    if classification:
        # Fill missing labels with mode-like value to avoid dropping many rows
        if y.isna().any():
            mode_val = y.dropna().mode()
            fill_val = mode_val.iloc[0] if len(mode_val) else "MISSING"
            y = y.fillna(fill_val)

        # Stratify only if enough samples per class
        stratify = None
        vc = y.value_counts(dropna=False)
        if (vc.min() >= 2) and (len(vc) >= 2):
            stratify = y

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=stratify
        )
        assert len(X_train) > 0 and len(X_test) > 0

        transformers = []
        if text_col is not None and text_col in X.columns:
            transformers.append((
                "text",
                Pipeline(steps=[
                    ("imputer", SimpleImputer(strategy="constant", fill_value="")),
                    ("tfidf", TfidfVectorizer(
                        lowercase=True,
                        strip_accents="unicode",
                        stop_words="english",
                        ngram_range=(1, 1),
                        max_features=20000,
                        min_df=2
                    ))
                ]),
                text_col
            ))

        if num_cols:
            transformers.append((
                "num",
                Pipeline(steps=[
                    ("imputer", SimpleImputer(strategy="median")),
                    ("scaler", StandardScaler(with_mean=False))
                ]),
                num_cols
            ))

        if cat_cols:
            transformers.append((
                "cat",
                Pipeline(steps=[
                    ("imputer", SimpleImputer(strategy="most_frequent")),
                    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True))
                ]),
                cat_cols
            ))

        if not transformers:
            # No usable features; trivial baseline accuracy (predict most frequent class)
            y_pred = np.full(shape=len(y_test), fill_value=y_train.mode().iloc[0])
            accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

        preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

        # Small CPU-friendly linear classifier; lbfgs is efficient for small dense, saga handles sparse; use liblinear for small/multiclass ovr.
        clf = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            C=1.0,
            class_weight=None
        )

        model = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", clf)
        ])

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Regression fallback
    # Coerce y to numeric, drop rows with missing y
    y_num = pd.to_numeric(y_raw, errors="coerce")
    valid = np.isfinite(y_num.to_numpy(dtype=float))
    X = X.loc[valid].reset_index(drop=True)
    y_num = y_num.loc[valid].reset_index(drop=True)
    assert len(X) > 2

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_num, test_size=0.25, random_state=RANDOM_STATE
    )
    assert len(X_train) > 0 and len(X_test) > 0

    transformers = []
    if text_col is not None and text_col in X.columns:
        transformers.append((
            "text",
            Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="constant", fill_value="")),
                ("tfidf", TfidfVectorizer(
                    lowercase=True,
                    strip_accents="unicode",
                    stop_words="english",
                    ngram_range=(1, 1),
                    max_features=20000,
                    min_df=2
                ))
            ]),
            text_col
        ))

    # Recompute num/cat on filtered X
    _, num_cols, cat_cols = _split_features(pd.concat([X, y_num.rename(target_col)], axis=1), target_col=target_col, text_col=text_col)

    if num_cols:
        transformers.append((
            "num",
            Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler(with_mean=False))
            ]),
            num_cols
        ))

    if cat_cols:
        transformers.append((
            "cat",
            Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True))
            ]),
            cat_cols
        ))

    if not transformers:
        # Trivial baseline: predict mean
        y_pred = np.full(shape=len(y_test), fill_value=float(np.mean(y_train)))
        accuracy = _safe_accuracy_proxy_regression(y_test, y_pred)
        print(f"ACCURACY={accuracy:.6f}")
        return

    preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)

    model = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", reg)
    ])

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = _safe_accuracy_proxy_regression(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used lightweight linear models (LogisticRegression / Ridge) for strong CPU baselines with low training energy vs. deep/ensemble methods.
# - TF-IDF with unigrams, min_df=2 and max_features cap limits vocabulary size to reduce RAM/CPU while preserving accuracy on text.
# - ColumnTransformer+Pipeline ensures single-pass, reproducible preprocessing and avoids redundant feature computation.
# - Robust CSV parsing fallback (sep=';', decimal=',') and column normalization improves resilience without expensive schema-specific logic.
# - Defensive target/feature selection enables end-to-end execution even if headers differ; regression fallback uses bounded proxy accuracy=(R2+1)/2 in [0,1].