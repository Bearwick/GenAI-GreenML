# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.dummy import DummyClassifier


DATASET_PATH = "voting_data.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c = "" if c is None else str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        out.append(c)
    return out


def _read_csv_robust(path):
    # Try default CSV parsing first
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(d):
        if d is None or d.empty:
            return True
        if d.shape[1] <= 1:
            return True
        # If the first column name contains separators, parsing may have failed
        first_col = str(d.columns[0])
        if (";" in first_col) or ("," in first_col and d.shape[1] == 1):
            return True
        return False

    if looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            # Last resort: try python engine with automatic separator detection
            df = pd.read_csv(path, sep=None, engine="python")

    return df


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _coerce_numeric_safely(df):
    # Coerce numeric-like object columns without touching true text columns too aggressively.
    # Only attempt coercion on columns where many values look numeric.
    for c in df.columns:
        if pd.api.types.is_numeric_dtype(df[c]):
            continue
        if pd.api.types.is_datetime64_any_dtype(df[c]):
            continue
        if pd.api.types.is_bool_dtype(df[c]):
            continue
        # Heuristic: sample and see if numeric coercion yields many non-NaNs
        s = df[c]
        if s.dtype == "object":
            sample = s.dropna().astype(str).head(200)
            if len(sample) == 0:
                continue
            coerced = pd.to_numeric(sample.str.replace(",", ".", regex=False), errors="coerce")
            ratio = float(np.mean(~coerced.isna()))
            if ratio >= 0.8:
                df[c] = pd.to_numeric(s.astype(str).str.replace(",", ".", regex=False), errors="coerce")
    return df


def _pick_target_and_features(df, preferred_target="label", preferred_text="text"):
    cols = list(df.columns)

    # Preferred case: use provided headers if present (but do not hard-fail)
    target_col = preferred_target if preferred_target in cols else None
    text_col = preferred_text if preferred_text in cols else None

    # If no explicit target, pick a reasonable fallback:
    if target_col is None:
        # Prefer object/categorical non-constant column as classification target
        obj_candidates = []
        for c in cols:
            nun = df[c].nunique(dropna=True)
            if nun >= 2 and (df[c].dtype == "object" or pd.api.types.is_bool_dtype(df[c])):
                obj_candidates.append((nun, c))
        if obj_candidates:
            # Choose one with fewer unique values (more likely a label)
            obj_candidates.sort(key=lambda x: (x[0], x[1]))
            target_col = obj_candidates[0][1]
        else:
            # Otherwise choose a numeric non-constant column as regression target
            num_candidates = []
            for c in cols:
                if pd.api.types.is_numeric_dtype(df[c]):
                    nun = df[c].nunique(dropna=True)
                    if nun >= 2:
                        num_candidates.append((nun, c))
            if num_candidates:
                num_candidates.sort(key=lambda x: (-x[0], x[1]))
                target_col = num_candidates[0][1]
            else:
                # Last resort: pick the last column
                target_col = cols[-1] if cols else None

    # Determine text feature column if present; else find best text-like column
    if text_col is None:
        text_like = []
        for c in cols:
            if c == target_col:
                continue
            if df[c].dtype == "object":
                # Prefer columns with higher average string length (more likely free text)
                sample = df[c].dropna().astype(str).head(200)
                if len(sample) == 0:
                    continue
                avg_len = float(sample.str.len().mean())
                text_like.append((avg_len, c))
        if text_like:
            text_like.sort(key=lambda x: (-x[0], x[1]))
            text_col = text_like[0][1]

    # Features: use all columns except target; keep text column if available
    feature_cols = [c for c in cols if c != target_col]

    return target_col, text_col, feature_cols


def _build_pipeline(text_col, numeric_cols, categorical_cols, is_classification):
    transformers = []

    if text_col is not None:
        # TF-IDF is a lightweight, sparse representation suitable for CPU baselines.
        transformers.append(
            ("text", TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_df=0.95, lowercase=True), text_col)
        )

    if numeric_cols:
        num_pipe = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="median")),
            ]
        )
        transformers.append(("num", num_pipe, numeric_cols))

    if categorical_cols:
        cat_pipe = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
            ]
        )
        transformers.append(("cat", cat_pipe, categorical_cols))

    pre = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    if is_classification:
        # Prefer MultinomialNB for sparse text; fall back to LogisticRegression for mixed features if needed.
        # Use a conservative solver and limited iterations to stay CPU-friendly.
        clf_nb = MultinomialNB(alpha=0.5)
        model = clf_nb
    else:
        # If regression is needed, we will handle via a dummy baseline and a bounded proxy score;
        # pipeline model not used in that path.
        model = DummyClassifier(strategy="most_frequent")

    pipe = Pipeline(steps=[("preprocess", pre), ("model", model)])
    return pipe


def _is_classification_target(y):
    # Classification if non-numeric, or numeric with small number of unique values.
    if y is None:
        return False
    if pd.api.types.is_numeric_dtype(y):
        nun = y.nunique(dropna=True)
        return nun >= 2 and nun <= 20
    return True


def _safe_accuracy_proxy_regression(y_true, y_pred):
    # Convert regression quality to a bounded [0,1] proxy using normalized MAE:
    # accuracy = max(0, 1 - MAE / (IQR + eps))
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    if y_true.size == 0:
        return 0.0
    mae = float(np.mean(np.abs(y_true - y_pred)))
    q75, q25 = np.percentile(y_true, [75, 25])
    iqr = float(q75 - q25)
    scale = iqr if iqr > 1e-12 else float(np.std(y_true) + 1e-12)
    acc = 1.0 - (mae / (scale + 1e-12))
    if not np.isfinite(acc):
        acc = 0.0
    return float(np.clip(acc, 0.0, 1.0))


def main():
    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Basic defensive cleanup
    if df is None or df.empty:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _coerce_numeric_safely(df)

    target_col, text_col, feature_cols = _pick_target_and_features(df, preferred_target="label", preferred_text="text")

    # Ensure we have something to work with
    if target_col is None or target_col not in df.columns:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Split X/y; avoid tuple indexing
    y = df[target_col]
    X = df[feature_cols] if feature_cols else pd.DataFrame(index=df.index)

    # If text_col is present, ensure it is a string column and handle NaNs
    if text_col is not None and text_col in X.columns:
        X[text_col] = X[text_col].astype("string").fillna("")

    # Remove rows with missing target
    mask_y = ~y.isna()
    X = X.loc[mask_y].copy()
    y = y.loc[mask_y].copy()

    # Replace inf in numeric columns
    X = X.replace([np.inf, -np.inf], np.nan)

    assert X.shape[0] > 0 and y.shape[0] > 0

    is_classification = _is_classification_target(y)

    # Determine column groups (excluding text from categoricals)
    numeric_cols = []
    categorical_cols = []

    for c in X.columns:
        if text_col is not None and c == text_col:
            continue
        if pd.api.types.is_numeric_dtype(X[c]):
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)

    # If we have no usable features, fall back to dummy baseline
    has_any_features = (text_col is not None and text_col in X.columns) or bool(numeric_cols) or bool(categorical_cols)

    if is_classification:
        # Ensure at least 2 classes; otherwise fallback to trivial baseline
        y_str = y.astype(str)
        classes = y_str.nunique(dropna=True)
        if classes < 2:
            dummy = DummyClassifier(strategy="most_frequent")
            X_train, X_test, y_train, y_test = train_test_split(
                np.zeros((len(y_str), 1)), y_str, test_size=0.25, random_state=RANDOM_STATE, stratify=None
            )
            assert len(y_train) > 0 and len(y_test) > 0
            dummy.fit(X_train, y_train)
            y_pred = dummy.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

        strat = y_str if classes >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_str, test_size=0.25, random_state=RANDOM_STATE, stratify=strat
        )
        assert len(y_train) > 0 and len(y_test) > 0

        if not has_any_features:
            dummy = DummyClassifier(strategy="most_frequent")
            dummy.fit(np.zeros((len(y_train), 1)), y_train)
            y_pred = dummy.predict(np.zeros((len(y_test), 1)))
            accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

        pipe = _build_pipeline(text_col if (text_col in X.columns if text_col else False) else None,
                               numeric_cols, categorical_cols, is_classification=True)

        # If the pipeline uses MultinomialNB but we have non-text numeric/cat features, it still works with nonnegative features.
        # However, OneHot and TF-IDF are nonnegative; numeric features might be negative.
        # If numeric exists, prefer LogisticRegression with sparse-friendly solver.
        if numeric_cols:
            pre = pipe.named_steps["preprocess"]
            lr = LogisticRegression(
                solver="saga",
                max_iter=200,
                n_jobs=1,
                random_state=RANDOM_STATE,
            )
            pipe = Pipeline(steps=[("preprocess", pre), ("model", lr)])

        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Regression fallback path (rare for this dataset, but required to be robust)
    y_num = pd.to_numeric(y, errors="coerce")
    mask = ~y_num.isna()
    X = X.loc[mask].copy()
    y_num = y_num.loc[mask].copy()

    if len(y_num) < 4:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_num, test_size=0.25, random_state=RANDOM_STATE
    )
    assert len(y_train) > 0 and len(y_test) > 0

    # Lightweight regression baseline: predict median
    y_pred = np.full(shape=(len(y_test),), fill_value=float(np.median(y_train)))
    accuracy = _safe_accuracy_proxy_regression(y_test.values, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used TF-IDF with sparse representation (1-2 grams, min_df=2) to keep memory/CPU low for text classification.
# - Preferred MultinomialNB for sparse text due to linear-time training; switched to sparse-friendly LogisticRegression (saga, low max_iter, n_jobs=1) only when numeric features exist to avoid NB issues with negative values.
# - Implemented ColumnTransformer + Pipeline for reproducible, single-pass preprocessing and to avoid duplicated work.
# - Robust CSV parsing fallback (default, then sep=';' and decimal=',') to reduce manual intervention and wasted reruns.
# - Defensive schema handling: auto-select target/features if headers mismatch; safe coercion of numeric-like columns; drop Unnamed columns.
# - Minimal stdout for energy efficiency; only prints final ACCURACY.
# - Regression fallback uses a constant median predictor and a bounded [0,1] normalized-MAE proxy score to keep evaluation stable and lightweight.