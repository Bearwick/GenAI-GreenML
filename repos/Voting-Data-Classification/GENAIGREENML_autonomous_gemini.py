# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

def load_data(path):
    # Robust CSV loading with fallbacks
    try:
        df = pd.read_csv(path)
        if df.shape[1] < 2:
            raise ValueError
    except:
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except:
            return pd.DataFrame()

    # Clean column names
    df.columns = [str(c).strip() for c in df.columns]
    df.columns = [" ".join(str(c).split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def main():
    dataset_path = 'voting_data.csv'
    df = load_data(dataset_path)

    if df.empty:
        print("ACCURACY=0.000000")
        return

    # Schema derivation
    all_cols = list(df.columns)
    
    # Identify target (prefer 'label', then 'target', then last column)
    target_col = None
    for candidate in ['label', 'target']:
        if candidate in all_cols:
            target_col = candidate
            break
    if not target_col:
        target_col = all_cols[-1]
    
    # Identify feature (prefer 'text', then first non-target column)
    feature_col = None
    for candidate in ['text', 'sentence', 'data']:
        if candidate in all_cols and candidate != target_col:
            feature_col = candidate
            break
    if not feature_col:
        remaining = [c for c in all_cols if c != target_col]
        feature_col = remaining[0] if remaining else None

    if not feature_col or not target_col:
        print("ACCURACY=0.000000")
        return

    # Preprocessing
    df = df.dropna(subset=[feature_col, target_col])
    X = df[feature_col].astype(str)
    y = df[target_col].astype(str)

    if len(X) == 0:
        print("ACCURACY=0.000000")
        return

    # Check for minimal class count
    if y.nunique() < 2:
        print("ACCURACY=1.000000")
        return

    # Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=y if y.nunique() > 1 else None
    )

    # Pipeline: TF-IDF + Logistic Regression (CPU friendly, low memory)
    # Using 1000 max features and lbfgs solver for energy efficiency
    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(
            stop_words='english', 
            max_features=1000, 
            ngram_range=(1, 1),
            sublinear_tf=True
        )),
        ('clf', LogisticRegression(
            max_iter=500, 
            multi_class='auto', 
            solver='lbfgs', 
            n_jobs=1
        ))
    ])

    # Fit and Predict
    try:
        pipeline.fit(X_train, y_train)
        predictions = pipeline.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
    except:
        accuracy = 0.000000

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary:
# 1. Model Choice: Logistic Regression was selected over LinearSVC or Deep Learning because it 
#    is computationally lightweight, provides fast convergence on CPU, and has a lower 
#    carbon footprint during training.
# 2. Vectorization: TfidfVectorizer with a limit of 1000 features ensures the feature matrix 
#    remains sparse and small, reducing RAM usage and CPU cycles compared to dense embeddings.
# 3. Efficiency: Avoided complex hyperparameter grid searches; used a single-pass 
#    pipeline with a fixed solver ('lbfgs') which is efficient for small-to-medium text datasets.
# 4. Robustness: Implemented multi-stage CSV parsing and dynamic column selection to ensure 
#    execution continues even if the input schema deviates from expectations.
# 5. Resource Control: Set n_jobs=1 to prevent aggressive CPU context switching, 
#    maintaining a steady and energy-efficient thermal profile.
# 6. Fallbacks: Included logic to handle single-class datasets or empty inputs without 
#    crashing, ensuring the primary goal of end-to-end reliability.
# 7. Preprocessing: Minimalist approach (stripping, basic stop word removal) reduces 
#    overhead while maintaining sufficient signal for classification.