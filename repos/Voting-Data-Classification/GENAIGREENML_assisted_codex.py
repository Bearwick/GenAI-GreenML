# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC

DATASET_PATH = "voting_data.csv"
DATASET_HEADERS = ["text", "label"]
RANDOM_STATE = 42


def normalize_name(name):
    return str(name).strip().lower().lstrip("\ufeff")


def is_expected_schema(df, expected_headers):
    cols_norm = {normalize_name(c) for c in df.columns}
    expected_norm = {normalize_name(h) for h in expected_headers}
    return expected_norm.issubset(cols_norm) and df.shape[1] >= len(expected_headers)


def read_csv_with_fallback(path, expected_headers):
    try:
        df = pd.read_csv(path)
    except Exception:
        return pd.read_csv(path, sep=";", decimal=",")
    if not is_expected_schema(df, expected_headers):
        try:
            df_alt = pd.read_csv(path, sep=";", decimal=",")
            if is_expected_schema(df_alt, expected_headers):
                return df_alt
        except Exception:
            pass
    return df


def resolve_columns(df, expected_headers):
    norm_map = {normalize_name(c): c for c in df.columns}
    resolved = {}
    for header in expected_headers:
        key = normalize_name(header)
        if key in norm_map:
            resolved[header] = norm_map[key]
    if len(resolved) < len(expected_headers):
        remaining = [c for c in df.columns if c not in resolved.values()]
        for header in expected_headers:
            if header not in resolved and remaining:
                resolved[header] = remaining.pop(0)
    return resolved


def load_features_labels(path):
    df = read_csv_with_fallback(path, DATASET_HEADERS)
    col_map = resolve_columns(df, DATASET_HEADERS)
    text_col = col_map.get("text", df.columns[0])
    label_col = col_map.get("label", df.columns[-1])
    texts = df[text_col].fillna("").astype(str)
    labels = df[label_col]
    mask = labels.notna()
    return texts[mask], labels[mask]


def train_and_evaluate(texts, labels):
    train_x, test_x, train_y, test_y = train_test_split(
        texts,
        labels,
        train_size=0.75,
        test_size=0.25,
        random_state=RANDOM_STATE,
        shuffle=True,
    )
    vectorizer = TfidfVectorizer()
    train_x_vec = vectorizer.fit_transform(train_x)
    test_x_vec = vectorizer.transform(test_x)
    model = LinearSVC(random_state=RANDOM_STATE)
    model.fit(train_x_vec, train_y)
    return float(model.score(test_x_vec, test_y))


def main():
    np.random.seed(RANDOM_STATE)
    texts, labels = load_features_labels(DATASET_PATH)
    accuracy = train_and_evaluate(texts, labels)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# Reduced imports to essentials to cut load time and memory usage.
# Added robust CSV parsing with schema resolution to avoid misparsing and rework.
# Removed model serialization, extra prediction calls, and unused modes to lower I/O and compute.
# Performed a single TF-IDF fit/transform pipeline to minimize redundant data processing.
# Fixed random seeds and random_state parameters for deterministic, reproducible results.