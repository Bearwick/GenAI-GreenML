# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import sys
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC

DATASET_HEADERS = "text,label"
SEED = 42
np.random.seed(SEED)


def read_csv_with_fallback(path, expected_headers):
    expected_norm = [h.strip().lower() for h in expected_headers]
    try:
        df = pd.read_csv(path, encoding="utf-8")
    except Exception:
        return pd.read_csv(path, sep=";", decimal=",", encoding="utf-8")
    cols_norm = [str(c).strip().lower() for c in df.columns]
    if len(df.columns) < len(expected_headers) or not set(expected_norm).issubset(cols_norm):
        try:
            df_alt = pd.read_csv(path, sep=";", decimal=",", encoding="utf-8")
            cols_norm_alt = [str(c).strip().lower() for c in df_alt.columns]
            if len(df_alt.columns) >= len(expected_headers) and set(expected_norm).issubset(cols_norm_alt):
                df = df_alt
            elif df_alt.shape[1] > df.shape[1]:
                df = df_alt
        except Exception:
            pass
    return df


def resolve_columns(df, expected_headers):
    cols_map = {str(c).strip().lower(): c for c in df.columns}
    resolved = []
    for h in expected_headers:
        key = h.strip().lower()
        if key in cols_map:
            resolved.append(cols_map[key])
    if len(resolved) < len(expected_headers):
        resolved = list(df.columns[:len(expected_headers)])
    if len(resolved) < len(expected_headers):
        raise ValueError("Required columns not found")
    return resolved[0], resolved[1]


def get_dataset_path():
    for arg in sys.argv[1:]:
        if os.path.isfile(arg) and arg.lower().endswith(".csv"):
            return arg
    csv_files = sorted([f for f in os.listdir(".") if f.lower().endswith(".csv")])
    if csv_files:
        return csv_files[0]
    raise FileNotFoundError("No CSV dataset found")


def load_data():
    expected_headers = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]
    path = get_dataset_path()
    df = read_csv_with_fallback(path, expected_headers)
    text_col, label_col = resolve_columns(df, expected_headers)
    labels = df[label_col]
    texts = df[text_col]
    mask = labels.notna()
    texts = texts[mask].fillna("").astype(str)
    labels = labels[mask]
    return texts, labels


def train_evaluate(texts, labels):
    train_x, test_x, train_y, test_y = train_test_split(
        texts, labels, train_size=0.75, test_size=0.25, random_state=SEED
    )
    vectorizer = TfidfVectorizer()
    train_vec = vectorizer.fit_transform(train_x)
    test_vec = vectorizer.transform(test_x)
    model = LinearSVC(random_state=SEED)
    model.fit(train_vec, train_y)
    accuracy = model.score(test_vec, test_y)
    return accuracy


def main():
    texts, labels = load_data()
    accuracy = train_evaluate(texts, labels)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()


# Optimization Summary
# - Removed unused imports and dead code paths to reduce startup time and memory.
# - Implemented single-pass CSV loading with a fallback parser to avoid redundant I/O.
# - Resolved columns dynamically from provided headers to prevent hardcoded schema assumptions.
# - Set deterministic seeds and random_state for reproducible data splits and model training.
# - Used model.score for accuracy to avoid extra prediction allocations.