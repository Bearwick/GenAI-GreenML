# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.sql.functions import when, col

def run_classification():
    spark = SparkSession.builder \
        .appName("GreenML_Classification") \
        .master("local[1]") \
        .config("spark.driver.host", "localhost") \
        .config("spark.sql.shuffle.partitions", "1") \
        .config("spark.ui.enabled", "false") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("ERROR")

    path = "heart.csv"
    try:
        df = spark.read.option("sep", " ") \
            .option("inferSchema", "true") \
            .option("ignoreLeadingWhiteSpace", "true") \
            .option("ignoreTrailingWhiteSpace", "true") \
            .csv(path)
        if len(df.columns) < 13:
            raise ValueError
    except:
        df = spark.read.option("sep", ";") \
            .option("decimal", ",") \
            .option("inferSchema", "true") \
            .csv(path)

    all_cols = df.columns
    feature_cols = all_cols[0:12]
    thal_col = all_cols[12]

    df = df.withColumn("label", when((col(thal_col) == 3.0) | (col(thal_col) == 7.0), 0.0).otherwise(1.0))

    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
    rf = RandomForestClassifier(labelCol="label", featuresCol="scaled_features", numTrees=200, seed=12345)

    pipeline = Pipeline(stages=[assembler, scaler, rf])

    train_df, test_df = df.randomSplit([0.5, 0.5], seed=12345)
    
    model = pipeline.fit(train_df)
    predictions = model.transform(test_df)

    evaluator = BinaryClassificationEvaluator(labelCol="label", metricName="areaUnderROC")
    auc = evaluator.evaluate(predictions)
    
    print(f"ACCURACY={auc:.6f}")
    
    spark.stop()

if __name__ == "__main__":
    run_classification()

# Optimization Summary
# 1. Replaced redundant Pandas-to-Spark data conversion with native Spark CSV reading to minimize memory overhead and data movement.
# 2. Implemented a Spark Pipeline to streamline feature engineering and model training, reducing multiple passes over the dataset.
# 3. Limited Spark parallelism (local[1] and shuffle.partitions=1) to suit the small dataset size, significantly reducing scheduling overhead.
# 4. Disabled Spark UI and lowered log levels to reduce background CPU and I/O consumption.
# 5. Consolidated multiple individual imports and removed unused ML algorithms (Logistic Regression, Linear Regression, etc.) to decrease library loading time.
# 6. Replaced the manual Python 'isSick' function with optimized Spark SQL built-in 'when' functions for vectorized processing.
# 7. Removed all intermediate 'show', 'print', and visualization calls to eliminate unnecessary computational tasks and I/O.
# 8. Set fixed seeds for both data splitting and the RandomForest model to ensure reproducibility and stable energy consumption across runs.