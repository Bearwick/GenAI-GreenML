# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
import pandas as pd

def is_sick(x):
    return 0 if x in (3, 7) else 1

def solve():
    spark = SparkSession.builder \
        .master("local[1]") \
        .config("spark.sql.shuffle.partitions", "1") \
        .config("spark.ui.enabled", "false") \
        .config("spark.driver.host", "127.0.0.1") \
        .getOrCreate()

    cols = [
        'age', 'sex', 'chest pain', 'resting blood pressure', 'serum cholesterol',
        'fasting blood sugar', 'resting electrocardiographic results',
        'maximum heart rate achieved', 'exercise induced angina',
        'ST depression induced by exercise relative to rest',
        'the slope of the peak exercise ST segment', 'number of major vessels ',
        'thal', 'last'
    ]

    try:
        data = pd.read_csv('heart.csv', sep=r'\s+', names=cols, engine='python')
        if data.shape[1] < 14:
            raise ValueError
    except Exception:
        data = pd.read_csv('heart.csv', sep=';', decimal=',', names=cols)

    df_pd = data.iloc[:, 0:13].copy()
    df_pd['label'] = df_pd['thal'].apply(is_sick)

    features_list = [
        'age', 'sex', 'chest pain', 'resting blood pressure', 'serum cholesterol',
        'fasting blood sugar', 'resting electrocardiographic results',
        'maximum heart rate achieved', 'exercise induced angina',
        'ST depression induced by exercise relative to rest',
        'the slope of the peak exercise ST segment', 'number of major vessels '
    ]

    sdf = spark.createDataFrame(df_pd)

    assembler = VectorAssembler(inputCols=features_list, outputCol="features")
    raw_data = assembler.transform(sdf)

    scaler = StandardScaler(inputCol="features", outputCol="Scaled_features", withStd=True, withMean=False)
    scaled_data = scaler.fit(raw_data).transform(raw_data)

    training, test = scaled_data.randomSplit([0.5, 0.5], seed=12345)

    rf = RandomForestClassifier(
        labelCol="label", 
        featuresCol="Scaled_features", 
        numTrees=200, 
        seed=12345
    )
    model = rf.fit(training)
    predictions = model.transform(test)

    evaluator = MulticlassClassificationEvaluator(
        labelCol="label", 
        predictionCol="prediction", 
        metricName="accuracy"
    )
    accuracy = evaluator.evaluate(predictions)

    print(f"ACCURACY={accuracy:.6f}")
    spark.stop()

if __name__ == "__main__":
    solve()

# Optimization Summary
# 1. Restricted Spark to single-threaded local mode (local[1]) and disabled the Web UI to minimize orchestration overhead.
# 2. Set shuffle partitions to 1 to eliminate the creation of hundreds of unnecessary empty tasks for a small dataset.
# 3. Streamlined data processing by removing intermediate show() calls and redundant print statements, reducing I/O operations.
# 4. Implemented robust CSV parsing with a fallback mechanism to handle different delimiters efficiently in one pass.
# 5. Reduced memory footprint by selecting necessary columns early in the Pandas stage before converting to a Spark DataFrame.
# 6. Ensured reproducibility and removed unnecessary computational search by fixing seeds for data splitting and the classifier.
# 7. Cleaned up unused library imports and commented-out code to reduce the runtime environment's initialization energy.