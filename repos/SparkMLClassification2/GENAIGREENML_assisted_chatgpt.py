# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator


SEED = 12345
DATASET_PATH = "heart.csv"
DATASET_HEADERS = "70.0 1.0 4.0 130.0 322.0 0.0 2.0 109.0 0.0 2.4 2.0 3.0 3.0 2"


def _read_csv_with_fallback(path: str, headers_hint: str) -> pd.DataFrame:
    names = [f"c{i}" for i in range(len(headers_hint.strip().split()))]

    def try_read(**kwargs):
        return pd.read_csv(path, header=None, names=names, **kwargs)

    df = try_read(sep=None, engine="python")
    if df.shape[1] != len(names) or df.isna().all(axis=None):
        df = try_read(sep=";", decimal=",")
    if df.shape[1] != len(names):
        df = try_read(delim_whitespace=True)
    if df.shape[1] != len(names):
        raise ValueError(f"Unexpected column count: got {df.shape[1]}, expected {len(names)}")
    return df


def _build_spark() -> SparkSession:
    builder = (
        SparkSession.builder.appName("SparkML_Heart_RF_Green")
        .config("spark.sql.shuffle.partitions", "8")
        .config("spark.default.parallelism", "8")
    )
    return builder.getOrCreate()


def _make_label_from_thal(series: pd.Series) -> pd.Series:
    return (~series.isin([3, 7])).astype("int32")


def run() -> float:
    spark = _build_spark()

    pdf = _read_csv_with_fallback(DATASET_PATH, DATASET_HEADERS)

    pdf13 = pdf.iloc[:, :13].copy()
    pdf13["label"] = _make_label_from_thal(pdf13.iloc[:, 12])

    df = spark.createDataFrame(pdf13)

    feature_cols = list(df.columns[:12])
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    scaler = StandardScaler(inputCol="features", outputCol="Scaled_features", withMean=False, withStd=True)

    assembled = assembler.transform(df)
    scaler_model = scaler.fit(assembled)
    data = scaler_model.transform(assembled).select("Scaled_features", "label")

    training, test = data.randomSplit([0.5, 0.5], seed=SEED)

    rf = RandomForestClassifier(
        labelCol="label",
        featuresCol="Scaled_features",
        numTrees=200,
        seed=SEED,
    )
    model = rf.fit(training)
    pred_test = model.transform(test)

    evaluator = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="rawPrediction", metricName="areaUnderROC")
    accuracy = float(evaluator.evaluate(pred_test))

    spark.stop()
    return accuracy


def main():
    accuracy = run()
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused imports, plotting, and all intermediate .show()/prints to reduce driver-side overhead and Spark actions.
# - Replaced row-wise Python apply() with a vectorized pandas isin() operation for labeling to minimize Python-level looping.
# - Avoided hard-coded feature names by deriving schema from DATASET_HEADERS and actual DataFrame columns, reducing brittleness and rework.
# - Added robust CSV parsing fallback (default -> ';' with decimal ',' -> whitespace) to prevent costly mis-parses and retries later in the pipeline.
# - Reduced Spark shuffle/parallelism defaults via config to cut unnecessary task overhead for small datasets while preserving outputs.
# - Selected only required columns after scaling (features + label) to reduce data movement and memory footprint in subsequent stages.
# - Set fixed seeds for split and model to ensure reproducibility and stable evaluation results.