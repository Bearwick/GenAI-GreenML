# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, DoubleType

from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator


SEED = 12345
DATASET_HEADERS = "70.0 1.0 4.0 130.0 322.0 0.0 2.0 109.0 0.0 2.4 2.0 3.0 3.0 2"


def _build_spark() -> SparkSession:
    builder = (
        SparkSession.builder.appName("HeartClassificationGreen")
        .config("spark.sql.shuffle.partitions", "8")
        .config("spark.default.parallelism", "8")
    )
    return builder.getOrCreate()


def _parse_headers(headers_str: str):
    toks = [t for t in headers_str.strip().split() if t]
    return [f"c{i}" for i in range(len(toks))]


def _read_csv_robust(path: str, ncols_expected: int) -> pd.DataFrame:
    def _read_try(**kwargs):
        return pd.read_csv(path, header=None, **kwargs)

    df = _read_try()
    if df.shape[1] != ncols_expected:
        df = _read_try(sep=";", decimal=",")
    if df.shape[1] != ncols_expected:
        df = _read_try(sep=r"\s+")
    if df.shape[1] != ncols_expected:
        df = _read_try(sep=r"\s+", engine="python")
    if df.shape[1] != ncols_expected:
        raise ValueError(f"Unexpected number of columns: got {df.shape[1]}, expected {ncols_expected}")
    return df


def _pandas_to_spark(spark: SparkSession, df: pd.DataFrame):
    schema = StructType([StructField(c, DoubleType(), True) for c in df.columns])
    return spark.createDataFrame(df.astype(np.float64, copy=False), schema=schema)


def classify() -> float:
    spark = _build_spark()
    spark.sparkContext.setLogLevel("ERROR")

    cols = _parse_headers(DATASET_HEADERS)
    ncols = len(cols)

    pdf = _read_csv_robust("heart.csv", ncols_expected=ncols)
    pdf.columns = cols

    feature_cols = cols[:12]
    thal_col = cols[12]

    pdf = pdf.iloc[:, :13].copy()
    pdf["label"] = (~pdf[thal_col].isin([3.0, 7.0])).astype(np.float64)

    sdf = _pandas_to_spark(spark, pdf)

    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    scaler = StandardScaler(inputCol="features", outputCol="Scaled_features", withMean=False, withStd=True)
    rf = RandomForestClassifier(
        labelCol="label",
        featuresCol="Scaled_features",
        numTrees=200,
        seed=SEED,
        featureSubsetStrategy="auto",
        subsamplingRate=1.0,
    )

    pipeline = Pipeline(stages=[assembler, scaler, rf])

    train_df, test_df = sdf.randomSplit([0.5, 0.5], seed=SEED)

    model = pipeline.fit(train_df)
    preds = model.transform(test_df)

    evaluator = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="rawPrediction", metricName="areaUnderROC")
    accuracy = float(evaluator.evaluate(preds))

    spark.stop()
    return accuracy


def main():
    accuracy = classify()
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed all `.show()`, plotting, and intermediate prints to avoid expensive Spark actions and driver-side overhead.
# - Combined feature assembly, scaling, and model training into a single Spark ML Pipeline to reduce repeated DataFrame materialization.
# - Replaced row-wise Python `apply` with vectorized pandas `isin` to minimize Python-loop overhead before Spark ingestion.
# - Ensured deterministic results by setting a fixed seed for both `randomSplit` and `RandomForestClassifier`.
# - Added robust CSV parsing with fallbacks (default, semicolon/decimal, whitespace) to avoid misparsing and reruns.
# - Reduced shuffle/parallelism defaults to limit unnecessary task overhead on small datasets, improving runtime/energy use.
# - Created an explicit numeric Spark schema and used float64 casting to avoid expensive type inference and conversions.