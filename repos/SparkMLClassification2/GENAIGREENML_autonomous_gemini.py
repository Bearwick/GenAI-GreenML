# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

def load_data(path):
    # Robust loading: try default, then common variations
    try:
        df = pd.read_csv(path)
        # Check if first row is data instead of headers
        if all(str(col).replace('.', '', 1).isdigit() for col in df.columns):
            df = pd.read_csv(path, header=None)
    except:
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except:
            df = pd.read_csv(path, sep=r'\s+', engine='python')
    
    # Normalize column names
    df.columns = [str(c).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def preprocess_and_train():
    try:
        df = load_data('heart.csv')
    except Exception:
        # Final fallback: create empty to trigger safe failure
        df = pd.DataFrame()

    if df.empty:
        print("ACCURACY=0.000000")
        return

    # Identify features and target (last column is usually the label in heart datasets)
    # Ensure columns are numeric
    for col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Drop columns that are entirely NaN after coercion
    df = df.dropna(axis=1, how='all')
    
    # Handle missing values in rows
    imputer = SimpleImputer(strategy='median')
    data_imputed = imputer.fit_transform(df)
    df = pd.DataFrame(data_imputed, columns=df.columns)

    if df.shape[1] < 2:
        print("ACCURACY=0.000000")
        return

    X = df.iloc[:, :-1]
    y = df.iloc[:, -1]

    # Handle classification labels (ensure they are discrete integers)
    # Heart dataset often uses 1/2 or 0/1.
    y = y.astype(int)

    # Check if we have at least 2 classes for classification
    unique_y = np.unique(y)
    if len(unique_y) < 2:
        # If only one class, accuracy is trivially 1.0 or 0.0
        print(f"ACCURACY={1.0 if len(unique_y) == 1 else 0.0:.6f}")
        return

    # Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if len(unique_y) > 1 else None
    )

    # Energy-efficient pipeline: Standard Scaler + Logistic Regression
    # Logistic Regression is CPU-friendly and efficient for small/medium datasets.
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(max_iter=1000, solver='lbfgs', penalty='l2'))
    ])

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    preprocess_and_train()

# Optimization Summary:
# 1. Model Choice: Logistic Regression was chosen as it is a highly energy-efficient, 
#    low-complexity linear model suitable for CPU-bound environments.
# 2. Preprocessing: Used a Pipeline with StandardScaler to ensure data compatibility 
#    with the solver while maintaining a small memory footprint.
# 3. Robustness: Implemented robust CSV loading and schema normalization to handle 
#    variations in delimiters, headers, and numeric formats without manual intervention.
# 4. CPU Efficiency: Avoided ensemble methods (like Random Forest/XGBoost) and 
#    Deep Learning to minimize FLOPs and energy consumption during training and inference.
# 5. Data Handling: Used median imputation and numeric coercion to ensure the 
#    pipeline runs end-to-end even with noisy or malformed input data.
# 6. Evaluation: Used a standard 80/20 split with stratification to maintain 
#    reproducibility and valid statistical estimation of model performance.
# 7. Fallback: Included defensive checks for empty datasets or single-class targets 
#    to prevent runtime crashes and return a valid (though trivial) accuracy score.