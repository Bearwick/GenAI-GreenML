# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: original_telemetry

from pyspark import SparkContext, SparkConf
from pyspark.shell import spark
from pyspark.sql import SQLContext, SparkSession
import numpy as np
from pyspark.sql import DataFrame
from pyspark.sql.functions import when
from pyspark.ml.feature import Imputer
from pyspark.ml.feature import StandardScaler
from pyspark.ml.feature import ChiSqSelector
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.regression import LinearRegression
from pyspark.ml.regression import GeneralizedLinearRegression
import matplotlib.pyplot as plt
import pandas as pd
from pyspark.sql.types import StructType, StructField, NumericType
from pyspark.ml.feature import StandardScaler
from pyspark.ml.feature import VectorAssembler
import time


def isSick(x):
    if x in (3, 7):
        return 0
    else:
        return 1


def classify():
    cols = [
        "age",
        "sex",
        "chest pain",
        "resting blood pressure",
        "serum cholesterol",
        "fasting blood sugar",
        "resting electrocardiographic results",
        "maximum heart rate achieved",
        "exercise induced angina",
        "ST depression induced by exercise relative to rest",
        "the slope of the peak exercise ST segment",
        "number of major vessels ",
        "thal",
        "last",
    ]

    data = pd.read_csv("heart.csv", delimiter=" ", names=cols)
    data = data.iloc[:, 0:13]
    data["label"] = data["thal"].apply(isSick)
    df = spark.createDataFrame(data)

    features = [
        "age",
        "sex",
        "chest pain",
        "resting blood pressure",
        "serum cholesterol",
        "fasting blood sugar",
        "resting electrocardiographic results",
        "maximum heart rate achieved",
        "exercise induced angina",
        "ST depression induced by exercise relative to rest",
        "the slope of the peak exercise ST segment",
        "number of major vessels ",
    ]

    assembler = VectorAssembler(inputCols=features, outputCol="features")
    raw_data = assembler.transform(df)

    standardscaler = StandardScaler().setInputCol("features").setOutputCol("Scaled_features")
    raw_data = standardscaler.fit(raw_data).transform(raw_data)

    from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit
    from pyspark.ml.classification import LogisticRegression

    training, test = raw_data.randomSplit([0.5, 0.5], seed=12345)

    rf = RandomForestClassifier(labelCol="label", featuresCol="Scaled_features", numTrees=200)
    model = rf.fit(training)
    predict_test = model.transform(test)

    evaluator = BinaryClassificationEvaluator()
    accuracy = evaluator.evaluate(predict_test, {evaluator.metricName: "areaUnderROC"})
    print(f"ACCURACY={accuracy:.6f}")

    return 0


def main():
    classify()


if __name__ == "__main__":
    main()