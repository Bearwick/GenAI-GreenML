# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
import random
from pyspark.sql import SparkSession, functions as F
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier

DATASET_HEADERS = "70.0 1.0 4.0 130.0 322.0 0.0 2.0 109.0 0.0 2.4 2.0 3.0 3.0 2"
DATA_PATH = "heart.csv"
SEED = 12345


def _is_number(x):
    try:
        float(str(x))
        return True
    except Exception:
        return False


def parse_header_tokens(header_str):
    return [h for h in str(header_str).strip().replace(",", " ").split() if h]


def parsing_wrong(df, expected_cols, header_tokens):
    if df.shape[1] != expected_cols:
        return True
    col_names = [str(c).strip() for c in df.columns]
    if col_names == header_tokens:
        return True
    if not isinstance(df.columns, pd.RangeIndex) and all(_is_number(c) for c in df.columns):
        return True
    return False


def normalize_columns(df, expected_cols):
    if df.shape[1] > expected_cols:
        df = df.iloc[:, :expected_cols]
    if df.shape[1] < expected_cols:
        raise ValueError("Unexpected number of columns")
    col_names = list(df.columns)
    needs_rename = isinstance(df.columns, pd.RangeIndex) or df.columns.duplicated().any()
    if not needs_rename and all(_is_number(c) for c in col_names):
        needs_rename = True
    if needs_rename:
        df.columns = [f"col_{i}" for i in range(df.shape[1])]
    else:
        df.columns = [str(c) for c in col_names]
    return df


def read_dataset(path, header_str):
    header_tokens = parse_header_tokens(header_str)
    expected_cols = len(header_tokens)
    df = pd.read_csv(path)
    if parsing_wrong(df, expected_cols, header_tokens):
        df = pd.read_csv(path, sep=';', decimal=',')
        if parsing_wrong(df, expected_cols, header_tokens):
            df = pd.read_csv(path, sep=r'\s+', header=None, engine='python')
    df = normalize_columns(df, expected_cols)
    return df


def prepare_dataframe(df):
    df = df.iloc[:, :-1].copy()
    label_source = df.columns[-1]
    label_values = pd.to_numeric(df[label_source], errors='coerce')
    df = df.drop(columns=[label_source])
    df = df.apply(pd.to_numeric, errors='coerce')
    feature_cols = df.columns.tolist()
    df['label'] = (~label_values.isin([3, 7])).astype(int)
    return df, feature_cols


def init_spark():
    spark = (SparkSession.builder
             .appName("HeartRF")
             .master("local[*]")
             .config("spark.sql.shuffle.partitions", "1")
             .config("spark.default.parallelism", "1")
             .config("spark.ui.showConsoleProgress", "false")
             .getOrCreate())
    spark.sparkContext.setLogLevel("ERROR")
    return spark


def train_and_evaluate(df, feature_cols, seed):
    spark = init_spark()
    spark_df = spark.createDataFrame(df)
    spark_df = spark_df.withColumn("label", F.col("label").cast("double"))
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    assembled = assembler.transform(spark_df.select(*feature_cols, "label"))
    scaler = StandardScaler(inputCol="features", outputCol="scaled_features", withStd=True, withMean=False)
    scaled = scaler.fit(assembled).transform(assembled).select("scaled_features", "label")
    train, test = scaled.randomSplit([0.5, 0.5], seed=seed)
    rf = RandomForestClassifier(labelCol="label", featuresCol="scaled_features", numTrees=200, seed=seed)
    model = rf.fit(train)
    predictions = model.transform(test).select("label", "prediction")
    accuracy = predictions.select((F.col("label") == F.col("prediction")).cast("double").alias("correct")) \
        .agg(F.avg("correct").alias("accuracy")).collect()[0]["accuracy"]
    spark.stop()
    return accuracy


def main():
    random.seed(SEED)
    np.random.seed(SEED)
    df = read_dataset(DATA_PATH, DATASET_HEADERS)
    df, feature_cols = prepare_dataframe(df)
    accuracy = train_and_evaluate(df, feature_cols, SEED)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()
# Optimization Summary
# Reduced data movement by trimming unused columns early and selecting only feature/label columns in Spark.
# Vectorized label generation and numeric conversion to avoid row-wise Python loops.
# Minimized Spark workload with low shuffle partitions and eliminated unnecessary actions.
# Ensured reproducibility with fixed seeds and deterministic splitting/model initialization.