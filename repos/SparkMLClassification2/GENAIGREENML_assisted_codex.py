# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import random
import numpy as np
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, DoubleType
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator

DATASET_PATH = "heart.csv"
DATASET_HEADERS = "70.0 1.0 4.0 130.0 322.0 0.0 2.0 109.0 0.0 2.4 2.0 3.0 3.0 2"
SEED = 12345

def parse_header_tokens(header_str):
    return [t for t in header_str.replace(",", " ").split() if t]

def is_parsing_wrong(df, expected_cols, header_tokens):
    if df is None or df.empty:
        return True
    if expected_cols is not None and df.shape[1] != expected_cols:
        return True
    if header_tokens and [str(c) for c in df.columns] == header_tokens:
        return True
    return False

def read_csv_robust(path, expected_cols, header_tokens):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if is_parsing_wrong(df, expected_cols, header_tokens):
        try:
            df = pd.read_csv(path, sep=";", decimal=",", header=None)
        except Exception:
            df = None
        if is_parsing_wrong(df, expected_cols, header_tokens):
            try:
                with open(path, "r") as f:
                    first_line = f.readline()
            except Exception:
                first_line = ""
            if "," in first_line:
                df = pd.read_csv(path, header=None)
            else:
                df = pd.read_csv(path, sep=r"\s+", header=None, engine="python")
    return df

def load_and_prepare_data(path, header_str):
    header_tokens = parse_header_tokens(header_str)
    expected_cols = len(header_tokens) if header_tokens else None
    df = read_csv_robust(path, expected_cols, header_tokens)
    if df is None or df.empty:
        raise ValueError("Failed to read dataset.")
    n_cols = df.shape[1]
    if n_cols < 13:
        raise ValueError("Dataset must have at least 13 columns.")
    df = df.copy()
    df.columns = [f"c{i}" for i in range(n_cols)]
    if not all(pd.api.types.is_numeric_dtype(t) for t in df.dtypes):
        df = df.apply(pd.to_numeric, errors="coerce")
    if df.isnull().values.any():
        df = df.dropna()
    df = df.iloc[:, :13]
    feature_cols = df.columns[:12].tolist()
    thal_col = df.columns[12]
    df["label"] = np.where(df[thal_col].isin([3.0, 7.0]), 0.0, 1.0)
    df = df.drop(columns=[thal_col])
    df["label"] = df["label"].astype(float)
    return df, feature_cols

def create_spark_session():
    spark = SparkSession.builder.appName("HeartClassification").config("spark.sql.shuffle.partitions", "4").getOrCreate()
    spark.sparkContext.setLogLevel("ERROR")
    return spark

def pandas_to_spark(spark, df):
    schema = StructType([StructField(c, DoubleType(), True) for c in df.columns])
    return spark.createDataFrame(df, schema=schema)

def train_and_evaluate(spark_df, feature_cols, seed):
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    assembled = assembler.transform(spark_df).select("label", "features")
    scaler = StandardScaler(inputCol="features", outputCol="Scaled_features", withStd=True, withMean=False)
    scaled = scaler.fit(assembled).transform(assembled).select("label", "Scaled_features")
    training, test = scaled.randomSplit([0.5, 0.5], seed=seed)
    rf = RandomForestClassifier(labelCol="label", featuresCol="Scaled_features", numTrees=200, seed=seed)
    model = rf.fit(training)
    predictions = model.transform(test).select("label", "prediction", "rawPrediction").cache()
    evaluator = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="rawPrediction", metricName="areaUnderROC")
    _ = evaluator.evaluate(predictions)
    accuracy_row = predictions.select((F.col("label") == F.col("prediction")).cast("double").alias("correct")).agg(F.avg("correct")).first()
    accuracy = accuracy_row[0] if accuracy_row and accuracy_row[0] is not None else 0.0
    predictions.unpersist()
    return float(accuracy)

def main():
    random.seed(SEED)
    np.random.seed(SEED)
    spark = create_spark_session()
    df, feature_cols = load_and_prepare_data(DATASET_PATH, DATASET_HEADERS)
    spark_df = pandas_to_spark(spark, df)
    accuracy = train_and_evaluate(spark_df, feature_cols, SEED)
    print(f"ACCURACY={accuracy:.6f}")
    spark.stop()

if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused imports, plotting, and display actions to eliminate unnecessary computation and I/O.
# - Used vectorized label creation and dropped the unused thal column to reduce data size and memory footprint.
# - Selected only required columns during transformations and cached predictions once to avoid recomputation for metrics.
# - Set fixed seeds and reduced Spark shuffle partitions to improve reproducibility and lower overhead.
# - Added robust CSV parsing with minimal re-reads and an explicit schema to avoid costly type inference.