# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder

SEED = 42
np.random.seed(SEED)

DATASET_HEADERS = "feature1,feature2,feature3,feature4,feature5,label"
EXPECTED_HEADERS = [h.strip() for h in DATASET_HEADERS.split(",")]

def read_csv_with_fallback(path, expected_headers):
    df = pd.read_csv(path)
    if df.shape[1] == 1 and len(expected_headers) > 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df

def find_label_column(columns, expected_headers):
    cols = [str(c).strip() for c in columns]
    lower_cols = [c.lower() for c in cols]
    if "label" in lower_cols:
        return cols[lower_cols.index("label")]
    expected_lower = [h.lower() for h in expected_headers]
    for h in reversed(expected_lower):
        if h in lower_cols:
            return cols[lower_cols.index(h)]
    return cols[-1]

df = read_csv_with_fallback("cat_dog_data.csv", EXPECTED_HEADERS)
df.columns = [str(c).strip() for c in df.columns]

label_col = find_label_column(df.columns, EXPECTED_HEADERS)

X = df.drop(columns=[label_col]).to_numpy(copy=False)
y = LabelEncoder().fit_transform(df[label_col].to_numpy(copy=False))
del df

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=SEED
)

model = SVC(kernel="linear", random_state=SEED)
model.fit(X_train, y_train)

accuracy = model.score(X_test, y_test)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Used a single CSV read with delimiter fallback to avoid repeated parsing when unnecessary.
# Trimmed column names and inferred the label column from expected headers to prevent hardcoding.
# Converted data to NumPy arrays early and used model.score to avoid storing predictions.
# Removed unused metric calculations and released the DataFrame to reduce memory footprint.
# Set a fixed random seed for reproducible train/test splits.