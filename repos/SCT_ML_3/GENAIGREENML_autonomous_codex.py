# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier
from sklearn.metrics import accuracy_score, r2_score

def read_csv_robust(path):
    try:
        df_local = pd.read_csv(path)
    except Exception:
        df_local = pd.read_csv(path, sep=';', decimal=',')
        return df_local
    if df_local.shape[1] == 1:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > df_local.shape[1]:
                df_local = df_alt
        except Exception:
            pass
    return df_local

df = read_csv_robust("cat_dog_data.csv")
df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
df = df.loc[:, [c for c in df.columns if not c.lower().startswith('unnamed')]]
assert df.shape[0] > 0 and df.shape[1] > 0

cols_lower = [c.lower() for c in df.columns]
target = None
for p in ["label", "target", "y", "class"]:
    if p in cols_lower:
        target = df.columns[cols_lower.index(p)]
        break
if target is None:
    numeric_candidates = []
    for c in df.columns:
        series = pd.to_numeric(df[c], errors='coerce')
        if series.notna().sum() > 0 and series.nunique(dropna=True) > 1:
            numeric_candidates.append(c)
    if numeric_candidates:
        target = numeric_candidates[-1]
    else:
        for c in df.columns[::-1]:
            if df[c].nunique(dropna=True) > 1:
                target = c
                break
    if target is None:
        target = df.columns[-1]

features = [c for c in df.columns if c != target]
if len(features) == 0:
    df["dummy_feature"] = 0
    features = ["dummy_feature"]

df_model = df[features + [target]].copy()
y_raw = df_model[target]
if y_raw.dtype == object:
    y_raw = y_raw.replace(r'^\s*$', np.nan, regex=True)
mask = y_raw.notna()
df_model = df_model.loc[mask].copy()
y_raw = y_raw.loc[mask]
assert df_model.shape[0] > 0

n_samples = len(y_raw)
unique_values = y_raw.nunique(dropna=True)
is_classification = False
if y_raw.dtype == object or str(y_raw.dtype).startswith('category') or y_raw.dtype == bool:
    is_classification = True
else:
    if unique_values <= max(20, int(0.2 * n_samples)):
        is_classification = True

if is_classification:
    le = LabelEncoder()
    y = le.fit_transform(y_raw.astype(str))
else:
    y_numeric = pd.to_numeric(y_raw, errors='coerce')
    mask_num = y_numeric.notna()
    df_model = df_model.loc[mask_num].copy()
    y = y_numeric.loc[mask_num].values
    assert df_model.shape[0] > 0

X = df_model[features].copy()
numeric_cols = []
categorical_cols = []
for c in X.columns:
    col_data = X[c]
    if col_data.dtype == bool:
        X[c] = col_data.astype(int)
        numeric_cols.append(c)
        continue
    if np.issubdtype(col_data.dtype, np.number):
        X[c] = pd.to_numeric(col_data, errors='coerce')
        numeric_cols.append(c)
        continue
    conv = pd.to_numeric(col_data, errors='coerce')
    num_ratio = conv.notna().mean()
    if num_ratio >= 0.5:
        X[c] = conv
        numeric_cols.append(c)
    else:
        X[c] = col_data.astype(str)
        categorical_cols.append(c)

if numeric_cols:
    X[numeric_cols] = X[numeric_cols].replace([np.inf, -np.inf], np.nan)

if len(X) < 2:
    X = pd.concat([X, X], ignore_index=True)
    y = np.concatenate([y, y])

stratify = None
if is_classification and len(np.unique(y)) >= 2:
    _, counts = np.unique(y, return_counts=True)
    if counts.min() >= 2:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=stratify
)
assert len(X_train) > 0 and len(X_test) > 0

transformers = []
if numeric_cols:
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False))
    ])
    transformers.append(("num", numeric_transformer, numeric_cols))
if categorical_cols:
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])
    transformers.append(("cat", categorical_transformer, categorical_cols))

if transformers:
    preprocess = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)
else:
    preprocess = "passthrough"

if is_classification:
    if len(np.unique(y)) < 2:
        model = DummyClassifier(strategy="most_frequent")
    else:
        model = LogisticRegression(max_iter=200, solver="liblinear", random_state=42)
else:
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if is_classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    if len(y_test) < 2 or np.all(y_test == y_test[0]):
        accuracy = 1.0 if np.allclose(y_pred, y_test[0]) else 0.0
    else:
        r2 = r2_score(y_test, y_pred)
        if not np.isfinite(r2):
            r2 = -1.0
        accuracy = float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# - Used lightweight linear/Dummy models with minimal preprocessing for CPU and energy efficiency.
# - Applied ColumnTransformer with simple imputers and one-hot encoding only when required for schema robustness.
# - Regression fallback reports a clipped (r2+1)/2 proxy to keep ACCURACY bounded in [0,1].