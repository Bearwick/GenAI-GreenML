# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score


DATASET_HEADERS = ["feature1", "feature2", "feature3", "feature4", "feature5", "label"]
RANDOM_SEED = 42


def _read_csv_with_fallback(path: str, expected_headers):
    df = pd.read_csv(path)
    if not _parsing_looks_ok(df, expected_headers):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _parsing_looks_ok(df: pd.DataFrame, expected_headers) -> bool:
    if df.empty:
        return False
    cols = list(df.columns)
    expected = list(expected_headers)
    if len(cols) == 1 and any(h in str(cols[0]) for h in expected):
        return False
    overlap = len(set(map(str, cols)) & set(map(str, expected)))
    return overlap >= 2 and "label" in df.columns


def _get_label_column(df: pd.DataFrame, expected_headers):
    if "label" in df.columns:
        return "label"
    for c in df.columns:
        if str(c).strip().lower() == "label":
            return c
    for c in df.columns:
        if str(c) in expected_headers and str(c).lower() == "label":
            return c
    raise KeyError("Label column not found.")


def _coerce_numeric_features(X: pd.DataFrame) -> pd.DataFrame:
    X_num = X.apply(pd.to_numeric, errors="coerce")
    return X_num.fillna(X_num.median(numeric_only=True))


def main():
    df = _read_csv_with_fallback("cat_dog_data.csv", DATASET_HEADERS)

    label_col = _get_label_column(df, DATASET_HEADERS)
    X = df.drop(columns=[label_col], copy=False)
    y = df[label_col]

    X = _coerce_numeric_features(X)

    encoder = LabelEncoder()
    y_encoded = encoder.fit_transform(y.astype(str))

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y_encoded,
        test_size=0.3,
        random_state=RANDOM_SEED,
        stratify=y_encoded if len(set(y_encoded)) > 1 else None,
    )

    model = LinearSVC(random_state=RANDOM_SEED)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced SVC(kernel="linear") with LinearSVC to avoid computing/storing the full kernel matrix for linear problems, reducing runtime and memory.
# - Removed classification_report generation and all non-required printing to eliminate extra computations and I/O overhead while preserving the core task/output (accuracy).
# - Used drop(columns=[...], copy=False) to reduce unnecessary DataFrame copies and data movement.
# - Added robust CSV parsing fallback (sep=';' and decimal=',') to prevent costly downstream failures/retries and ensure stable ingestion.
# - Coerced features to numeric once and filled missing values via median to avoid repeated type conversions and potential model fit errors.
# - Set fixed random seeds and used stratified splitting when feasible for reproducibility with minimal overhead.