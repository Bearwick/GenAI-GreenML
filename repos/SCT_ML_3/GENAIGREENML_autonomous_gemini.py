# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
import sys
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score

def run_pipeline():
    # 1. Robust CSV Parsing
    file_path = 'cat_dog_data.csv'
    try:
        df = pd.read_csv(file_path)
        # If pandas incorrectly parsed a single column, retry with common delimiters
        if df.shape[1] <= 1:
            df = pd.read_csv(file_path, sep=';', decimal=',')
    except Exception:
        # If reading fails, exit with zero accuracy to remain end-to-end safe
        print(f"ACCURACY=0.000000")
        return

    # 2. Schema Normalization
    # Strip whitespace and collapse internal whitespace
    df.columns = [" ".join(str(c).strip().split()) for c in df.columns]
    # Drop 'Unnamed' columns
    df = df.loc[:, ~df.columns.str.contains('^Unnamed', case=False)]

    if df.empty:
        print(f"ACCURACY=0.000000")
        return

    # 3. Target and Feature Identification
    # Prefer 'label' but fallback to the last column
    target_col = 'label' if 'label' in df.columns else df.columns[-1]
    
    # Identify feature columns (numeric preferred)
    X_raw = df.drop(columns=[target_col])
    
    # Robust numeric conversion for features
    for col in X_raw.columns:
        X_raw[col] = pd.to_numeric(X_raw[col], errors='coerce')
    
    # Drop columns that are entirely NaN after coercion
    X_raw = X_raw.dropna(axis=1, how='all')
    feature_list = X_raw.columns.tolist()

    if not feature_list:
        print(f"ACCURACY=0.000000")
        return

    # 4. Preprocess Target
    y_raw = df[target_col].astype(str).str.strip()
    unique_classes = y_raw.unique()

    # 5. Pipeline Construction
    # Energy efficient: Logistic Regression is a lightweight linear model.
    # Pipeline handles scaling and imputation in one pass.
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(
            max_iter=500, 
            solver='lbfgs', 
            class_weight='balanced',
            random_state=42
        ))
    ])

    # 6. Training and Evaluation
    if len(unique_classes) < 2:
        # Fallback for single class datasets
        accuracy = 1.0
    else:
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X_raw[feature_list], y_raw, test_size=0.2, random_state=42
            )
            
            if len(X_train) > 0:
                pipeline.fit(X_train, y_train)
                predictions = pipeline.predict(X_test)
                accuracy = accuracy_score(y_test, predictions)
            else:
                accuracy = 0.0
        except Exception:
            # Fallback for unexpected data-related failures
            accuracy = 0.0

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# OPTIMIZATION SUMMARY
# 1. Model Selection: Used Logistic Regression instead of large ensembles (Random Forest/XGBoost) to minimize CPU instructions per inference.
# 2. Lightweight Preprocessing: StandardScaling and Mean Imputation are O(n) operations, ensuring low memory and power consumption.
# 3. Computational Efficiency: Avoided expensive hyperparameter tuning and cross-validation to reduce energy footprint during training.
# 4. Robustness: Implemented multi-stage CSV parsing and schema normalization to prevent crashes on non-standard formatting.
# 5. Scalability: The pipeline uses scikit-learn's optimized C-based implementations for matrix operations, which are highly efficient on CPU.
# 6. Fallback Logic: Included checks for empty datasets or single-class targets to ensure the script returns a valid value instead of an error.