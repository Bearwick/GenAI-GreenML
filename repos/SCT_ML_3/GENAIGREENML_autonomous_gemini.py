# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score

def load_data(filepath):
    """Robust CSV loader with fallback parameters."""
    try:
        df = pd.read_csv(filepath)
        # Check if single column (wrong separator)
        if df.shape[1] <= 1:
            raise ValueError
    except Exception:
        df = pd.read_csv(filepath, sep=';', decimal=',')
    return df

def clean_columns(df):
    """Normalize column names and remove artifacts."""
    df.columns = [str(c).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def run_pipeline():
    filename = "cat_dog_data.csv"
    try:
        df = load_data(filename)
    except Exception:
        # Create empty df to trigger fail-safe if file missing
        df = pd.DataFrame()

    if df.empty:
        # Exit gracefully if no data
        print("ACCURACY=0.000000")
        return

    df = clean_columns(df)

    # Identify target and features
    target_col = 'label' if 'label' in df.columns else df.columns[-1]
    feature_cols = [c for c in df.columns if c != target_col]

    if not feature_cols:
        print("ACCURACY=0.000000")
        return

    # Basic data cleaning: handle NaN and convert types
    X = df[feature_cols].copy()
    y = df[target_col].copy()

    # Drop rows where target is NaN
    mask = y.notna()
    X = X[mask]
    y = y[mask]

    if len(X) < 2:
        print("ACCURACY=0.000000")
        return

    # Determine task: Classification or Regression
    # If target is object or has few unique values relative to size
    is_classification = False
    if y.dtype == 'object' or y.nunique() < 20:
        is_classification = True
        # Ensure categories are encoded for the model
        y = y.astype(str)
        unique_classes = np.unique(y)
        if len(unique_classes) < 2:
            # Trivial case
            print("ACCURACY=1.000000")
            return
    else:
        y = pd.to_numeric(y, errors='coerce')
        y = y.fillna(y.mean())

    # Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Preprocessing
    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ],
        remainder='drop'
    )

    if is_classification:
        # Energy-efficient linear model
        model = LogisticRegression(max_iter=1000, solver='liblinear')
    else:
        model = Ridge()

    clf = Pipeline(steps=[('preprocessor', preprocessor),
                          ('classifier', model)])

    # Fit
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    # Metric
    if is_classification:
        accuracy = accuracy_score(y_test, y_pred)
    else:
        # For regression, we use a clipped R2-based proxy for "accuracy" in [0,1]
        score = r2_score(y_test, y_pred)
        accuracy = max(0, min(1, score))

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary
# 1. Model Choice: LogisticRegression (with liblinear) and Ridge are chosen for high energy efficiency on CPU.
# 2. They have O(n_features * n_samples) complexity, making them significantly faster than non-linear SVMs or Ensembles.
# 3. Robustness: Implemented a robust CSV reader and column cleaner to handle malformed files or whitespace.
# 4. Pipeline: Used sklearn.Pipeline to prevent data leakage and minimize redundant transformations.
# 5. Preprocessing: SimpleImputer and StandardScaler ensure convergence for linear models with minimal compute overhead.
# 6. Memory: Avoided heavy deep learning or large-scale embeddings to stay within low-resource CPU constraints.
# 7. Fallback: Includes logic to switch between classification and regression based on target distribution to ensure end-to-end execution.
# 8. Constraints: All categorical features are One-Hot Encoded only if present; otherwise, numerical scaling is preferred.