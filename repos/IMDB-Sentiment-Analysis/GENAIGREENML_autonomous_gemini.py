# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

def robust_read_csv(filepath):
    """
    Reads CSV with fallback for different delimiters and normalizes column headers.
    """
    try:
        df = pd.read_csv(filepath, engine='python')
    except Exception:
        try:
            df = pd.read_csv(filepath, sep=';', decimal=',', engine='python')
        except Exception:
            return pd.DataFrame()
    
    # Normalize column names: strip whitespace and collapse internal spaces
    df.columns = [' '.join(str(c).strip().split()) for c in df.columns]
    # Drop 'Unnamed' columns
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def get_schema_mapping(df):
    """
    Identifies target and feature columns from the dataframe.
    """
    cols = df.columns.tolist()
    if not cols:
        return None, None
    
    # Identify target candidate: prefer 'class', 'label', 'target' or column with few unique values
    target_candidates = [c for c in cols if c.lower() in ['class', 'label', 'target', 'sentiment']]
    if target_candidates:
        target_col = target_candidates[0]
    else:
        # Fallback: pick the column with the fewest unique values (likely the class)
        counts = {c: df[c].nunique() for c in cols if df[c].dtype == 'object' or df[c].nunique() < 20}
        target_col = min(counts, key=counts.get) if counts else cols[0]
    
    # Identify text feature: prefer 'text', 'review' or column with highest average string length
    text_candidates = [c for c in cols if c.lower() in ['text', 'review', 'body', 'content']]
    if text_candidates:
        text_col = text_candidates[0]
    else:
        text_col = [c for c in cols if c != target_col][0]
        
    return target_col, text_col

def prepare_pipeline():
    # Load training and testing data as indicated in the source code
    df_train = robust_read_csv('movie_review_train.csv')
    df_test = robust_read_csv('movie_review_test.csv')

    if df_train.empty or df_test.empty:
        # Trivial fallback if files are missing or unreadable
        print(f"ACCURACY={0.000000:.6f}")
        return

    target_col, text_col = get_schema_mapping(df_train)

    # Clean missing values
    df_train = df_train.dropna(subset=[target_col, text_col])
    df_test = df_test.dropna(subset=[target_col, text_col])

    X_train_raw = df_train[text_col].astype(str)
    X_test_raw = df_test[text_col].astype(str)

    # Encode labels
    le = LabelEncoder()
    y_train = le.fit_transform(df_train[target_col].astype(str))
    
    # Use transform on test and handle unseen labels if any
    try:
        y_test = le.transform(df_test[target_col].astype(str))
    except ValueError:
        # If test contains labels not in train, we manually map them or skip
        y_test = df_test[target_col].astype(str).apply(lambda x: le.transform([x])[0] if x in le.classes_ else -1)

    # Filtering out rows with unseen labels in test to maintain evaluation integrity
    mask = y_test != -1
    X_test_raw = X_test_raw[mask]
    y_test = y_test[mask]

    if len(np.unique(y_train)) < 2:
        # Fallback for single-class scenario
        accuracy = 1.0 if len(y_test) > 0 else 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Energy-efficient feature extraction: CountVectorizer with frequency filtering
    # min_df and max_df prune the vocabulary size significantly, reducing memory and CPU cycles.
    vect = CountVectorizer(stop_words='english', min_df=0.03, max_df=0.8)
    
    X_train_vec = vect.fit_transform(X_train_raw)
    X_test_vec = vect.transform(X_test_raw)

    # Energy-efficient model: Multinomial Naive Bayes
    # NB is computationally inexpensive (linear complexity) and performs well on text.
    model = MultinomialNB()
    model.fit(X_train_vec, y_train)

    # Evaluation
    predictions = model.predict(X_test_vec)
    accuracy = accuracy_score(y_test, predictions)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    prepare_pipeline()

# Optimization Summary:
# 1. Used Multinomial Naive Bayes: A highly efficient O(N*D) classifier ideal for CPU-only environments.
# 2. Optimized CountVectorizer: Used min_df (3%) and max_df (80%) to reduce the feature space.
#    This significantly lowers memory usage and speeds up matrix multiplications.
# 3. Robust CSV Parsing: Implemented a multi-stage loading strategy with header normalization
#    to prevent failures due to delimiter issues or whitespace in column names.
# 4. Memory Efficiency: Avoided heavy embeddings (Word2Vec/BERT) which require significant RAM and GPU/High-CPU power.
# 5. Pipeline Integrity: Used standard LabelEncoder and defensive masking for unseen labels in the test set.
# 6. Target Fallback: Included checks for single-class datasets to ensure the script never hard-fails.