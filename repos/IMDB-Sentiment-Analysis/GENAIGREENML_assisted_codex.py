# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
import random
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

SEED = 42
random.seed(SEED)
np.random.seed(SEED)

DATASET_HEADERS = "class,text"

def _missing_expected_columns(df, expected_headers):
    cols = [str(c).strip().lower() for c in df.columns]
    for h in expected_headers:
        if h.strip().lower() in cols:
            return False
    return True

def standardize_columns(df, expected_headers):
    expected = [h.strip() for h in expected_headers if h.strip()]
    lower_map = {str(col).strip().lower(): col for col in df.columns}
    rename_map = {}
    for exp in expected:
        key = exp.lower()
        if key in lower_map:
            rename_map[lower_map[key]] = exp
    if len(rename_map) == len(expected):
        return df.rename(columns=rename_map)
    if df.shape[1] >= len(expected):
        df = df.iloc[:, :len(expected)].copy()
        df.columns = expected
        return df
    return df

def read_csv_robust(path, expected_headers):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is None or df.shape[1] < len(expected_headers) or (_missing_expected_columns(df, expected_headers) and df.shape[1] == 1):
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            if df is None:
                raise
    return standardize_columns(df, expected_headers)

def prepare_features_labels(df, class_col, text_col):
    texts = df[text_col].fillna('')
    if texts.dtype != object:
        texts = texts.astype(str)
    labels = df[class_col].map({'Neg': 0, 'Pos': 1})
    return texts, labels

def main():
    expected_headers = [h.strip() for h in DATASET_HEADERS.split(',') if h.strip()]
    train_df = read_csv_robust('movie_review_train.csv', expected_headers)
    test_df = read_csv_robust('movie_review_test.csv', expected_headers)

    if len(expected_headers) >= 2:
        class_col, text_col = expected_headers[0], expected_headers[1]
    elif len(train_df.columns) >= 2:
        class_col, text_col = train_df.columns[0], train_df.columns[1]
    else:
        class_col = train_df.columns[0]
        text_col = train_df.columns[0]

    train_df = train_df[[class_col, text_col]]
    test_df = test_df[[class_col, text_col]]

    X_train, y_train = prepare_features_labels(train_df, class_col, text_col)
    X_test, y_test = prepare_features_labels(test_df, class_col, text_col)

    vectorizer = CountVectorizer(stop_words='english', min_df=0.03, max_df=0.8)
    X_train_vec = vectorizer.fit_transform(X_train)

    model = MultinomialNB()
    model.fit(X_train_vec, y_train)

    accuracy = model.score(vectorizer.transform(X_test), y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Combined fitting and transformation steps to minimize redundant data passes.
# - Removed unused metric calculations, probability predictions, and plotting to cut computation.
# - Standardized and trimmed columns to only required fields for lower memory usage.
# - Used model.score with on-the-fly test transformation to avoid extra intermediate storage.
# - Added robust CSV parsing with delimiter fallback for reliable input handling.