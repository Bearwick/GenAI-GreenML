# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline

COLS = ['class', 'text']
LABEL_MAP = {'Neg': 0, 'Pos': 1}

def load_data(train_path, test_path):
    train = pd.read_csv(train_path, usecols=COLS)
    test = pd.read_csv(test_path, usecols=COLS)
    X_train = train['text']
    y_train = train['class'].map(LABEL_MAP)
    X_test = test['text']
    y_test = test['class'].map(LABEL_MAP)
    return X_train, y_train, X_test, y_test

def train_and_evaluate(X_train, y_train, X_test, y_test):
    model = make_pipeline(
        CountVectorizer(stop_words='english', min_df=0.03, max_df=0.8),
        MultinomialNB()
    )
    model.fit(X_train, y_train)
    return model.score(X_test, y_test)

def main():
    X_train, y_train, X_test, y_test = load_data('movie_review_train.csv', 'movie_review_test.csv')
    accuracy = train_and_evaluate(X_train, y_train, X_test, y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# Removed unused analysis steps, ROC computation, and plotting to cut redundant processing.
# Read only required columns via usecols and skipped storing extra label columns to reduce memory use.
# Used a pipeline with score to avoid manual predictions/probabilities and intermediate matrices.
# Limited imports and kept work inside functions for clearer scope-based cleanup.