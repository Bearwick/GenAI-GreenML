# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is None or df.shape[1] <= 1:
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df is None or df2.shape[1] > df.shape[1]:
                df = df2
        except Exception:
            pass
    if df is None:
        raise RuntimeError("Failed to read dataset.")
    return df


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if isinstance(c, str) and c.strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _pick_target_and_features(df, provided_headers=None):
    cols = list(df.columns)

    # Prefer headers if present and valid
    target = None
    text_col = None

    if provided_headers:
        ph = [h.strip() for h in provided_headers if isinstance(h, str)]
        for h in ph:
            if h in cols:
                pass

    # Try canonical names first (case-insensitive match)
    lower_map = {str(c).strip().lower(): c for c in cols}
    if "class" in lower_map:
        target = lower_map["class"]
    if "text" in lower_map:
        text_col = lower_map["text"]

    # If missing, pick a reasonable target: prefer low-cardinality non-constant object/categorical else numeric
    if target is None or target not in cols:
        # Candidate categoricals/objects with >1 unique
        cand_obj = []
        for c in cols:
            s = df[c]
            nun = s.nunique(dropna=True)
            if nun > 1 and (s.dtype == "object" or str(s.dtype).startswith("category")):
                cand_obj.append((nun, c))
        # Prefer smallest >1 cardinality for classification
        if cand_obj:
            cand_obj.sort(key=lambda x: x[0])
            target = cand_obj[0][1]
        else:
            # numeric target for regression
            cand_num = []
            for c in cols:
                s = pd.to_numeric(df[c], errors="coerce")
                nun = s.nunique(dropna=True)
                if nun > 1:
                    cand_num.append((nun, c))
            if cand_num:
                cand_num.sort(key=lambda x: -x[0])
                target = cand_num[0][1]

    if target is None or target not in cols:
        # Ultimate fallback: first column
        target = cols[0]

    # Features: all except target, but prefer including a likely text column if present
    feature_cols = [c for c in cols if c != target]
    if not feature_cols:
        feature_cols = [target]
    if text_col is None or text_col not in feature_cols:
        # Guess a text column: object column with longest average string length
        best = None
        for c in feature_cols:
            if df[c].dtype == "object":
                s = df[c].astype(str)
                avg_len = s.str.len().replace([np.inf, -np.inf], np.nan).fillna(0).mean()
                if best is None or avg_len > best[0]:
                    best = (avg_len, c)
        if best is not None:
            text_col = best[1]

    return target, feature_cols, text_col


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    denom = np.var(y_true)
    if not np.isfinite(denom) or denom <= 1e-12:
        return 0.0
    mse = np.mean((y_true - y_pred) ** 2)
    r2 = 1.0 - (mse / denom)
    # bound into [0,1] for stable "accuracy" proxy
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    dataset_path = "movie_review_train.csv"
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(dataset_path)

    df = _read_csv_robust(dataset_path)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Basic sanity
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col, feature_cols, text_col = _pick_target_and_features(df, provided_headers=["class", "text"])

    # Build y and X with minimal assumptions
    y_raw = df[target_col]
    X = df[feature_cols].copy()

    # Drop rows with missing target
    y_na = y_raw.isna()
    if y_na.any():
        X = X.loc[~y_na].copy()
        y_raw = y_raw.loc[~y_na].copy()

    assert X.shape[0] > 0

    # Determine task: classification if y is object or low cardinality; else regression
    y_obj = (y_raw.dtype == "object") or str(y_raw.dtype).startswith("category")
    if y_obj:
        y = y_raw.astype(str).str.strip()
    else:
        y_num = pd.to_numeric(y_raw, errors="coerce")
        # If too many NaNs, treat as classification on string form
        if y_num.notna().mean() < 0.7:
            y = y_raw.astype(str).str.strip()
            y_obj = True
        else:
            y = y_num

    # Identify columns for preprocessing
    text_features = []
    if text_col is not None and text_col in X.columns:
        text_features = [text_col]

    # Other categorical/non-text
    other_cols = [c for c in X.columns if c not in text_features]
    cat_cols = []
    num_cols = []
    for c in other_cols:
        if X[c].dtype == "object" or str(X[c].dtype).startswith("category"):
            cat_cols.append(c)
        else:
            num_cols.append(c)

    # If no explicit text column, treat all object columns as categorical (still works)
    # If text column exists, keep it as text and not as categorical
    transformers = []

    if text_features:
        transformers.append(
            ("text", TfidfVectorizer(lowercase=True, stop_words="english", max_features=20000, ngram_range=(1, 2)),
             text_features[0])
        )

    if cat_cols:
        cat_pipe = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ])
        transformers.append(("cat", cat_pipe, cat_cols))

    if num_cols:
        num_pipe = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
        ])
        transformers.append(("num", num_pipe, num_cols))

    # If nothing to transform (edge case), make a dummy numeric feature
    if not transformers:
        X = pd.DataFrame({"_dummy": np.zeros(len(y_raw), dtype=float)})
        feature_cols = ["_dummy"]
        num_cols = ["_dummy"]
        transformers = [("num", Pipeline(steps=[("imputer", SimpleImputer(strategy="median"))]), num_cols)]

    preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    # Split
    if y_obj:
        # Ensure at least 2 classes for classification
        classes = pd.Series(y).nunique(dropna=True)
        if classes >= 2:
            strat = y
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=strat
            )
            assert len(y_train) > 0 and len(y_test) > 0

            model = LogisticRegression(
                solver="liblinear",
                max_iter=200,
            )

            clf = Pipeline(steps=[
                ("prep", preprocessor),
                ("model", model),
            ])
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
        else:
            # Trivial baseline if single class
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            assert len(y_train) > 0 and len(y_test) > 0
            majority = pd.Series(y_train).mode(dropna=True)
            pred = majority.iloc[0] if len(majority) else (pd.Series(y_train).iloc[0] if len(y_train) else "")
            y_pred = np.array([pred] * len(y_test), dtype=object)
            accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Regression
        y = pd.to_numeric(y, errors="coerce")
        mask = y.notna()
        X = X.loc[mask].copy()
        y = y.loc[mask].copy()
        assert len(y) > 1

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        assert len(y_train) > 0 and len(y_test) > 0

        reg = Ridge(alpha=1.0, random_state=42)
        pipe = Pipeline(steps=[
            ("prep", preprocessor),
            ("model", reg),
        ])
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = _bounded_regression_score(y_test.values, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression(liblinear) / Ridge) for CPU-friendly, energy-efficient training.
# - TF-IDF with capped max_features and limited n-grams provides strong text baselines without embeddings or deep nets.
# - ColumnTransformer/Pipeline ensures single-pass, reproducible preprocessing and avoids redundant transforms.
# - Robust CSV parsing fallback (default then ';' with decimal=',') reduces reruns/manual fixes (saves energy).
# - Defensive schema handling: derives target/features from available columns, drops "Unnamed:*", normalizes headers.
# - Regression fallback uses bounded (r2+1)/2 score in [0,1] as an "accuracy" proxy to keep output stable.