# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression


def _find_dataset_path() -> str:
    candidates = [
        "data.csv",
        "dataset.csv",
        "train.csv",
        "training.csv",
        "text.csv",
        "input.csv",
        "data/dataset.csv",
        "data/train.csv",
        "data/data.csv",
        "dataset/train.csv",
    ]
    for p in candidates:
        if os.path.exists(p) and os.path.isfile(p):
            return p

    for fname in os.listdir("."):
        if fname.lower().endswith(".csv") and os.path.isfile(fname):
            return fname

    raise FileNotFoundError("No CSV dataset found in the working directory.")


def _basic_text_clean(s: str) -> str:
    if s is None:
        return ""
    s = str(s).lower()
    s = re.sub(r"http\S+|www\.\S+", " ", s)
    s = re.sub(r"<[^>]+>", " ", s)
    s = re.sub(r"[^a-z0-9\s]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def main() -> None:
    path = _find_dataset_path()
    df = pd.read_csv(path)

    expected = {"class", "text"}
    if not expected.issubset(set(df.columns)):
        raise ValueError(f"Dataset must contain columns: {sorted(expected)}")

    df = df[["class", "text"]].copy()
    df["text"] = df["text"].fillna("").map(_basic_text_clean)
    df = df.dropna(subset=["class"])
    df = df[df["text"].str.len() > 0]

    X = df[["text"]]
    y = df["class"]

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y if y.nunique() > 1 else None,
    )

    preprocess = ColumnTransformer(
        transformers=[
            (
                "txt",
                TfidfVectorizer(
                    lowercase=False,
                    strip_accents="unicode",
                    ngram_range=(1, 2),
                    max_features=20000,
                    min_df=2,
                    sublinear_tf=True,
                    dtype=float,
                ),
                "text",
            )
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    clf = LogisticRegression(
        solver="liblinear",
        C=1.0,
        max_iter=300,
        n_jobs=1,
    )

    model = Pipeline(
        steps=[
            ("preprocess", preprocess),
            ("clf", clf),
        ]
    )

    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    accuracy = accuracy_score(y_test, preds)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Used TF-IDF (sparse, CPU-friendly) instead of neural embeddings to minimize compute and memory.
# - Applied lightweight regex-based cleaning to improve signal without heavy NLP dependencies.
# - Limited feature space (max_features, min_df) and used sublinear_tf for better scaling/efficiency.
# - Selected LogisticRegression with liblinear for small/medium datasets: fast convergence and low overhead.
# - Kept pipeline fully reproducible (fixed random_state, deterministic preprocessing), no model saving/plots.