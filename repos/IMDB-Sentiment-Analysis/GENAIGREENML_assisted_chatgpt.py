# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB


RANDOM_SEED = 42


def _read_csv_robust(path: str, expected_headers: tuple[str, ...]) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] != len(expected_headers):
        df = pd.read_csv(path, sep=";", decimal=",")
    if df.shape[1] != len(expected_headers):
        df = pd.read_csv(path, header=0, names=list(expected_headers), sep=None, engine="python")
        if df.shape[1] != len(expected_headers):
            df = pd.read_csv(path, header=0, names=list(expected_headers), sep=";", decimal=",")
    df.columns = list(expected_headers)
    return df


def _map_labels(series: pd.Series) -> pd.Series:
    return series.map({"Neg": 0, "Pos": 1}).astype("int8")


def main() -> None:
    np.random.seed(RANDOM_SEED)

    expected_headers = ("class", "text")
    train_df = _read_csv_robust("movie_review_train.csv", expected_headers)
    test_df = _read_csv_robust("movie_review_test.csv", expected_headers)

    train_df["label"] = _map_labels(train_df["class"])
    test_df["label"] = _map_labels(test_df["class"])

    X_train = train_df["text"].astype(str, copy=False)
    y_train = train_df["label"].to_numpy(copy=False)
    X_test = test_df["text"].astype(str, copy=False)
    y_test = test_df["label"].to_numpy(copy=False)

    vect = CountVectorizer(stop_words="english", min_df=0.03, max_df=0.8)
    X_train_vec = vect.fit_transform(X_train)
    X_test_vec = vect.transform(X_test)

    clf = MultinomialNB()
    clf.fit(X_train_vec, y_train)

    y_pred = clf.predict(X_test_vec)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused exploratory steps (head(), value_counts(), vocabulary inspection) to avoid redundant computation.
# - Dropped probability prediction, ROC computation, confusion-matrix-derived metrics, and plotting to eliminate unnecessary heavy operations and dependencies.
# - Used a single end-to-end pipeline of fit_transform/transform/predict focused only on the required evaluation intent (accuracy).
# - Ensured robust CSV parsing with a fallback strategy to reduce reruns and failures due to delimiter/decimal mismatches.
# - Reduced memory footprint by storing labels as int8 and using NumPy views (to_numpy(copy=False)) where possible.
# - Avoided unnecessary intermediate variables and repeated metric calculations to minimize overhead.
# - Set a fixed random seed to ensure reproducibility (even though this model path is deterministic in practice).