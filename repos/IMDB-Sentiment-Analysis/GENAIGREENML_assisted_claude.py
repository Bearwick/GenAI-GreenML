# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

SEED = 1
np.random.seed(SEED)

def load_csv(path):
    df = pd.read_csv(path, header=0, names=['Class', 'text'])
    if df.shape[1] != 2:
        df = pd.read_csv(path, header=0, names=['Class', 'text'], sep=';', decimal=',')
    return df

docs = load_csv('movie_review_train.csv')
docs_test = load_csv('movie_review_test.csv')

label_map = {'Neg': 0, 'Pos': 1}
y = docs['Class'].map(label_map).values
y_test = docs_test['Class'].map(label_map).values

vect = CountVectorizer(stop_words='english', min_df=0.03, max_df=0.8)
X_train_transformed = vect.fit_transform(docs['text'])
X_test_transformed = vect.transform(docs_test['text'])

mnb = MultinomialNB()
mnb.fit(X_train_transformed, y)

y_pred_class = mnb.predict(X_test_transformed)

accuracy = metrics.accuracy_score(y_test, y_pred_class)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Removed all print statements, plots, and visualizations per requirements.
# Combined fit and transform into fit_transform to avoid redundant pass over training data.
# Removed intermediate DataFrames and unused variables (confusion matrix, ROC, etc.).
# Removed redundant imports and duplicate computations (e.g., precision computed twice).
# Used .values for label arrays to avoid pandas Series overhead during model fitting.
# Implemented robust CSV fallback with sep=';' and decimal=','.
# Set fixed random seed for reproducibility.
# Kept only essential computation: vectorization, model training, prediction, and accuracy.