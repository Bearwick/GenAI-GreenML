# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

docs = pd.read_csv('movie_review_train.csv', header=0, names=['Class', 'text'])
docs_test = pd.read_csv('movie_review_test.csv', header=0, names=['Class', 'text'])

label_map = {'Neg': 0, 'Pos': 1}
y = docs.Class.map(label_map).values
y_test = docs_test.Class.map(label_map).values

vect = CountVectorizer(stop_words='english', min_df=0.03, max_df=0.8)
X_train_transformed = vect.fit_transform(docs.text)
X_test_transformed = vect.transform(docs_test.text)

mnb = MultinomialNB()
mnb.fit(X_train_transformed, y)

y_pred_class = mnb.predict(X_test_transformed)
accuracy = accuracy_score(y_test, y_pred_class)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Combined fit and transform into fit_transform to avoid redundant pass over training data
# Removed unused intermediate variables (confusion matrix, ROC, precision, recall, etc.)
# Removed all plots, prints, and visualizations not needed for final accuracy
# Removed redundant .values and DataFrame column accesses; used numpy arrays directly
# Removed predict_proba call since it was only used for ROC which is removed
# Removed unnecessary imports (metrics, roc_curve, auc, matplotlib)
# Kept only essential pipeline: load, vectorize, train, predict, report accuracy