# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
import re

# Load dataset
df = pd.read_csv("dataset.csv")

# Drop rows with missing values in key columns
df = df.dropna(subset=["class", "text"])

# Lightweight text preprocessing function
def preprocess_text(text):
    if not isinstance(text, str):
        text = str(text)
    text = text.lower()
    text = re.sub(r"[^a-z0-9\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

df["text_clean"] = df["text"].apply(preprocess_text)

X = df["text_clean"]
y = df["class"]

# Stratified split for reproducibility
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Pipeline: TF-IDF + Logistic Regression
# TF-IDF is lightweight and effective for text classification on CPU
# Logistic Regression is energy-efficient, fast to train, and works well for small-scale tasks
# Sublinear TF helps with term frequency scaling; max_features caps vocabulary for efficiency
pipeline = Pipeline([
    ("tfidf", TfidfVectorizer(
        max_features=20000,
        sublinear_tf=True,
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.95,
        dtype=np.float32  # float32 reduces memory usage
    )),
    ("clf", LogisticRegression(
        max_iter=1000,
        solver="saga",       # Efficient solver for larger datasets
        C=1.0,
        random_state=42,
        n_jobs=-1
    ))
])

pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Text preprocessing: minimal regex-based cleaning (lowercase, remove special chars) - very fast on CPU
# 2. TF-IDF Vectorizer: lightweight feature extraction with max_features=20000 to cap dimensionality
#    - ngram_range=(1,2) captures bigrams for better context without excessive feature explosion
#    - sublinear_tf=True improves performance on skewed term frequencies
#    - dtype=np.float32 halves memory compared to float64
#    - min_df=2 and max_df=0.95 prune rare/common terms for efficiency
# 3. Logistic Regression: energy-efficient linear model, fast training on CPU
#    - saga solver is efficient for medium-to-large datasets
#    - No deep learning or GPU required
# 4. Stratified train/test split ensures balanced evaluation
# 5. Entire pipeline is CPU-friendly and completes in seconds for small-to-medium datasets