# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

train = pd.read_csv('movie_review_train.csv', header=0, names=['Class', 'text'])
test = pd.read_csv('movie_review_test.csv', header=0, names=['Class', 'text'])

y_train = train['Class'].map({'Neg': 0, 'Pos': 1})
y_test = test['Class'].map({'Neg': 0, 'Pos': 1})

vect = CountVectorizer(stop_words='english', min_df=0.03, max_df=0.8)
X_train_dtm = vect.fit_transform(train['text'])
X_test_dtm = vect.transform(test['text'])

mnb = MultinomialNB()
mnb.fit(X_train_dtm, y_train)

y_pred = mnb.predict(X_test_dtm)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Utilized fit_transform() to combine vocabulary learning and document-term matrix construction, reducing redundant passes over the training data.
# 2. Eliminated intermediate variable assignments (e.g., X, y, X_test) and data copies to reduce memory overhead and data movement.
# 3. Removed high-latency metrics and visualizations (ROC curves, confusion matrices, predict_proba) that were not required for the final output.
# 4. Streamlined the target label encoding by applying the map function directly to the relevant dataframe columns.
# 5. Minimized the import footprint by specifically importing only the necessary classes and functions from sklearn.
# 6. Avoided redundant attribute lookups and interactive display calls (like .head() and .vocabulary_) that increase execution time in script environments.
# 7. Preserved existing CountVectorizer hyper-parameters (min_df, max_df) which inherently optimize performance by reducing the feature space dimensionality.