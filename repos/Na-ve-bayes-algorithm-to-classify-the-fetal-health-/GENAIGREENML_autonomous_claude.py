# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings("ignore")

# Robust CSV loading
df = None
for sep, decimal in [(',', '.'), (';', ','), ('\t', '.')]:
    try:
        df = pd.read_csv("fetal_health.csv", sep=sep, decimal=decimal)
        if df.shape[1] > 1:
            break
    except Exception:
        continue

if df is None or df.shape[1] <= 1:
    try:
        df = pd.read_csv("fetal_health.csv", sep=';', decimal=',')
    except Exception:
        df = pd.read_csv("fetal_health.csv")

# Normalize column names
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
df = df.loc[:, ~df.columns.str.startswith('Unnamed')]

# Identify target column
target_col = None
expected_target = 'fetal_health'
for col in df.columns:
    if col.lower().replace('_', '').replace(' ', '') == 'fetalhealth':
        target_col = col
        break

if target_col is None:
    # Fallback: use last column as target
    target_col = df.columns[-1]

# Coerce all columns to numeric
for col in df.columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Drop rows with NaN in target
df = df.dropna(subset=[target_col])

# Separate features and target
feature_cols = [c for c in df.columns if c != target_col]
X = df[feature_cols].copy()
y = df[target_col].copy()

# Handle NaN/inf in features
X = X.replace([np.inf, -np.inf], np.nan)
X = X.fillna(X.median())

# Assert dataset is not empty
assert X.shape[0] > 0, "Dataset is empty after preprocessing"
assert len(feature_cols) > 0, "No feature columns found"

# Determine task type
n_classes = y.nunique()
is_classification = n_classes >= 2 and n_classes <= 50

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y if is_classification else None
)

assert X_train.shape[0] > 0, "Training set is empty"
assert X_test.shape[0] > 0, "Test set is empty"

if is_classification:
    # Gaussian Naive Bayes is lightweight and energy-efficient
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('clf', GaussianNB())
    ])
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
else:
    # Fallback to regression with R^2 clamped to [0,1]
    from sklearn.linear_model import Ridge
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('reg', Ridge(alpha=1.0))
    ])
    pipe.fit(X_train, y_train)
    r2 = pipe.score(X_test, y_test)
    accuracy = max(0.0, min(1.0, r2))

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Gaussian Naive Bayes chosen as the primary classifier: O(n*d) training,
#    extremely lightweight, no iterative optimization, minimal memory footprint.
# 2. StandardScaler used to normalize features before