# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score

warnings.filterwarnings("ignore")


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _try_read_csv(path):
    # Robust parsing fallback: default first; if single-column or too few columns, retry with ; and decimal=,
    df0 = pd.read_csv(path)
    if df0.shape[1] <= 1:
        df1 = pd.read_csv(path, sep=";", decimal=",")
        if df1.shape[1] > df0.shape[1]:
            return df1
    return df0


def _pick_target_column(df, headers_hint):
    cols = list(df.columns)
    # Prefer provided header hint match (case-insensitive, normalized)
    hint_norm = [re.sub(r"\s+", " ", h.strip()) for h in headers_hint]
    hint_lower_map = {h.lower(): h for h in hint_norm}

    for c in cols:
        if c.strip().lower() in hint_lower_map and c.strip().lower() == "fetal_health":
            return c

    # Otherwise prefer any column whose normalized name contains 'target'/'label'/'class'/'health'
    for c in cols:
        cl = c.strip().lower()
        if any(k in cl for k in ("target", "label", "class", "health")):
            return c

    # Else choose a numeric-like non-constant column with few unique values (classification-like)
    best = None
    best_score = None
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        nun = s.nunique(dropna=True)
        if nun < 2:
            continue
        # Score: prefer low cardinality but not 1; cap at 20
        score = nun if nun <= 20 else 1000 + nun
        if best is None or score < best_score:
            best, best_score = c, score

    if best is not None:
        return best

    # Fallback: last column
    return cols[-1]


def _bounded_regression_score(y_true, y_pred):
    # Stable "accuracy" proxy in [0,1]: 1/(1+RMSE/STD). If STD==0 => 0.
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2))) if y_true.size else 0.0
    std = float(np.std(y_true)) if y_true.size else 0.0
    if not np.isfinite(rmse) or not np.isfinite(std) or std <= 1e-12:
        return 0.0
    return float(1.0 / (1.0 + (rmse / std)))


# ----------------------- Load -----------------------
DATASET_PATH = "fetal_health.csv"
DATASET_HEADERS = [
    "baseline value",
    "accelerations",
    "fetal_movement",
    "uterine_contractions",
    "light_decelerations",
    "severe_decelerations",
    "prolongued_decelerations",
    "abnormal_short_term_variability",
    "mean_value_of_short_term_variability",
    "percentage_of_time_with_abnormal_long_term_variability",
    "mean_value_of_long_term_variability",
    "histogram_width",
    "histogram_min",
    "histogram_max",
    "histogram_number_of_peaks",
    "histogram_number_of_zeroes",
    "histogram_mode",
    "histogram_mean",
    "histogram_median",
    "histogram_variance",
    "histogram_tendency",
    "fetal_health",
]

if not os.path.exists(DATASET_PATH):
    # Minimal fallback to avoid crashing if path differs; try common relative alternatives
    alt_paths = [
        os.path.join(os.getcwd(), DATASET_PATH),
        os.path.join(os.path.dirname(__file__) if "__file__" in globals() else os.getcwd(), DATASET_PATH),
    ]
    found = None
    for p in alt_paths:
        if os.path.exists(p):
            found = p
            break
    if found is None:
        raise FileNotFoundError(DATASET_PATH)
    DATASET_PATH = found

df = _try_read_csv(DATASET_PATH)
df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)
df = df.copy()

assert df.shape[0] > 0 and df.shape[1] > 0, "Empty dataset after loading."

# ----------------------- Basic cleaning -----------------------
# Coerce numerics where possible while keeping object columns for categorical handling.
for c in df.columns:
    if df[c].dtype == "object":
        # Try numeric coercion; only adopt if it yields a meaningful amount of numeric data
        coerced = pd.to_numeric(df[c].astype(str).str.replace(",", ".", regex=False), errors="coerce")
        non_na_ratio = float(coerced.notna().mean()) if len(coerced) else 0.0
        if non_na_ratio >= 0.9:
            df[c] = coerced

# Replace inf with NaN
df = df.replace([np.inf, -np.inf], np.nan)

# ----------------------- Target/Features -----------------------
target_col = _pick_target_column(df, DATASET_HEADERS)

# Keep at least one feature; if only one column exists, create dummy feature from index
feature_cols = [c for c in df.columns if c != target_col]
if len(feature_cols) == 0:
    df["__idx__"] = np.arange(len(df))
    feature_cols = ["__idx__"]

X = df[feature_cols]
y_raw = df[target_col]

# Drop rows where target missing
mask = ~pd.isna(y_raw)
X = X.loc[mask]
y_raw = y_raw.loc[mask]

assert X.shape[0] > 1, "Not enough rows after dropping missing target."

# Determine task
y_num = pd.to_numeric(y_raw, errors="coerce")
y_is_numeric = y_num.notna().mean() >= 0.95

task = "classification"
y = None

if y_is_numeric:
    y_num = y_num.astype(float)
    n_unique = int(pd.Series(y_num).nunique(dropna=True))
    if n_unique >= 2 and n_unique <= 20:
        # Classification-like numeric labels
        y = y_num.astype(int)
        # If labels look like floats but represent categories, still ok via int cast
        task = "classification"
    elif n_unique >= 2:
        y = y_num
        task = "regression"
    else:
        # Constant target: regression fallback with trivial score
        y = y_num
        task = "trivial"
else:
    # Object labels -> classification if >=2 classes
    y_obj = y_raw.astype(str)
    n_unique = int(pd.Series(y_obj).nunique(dropna=True))
    if n_unique >= 2:
        y = y_obj
        task = "classification"
    else:
        task = "trivial"
        y = y_obj

# ----------------------- Split -----------------------
random_state = 42

if task == "classification":
    # If too few samples per class, avoid stratify to prevent split errors
    y_series = pd.Series(y)
    value_counts = y_series.value_counts(dropna=True)
    stratify = y if (value_counts.min() >= 2 and len(value_counts) >= 2) else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=random_state, stratify=stratify
    )
elif task == "regression":
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=random_state
    )
else:
    # trivial
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=random_state
    )

assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Train/test split failed."

# ----------------------- Preprocessing -----------------------
numeric_selector = make_column_selector(dtype_include=np.number)
categorical_selector = make_column_selector(dtype_exclude=np.number)

numeric_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False)),
    ]
)

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_selector),
        ("cat", categorical_transformer, categorical_selector),
    ],
    remainder="drop",
    sparse_threshold=0.0,
)

# ----------------------- Model -----------------------
accuracy = 0.0

if task == "classification":
    # Prefer GaussianNB for CPU-friendly baseline; use LogisticRegression if features become high-dimensional.
    # We'll decide after preprocessing dimensionality inspection without extra passes by fitting preprocess once.
    X_train_p = preprocess.fit_transform(X_train)
    X_test_p = preprocess.transform(X_test)

    # Defensive: if preprocessing yields no features, fallback to majority class
    if X_train_p.shape[1] == 0:
        y_pred = np.full(shape=len(y_test), fill_value=pd.Series(y_train).mode(dropna=True).iloc[0])
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # If many one-hot features, LogisticRegression can be more stable than NB on sparse-ish binary expansions.
        if X_train_p.shape[1] > 200:
            clf = LogisticRegression(
                solver="lbfgs",
                max_iter=200,
                n_jobs=1,
                multi_class="auto",
            )
            clf.fit(X_train_p, y_train)
            y_pred = clf.predict(X_test_p)
        else:
            clf = GaussianNB()
            clf.fit(X_train_p, y_train)
            y_pred = clf.predict(X_test_p)

        accuracy = float(accuracy_score(y_test, y_pred))

elif task == "regression":
    reg = Pipeline(
        steps=[
            ("preprocess", preprocess),
            ("model", Ridge(alpha=1.0, random_state=random_state)),
        ]
    )
    reg.fit(X_train, y_train)
    pred = reg.predict(X_test)
    accuracy = _bounded_regression_score(y_test, pred)

else:
    # Trivial: predict the most frequent class (classification-like) or mean (numeric)
    y_test_series = pd.Series(y_test)
    y_train_series = pd.Series(y_train)
    if y_is_numeric and pd.to_numeric(y_train_series, errors="coerce").notna().mean() >= 0.95:
        mu = float(pd.to_numeric(y_train_series, errors="coerce").mean())
        pred = np.full(len(y_test_series), mu)
        accuracy = _bounded_regression_score(pd.to_numeric(y_test_series, errors="coerce").fillna(mu), pred)
    else:
        mode = y_train_series.mode(dropna=True)
        fill = mode.iloc[0] if len(mode) else (y_train_series.iloc[0] if len(y_train_series) else "")
        pred = np.full(len(y_test_series), fill)
        accuracy = float(accuracy_score(y_test_series, pred)) if len(y_test_series) else 0.0

accuracy = float(np.clip(accuracy, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Chose lightweight, CPU-friendly baselines (GaussianNB / LogisticRegression / Ridge) to minimize compute and memory.
# - Used ColumnTransformer + simple imputers + StandardScaler/OneHotEncoder for reproducible preprocessing with minimal feature engineering.
# - Implemented robust CSV parsing fallback (default, then ';' + decimal=',') to avoid costly manual fixes and ensure end-to-end execution.
# - Added defensive schema handling: normalize headers, drop 'Unnamed:' columns, auto-pick target, and fallback paths if target/classes are invalid.
# - Avoided large ensembles/deep learning; limited iterations and set n_jobs=1 to reduce energy use and improve determinism on CPU.
# - For regression fallback, reported a bounded [0,1] proxy score: 1/(1+RMSE/STD), ensuring stable "ACCURACY" formatting.