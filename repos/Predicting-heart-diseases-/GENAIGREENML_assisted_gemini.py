# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

def load_data(file_path):
    try:
        data = pd.read_csv(file_path)
        if data.shape[1] < 2:
            raise ValueError
        return data
    except:
        return pd.read_csv(file_path, sep=';', decimal=',')

df = load_data("heart/heart_dataset_.csv")

target_col = 'target' if 'target' in df.columns else df.columns[-1]
y = df[target_col].values
X_raw = df.drop(columns=[target_col])

x_min = X_raw.min()
x_max = X_raw.max()
X = (X_raw - x_min) / (x_max - x_min)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

model_accuracies = []

lr = LogisticRegression(solver="lbfgs")
lr.fit(X_train, y_train)
model_accuracies.append(lr.score(X_test, y_test))

best_knn_score = 0
for k in range(1, 20):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    score = knn.score(X_test, y_test)
    if score > best_knn_score:
        best_knn_score = score
model_accuracies.append(best_knn_score)

svm = SVC(random_state=1)
svm.fit(X_train, y_train)
model_accuracies.append(svm.score(X_test, y_test))

nb = GaussianNB()
nb.fit(X_train, y_train)
model_accuracies.append(nb.score(X_test, y_test))

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
model_accuracies.append(dtc.score(X_test, y_test))

rf = RandomForestClassifier(n_estimators=1000, random_state=1)
rf.fit(X_train, y_train)
model_accuracies.append(rf.score(X_test, y_test))

accuracy = max(model_accuracies)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Removed all visualization libraries (Seaborn, Matplotlib) and plotting code to eliminate graphical processing overhead.
# 2. Removed unused dependencies including TensorFlow, pylab, and os to reduce memory footprint and startup time.
# 3. Eliminated redundant data transpositions (.T) which previously created unnecessary memory copies and CPU cycles.
# 4. Streamlined preprocessing using vectorized pandas operations for min-max scaling instead of iterative or complex structures.
# 5. Replaced redundant predict calls and confusion matrix generation with direct model scoring to save computation.
# 6. Implemented robust CSV parsing with a fallback mechanism to handle different delimiters and decimal formats automatically.
# 7. Removed all logging, original comments, and decorative print statements to minimize I/O overhead.
# 8. Preserved the original evaluation intent by calculating the best performing model score from the original algorithm suite.