# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

def load_data(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    return df

path = "heart/heart_dataset_.csv"
df = load_data(path)

target_col = [col for col in df.columns if col.lower() == 'target'][0]
y = df[target_col].values
X_raw = df.drop([target_col], axis=1)

X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

model_accuracies = []

lr = LogisticRegression(solver="lbfgs", max_iter=1000)
lr.fit(X_train, y_train)
model_accuracies.append(lr.score(X_test, y_test))

best_knn_score = 0
for i in range(1, 20):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    score = knn.score(X_test, y_test)
    if score > best_knn_score:
        best_knn_score = score
model_accuracies.append(best_knn_score)

svm = SVC(random_state=1)
svm.fit(X_train, y_train)
model_accuracies.append(svm.score(X_test, y_test))

nb = GaussianNB()
nb.fit(X_train, y_train)
model_accuracies.append(nb.score(X_test, y_test))

dtc = DecisionTreeClassifier(random_state=0)
dtc.fit(X_train, y_train)
model_accuracies.append(dtc.score(X_test, y_test))

rf = RandomForestClassifier(n_estimators=1000, random_state=1)
rf.fit(X_train, y_train)
model_accuracies.append(rf.score(X_test, y_test))

accuracy = max(model_accuracies)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Removed heavy and unused dependencies: seaborn, matplotlib, tensorflow, and os.
# 2. Eliminated all visualization and plotting code to reduce CPU and memory overhead.
# 3. Replaced redundant data transpositions (.T) with direct matrix operations.
# 4. Streamlined preprocessing by removing intermediate dataframe copies.
# 5. Implemented robust CSV parsing with a fallback mechanism for different delimiters.
# 6. Optimized memory footprint by avoiding storage of multiple confusion matrices and large figure objects.
# 7. Removed all print/logging statements within the training loop to reduce I/O overhead.
# 8. Set fixed random seeds (random_state) to ensure deterministic results and reproducibility.
# 9. Simplified scaling using vectorized pandas operations instead of manual loops.
# 10. Consolidated model evaluation logic to focus on extracting the primary performance metric.