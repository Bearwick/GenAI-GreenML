# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import re
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")

def find_csv_file():
    candidates = ["heart/heart_dataset_.csv", "heart_dataset_.csv", "heart.csv", "data.csv", "dataset.csv"]
    for p in candidates:
        if os.path.isfile(p):
            return p
    csv_files = []
    for root, dirs, files in os.walk("."):
        for f in files:
            if f.lower().endswith(".csv"):
                csv_files.append(os.path.join(root, f))
        if csv_files:
            break
    if csv_files:
        csv_files.sort()
        return csv_files[0]
    return None

path = find_csv_file()
if path is None:
    raise FileNotFoundError("No CSV file found.")

def robust_read_csv(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    df_alt = None
    try:
        df_alt = pd.read_csv(path, sep=";", decimal=",")
    except Exception:
        df_alt = None
    if df is None:
        df = df_alt
    elif df_alt is not None and df.shape[1] <= 1 and df_alt.shape[1] > df.shape[1]:
        df = df_alt
    if df is None:
        raise ValueError("Failed to read CSV.")
    return df

df = robust_read_csv(path)

def normalize_columns(cols):
    cleaned = []
    for c in cols:
        c = str(c).replace("\ufeff", "")
        c = re.sub(r"\s+", " ", c.strip())
        cleaned.append(c)
    return cleaned

df.columns = normalize_columns(df.columns)
df = df.loc[:, [c for c in df.columns if not str(c).startswith("Unnamed")]]
df = df.dropna(axis=1, how="all")
assert df.shape[0] > 0 and df.shape[1] > 0

lower_map = {c.lower(): c for c in df.columns}
target_col = None
for cand in ["target", "label", "class", "y", "outcome"]:
    if cand in lower_map:
        target_col = lower_map[cand]
        break

numeric_info = {}
for col in df.columns:
    series_num = pd.to_numeric(df[col], errors="coerce")
    numeric_info[col] = (series_num.notna().mean(), series_num.nunique(dropna=True), series_num)

if target_col is None:
    numeric_cols_detected = [col for col, info in numeric_info.items() if info[0] >= 0.8]
    non_constant = [(col, numeric_info[col][1]) for col in numeric_cols_detected if numeric_info[col][1] > 1]
    if non_constant:
        target_col = sorted(non_constant, key=lambda x: x[1])[0][0]
    elif numeric_cols_detected:
        target_col = numeric_cols_detected[0]
    else:
        target_col = df.columns[-1]

feature_cols = [c for c in df.columns if c != target_col]
if not feature_cols:
    df["__constant__"] = 0.0
    feature_cols = ["__constant__"]

df = df.replace([np.inf, -np.inf], np.nan)

target_series_raw = df[target_col]
target_num = pd.to_numeric(target_series_raw, errors="coerce")
target_num_ratio = target_num.notna().mean()
if pd.api.types.is_numeric_dtype(target_series_raw) or target_num_ratio >= 0.8:
    target_series = target_num
    target_is_numeric = True
else:
    target_series = target_series_raw
    target_is_numeric = False

mask = target_series.notna()
df = df.loc[mask].copy()
target_series = target_series.loc[mask]

assert len(df) > 0

numeric_cols = []
categorical_cols = []
for col in feature_cols:
    if col not in df.columns:
        continue
    series = df[col]
    if pd.api.types.is_numeric_dtype(series):
        df[col] = pd.to_numeric(series, errors="coerce")
        numeric_cols.append(col)
    else:
        series_num = pd.to_numeric(series, errors="coerce")
        if series_num.notna().mean() >= 0.8:
            df[col] = series_num
            numeric_cols.append(col)
        else:
            categorical_cols.append(col)

numeric_cols = [col for col in numeric_cols if df[col].notna().sum() > 0]
categorical_cols = [col for col in categorical_cols if df[col].notna().sum() > 0]

if not numeric_cols and not categorical_cols:
    df["__constant__"] = 0.0
    numeric_cols = ["__constant__"]
    feature_cols = ["__constant__"]
else:
    feature_cols = numeric_cols + categorical_cols

X = df[feature_cols]
y = target_series

n_samples = len(y)
if target_is_numeric:
    unique_vals = y.nunique(dropna=True)
    if unique_vals <= 20 and unique_vals <= max(2, int(0.2 * n_samples)):
        task = "classification"
    else:
        task = "regression"
else:
    unique_vals = y.nunique(dropna=True)
    task = "classification"

if task == "classification" and unique_vals < 2:
    task = "classification_dummy"

transformers = []
if numeric_cols:
    num_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ])
    transformers.append(("num", num_pipe, numeric_cols))
if categorical_cols:
    cat_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])
    transformers.append(("cat", cat_pipe, categorical_cols))
preprocessor = ColumnTransformer(transformers=transformers, remainder="drop")

if task == "classification":
    model = LogisticRegression(max_iter=200, solver="liblinear")
elif task == "classification_dummy":
    model = DummyClassifier(strategy="most_frequent")
else:
    model = Ridge(alpha=1.0)

pipeline = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

if n_samples < 2:
    X_train, X_test, y_train, y_test = X, X, y, y
else:
    stratify = y if task in ["classification", "classification_dummy"] and unique_vals > 1 else None
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=None
        )

assert len(X_train) > 0 and len(X_test) > 0

pipeline.fit(X_train, y_train)

if task in ["classification", "classification_dummy"]:
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
else:
    y_pred = pipeline.predict(X_test)
    try:
        r2 = r2_score(y_test, y_pred)
        if np.isnan(r2):
            raise ValueError("NaN r2")
        accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))
    except Exception:
        mse = np.mean((np.array(y_test) - np.array(y_pred)) ** 2)
        accuracy = 1.0 / (1.0 + mse) if mse is not None else 0.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Lightweight linear models (LogisticRegression/Ridge) with limited iterations keep CPU usage low while providing solid baselines.
# - A single ColumnTransformer pipeline performs imputation, scaling, and one-hot encoding efficiently without redundant passes.
# - Regression fallback maps RÂ² to a bounded [0,1] accuracy proxy (clipped) to maintain stable evaluation on continuous targets.