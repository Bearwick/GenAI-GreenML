# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

RANDOM_STATE = 0

try:
    df = pd.read_csv("heart/heart_dataset_.csv")
    if df.shape[1] < 2:
        df = pd.read_csv("heart/heart_dataset_.csv", sep=';', decimal=',')
except Exception:
    df = pd.read_csv("heart/heart_dataset_.csv", sep=';', decimal=',')

expected_cols = ["age", "sex", "cp", "trestbps", "chol", "fbs", "restecg",
                 "thalach", "exang", "oldpeak", "slope", "ca", "thal", "target"]
if list(df.columns) != expected_cols and df.shape[1] == len(expected_cols):
    df.columns = expected_cols

y = df["target"].values
x_data = df.drop(["target"], axis=1)
x_min = x_data.min()
x_range = x_data.max() - x_min
x_range[x_range == 0] = 1
x = (x_data - x_min) / x_range

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=RANDOM_STATE)

accuracies = {}

lr = LogisticRegression(solver="lbfgs", random_state=RANDOM_STATE, max_iter=1000)
lr.fit(x_train, y_train)
accuracies['Logistic Regression'] = lr.score(x_test, y_test) * 100

best_knn_score = 0
for k in range(1, 20):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(x_train, y_train)
    score = knn.score(x_test, y_test)
    if score > best_knn_score:
        best_knn_score = score
accuracies['KNN'] = best_knn_score * 100

svm = SVC(random_state=1)
svm.fit(x_train, y_train)
accuracies['SVM'] = svm.score(x_test, y_test) * 100

nb = GaussianNB()
nb.fit(x_train, y_train)
accuracies['Naive Bayes'] = nb.score(x_test, y_test) * 100

dtc = DecisionTreeClassifier(random_state=RANDOM_STATE)
dtc.fit(x_train, y_train)
accuracies['Decision Tree'] = dtc.score(x_test, y_test) * 100

rf = RandomForestClassifier(n_estimators=1000, random_state=1)
rf.fit(x_train, y_train)
accuracies['Random Forest'] = rf.score(x_test, y_test) * 100

best_name = max(accuracies, key=accuracies.get)
accuracy = accuracies[best_name] / 100.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Removed all matplotlib/seaborn imports and plotting calls to eliminate unnecessary computation and I/O.
# Removed all print/logging statements except the required accuracy output.
# Removed TensorFlow import which was unused, saving significant import time and memory.
# Removed unnecessary .T transpose operations on train/test arrays (sklearn accepts row-major directly).
# Consolidated KNN search loop to track best score without storing a full list.
# Added robust CSV parsing fallback with sep=';' and decimal=','.
# Set random_state on Decision