# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "heart/heart_dataset_.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    df = pd.read_csv(path)
    # Heuristic: if only one column and it contains separators, parsing likely wrong
    if df.shape[1] <= 1:
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df2.shape[1] > df.shape[1]:
                df = df2
        except Exception:
            pass
    return df


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _coerce_numeric_columns(df):
    # Coerce object columns that look numeric to numeric, safely
    for c in df.columns:
        if df[c].dtype == "object":
            s = df[c].astype(str).str.strip()
            # If many values match numeric-like patterns, try to convert
            numeric_like = s.str.match(r"^[\+\-]?\d+([.,]\d+)?$").mean() if len(s) else 0.0
            if numeric_like >= 0.7:
                s2 = s.str.replace(",", ".", regex=False)
                df[c] = pd.to_numeric(s2, errors="coerce")
    return df


def _choose_target(df, preferred=("target", "label", "y", "outcome", "class")):
    cols_lower = {c.lower(): c for c in df.columns}
    for p in preferred:
        if p in cols_lower:
            return cols_lower[p]

    # Prefer a non-constant numeric column with low cardinality (likely classification)
    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    candidates = []
    for c in numeric_cols:
        s = df[c].replace([np.inf, -np.inf], np.nan).dropna()
        if s.empty:
            continue
        nunique = s.nunique(dropna=True)
        if nunique <= 20 and nunique >= 2:
            candidates.append((nunique, -s.var(ddof=0), c))
    if candidates:
        candidates.sort()
        return candidates[0][2]

    # Otherwise, choose numeric column with highest variance that is not constant
    best = None
    best_var = -1.0
    for c in numeric_cols:
        s = df[c].replace([np.inf, -np.inf], np.nan).dropna()
        if s.nunique(dropna=True) < 2:
            continue
        v = float(s.var(ddof=0))
        if np.isfinite(v) and v > best_var:
            best_var = v
            best = c
    if best is not None:
        return best

    # Fallback: last column
    return df.columns[-1]


def _is_classification_target(y):
    # Classification if few unique values and integer-like
    y_no_na = pd.Series(y).replace([np.inf, -np.inf], np.nan).dropna()
    if y_no_na.empty:
        return False
    nunique = int(y_no_na.nunique())
    if nunique < 2:
        return False
    if nunique <= 20:
        # If all values are near integers, treat as classification
        vals = y_no_na.to_numpy()
        if np.all(np.isfinite(vals)):
            if np.all(np.isclose(vals, np.round(vals), atol=1e-6)):
                return True
    return False


def _bounded_regression_score(y_true, y_pred):
    # Stable proxy in [0,1]: 1 / (1 + MSE / Var(y_true)), with safeguards
    yt = np.asarray(y_true, dtype=float)
    yp = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(yt) & np.isfinite(yp)
    if mask.sum() == 0:
        return 0.0
    yt = yt[mask]
    yp = yp[mask]
    mse = float(np.mean((yt - yp) ** 2))
    var = float(np.var(yt))
    denom = var if var > 1e-12 else (mse + 1e-12)
    score = 1.0 / (1.0 + mse / (denom + 1e-12))
    return float(np.clip(score, 0.0, 1.0))


def main():
    if not os.path.exists(DATASET_PATH):
        # Minimal fallback: cannot read dataset; keep deterministic output
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Defensive: if header row got swallowed into data, attempt to re-read with header=None is too risky;
    # proceed with what we have.
    assert df.shape[0] > 0 and df.shape[1] > 0

    df = _coerce_numeric_columns(df)

    target_col = _choose_target(df)
    # If target column is missing for any reason, fallback to last column
    if target_col not in df.columns:
        target_col = df.columns[-1]

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # Replace inf with nan to keep imputers safe
    X = X.replace([np.inf, -np.inf], np.nan)
    y_series = pd.Series(y_raw).replace([np.inf, -np.inf], np.nan)

    # Drop rows where target is missing
    valid = y_series.notna()
    X = X.loc[valid].copy()
    y_series = y_series.loc[valid].copy()

    assert X.shape[0] > 0

    # If no features left, create a constant feature
    if X.shape[1] == 0:
        X = pd.DataFrame({"const": np.ones(len(y_series), dtype=float)})

    # Decide task type
    task_is_classification = False
    # Try to coerce y to numeric if possible
    y_num = pd.to_numeric(y_series, errors="coerce")
    if y_num.notna().mean() >= 0.9:
        y_used = y_num
        task_is_classification = _is_classification_target(y_used)
    else:
        y_used = y_series.astype(str)
        # If strings have few unique classes, classification; else not meaningful but still classification
        nunique = y_used.nunique(dropna=True)
        task_is_classification = nunique >= 2 and nunique <= 50

    # Ensure split feasibility
    if len(y_used) < 4:
        # Too small; trivial baseline
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    stratify = None
    if task_is_classification:
        # Only stratify if each class has at least 2 instances
        vc = pd.Series(y_used).value_counts(dropna=False)
        if (vc.min() >= 2) and (vc.size >= 2):
            stratify = y_used

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y_used,
        test_size=0.2,
        random_state=RANDOM_STATE,
        stratify=stratify,
    )
    assert len(X_train) > 0 and len(X_test) > 0

    # Preprocessing: numeric -> median impute + standardize; categorical -> most_frequent + onehot
    numeric_selector = make_column_selector(dtype_include=np.number)
    categorical_selector = make_column_selector(dtype_exclude=np.number)

    numeric_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_pipe, numeric_selector),
            ("cat", categorical_pipe, categorical_selector),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if task_is_classification:
        # If classification but target has <2 classes in train, fallback to regression-like score
        y_train_series = pd.Series(y_train)
        if y_train_series.nunique(dropna=True) < 2:
            task_is_classification = False

    if task_is_classification:
        # Lightweight linear classifier; liblinear is CPU-friendly for small datasets
        clf = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            random_state=RANDOM_STATE,
        )
        model = Pipeline(steps=[("prep", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Regression fallback with bounded [0,1] score proxy
        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        model = Pipeline(steps=[("prep", preprocessor), ("model", reg)])

        # Coerce y to numeric; if impossible, use factor codes
        y_train_num = pd.to_numeric(pd.Series(y_train), errors="coerce")
        y_test_num = pd.to_numeric(pd.Series(y_test), errors="coerce")
        if y_train_num.notna().mean() < 0.9 or y_test_num.notna().mean() < 0.9:
            y_all = pd.concat([pd.Series(y_train), pd.Series(y_test)], axis=0).astype(str)
            codes, _ = pd.factorize(y_all, sort=True)
            y_train_num = pd.Series(codes[: len(y_train)], index=pd.Series(y_train).index).astype(float)
            y_test_num = pd.Series(codes[len(y_train) :], index=pd.Series(y_test).index).astype(float)

        model.fit(X_train, y_train_num)
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_score(y_test_num.to_numpy(), y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used robust CSV loading with a low-cost fallback (semicolon separator + comma decimals) to avoid expensive manual fixes.
# - Normalized/dropped noisy columns (e.g., Unnamed) and coerced numeric-like object columns to reduce one-hot expansion.
# - Chose lightweight, CPU-friendly baselines: LogisticRegression(liblinear) for classification; Ridge for regression fallback.
# - Implemented a single sklearn Pipeline + ColumnTransformer to avoid repeated preprocessing and ensure reproducibility.
# - Kept feature engineering minimal: median/mode imputation, standardization for numeric stability, and sparse one-hot for categoricals.
# - Added defensive schema/target selection and bounded regression scoring in [0,1] to guarantee end-to-end execution.