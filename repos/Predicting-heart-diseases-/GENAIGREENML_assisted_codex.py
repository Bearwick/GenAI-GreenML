# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd
import random
import warnings
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.exceptions import ConvergenceWarning

DATASET_HEADERS = "age,sex,cp,trestbps,chol,fbs,restecg,thalach,exang,oldpeak,slope,ca,thal,target"
EXPECTED_HEADERS = [h.strip() for h in DATASET_HEADERS.split(",")]

def _normalize(col):
    return str(col).strip().lower()

_EXPECTED_MAP = {_normalize(h): h for h in EXPECTED_HEADERS}

def _align_columns(df):
    rename_map = {}
    for col in df.columns:
        norm = _normalize(col)
        if norm in _EXPECTED_MAP:
            rename_map[col] = _EXPECTED_MAP[norm]
    if rename_map:
        df = df.rename(columns=rename_map)
    if not set(EXPECTED_HEADERS).issubset(df.columns) and df.shape[1] == len(EXPECTED_HEADERS):
        df.columns = EXPECTED_HEADERS
    return df

def _read_csv_robust(path):
    df = pd.read_csv(path)
    df = _align_columns(df)
    if df.shape[1] == 1 or not set(EXPECTED_HEADERS).issubset(df.columns):
        df_alt = pd.read_csv(path, sep=";", decimal=",")
        df_alt = _align_columns(df_alt)
        if df_alt.shape[1] > 1:
            if set(EXPECTED_HEADERS).issubset(df_alt.columns) or df.shape[1] == 1 or df_alt.shape[1] > df.shape[1]:
                df = df_alt
    if not set(EXPECTED_HEADERS).issubset(df.columns) and df.shape[1] == len(EXPECTED_HEADERS):
        df.columns = EXPECTED_HEADERS
    return df

def main():
    np.random.seed(0)
    random.seed(0)
    warnings.filterwarnings("ignore", category=ConvergenceWarning)

    df = _read_csv_robust("heart/heart_dataset_.csv")
    df = df.apply(pd.to_numeric, errors="coerce")
    if df.isna().any().any():
        try:
            median_vals = df.median(numeric_only=True)
        except TypeError:
            median_vals = df.median()
        df = df.fillna(median_vals)

    norm_cols = {_normalize(c): c for c in df.columns}
    if "target" in norm_cols:
        target_col = norm_cols["target"]
    elif len(df.columns) == len(EXPECTED_HEADERS):
        target_col = _EXPECTED_MAP.get("target", df.columns[-1])
        if target_col not in df.columns:
            target_col = df.columns[-1]
    else:
        target_col = df.columns[-1]

    y = df[target_col].to_numpy()
    x_data = df.drop(columns=[target_col])
    min_vals = x_data.min()
    range_vals = x_data.max() - min_vals
    range_vals = range_vals.replace(0, 1)
    x_scaled = (x_data - min_vals) / range_vals
    x_scaled = x_scaled.to_numpy()

    x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=0)

    accuracies = {}

    lr = LogisticRegression(solver="lbfgs")
    lr.fit(x_train, y_train)
    accuracies["Logistic Regression"] = lr.score(x_test, y_test)

    best_knn_score = -1.0
    for k in range(1, 20):
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(x_train, y_train)
        score = knn.score(x_test, y_test)
        if score > best_knn_score:
            best_knn_score = score
    accuracies["KNN"] = best_knn_score

    svm = SVC(random_state=1)
    svm.fit(x_train, y_train)
    accuracies["SVM"] = svm.score(x_test, y_test)

    nb = GaussianNB()
    nb.fit(x_train, y_train)
    accuracies["Naive Bayes"] = nb.score(x_test, y_test)

    dtc = DecisionTreeClassifier()
    dtc.fit(x_train, y_train)
    accuracies["Decision Tree"] = dtc.score(x_test, y_test)

    rf = RandomForestClassifier(n_estimators=1000, random_state=1)
    rf.fit(x_train, y_train)
    accuracies["Random Forest"] = rf.score(x_test, y_test)

    accuracy = max(accuracies.values())
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed visualization and exploratory analysis to avoid unnecessary computation and I/O.
# - Performed a single min-max scaling pass and converted features once to NumPy to reduce repeated conversions.
# - Eliminated redundant transposes and unused model predictions while preserving model evaluations.
# - Implemented robust CSV parsing with delimiter fallback and column alignment for reliable input handling.
# - Fixed random seeds and suppressed convergence warnings for deterministic, clean execution.