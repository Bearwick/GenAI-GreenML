# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import random
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.exceptions import ConvergenceWarning

DATASET_PATH = "heart/heart_dataset_.csv"
DATASET_HEADERS = [
    "age", "sex", "cp", "trestbps", "chol", "fbs", "restecg",
    "thalach", "exang", "oldpeak", "slope", "ca", "thal", "target"
]

def _normalize_headers(headers):
    return [str(h).strip().lower() for h in headers]

def _numeric_like(headers):
    for h in headers:
        try:
            float(str(h).replace(",", "."))
        except ValueError:
            return False
    return True

def read_csv_robust(path, expected_headers):
    expected_len = len(expected_headers)

    def parsed_ok(df, check_numeric):
        if len(df.columns) != expected_len:
            return False
        if check_numeric and _numeric_like(df.columns):
            return False
        return True

    df = pd.read_csv(path)
    if not parsed_ok(df, True):
        df_alt = pd.read_csv(path, sep=";", decimal=",")
        if parsed_ok(df_alt, True):
            df = df_alt
        else:
            df_noheader = pd.read_csv(path, header=None)
            if parsed_ok(df_noheader, False):
                df = df_noheader
            else:
                df_noheader_alt = pd.read_csv(path, sep=";", decimal=",", header=None)
                if parsed_ok(df_noheader_alt, False):
                    df = df_noheader_alt

    if len(df.columns) == expected_len:
        if _numeric_like(df.columns):
            df.columns = expected_headers
        else:
            norm_cols = _normalize_headers(df.columns)
            expected_norm = _normalize_headers(expected_headers)
            if norm_cols != expected_norm:
                if set(norm_cols) == set(expected_norm):
                    mapping = {n: col for n, col in zip(norm_cols, df.columns)}
                    df = df[[mapping[name] for name in expected_norm]]
                df.columns = expected_headers
    return df

def main():
    random.seed(0)
    np.random.seed(0)
    warnings.filterwarnings("ignore", category=ConvergenceWarning)

    df = read_csv_robust(DATASET_PATH, DATASET_HEADERS)
    expected_norm = _normalize_headers(DATASET_HEADERS)
    target_norm = expected_norm[-1] if expected_norm else None
    target_col = None
    if target_norm is not None:
        for col in df.columns:
            if str(col).strip().lower() == target_norm:
                target_col = col
                break
    if target_col is None:
        target_col = df.columns[-1]

    feature_cols = [c for c in df.columns if c != target_col]
    y = df[target_col].to_numpy()
    X = df[feature_cols].to_numpy(dtype=float)
    del df

    min_vals = np.nanmin(X, axis=0)
    max_vals = np.nanmax(X, axis=0)
    range_vals = max_vals - min_vals
    with np.errstate(divide="ignore", invalid="ignore"):
        X -= min_vals
        X /= range_vals

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=0
    )
    model = LogisticRegression(solver="lbfgs", random_state=0)
    model.fit(X_train, y_train)
    accuracy = model.score(X_test, y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed exploratory analysis, plotting, and extra model training to avoid redundant computation.
# - Used vectorized NumPy scaling without transposes to minimize data movement and memory overhead.
# - Implemented robust CSV parsing with header normalization and limited fallback reads.
# - Fixed random seeds and suppressed non-critical warnings for deterministic, quiet execution.