# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def load_data(path):
    features = [
        "texture_worst", "radius_se", "symmetry_worst", "concave points_mean",
        "area_se", "area_worst", "radius_worst", "concave points_worst",
        "concavity_mean", "fractal_dimension_se"
    ]
    target = "diagnosis"
    cols_to_load = [target] + features

    try:
        df = pd.read_csv(path, usecols=cols_to_load)
        if len(df.columns) < len(cols_to_load):
            raise ValueError
    except (ValueError, pd.errors.ParserError):
        df = pd.read_csv(path, sep=';', decimal=',', usecols=cols_to_load)
    
    return df, features, target

def run_pipeline():
    df, feature_names, target_name = load_data('Cancer_Data.csv')
    
    df[target_name] = df[target_name].map({'M': 1, 'B': 0})
    
    X = df[feature_names].astype('float32')
    y = df[target_name].astype('int8')

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = LogisticRegression(solver='liblinear', random_state=42)
    model.fit(X_train_scaled, y_train)

    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary
# 1. Memory Reduction: Used 'usecols' in pd.read_csv to load only required features and target, minimizing RAM usage.
# 2. Precision Optimization: Downcast features to float32 and target to int8 to reduce memory footprint and speed up processing.
# 3. Eliminated Redundant Compute: Removed plotting (matplotlib/seaborn) and high-level metric reporting (classification_report/confusion_matrix).
# 4. Reduced I/O: Removed joblib model and scaler persistence (saving to disk) to save energy and runtime associated with storage.
# 5. Library Streamlining: Removed unused imports to reduce startup overhead and memory consumption.
# 6. Robust Parsing: Implemented a memory-efficient fallback mechanism for CSV parsing without loading full data into memory twice.
# 7. Computation Efficiency: Retained the lightweight 'liblinear' solver which is efficient for small-to-medium binary classification tasks.