# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def load_and_preprocess(filepath, feature_names, target_col):
    cols_to_load = feature_names + [target_col]
    try:
        df = pd.read_csv(filepath, usecols=cols_to_load)
    except (ValueError, KeyError):
        try:
            df = pd.read_csv(filepath, sep=';', decimal=',', usecols=cols_to_load)
        except Exception:
            df = pd.read_csv(filepath)
            df = df[cols_to_load]
    except FileNotFoundError:
        return None, None

    df[target_col] = df[target_col].map({'M': 1, 'B': 0})
    return df[feature_names], df[target_col]

feature_names = [
    "texture_worst", "radius_se", "symmetry_worst", "concave points_mean",
    "area_se", "area_worst", "radius_worst", "concave points_worst",
    "concavity_mean", "fractal_dimension_se"
]
target_column = "diagnosis"

X, y = load_and_preprocess('Cancer_Data.csv', feature_names, target_column)

if X is not None:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = LogisticRegression(solver='liblinear', random_state=42)
    model.fit(X_train_scaled, y_train)

    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Selective Column Loading: Utilized 'usecols' in pandas.read_csv to load only the 11 necessary columns, significantly reducing memory allocation and I/O overhead.
# - Removed Redundant Dependencies: Eliminated matplotlib, seaborn, and joblib, reducing the environment footprint, startup time, and energy consumption associated with visualization and disk serialization.
# - Efficient Memory Management: Avoided creating intermediate data structures and large copies of the dataframe by performing mapping and column selection early.
# - Optimized Preprocessing: Retained only the essential StandardScaler and LogisticRegression components, removing high-overhead reporting functions like classification_report.
# - Robust CSV Fallback: Implemented a memory-efficient fallback mechanism for different CSV formats to ensure reliable execution without manual intervention.
# - Reduced I/O Side Effects: Removed all file-saving operations (joblib.dump), which avoids unnecessary disk wear and energy usage during model training.