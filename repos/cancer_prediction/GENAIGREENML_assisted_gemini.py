# Generated by generate_llm_code_2.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import joblib

def train_optimized():
    feature_names = [
        "texture_worst", "radius_se", "symmetry_worst", "concave points_mean",
        "area_se", "area_worst", "radius_worst", "concave points_worst",
        "concavity_mean", "fractal_dimension_se"
    ]
    target_col = "diagnosis"

    try:
        # Optimization: Only load necessary columns to reduce memory usage and I/O time
        df = pd.read_csv('Cancer_Data.csv', usecols=feature_names + [target_col])
    except FileNotFoundError:
        return

    # Optimization: Vectorized mapping and conversion to numpy array to minimize overhead
    y = df.pop(target_col).map({'M': 1, 'B': 0}).values
    X = df.values

    # Preservation of original split parameters
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Optimization: In-place scaling to avoid creating extra memory copies
    scaler = StandardScaler(copy=False)
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Optimization: Using a simple, lightweight model suitable for small datasets
    model = LogisticRegression(solver='liblinear', random_state=42)
    model.fit(X_train, y_train)

    # Evaluation
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

    # Efficient serialization
    joblib.dump(model, 'breast_cancer_model.joblib')
    joblib.dump(scaler, 'breast_cancer_scaler.joblib')

if __name__ == "__main__":
    train_optimized()

# OPTIMIZATIONS APPLIED:
# 1. Selective Column Loading: Used 'usecols' in pd.read_csv to prevent loading unnecessary data into memory.
# 2. Reduced Data Movement: Utilized df.pop() and .values to extract target and features without redundant dataframe copying.
# 3. Memory Footprint Reduction: Set copy=False in StandardScaler to perform scaling operations in-place on numpy arrays.
# 4. Computational Efficiency: Removed heavy visualization libraries (matplotlib/seaborn) and complex metrics (classification_report) to save CPU cycles and RAM.
# 5. Optimized Preprocessing: Replaced dataframe-based mapping with direct numpy array conversion, which is faster for scikit-learn inputs.
# 6. Streamlined Execution: Removed interactive print statements and unnecessary loops to minimize runtime overhead.