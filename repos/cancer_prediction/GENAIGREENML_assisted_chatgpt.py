# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import sys
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


FEATURE_NAMES = (
    "texture_worst",
    "radius_se",
    "symmetry_worst",
    "concave points_mean",
    "area_se",
    "area_worst",
    "radius_worst",
    "concave points_worst",
    "concavity_mean",
    "fractal_dimension_se",
)


def load_data(path: str) -> pd.DataFrame:
    try:
        return pd.read_csv(path)
    except FileNotFoundError:
        sys.exit(1)


def prepare_xy(df: pd.DataFrame):
    y = df["diagnosis"].map({"M": 1, "B": 0}).astype("int8")
    X = df.loc[:, FEATURE_NAMES]
    return X, y


def train_and_evaluate(X, y, random_state: int = 42, test_size: float = 0.3) -> float:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )

    scaler = StandardScaler(copy=False)
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = LogisticRegression(solver="liblinear", random_state=random_state)
    model.fit(X_train_scaled, y_train)

    y_pred = model.predict(X_test_scaled)
    return accuracy_score(y_test, y_pred)


def main():
    df = load_data("Cancer_Data.csv")
    X, y = prepare_xy(df)
    accuracy = train_and_evaluate(X, y)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Removed unused imports (joblib, plotting libs, extra metrics) to cut startup time and memory.
# - Removed confusion-matrix visualization and model/scaler saving to avoid expensive I/O and rendering.
# - Used a single accuracy computation to preserve the required output with minimal overhead.
# - Converted target to int8 to reduce memory footprint without changing labels/behavior.
# - StandardScaler(copy=False) reduces unnecessary data copies during scaling to lower memory traffic.
# - Used df.loc[:, FEATURE_NAMES] and tuple constant for features to avoid repeated list allocations and keep selection explicit.
# - Kept fixed random_state for reproducibility and stable results.