# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

RANDOM_SEED = 42
DATASET_PATH = "Cancer_Data.csv"
DATASET_HEADERS = [
    "id",
    "diagnosis",
    "radius_mean",
    "texture_mean",
    "perimeter_mean",
    "area_mean",
    "smoothness_mean",
    "compactness_mean",
    "concavity_mean",
    "concave points_mean",
    "symmetry_mean",
    "fractal_dimension_mean",
    "radius_se",
    "texture_se",
    "perimeter_se",
    "area_se",
    "smoothness_se",
    "compactness_se",
    "concavity_se",
    "concave points_se",
    "symmetry_se",
    "fractal_dimension_se",
    "radius_worst",
    "texture_worst",
    "perimeter_worst",
    "area_worst",
    "smoothness_worst",
    "compactness_worst",
    "concavity_worst",
    "concave points_worst",
    "symmetry_worst",
    "fractal_dimension_worst",
]

FEATURE_NAMES = [
    "texture_worst",
    "radius_se",
    "symmetry_worst",
    "concave points_mean",
    "area_se",
    "area_worst",
    "radius_worst",
    "concave points_worst",
    "concavity_mean",
    "fractal_dimension_se",
]


def _read_csv_with_fallback(path: str, expected_headers: list[str]) -> pd.DataFrame:
    df = pd.read_csv(path)
    looks_wrong = df.shape[1] <= 2 or (expected_headers and expected_headers[0] not in df.columns)
    if looks_wrong:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _resolve_columns(df: pd.DataFrame, requested: list[str]) -> list[str]:
    colset = set(df.columns)
    missing = [c for c in requested if c not in colset]
    if missing:
        raise KeyError(f"Missing required columns: {missing}")
    return requested


def main() -> None:
    df = _read_csv_with_fallback(DATASET_PATH, DATASET_HEADERS)

    diag_col = "diagnosis"
    if diag_col not in df.columns:
        raise KeyError("Missing target column 'diagnosis'")

    feature_cols = _resolve_columns(df, FEATURE_NAMES)

    y = df[diag_col].map({"M": 1, "B": 0})
    X = df.loc[:, feature_cols]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=RANDOM_SEED
    )

    model = make_pipeline(
        StandardScaler(),
        LogisticRegression(solver="liblinear", random_state=RANDOM_SEED),
    )
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused imports, reports, plots, and file-saving to cut runtime, I/O, and dependency overhead while keeping the same training/evaluation intent.
# - Used a single scikit-learn Pipeline (StandardScaler + LogisticRegression) to avoid storing separate scaled arrays and reduce memory footprint/data movement.
# - Loaded only needed columns via a single DataFrame slice (df.loc[:, feature_cols]) and avoided extra intermediate structures.
# - Implemented robust CSV parsing fallback (default read_csv, then retry with sep=';' and decimal=',') to prevent costly downstream failures/retries.
# - Centralized fixed RANDOM_SEED for deterministic split/model behavior and reproducible results without extra computation.
# - Added explicit column resolution against df.columns to avoid assumptions about schema and prevent hidden errors that waste compute.