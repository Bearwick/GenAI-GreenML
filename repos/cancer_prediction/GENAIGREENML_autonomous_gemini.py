# Generated by generate_llm_code_2.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

df = pd.read_csv('Cancer_Data.csv')

df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})
y = df['diagnosis']

feature_names = [
    "texture_worst",
    "radius_se",
    "symmetry_worst",
    "concave points_mean",
    "area_se",
    "area_worst",
    "radius_worst",
    "concave points_worst",
    "concavity_mean",
    "fractal_dimension_se"
]
X = df[feature_names]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = LogisticRegression(solver='liblinear', penalty='l2', random_state=42)
model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

"""
DESIGN JUSTIFICATION FOR ENERGY EFFICIENCY:
1. Algorithm Selection: Logistic Regression was chosen over Deep Learning or Ensembles (like XGBoost) because it is mathematically simple, requiring significantly fewer FLOPs (Floating Point Operations) for both training and inference.
2. Dimensionality Reduction: Only 10 high-impact features were utilized instead of the full set. This reduces the memory footprint and the number of operations required per prediction.
3. Preprocessing: StandardScaler is a low-complexity linear transformation that ensures fast model convergence without heavy computational overhead.
4. Solver Choice: The 'liblinear' solver is highly efficient for small-scale binary classification tasks on CPU, minimizing the CPU cycles and time required for convergence compared to iterative solvers like 'lbfgs' on small datasets.
5. Resource Management: By avoiding specialized libraries (like TensorFlow/PyTorch) and hardware (GPU), the solution minimizes the energy profile associated with loading large runtimes and powering dedicated hardware.
"""