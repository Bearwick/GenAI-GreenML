# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Step 1: Robust Data Loading
dataset_path = 'Cancer_Data.csv'
try:
    df = pd.read_csv(dataset_path)
    if df.shape[1] <= 1:
        raise ValueError
except Exception:
    df = pd.read_csv(dataset_path, sep=';', decimal=',')

# Step 2: Schema Normalization
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Step 3: Identify Target and Features
# Based on context 'diagnosis' is target. Fallback to second column if missing.
target_col = None
potential_targets = ['diagnosis', 'target', 'label', 'class']
for pt in potential_targets:
    if pt in df.columns:
        target_col = pt
        break

if target_col is None:
    # Fallback: find a non-numeric column or pick the second column (typical for ID, Target, Features structure)
    cat_cols = df.select_dtypes(include=['object']).columns
    if len(cat_cols) > 0:
        target_col = cat_cols[0]
    else:
        target_col = df.columns[1]

# Drop ID-like columns to prevent leakage/noise
id_cols = ['id', 'uuid', 'index']
features = [c for c in df.columns if c != target_col and c.lower() not in id_cols]

# Step 4: Data Cleaning & Preprocessing
# Ensure data is not empty
if df.empty:
    import sys
    sys.exit(0)

X = df[features].copy()
y = df[target_col].copy()

# Robust coercion of features to numeric
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors='coerce')

# Target encoding (Classification context)
le = LabelEncoder()
y = le.fit_transform(y.astype(str))

# Handle edge case: only one class in target
if len(np.unique(y)) < 2:
    # Trivial baseline if classification is impossible
    accuracy = 1.000000
    print(f"ACCURACY={accuracy:.6f}")
else:
    # Step 5: Energy-Efficient Modeling
    # Logistic Regression is highly CPU-efficient, linear in complexity, and fits the breast cancer task well.
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # Pipeline: Impute missing values -> Scale -> Logistic Regression
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler()),
        ('model', LogisticRegression(solver='lbfgs', max_iter=1000, penalty='l2', random_state=42))
    ])

    # Step 6: Training and Evaluation
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    # Step 7: Final Output
    print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Model Choice: Logistic Regression was selected over ensembles (Random Forest/XGBoost) to minimize CPU cycles and memory usage. 
# 2. Solver Selection: 'lbfgs' is efficient for small-to-medium datasets and converges quickly on standardized data.
# 3. Preprocessing: SimpleImputer with median and StandardScaler ensures stability for the linear model without heavy computation.
# 4. Schema Handling: Implemented robust column normalization and automated feature/target detection to prevent crashes on varied CSV formats.
# 5. Resource Efficiency: The pipeline avoids any deep learning or high-dimensional embeddings, making it suitable for low-power CPU environments.
# 6. Memory Management: Unnecessary ID columns are dropped early, and internal data copies are minimized.