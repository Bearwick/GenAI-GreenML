# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset (assuming data.csv based on provided headers)
df = pd.read_csv('data.csv')

# Drop unnecessary identifier and potential trailing empty column
cols_to_drop = ['id']
if 'Unnamed: 32' in df.columns:
    cols_to_drop.append('Unnamed: 32')
df = df.drop(columns=cols_to_drop)

# Encode target variable: Malignant (M) -> 1, Benign (B) -> 0
le = LabelEncoder()
df['diagnosis'] = le.fit_transform(df['diagnosis'])

# Split features and target
X = df.drop('diagnosis', axis=1)
y = df['diagnosis']

# Reproducible split with 80/20 ratio
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling for faster convergence and better performance of linear model
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Logistic Regression chosen for extreme computational efficiency and interpretability
# Using lbfgs solver which is energy-efficient for small-to-medium tabular datasets
model = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42)
model.fit(X_train_scaled, y_train)

# Model evaluation
y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Model Selection: Logistic Regression was chosen over Deep Learning or Ensemble methods. 
#    It requires significantly fewer FLOPs (Floating Point Operations) during training and inference.
# 2. Algorithm Efficiency: The 'lbfgs' solver is an optimization algorithm that converges quickly 
#    on small datasets like this one, minimizing CPU cycles and energy consumption.
# 3. Dimensionality: The solution avoids high-dimensional embeddings or feature expansion, 
#    keeping the memory footprint minimal.
# 4. Preprocessing: Simple StandardScaler was used to ensure the gradient-based optimizer 
#    converges in fewer iterations, directly reducing the total execution time.
# 5. Hardware: The entire pipeline is optimized for single-thread CPU execution, 
#    removing the need for power-hungry GPU/TPU resources.
# 6. Data Handling: Dropping irrelevant columns ('id') early reduces the memory bandwidth 
#    requirement during matrix operations.