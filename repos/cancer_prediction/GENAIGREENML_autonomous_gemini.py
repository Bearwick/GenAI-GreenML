# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

df = pd.read_csv('Cancer_Data.csv')
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df = df.drop(columns=['id'], errors='ignore')

df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})

X = df.drop('diagnosis', axis=1)
y = df['diagnosis']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = LogisticRegression(solver='liblinear', penalty='l2', C=1.0, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

"""
JUSTIFICATION FOR ENERGY-EFFICIENT DESIGN:
1. Model Selection: Logistic Regression was chosen over Deep Learning or Ensembles (like Random Forest/XGBoost). It has a closed-form approximation or simple iterative convergence (O(n*p)), requiring significantly fewer CPU cycles.
2. Solver Efficiency: The 'liblinear' solver is highly optimized for small to medium binary classification tasks, ensuring fast convergence and minimal memory footprint.
3. Preprocessing: Standard scaling is a linear operation (O(n)) that prepares the data for the solver without high-dimensional mapping or memory-intensive transformations.
4. Minimal Overhead: The pipeline avoids iterative hyperparameter optimization (like GridSearch) and complex cross-validation, which drastically reduces the total energy consumed during the training phase.
5. Resource Usage: The implementation is designed for standard CPU execution, eliminating the high idle-power costs and specialized hardware requirements of GPUs.
"""