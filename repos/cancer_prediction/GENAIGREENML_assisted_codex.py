# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline

DATASET_HEADERS = '"id","diagnosis","radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean","compactness_mean","concavity_mean","concave points_mean","symmetry_mean","fractal_dimension_mean","radius_se","texture_se","perimeter_se","area_se","smoothness_se","compactness_se","concavity_se","concave points_se","symmetry_se","fractal_dimension_se","radius_worst","texture_worst","perimeter_worst","area_worst","smoothness_worst","compactness_worst","concavity_worst","concave points_worst","symmetry_worst","fractal_dimension_worst",'

def parse_headers(header_str):
    parts = header_str.replace("\n", "").split(",")
    return [p.strip().strip('"') for p in parts if p.strip().strip('"')]

EXPECTED_HEADERS = parse_headers(DATASET_HEADERS)

def parsing_suspect(df, expected_headers):
    if df.shape[1] <= 1:
        return True
    if any(";" in str(c) for c in df.columns):
        return True
    if expected_headers:
        matches = sum(1 for h in expected_headers if h in df.columns)
        if matches < max(1, len(expected_headers) // 2):
            return True
        if "diagnosis" in expected_headers and "diagnosis" not in df.columns:
            return True
    return False

def read_csv_robust(path, expected_headers):
    df = pd.read_csv(path)
    if parsing_suspect(df, expected_headers):
        df = pd.read_csv(path, sep=";", decimal=",")
    df = df.dropna(axis=1, how="all")
    unnamed_cols = [c for c in df.columns if str(c).startswith("Unnamed")]
    if unnamed_cols:
        df = df.drop(columns=unnamed_cols)
    df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]
    return df

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

df = read_csv_robust("Cancer_Data.csv", EXPECTED_HEADERS)

if "diagnosis" not in df.columns:
    raise ValueError("diagnosis column not found")

diagnosis_series = df["diagnosis"]
if diagnosis_series.dtype.kind in ("O", "U", "S"):
    y_series = diagnosis_series.astype(str).str.strip().map({"M": 1, "B": 0})
else:
    y_series = diagnosis_series
y = y_series.astype("int8", copy=False).to_numpy(copy=False)

feature_names = [
    "texture_worst",
    "radius_se",
    "symmetry_worst",
    "concave points_mean",
    "area_se",
    "area_worst",
    "radius_worst",
    "concave points_worst",
    "concavity_mean",
    "fractal_dimension_se",
]

available_features = [f for f in feature_names if f in df.columns]
if not available_features:
    raise ValueError("Required feature columns not found")

X = df[available_features].to_numpy(dtype=float, copy=False)
del df

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=RANDOM_STATE
)

model = make_pipeline(
    StandardScaler(),
    LogisticRegression(solver="liblinear", random_state=RANDOM_STATE)
)
model.fit(X_train, y_train)
accuracy = model.score(X_test, y_test)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Added robust CSV parsing with delimiter/decimal fallback and column cleanup to prevent incorrect reads.
# - Converted selected columns directly to NumPy arrays with minimal copying and removed unused columns to reduce memory.
# - Used a pipeline to combine scaling and modeling without retaining extra intermediate datasets.
# - Applied fixed random seeds and explicit random_state settings for reproducible results.