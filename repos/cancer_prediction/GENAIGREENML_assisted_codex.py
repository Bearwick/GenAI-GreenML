# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline

SEED = 42
np.random.seed(SEED)

DATASET_PATH = "Cancer_Data.csv"
DATASET_HEADERS_RAW = (
    '"id","diagnosis","radius_mean","texture_mean","perimeter_mean","area_mean",'
    '"smoothness_mean","compactness_mean","concavity_mean","concave points_mean",'
    '"symmetry_mean","fractal_dimension_mean","radius_se","texture_se","perimeter_se",'
    '"area_se","smoothness_se","compactness_se","concavity_se","concave points_se",'
    '"symmetry_se","fractal_dimension_se","radius_worst","texture_worst","perimeter_worst",'
    '"area_worst","smoothness_worst","compactness_worst","concavity_worst",'
    '"concave points_worst","symmetry_worst","fractal_dimension_worst",'
)

def parse_headers(raw):
    headers = []
    for part in raw.split(','):
        part = part.strip()
        if part:
            part = part.strip('"').strip("'").strip()
            if part:
                headers.append(part)
    return headers

def normalize(name):
    return ''.join(ch for ch in str(name).lower() if ch.isalnum())

EXPECTED_HEADERS = parse_headers(DATASET_HEADERS_RAW)
EXPECTED_MAP = {}
for h in EXPECTED_HEADERS:
    key = normalize(h)
    if key not in EXPECTED_MAP:
        EXPECTED_MAP[key] = h
EXPECTED_NORMS = set(EXPECTED_MAP.keys())

TARGET_FEATURES_RAW = [
    "texture_worst",
    "radius_se",
    "symmetry_worst",
    "concave points_mean",
    "area_se",
    "area_worst",
    "radius_worst",
    "concave points_worst",
    "concavity_mean",
    "fractal_dimension_se",
]

TARGET_NORMS = [normalize(n) for n in TARGET_FEATURES_RAW]
if any(n not in EXPECTED_MAP for n in TARGET_NORMS):
    raise KeyError

def read_csv_robust(path, expected_norms):
    try:
        df = pd.read_csv(path)
    except FileNotFoundError:
        raise SystemExit(1)
    def valid(d):
        if d.shape[1] <= 1:
            return False
        if expected_norms:
            norm_cols = {normalize(c) for c in d.columns}
            if len(norm_cols & expected_norms) < max(1, len(expected_norms) // 2):
                return False
        obj_cols = d.select_dtypes(include=["object"]).columns
        if len(obj_cols) > max(2, d.shape[1] // 3):
            return False
        return True
    if not valid(df):
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if valid(df_alt):
                df = df_alt
        except Exception:
            pass
    return df

df = read_csv_robust(DATASET_PATH, EXPECTED_NORMS)
df.columns = [str(c).strip() for c in df.columns]
actual_map = {}
for c in df.columns:
    key = normalize(c)
    if key not in actual_map:
        actual_map[key] = c

diag_key = normalize("diagnosis")
if diag_key not in EXPECTED_MAP or diag_key not in actual_map:
    raise KeyError
diag_col = actual_map[diag_key]

feature_cols = []
for n in TARGET_NORMS:
    if n not in actual_map:
        raise KeyError
    feature_cols.append(actual_map[n])

df = df[[diag_col] + feature_cols]

y_series = df[diag_col]
if y_series.dtype == object or str(y_series.dtype).startswith("category"):
    y_clean = y_series.astype(str).str.strip()
    y_mapped = y_clean.map({'M': 1, 'B': 0, 'm': 1, 'b': 0, 'malignant': 1, 'benign': 0})
    if y_mapped.isnull().any():
        y = pd.to_numeric(y_series, errors='coerce')
    else:
        y = y_mapped
else:
    y = y_series

y = y.astype(int).to_numpy()
X = df[feature_cols].to_numpy(dtype=float, copy=False)

del df, y_series

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=SEED
)

model = make_pipeline(
    StandardScaler(),
    LogisticRegression(solver="liblinear", random_state=SEED)
)
model.fit(X_train, y_train)
accuracy = model.score(X_test, y_test)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Limited dependencies and removed visualization and artifact saving to cut unnecessary overhead.
# - Added conditional CSV parsing fallback and normalized column mapping to avoid redundant data reads and mismatches.
# - Selected only required columns and converted directly to NumPy arrays with copy=False to reduce memory use and data movement.
# - Used a pipeline to combine scaling and modeling without storing extra scaled datasets.
# - Cleared intermediate objects early to free memory during training.