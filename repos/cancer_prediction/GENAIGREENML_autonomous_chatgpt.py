# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _find_dataset_path() -> str:
    candidates = [
        "data.csv",
        "dataset.csv",
        "train.csv",
        "breast_cancer.csv",
        "wdbc.csv",
        "input.csv",
    ]
    for c in candidates:
        if os.path.exists(c) and os.path.isfile(c):
            return c
    csv_files = [f for f in os.listdir(".") if f.lower().endswith(".csv") and os.path.isfile(f)]
    if len(csv_files) == 1:
        return csv_files[0]
    if len(csv_files) > 1:
        header_target = {
            "id",
            "diagnosis",
            "radius_mean",
            "texture_mean",
            "perimeter_mean",
            "area_mean",
        }
        best = None
        best_score = -1
        for f in csv_files:
            try:
                tmp = pd.read_csv(f, nrows=0)
                cols = set(map(str, tmp.columns))
                score = len(header_target.intersection(cols))
                if score > best_score:
                    best_score = score
                    best = f
            except Exception:
                continue
        if best is not None and best_score >= 3:
            return best
    raise FileNotFoundError("No suitable CSV dataset found in the current directory.")


def _load_data(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    df.columns = [str(c).strip().strip('"').strip("'") for c in df.columns]
    return df


def _prepare_xy(df: pd.DataFrame):
    if "diagnosis" not in df.columns:
        raise ValueError("Missing target column 'diagnosis'.")
    y_raw = df["diagnosis"].astype(str).str.strip()
    y = y_raw.map({"M": 1, "B": 0})
    if y.isna().any():
        unique_vals = sorted(y_raw.unique().tolist())
        raise ValueError(f"Unexpected values in diagnosis column: {unique_vals}")
    X = df.drop(columns=["diagnosis"], errors="ignore")

    drop_cols = [c for c in ["id"] if c in X.columns]
    if drop_cols:
        X = X.drop(columns=drop_cols)

    numeric_cols = X.columns.tolist()
    return X, y.astype(int), numeric_cols


def main():
    path = _find_dataset_path()
    df = _load_data(path)
    X, y, numeric_cols = _prepare_xy(df)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
        ],
        remainder="drop",
        sparse_threshold=0.0,
    )

    clf = LogisticRegression(
        solver="lbfgs",
        max_iter=200,
        n_jobs=None,
    )

    model = Pipeline(
        steps=[
            ("preprocess", preprocessor),
            ("clf", clf),
        ]
    )

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Chose LogisticRegression (linear model) for strong baseline accuracy with minimal compute and memory on CPU.
# - Used a single StandardScaler + median imputation pipeline: cheap preprocessing, stable optimization, reproducible.
# - Dropped high-cardinality/non-predictive identifier ('id') to avoid wasted computation and potential leakage.
# - Avoided ensembles/deep learning to reduce training energy; kept max_iter modest for fast convergence.
# - Used ColumnTransformer/Pipeline for deterministic, one-pass preprocessing without manual feature handling.