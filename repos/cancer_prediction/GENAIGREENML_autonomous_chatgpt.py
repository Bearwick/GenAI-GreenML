# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "Cancer_Data.csv"


def _normalize_columns(cols):
    out = []
    for c in cols:
        c = "" if c is None else str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        out.append(c)
    return out


def _drop_unnamed_columns(df):
    drop_cols = []
    for c in df.columns:
        if c is None:
            continue
        cs = str(c).strip()
        if cs.lower().startswith("unnamed:") or cs == "":
            drop_cols.append(c)
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _robust_read_csv(path):
    # Try default parse
    try:
        df0 = pd.read_csv(path)
    except Exception:
        df0 = None

    # Retry with common European format
    try:
        df1 = pd.read_csv(path, sep=";", decimal=",")
    except Exception:
        df1 = None

    # Heuristic: choose the parse that yields more columns and non-empty data
    candidates = []
    for df in (df0, df1):
        if isinstance(df, pd.DataFrame) and df.shape[0] > 0 and df.shape[1] > 0:
            candidates.append(df)

    if not candidates:
        # Last resort: read as raw with python engine
        df2 = pd.read_csv(path, engine="python")
        return df2

    def score(df):
        # Prefer more columns and fewer all-null columns
        null_cols = int(df.isna().all(axis=0).sum())
        return (df.shape[1], -null_cols, df.shape[0])

    best = sorted(candidates, key=score, reverse=True)[0]
    return best


def _is_probably_id(colname):
    s = str(colname).strip().lower()
    return s in {"id", "index"} or s.endswith("_id") or s.endswith("id")


def _choose_target(df):
    cols_lower = {str(c).strip().lower(): c for c in df.columns}
    for name in ["diagnosis", "target", "label", "class", "y"]:
        if name in cols_lower:
            return cols_lower[name]

    # Prefer low-cardinality non-numeric columns (classification-like)
    obj_cols = [c for c in df.columns if df[c].dtype == "object" or pd.api.types.is_string_dtype(df[c])]
    best_obj = None
    best_card = None
    for c in obj_cols:
        if _is_probably_id(c):
            continue
        nun = df[c].nunique(dropna=True)
        if 2 <= nun <= 20:
            if best_card is None or nun < best_card:
                best_obj, best_card = c, nun
    if best_obj is not None:
        return best_obj

    # Otherwise pick a numeric, non-constant column (regression fallback)
    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    for c in num_cols:
        if _is_probably_id(c):
            continue
        if df[c].nunique(dropna=True) > 1:
            return c

    # Last resort: first column
    return df.columns[0]


def _safe_accuracy_from_r2(r2):
    # Map potentially negative R2 into [0,1] with clipping; stable proxy
    if not np.isfinite(r2):
        return 0.0
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    if not os.path.exists(DATASET_PATH):
        # Minimal stdout requirement: do not print; raise to surface file issue
        raise FileNotFoundError(f"Dataset not found at path: {DATASET_PATH}")

    df = _robust_read_csv(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed_columns(df)

    # Drop fully empty columns and rows
    df = df.dropna(axis=1, how="all")
    df = df.dropna(axis=0, how="all")

    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _choose_target(df)

    # Separate X/y
    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # Drop likely ID-like columns to reduce noise
    id_like = [c for c in X.columns if _is_probably_id(c)]
    if id_like:
        X = X.drop(columns=id_like, errors="ignore")

    # Coerce numerics defensively (only for columns already numeric-ish)
    # If a column is object but looks numeric, convert; otherwise keep as categorical.
    X2 = X.copy()
    for c in X2.columns:
        if pd.api.types.is_numeric_dtype(X2[c]):
            X2[c] = pd.to_numeric(X2[c], errors="coerce")
        else:
            # attempt numeric conversion if it succeeds for many entries
            conv = pd.to_numeric(X2[c], errors="coerce")
            non_na_conv = conv.notna().mean() if len(conv) else 0.0
            if non_na_conv >= 0.8:
                X2[c] = conv

    # Decide task type
    y = y_raw.copy()
    is_classification = False

    # If y is object or low-cardinality numeric -> classification
    if (y.dtype == "object") or pd.api.types.is_string_dtype(y):
        is_classification = True
    else:
        y_num = pd.to_numeric(y, errors="coerce")
        # If few unique values, treat as classification
        nun = y_num.nunique(dropna=True)
        if nun >= 2 and nun <= 20:
            is_classification = True
        y = y_num

    # Clean y for classification/regression
    if is_classification:
        y = y.astype("object")
        # Drop rows with missing y
        mask = y.notna()
        X2 = X2.loc[mask].reset_index(drop=True)
        y = y.loc[mask].reset_index(drop=True)

        # Ensure at least two classes
        classes = pd.Series(y).astype(str).unique()
        if len(classes) < 2:
            is_classification = False
            y = pd.to_numeric(y_raw, errors="coerce")
            mask = y.notna()
            X2 = X2.loc[mask].reset_index(drop=True)
            y = y.loc[mask].reset_index(drop=True)
    else:
        y = pd.to_numeric(y, errors="coerce")
        mask = y.notna()
        X2 = X2.loc[mask].reset_index(drop=True)
        y = y.loc[mask].reset_index(drop=True)

    assert len(y) > 2 and X2.shape[0] == len(y)

    # Build preprocessing
    numeric_features = [c for c in X2.columns if pd.api.types.is_numeric_dtype(X2[c])]
    categorical_features = [c for c in X2.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Train/test split
    if is_classification:
        # Use stratify when feasible (>=2 samples per class)
        y_str = y.astype(str)
        vc = y_str.value_counts()
        stratify = y_str if (vc.min() >= 2 and len(vc) >= 2) else None
        X_train, X_test, y_train, y_test = train_test_split(
            X2, y_str, test_size=0.2, random_state=42, stratify=stratify
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X2, y, test_size=0.2, random_state=42
        )

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    if is_classification:
        model = LogisticRegression(
            solver="liblinear",  # efficient on small/medium datasets
            max_iter=200,
            random_state=42,
        )
        clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Lightweight regression fallback
        model = Ridge(alpha=1.0, random_state=42)
        reg = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        reg.fit(X_train, y_train)
        r2 = float(reg.score(X_test, y_test))
        accuracy = _safe_accuracy_from_r2(r2)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses pandas+sklearn CPU-friendly pipeline with simple models (LogisticRegression liblinear / Ridge) to minimize compute and energy.
# - Robust CSV parsing (default then ;/, fallback), column normalization, and dropping Unnamed columns reduce wasted processing and failures.
# - ColumnTransformer with SimpleImputer + StandardScaler + OneHotEncoder avoids manual feature loops and is reproducible/end-to-end.
# - Drops likely ID columns to reduce noise and dimensionality; keeps feature engineering minimal to limit CPU cost.
# - Defensive target selection: prefers known label names, otherwise low-cardinality categorical, else numeric non-constant for regression fallback.
# - If regression fallback is used, prints ACCURACY as a bounded proxy: clip((R2+1)/2) into [0,1] for stability and comparability.