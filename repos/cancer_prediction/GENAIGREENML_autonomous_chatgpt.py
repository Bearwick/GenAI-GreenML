# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        normed.append(c2)
    return normed


def _read_csv_robust(path):
    # Attempt 1: default CSV parsing
    try:
        df1 = pd.read_csv(path)
    except Exception:
        df1 = None

    # Attempt 2: common European format fallback
    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
    except Exception:
        df2 = None

    if df1 is None and df2 is None:
        raise FileNotFoundError(f"Could not read CSV file: {path}")

    def _score_df(df):
        if df is None or df.shape[0] == 0:
            return -1
        score = 0
        score += int(df.shape[1] >= 2) * 2
        # Prefer fewer "Unnamed" columns
        unnamed = sum(str(c).startswith("Unnamed") for c in df.columns)
        score -= unnamed
        # Prefer more non-null values overall
        score += int(df.notna().mean().mean() * 10)
        return score

    return df1 if _score_df(df1) >= _score_df(df2) else df2


def _drop_unnamed(df):
    cols = [c for c in df.columns if not str(c).startswith("Unnamed")]
    return df[cols]


def _coerce_numeric_inplace(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")


def _pick_target(df):
    # Prefer known header if present
    if "diagnosis" in df.columns:
        return "diagnosis"

    # Otherwise choose a non-constant numeric column with highest cardinality
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    best = None
    best_score = -1
    for c in numeric_cols:
        s = df[c]
        nunq = int(s.nunique(dropna=True))
        if nunq >= 2:
            score = nunq
            if score > best_score:
                best = c
                best_score = score
    if best is not None:
        return best

    # Fallback: any column with >=2 unique values
    for c in df.columns:
        if df[c].nunique(dropna=True) >= 2:
            return c

    return None


def _is_classification_target(y):
    if y is None or len(y) == 0:
        return False
    if y.dtype == "O" or pd.api.types.is_string_dtype(y) or pd.api.types.is_bool_dtype(y) or pd.api.types.is_categorical_dtype(y):
        return True
    # numeric: treat as classification if small number of unique values
    nunq = int(pd.Series(y).nunique(dropna=True))
    return 2 <= nunq <= 20


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    var = np.var(y_true)
    mse = np.mean((y_true - y_pred) ** 2)
    if not np.isfinite(var) or var <= 0:
        return 0.0
    r2 = 1.0 - (mse / (var + 1e-12))
    return float(np.clip(r2, 0.0, 1.0))


# ---- Main script ----
# Resolve dataset path without assuming exact filename
candidate_paths = [
    "Cancer_Data.csv",
    "cancer_data.csv",
    "data.csv",
    "dataset.csv",
    "train.csv",
]
csv_path = None
for p in candidate_paths:
    if os.path.exists(p):
        csv_path = p
        break
if csv_path is None:
    # If only one CSV exists in cwd, use it
    csvs = [f for f in os.listdir(".") if f.lower().endswith(".csv")]
    if len(csvs) == 1:
        csv_path = csvs[0]
    elif len(csvs) > 1:
        # Choose the largest CSV (most likely the dataset)
        csv_path = max(csvs, key=lambda f: os.path.getsize(f))
    else:
        raise FileNotFoundError("No CSV file found in current directory.")

df = _read_csv_robust(csv_path)
df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)

assert df.shape[0] > 0 and df.shape[1] > 0

target_col = _pick_target(df)
assert target_col is not None

y_raw = df[target_col].copy()
X = df.drop(columns=[target_col])

# Remove ID-like columns if clearly identifier-ish to reduce noise/compute
id_like = []
for c in X.columns:
    cl = str(c).strip().lower()
    if cl in ("id", "identifier") or cl.endswith("_id") or cl == "uid":
        id_like.append(c)
if id_like:
    X = X.drop(columns=id_like)

# Determine column types
numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = [c for c in X.columns if c not in numeric_features]

# Coerce numerics safely (handles numeric strings that got read as object)
_all_numeric_candidates = list(set(numeric_features + [c for c in X.columns if c not in categorical_features]))
_coerce_numeric_inplace(X, _all_numeric_candidates)
numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = [c for c in X.columns if c not in numeric_features]

# Prepare target
classification = _is_classification_target(y_raw)

if classification:
    # Map common diagnosis labels if present; otherwise factorize
    y = y_raw.copy()
    if y.dtype == "O" or pd.api.types.is_string_dtype(y):
        y_str = y.astype(str).str.strip()
        # Known mapping for breast cancer diagnosis: M/B
        if set(y_str.dropna().unique()).issubset({"M", "B"}):
            y = y_str.map({"M": 1, "B": 0})
        else:
            y, _ = pd.factorize(y_str, sort=True)
    else:
        # numeric classification target
        y = pd.to_numeric(y, errors="coerce")
    # Drop rows with missing target
    valid = pd.Series(y).notna().to_numpy()
    X = X.loc[valid].reset_index(drop=True)
    y = pd.Series(y).loc[valid].reset_index(drop=True)

    # If still not enough classes, fallback to regression path
    if int(pd.Series(y).nunique(dropna=True)) < 2:
        classification = False
else:
    y = pd.to_numeric(y_raw, errors="coerce")
    valid = pd.Series(y).notna().to_numpy()
    X = X.loc[valid].reset_index(drop=True)
    y = pd.Series(y).loc[valid].reset_index(drop=True)

assert X.shape[0] > 1

# Build preprocessing
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=True, with_std=True)),
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Train/test split
if classification:
    # stratify only if at least 2 samples per class
    y_counts = pd.Series(y).value_counts(dropna=True)
    stratify = y if (y_counts.min() >= 2 and y_counts.shape[0] >= 2) else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=stratify
    )
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

assert X_train.shape[0] > 0 and X_test.shape[0] > 0

# Lightweight models
if classification:
    # Use liblinear: efficient on small/medium data, deterministic with random_state for this solver
    clf = LogisticRegression(
        solver="liblinear",
        max_iter=300,
        random_state=42,
    )
    model = Pipeline(steps=[
        ("preprocess", preprocess),
        ("model", clf),
    ])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
else:
    reg = Ridge(alpha=1.0, random_state=42)
    model = Pipeline(steps=[
        ("preprocess", preprocess),
        ("model", reg),
    ])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = _bounded_regression_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Robust CSV read: tries default then (sep=';', decimal=',') and picks best-looking parse to avoid expensive manual debugging.
# - Column normalization + dropping 'Unnamed:*' avoids wasted preprocessing on index artifacts.
# - Simple, CPU-friendly models: LogisticRegression(liblinear) for classification; Ridge for regression fallback.
# - Minimal preprocessing via ColumnTransformer: median imputation + StandardScaler for numerics; most_frequent + OneHot for categoricals.
# - Sparse one-hot output reduces memory/compute when categoricals exist.
# - Defensive target selection and fallbacks ensure end-to-end run even with schema issues or degenerate targets.
# - Regression "accuracy" uses clipped R2 in [0,1] for stability and consistent reporting format.