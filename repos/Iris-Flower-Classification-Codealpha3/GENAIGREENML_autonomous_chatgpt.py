# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler


warnings.filterwarnings("ignore")


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = "" if c is None else str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        normed.append(c2)
    return normed


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if isinstance(c, str) and c.strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _try_read_csv(path):
    # Try default parsing first; if it yields a degenerate single-column frame, retry with ; and decimal=,
    df1 = pd.read_csv(path)
    if df1.shape[1] <= 1:
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df2.shape[1] > df1.shape[1]:
                return df2
        except Exception:
            return df1
    return df1


def _choose_target(df):
    cols = list(df.columns)

    # Prefer a "Species"-like column if present (typical for iris); otherwise pick best candidate.
    preferred = None
    for c in cols:
        if isinstance(c, str) and c.strip().lower() in ("species", "target", "label", "class", "y"):
            preferred = c
            break
    if preferred is not None:
        return preferred

    # Heuristic: choose non-constant column with lowest unique count among object/categorical;
    # fallback to a numeric column with reasonable variance.
    nunique = df.nunique(dropna=True)

    cat_candidates = []
    num_candidates = []

    for c in cols:
        if nunique.get(c, 0) <= 1:
            continue
        if pd.api.types.is_numeric_dtype(df[c]):
            num_candidates.append(c)
        else:
            cat_candidates.append(c)

    if cat_candidates:
        # pick the one with smallest unique count but >=2 (likely label)
        cat_candidates.sort(key=lambda x: (nunique.get(x, 10**9), x))
        return cat_candidates[0]

    if num_candidates:
        # pick numeric column with moderate unique count (avoid IDs) by minimizing unique ratio
        n = len(df)
        num_candidates.sort(key=lambda x: (nunique.get(x, 10**9) / max(n, 1), x))
        return num_candidates[0]

    # Last resort: first column
    return cols[0] if cols else None


def _safe_accuracy_from_regression(y_true, y_pred):
    # Stable bounded proxy in [0,1]: 1 / (1 + normalized MAE), using robust scale
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = np.nanmean(np.abs(y_true - y_pred))
    scale = np.nanstd(y_true)
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = np.nanmean(np.abs(y_true)) + 1e-12
    norm_mae = mae / (scale + 1e-12)
    score = 1.0 / (1.0 + max(0.0, float(norm_mae)))
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    # Locate dataset file robustly
    candidate_paths = [
        "Iris.csv",
        "iris.csv",
        "data.csv",
        "dataset.csv",
        os.path.join("data", "Iris.csv"),
        os.path.join("data", "iris.csv"),
        os.path.join("input", "Iris.csv"),
        os.path.join("input", "iris.csv"),
    ]
    csv_path = None
    for p in candidate_paths:
        if os.path.exists(p) and os.path.isfile(p):
            csv_path = p
            break
    if csv_path is None:
        # As a last resort, pick the first csv in current directory
        for fn in os.listdir("."):
            if fn.lower().endswith(".csv") and os.path.isfile(fn):
                csv_path = fn
                break

    if csv_path is None:
        # No dataset found; run a trivial constant baseline to satisfy end-to-end requirement.
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _try_read_csv(csv_path)

    # Normalize column names and drop unnamed
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Basic row cleanup
    df = df.copy()
    df = df.replace([np.inf, -np.inf], np.nan)

    # If empty after load, exit gracefully
    if df.shape[0] == 0 or df.shape[1] == 0:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Drop an "Id"-like column if present (common non-predictive identifier)
    for c in list(df.columns):
        if isinstance(c, str) and c.strip().lower() in ("id", "index"):
            df = df.drop(columns=[c])
            break

    # Choose target robustly
    target_col = _choose_target(df)
    if target_col is None or target_col not in df.columns:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    y_raw = df[target_col]
    X = df.drop(columns=[target_col])

    # If no features remain, use trivial baseline
    if X.shape[1] == 0 or X.shape[0] == 0:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Coerce numeric-looking columns safely without touching obvious categoricals
    for c in X.columns:
        if pd.api.types.is_numeric_dtype(X[c]):
            X[c] = pd.to_numeric(X[c], errors="coerce")

    # Decide classification vs regression
    # Classification if y has few unique values or is non-numeric; regression otherwise.
    y_nunique = y_raw.nunique(dropna=True)
    y_is_numeric = pd.api.types.is_numeric_dtype(y_raw)

    is_classification = (not y_is_numeric) or (y_nunique <= 50 and y_nunique >= 2)

    # Prepare y
    if is_classification:
        y = y_raw.astype("object").where(~y_raw.isna(), other="__MISSING__")
        # If only one class remains, fallback to regression proxy
        if y.nunique(dropna=True) < 2:
            is_classification = False
    if not is_classification:
        y = pd.to_numeric(y_raw, errors="coerce")

    # Drop rows with missing target for modeling (lightweight & safe)
    mask = ~pd.isna(y)
    X = X.loc[mask].reset_index(drop=True)
    y = y.loc[mask].reset_index(drop=True)

    # Assert dataset not empty after preprocessing
    assert X.shape[0] > 0 and X.shape[1] > 0

    # Infer feature types
    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if is_classification:
        # Lightweight linear model; good baseline on CPU
        model = LogisticRegression(
            max_iter=200,
            solver="lbfgs",
            n_jobs=1,
        )

        # Stratify only if enough samples per class
        stratify = None
        try:
            vc = y.value_counts()
            if vc.min() >= 2 and y.nunique() >= 2:
                stratify = y
        except Exception:
            stratify = None

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify
        )
        assert X_train.shape[0] > 0 and X_test.shape[0] > 0

        clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # CPU-friendly linear regressor
        model = Ridge(alpha=1.0, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        assert X_train.shape[0] > 0 and X_test.shape[0] > 0

        reg = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)

        # Convert regression error into bounded "accuracy" proxy in [0,1]
        accuracy = _safe_accuracy_from_regression(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Chose lightweight linear models (LogisticRegression/Ridge) for strong CPU baselines with low energy use.
# - Used a single sklearn Pipeline + ColumnTransformer to avoid redundant preprocessing and ensure reproducibility.
# - Applied median/most_frequent imputation and StandardScaler only for numeric features; OHE for categoricals with sparse output to reduce memory/compute.
# - Implemented robust CSV parsing fallback (default, then ; with decimal=,) and defensive schema handling (normalize headers, drop Unnamed, auto-select target).
# - Avoided cross-validation/plotting and minimized stdout to reduce runtime and overhead.
# - Regression fallback outputs a bounded accuracy proxy: 1/(1+normalized MAE) in [0,1] for stable reporting as ACCURACY=.