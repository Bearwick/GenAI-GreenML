# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler


warnings.filterwarnings("ignore")


DATASET_PATH = "Iris.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    # Try default parsing first; if it looks wrong, retry with alternative European CSV conventions.
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _safe_choose_target(df):
    # Prefer a non-constant object/categorical column with low-to-moderate unique count (classification).
    n = len(df)
    if n == 0:
        return None, "none"
    candidates = []
    for c in df.columns:
        s = df[c]
        nun = s.nunique(dropna=True)
        if nun <= 1:
            continue
        # Ignore likely ID-like columns
        if re.search(r"\bid\b", str(c).lower()) and pd.api.types.is_numeric_dtype(s):
            continue
        candidates.append((c, nun, s.dtype))

    # First: categorical-ish (object/category/bool) with reasonable unique count
    for c, nun, dt in candidates:
        if pd.api.types.is_object_dtype(dt) or pd.api.types.is_categorical_dtype(dt) or pd.api.types.is_bool_dtype(dt):
            # Avoid extremely high cardinality text columns
            if 2 <= nun <= max(20, int(np.sqrt(n)) + 2):
                return c, "classification"

    # Second: numeric with few unique values (encoded classes)
    for c, nun, dt in candidates:
        if pd.api.types.is_numeric_dtype(dt) and 2 <= nun <= max(20, int(np.sqrt(n)) + 2):
            return c, "classification"

    # Fallback: choose a non-constant numeric column for regression
    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    for c in numeric_cols:
        if df[c].nunique(dropna=True) > 1:
            return c, "regression"

    # Final fallback: any column with variability; will be treated as classification if possible
    if candidates:
        return candidates[0][0], "classification"
    return None, "none"


def _coerce_numeric_where_possible(df, exclude_cols):
    # Coerce object columns that look numeric to numeric to avoid losing signal.
    for c in df.columns:
        if c in exclude_cols:
            continue
        if pd.api.types.is_object_dtype(df[c]):
            coerced = pd.to_numeric(df[c], errors="coerce")
            # Convert if it yields some valid numbers and doesn't destroy almost everything
            non_nan = coerced.notna().mean() if len(coerced) else 0.0
            if non_nan >= 0.7:
                df[c] = coerced
    return df


def _bounded_regression_score(y_true, y_pred):
    # Stable proxy in [0,1]: 1 / (1 + normalized MAE); robust to scale via IQR.
    y_true = np.asarray(y_true).ravel()
    y_pred = np.asarray(y_pred).ravel()
    mae = np.mean(np.abs(y_true - y_pred)) if y_true.size else 0.0
    q75, q25 = np.percentile(y_true, [75, 25]) if y_true.size else (0.0, 0.0)
    iqr = float(q75 - q25)
    scale = iqr if iqr > 1e-12 else (float(np.std(y_true)) if float(np.std(y_true)) > 1e-12 else 1.0)
    score = 1.0 / (1.0 + (mae / scale))
    return float(np.clip(score, 0.0, 1.0))


def main():
    if not os.path.exists(DATASET_PATH):
        # Minimal, non-crashing fallback if file missing.
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Ensure we have data
    df = df.dropna(how="all")
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col, task = _safe_choose_target(df)
    if target_col is None:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Coerce potential numeric objects except the target
    df = _coerce_numeric_where_possible(df, exclude_cols={target_col})

    # Build X/y with defensive cleaning
    y = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # Drop constant columns in X (saves compute and avoids singularities)
    non_constant = []
    for c in X.columns:
        if X[c].nunique(dropna=True) > 1:
            non_constant.append(c)
    X = X[non_constant] if non_constant else X

    # If X ends up empty, create a trivial feature to keep pipeline valid
    if X.shape[1] == 0:
        X = pd.DataFrame({"__bias__": np.ones(len(df), dtype=np.float32)})

    # Split features by dtype
    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Decide final task after seeing y distribution
    y_is_numeric = pd.api.types.is_numeric_dtype(y)
    if task == "classification":
        # If y is object, keep as-is; if numeric, still can be class labels.
        y_clean = y.astype(str) if not y_is_numeric else y
        # Handle rare case: too few classes
        if pd.Series(y_clean).nunique(dropna=True) < 2:
            task = "regression"
        else:
            # Stratify only if each class has >=2 samples
            vc = pd.Series(y_clean).value_counts(dropna=True)
            stratify = y_clean if (vc.min() >= 2 and len(vc) >= 2) else None

            X_train, X_test, y_train, y_test = train_test_split(
                X, y_clean, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
            )
            assert len(X_train) > 0 and len(X_test) > 0

            # Light, CPU-friendly baseline for tabular multi-class: multinomial logistic regression.
            clf = LogisticRegression(
                solver="lbfgs",
                max_iter=200,
                n_jobs=1,
                multi_class="auto",
            )

            model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)

            accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

    # Regression fallback path
    y_num = pd.to_numeric(y, errors="coerce") if not y_is_numeric else y.astype(float)
    # If y cannot be coerced, map categories to codes
    if pd.isna(y_num).all():
        y_num = pd.Series(pd.factorize(y.astype(str))[0], index=y.index).astype(float)

    # Drop rows where y is NaN (can't learn)
    mask = ~pd.isna(y_num)
    Xr = X.loc[mask].copy()
    yr = y_num.loc[mask].copy()
    assert len(Xr) > 1

    X_train, X_test, y_train, y_test = train_test_split(
        Xr, yr, test_size=0.2, random_state=RANDOM_STATE
    )
    assert len(X_train) > 0 and len(X_test) > 0

    reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
    model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = _bounded_regression_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Chose lightweight, CPU-friendly models: LogisticRegression for classification; Ridge for regression fallback.
# - Used a single sklearn Pipeline + ColumnTransformer to avoid redundant preprocessing work and ensure reproducibility.
# - Robust CSV reading: default read_csv first, then fallback to sep=';' and decimal=',' if parsing yields 1 column.
# - Defensive schema handling: normalize column names, drop 'Unnamed' columns, auto-select target if expected missing.
# - Minimal feature engineering: median imputation + standard scaling for numeric; most_frequent + one-hot for categoricals.
# - Dropped constant feature columns to reduce compute and avoid numerical issues.
# - Regression fallback uses a bounded [0,1] "accuracy" proxy: 1/(1+normalized MAE) with IQR-based scaling for stability.