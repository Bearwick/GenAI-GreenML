# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

df = pd.read_csv("Iris.csv", usecols=["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"])

df["Species"], _ = pd.factorize(df["Species"])

X = df.iloc[:, :-1].values.astype("float32")
y = df.iloc[:, -1].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

model = LogisticRegression(solver='liblinear', multi_class='auto')
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Used 'usecols' in pd.read_csv to prevent loading the 'Id' column into memory, reducing initial data overhead.
# 2. Replaced the heavy RandomForestClassifier (100 trees) with LogisticRegression, which is significantly more energy-efficient and faster for small, low-dimensional datasets like Iris.
# 3. Substituted LabelEncoder with pd.factorize for a more direct and efficient categorical-to-numeric conversion.
# 4. Cast feature data to 'float32' to reduce the memory footprint compared to the default 'float64'.
# 5. Removed redundant computational steps including cross-validation, complex classification reports, and heatmaps.
# 6. Optimized library imports to include only necessary modules, reducing the initialization time and memory usage of the Python interpreter.
# 7. Eliminated plotting and visualization libraries (Matplotlib/Seaborn) to reduce CPU cycles and energy consumption associated with rendering.