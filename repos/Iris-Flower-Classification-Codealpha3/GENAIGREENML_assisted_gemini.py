# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

def run_process():
    data_path = "Iris.csv"
    try:
        df = pd.read_csv(data_path)
        if df.shape[1] <= 1:
            raise ValueError
    except Exception:
        df = pd.read_csv(data_path, sep=';', decimal=',')

    if "Id" in df.columns:
        df.drop(columns=["Id"], inplace=True)

    target = "Species"
    x_data = df.drop(columns=[target]).values.astype(np.float32)
    y_data = LabelEncoder().fit_transform(df[target])

    x_train, x_test, y_train, y_test = train_test_split(
        x_data, y_data, test_size=0.2, random_state=42, stratify=y_data
    )

    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(x_train, y_train)

    accuracy = accuracy_score(y_test, clf.predict(x_test))
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_process()

# Optimization Summary
# 1. Reduced memory footprint by casting features to float32 instead of the default float64.
# 2. Removed heavy visualization dependencies (Matplotlib, Seaborn) to reduce environment overhead and runtime.
# 3. Eliminated redundant computations by removing cross-validation and duplicate metric calculations.
# 4. Streamlined data loading with a robust fallback mechanism to ensure single-pass parsing.
# 5. Removed all logging, descriptive prints, and plots to minimize CPU cycles and I/O operations.
# 6. Used .values to convert dataframes to NumPy arrays early, reducing overhead of pandas index alignment during training.
# 7. Maintained model consistency using fixed seeds to ensure reproducibility with minimal complexity.