# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

def load_data(file_path):
    try:
        df = pd.read_csv(file_path)
        if df.shape[1] <= 1:
            raise ValueError
    except (pd.errors.ParserError, ValueError):
        df = pd.read_csv(file_path, sep=';', decimal=',')
    return df

df = load_data("Iris.csv")

if 'Id' in df.columns:
    df.drop(columns=['Id'], inplace=True)

target_col = [col for col in df.columns if 'Species' in col][0]
df[target_col] = df[target_col].astype('category').cat.codes

X = df.drop(columns=[target_col]).astype('float32')
y = df[target_col]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Reduced memory footprint by casting feature data to float32.
# 2. Replaced LabelEncoder with pandas categorical codes for faster, more efficient target encoding.
# 3. Implemented robust CSV parsing with conditional fallback to handle different delimiters and decimal formats.
# 4. Eliminated redundant computations by removing unused cross-validation and detailed metrics (report/confusion matrix).
# 5. Removed visualization libraries (matplotlib/seaborn) to reduce package overhead and execution time.
# 6. Optimized data loading by identifying and dropping the 'Id' column immediately.
# 7. Set n_jobs=1 for RandomForest to avoid the overhead of multiprocessing on a small-scale dataset.
# 8. Minimized I/O and CPU cycles by removing all logging, plotting, and intermediate data structures.