# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

df = pd.read_csv('iris.csv')
df = df.drop(columns=['Id'])
df = df.dropna()

X = df.drop(columns=['Species'])
y = df['Species']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = LogisticRegression(
    max_iter=500,
    solver='lbfgs',
    multi_class='auto',
    n_jobs=1
)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Model Selection: Logistic Regression was chosen for its minimal computational complexity (O(features)) compared to ensembles or neural networks.
# 2. Convergence Optimization: StandardScaler was implemented to normalize the feature space, which allows the L-BFGS solver to converge in fewer iterations, saving CPU cycles.
# 3. Data Efficiency: The 'Id' column was removed to reduce memory consumption and prevent the model from learning noise.
# 4. Energy Efficiency: By using a linear model with a robust solver, we avoid the heavy energy overhead of GPU initialization and high-dimensional parameter optimization.
# 5. CPU Utilization: n_jobs=1 was specified because the overhead of spawning multiple processes for a dataset of 150 samples consumes more energy than it saves through parallelization.
# 6. Pipeline: The code utilizes a direct, linear pipeline to ensure minimal memory state transitions and efficient garbage collection.