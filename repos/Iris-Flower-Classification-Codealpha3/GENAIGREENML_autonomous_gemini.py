# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

# Robust CSV Loading
def load_data(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError
    except Exception:
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            # Create a dummy dataframe if all loads fail to prevent crash
            return pd.DataFrame()
    
    # Normalize column names
    df.columns = [" ".join(str(col).strip().split()) for col in df.columns]
    # Drop index-like columns (Id, Unnamed, etc.)
    cols_to_drop = [c for c in df.columns if 'id' in c.lower() or 'unnamed' in c.lower()]
    df = df.drop(columns=cols_to_drop)
    return df

# Main Execution
dataset_path = "Iris.csv"
df = load_data(dataset_path)

if df.empty:
    # Fallback accuracy if data cannot be loaded
    print(f"ACCURACY={0.000000:.6f}")
else:
    # Identify Target and Features
    # Priority: Column named 'Species' (case insensitive), otherwise last column
    potential_targets = [c for c in df.columns if 'species' in c.lower() or 'class' in c.lower() or 'target' in c.lower()]
    if potential_targets:
        target_col = potential_targets[0]
    else:
        target_col = df.columns[-1]

    X = df.drop(columns=[target_col])
    y = df[target_col]

    # Preprocessing numeric features
    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
    # Force numeric conversion for safety
    for col in numeric_features:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    
    # Drop rows where target is NaN
    valid_indices = y.dropna().index
    X = X.loc[valid_indices]
    y = y.loc[valid_indices]

    if len(X) > 0:
        # Label encode target if it's categorical
        if y.dtype == 'object' or len(np.unique(y)) < 10:
            le = LabelEncoder()
            y = le.fit_transform(y.astype(str))
            is_classification = True
        else:
            is_classification = False

        # Split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Pipeline: Impute -> Scale -> Model
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])

        preprocessor = ColumnTransformer(
            transformers=[('num', numeric_transformer, numeric_features)],
            remainder='drop'
        )

        if is_classification:
            # Logistic Regression is energy-efficient and sufficient for Iris
            model = Pipeline(steps=[
                ('preprocessor', preprocessor),
                ('classifier', LogisticRegression(max_iter=200, solver='liblinear'))
            ])
            model.fit(X_train, y_train)
            preds = model.predict(X_test)
            accuracy = accuracy_score(y_test, preds)
        else:
            # Fallback regression to accuracy proxy
            from sklearn.linear_model import Ridge
            model = Pipeline(steps=[
                ('preprocessor', preprocessor),
                ('regressor', Ridge())
            ])
            model.fit(X_train, y_train)
            score = model.score(X_test, y_test)
            accuracy = max(0, min(1, score)) # Bound R^2 as proxy

        print(f"ACCURACY={accuracy:.6f}")
    else:
        print(f"ACCURACY={0.000000:.6f}")

# Optimization Summary
# 1. Model Choice: Logistic Regression was selected over Random Forest or SVM. 
#    It uses significantly fewer CPU cycles for both training and inference on small, low-dimensional datasets like Iris.
# 2. Solver Selection: Used 'liblinear' which is computationally efficient for small datasets.
# 3. Data Loading: Robust parsing logic prevents execution failure, ensuring reliability without manual intervention.
# 4. Preprocessing: Scikit-learn Pipelines ensure that transformations (scaling/imputing) are only computed once and 
#    prevent data leakage without redundant code.
# 5. Resource Efficiency: Avoided heavy libraries (TensorFlow/PyTorch) and large ensembles (XGBoost) which require 
#    higher memory and power consumption for a task this size.
# 6. Feature Selection: Automatically drops 'Id' columns to reduce noise and dimensionality, saving processing time.
# 7. Numerical Safety: Forced numeric coercion and median imputation handle dirty data without crashing the pipeline.