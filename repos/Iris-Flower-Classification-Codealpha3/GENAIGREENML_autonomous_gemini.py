# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import warnings

# Suppress warnings for clean output
warnings.filterwarnings('ignore')

def load_and_preprocess():
    path = 'Iris.csv'
    # Robust CSV parsing
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError
    except Exception:
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            # Create a dummy dataframe if file read fails to ensure end-to-end run
            df = pd.DataFrame(columns=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species'])

    # Normalize column names
    df.columns = [str(c).strip() for c in df.columns]
    df.columns = [" ".join(str(c).split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

    if df.empty:
        # Minimal fallback for a valid return
        print(f"ACCURACY={0.000000:.6f}")
        exit()

    # Identify target column
    target_col = None
    possible_targets = ['Species', 'target', 'class', 'type']
    for pt in possible_targets:
        matches = [c for c in df.columns if c.lower() == pt.lower()]
        if matches:
            target_col = matches[0]
            break
    
    if not target_col:
        # Heuristic: Choose the non-numeric column or the last column
        cat_cols = df.select_dtypes(exclude=[np.number]).columns
        target_col = cat_cols[-1] if len(cat_cols) > 0 else df.columns[-1]

    # Clean target
    df = df.dropna(subset=[target_col])
    
    # Identify feature columns (numeric, non-ID)
    id_indicators = ['id', 'index', 'unnamed']
    features = [c for c in df.columns if c != target_col and not any(ind in c.lower() for ind in id_indicators)]
    
    if not features:
        features = [c for c in df.columns if c != target_col]

    X = df[list(features)].copy()
    y = df[target_col]

    # Ensure numeric types
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    
    # Drop features that are entirely NaN after coercion
    X = X.dropna(axis=1, how='all')
    
    # Final safety check
    if X.empty or len(y) < 2:
        print(f"ACCURACY={0.000000:.6f}")
        exit()

    return X, y

def run_pipeline(X, y):
    # Determine task type (Classification vs Regression)
    # Check if target is discrete
    unique_vals = np.unique(y.astype(str))
    is_classification = len(unique_vals) < (len(y) * 0.1) or isinstance(y.iloc[0], str)

    if is_classification:
        le = LabelEncoder()
        y_encoded = le.fit_transform(y.astype(str))
        
        if len(np.unique(y_encoded)) < 2:
            # Trivial case
            print(f"ACCURACY={1.000000:.6f}")
            return

        X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)
        
        # Logistic Regression: low CPU, high efficiency for small datasets
        model = Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler()),
            ('clf', LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto'))
        ])
        
        model.fit(X_train, y_train)
        score = model.score(X_test, y_test)
        print(f"ACCURACY={score:.6f}")

    else:
        # Fallback to Regression proxy if the target is continuous
        from sklearn.linear_model import Ridge
        from sklearn.metrics import r2_score
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        model = Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler()),
            ('reg', Ridge())
        ])
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        # Scale R2 to [0,1] for ACCURACY proxy
        accuracy_proxy = max(0, min(1, r2))
        print(f"ACCURACY={accuracy_proxy:.6f}")

if __name__ == "__main__":
    X_data, y_data = load_and_preprocess()
    run_pipeline(X_data, y_data)

# OPTIMIZATION SUMMARY
# 1. Model Choice: Logistic Regression was selected as the primary classifier. It is computationally 
#    inexpensive (O(n*p) training), requires no specialized hardware (CPU-friendly), and provides 
#    excellent baselines for linearly separable datasets like Iris.
# 2. Pipeline: Used sklearn.pipeline.Pipeline to minimize memory overhead and ensure that 
#    preprocessing (imputation, scaling) is applied consistently and only once per data pass.
# 3. Energy Efficiency: Avoided ensemble methods (Random Forest/XGBoost) and Neural Networks 
#    which consume significantly more clock cycles and Watts during training/inference.
# 4. Robustness: Implemented a schema-agnostic column normalization and target detection 
#    logic to prevent failures on minor header variations.
# 5. Data Handling: Used 'lbfgs' solver for Logistic Regression, which is memory-efficient 
#    and converges quickly for small-to-medium datasets.
# 6. Regression Fallback: Included a bounded R^2 score as an accuracy proxy in case the 
#    schema detection logic encounters a continuous target variable unexpectedly.