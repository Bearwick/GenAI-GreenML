# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, mean_squared_error

warnings.filterwarnings("ignore")

DATASET_PATH = "Iris.csv"

def normalize_col(col):
    return re.sub(r'\s+', ' ', str(col).strip())

provided_headers = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]
provided_headers_clean = [normalize_col(h) for h in provided_headers]

def read_dataset(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is None or df.shape[1] == 1:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df is None or df_alt.shape[1] > df.shape[1]:
                df = df_alt
        except Exception:
            pass
    return df

df = read_dataset(DATASET_PATH)
if df is None:
    raise RuntimeError("Failed to load dataset")

clean_cols = []
for i, col in enumerate(df.columns):
    col_clean = normalize_col(col)
    if col_clean == '':
        col_clean = f'col_{i}'
    clean_cols.append(col_clean)
df.columns = clean_cols
df = df.loc[:, ~df.columns.str.contains('^Unnamed', case=False, regex=True)]

seen = {}
new_cols = []
for c in df.columns:
    if c in seen:
        seen[c] += 1
        new_cols.append(f"{c}_{seen[c]}")
    else:
        seen[c] = 0
        new_cols.append(c)
df.columns = new_cols

assert df.shape[0] > 0 and df.shape[1] > 0

def choose_target_column(dataframe):
    cols = dataframe.columns.tolist()
    col_map = {c.lower(): c for c in cols}
    for h in provided_headers_clean:
        hl = h.lower()
        if hl in col_map and any(k in hl for k in ['target', 'label', 'class', 'species', 'output', 'y']):
            return col_map[hl]
    keywords = ['target', 'label', 'class', 'species', 'output', 'y']
    for key in keywords:
        for c in cols:
            if key in c.lower():
                return c
    object_cols = [c for c in cols if dataframe[c].dtype == object or str(dataframe[c].dtype).startswith('category')]
    object_cols = [c for c in object_cols if 'id' not in c.lower()]
    for c in object_cols:
        nunique = dataframe[c].nunique(dropna=True)
        if nunique >= 2 and nunique <= max(20, int(len(dataframe) * 0.5)):
            return c
    numeric_scores = {}
    for c in cols:
        if 'id' in c.lower():
            continue
        series = pd.to_numeric(dataframe[c], errors='coerce')
        if series.notna().sum() >= max(1, int(len(dataframe) * 0.5)):
            if series.nunique(dropna=True) > 1:
                numeric_scores[c] = series.var()
    if numeric_scores:
        return max(numeric_scores, key=numeric_scores.get)
    return cols[-1] if cols else None

target_col = choose_target_column(df)
if target_col is None:
    raise RuntimeError("No target column found")

df = df.replace([np.inf, -np.inf], np.nan)
y_raw = df[target_col]

y_numeric = pd.to_numeric(y_raw, errors='coerce')
numeric_ratio = float(y_numeric.notna().mean()) if len(y_numeric) > 0 else 0.0

if y_raw.dtype == object or str(y_raw.dtype).startswith('category'):
    if numeric_ratio >= 0.8:
        y = y_numeric
        unique_vals = y.nunique(dropna=True)
        class_threshold = max(2, min(20, int(len(y) * 0.2)))
        task = 'classification' if unique_vals <= class_threshold else 'regression'
    else:
        y = y_raw
        task = 'classification'
else:
    y = y_numeric
    unique_vals = y.nunique(dropna=True)
    class_threshold = max(2, min(20, int(len(y) * 0.2)))
    task = 'classification' if unique_vals <= class_threshold else 'regression'

mask = y.notna()
df = df.loc[mask].reset_index(drop=True)
y = y.loc[mask].reset_index(drop=True)

assert df.shape[0] > 0

feature_cols = [c for c in df.columns if c != target_col]

filtered_features = []
for c in feature_cols:
    if df[c].nunique(dropna=True) <= 1:
        continue
    filtered_features.append(c)
feature_cols = filtered_features

forced_dummy = False
if not feature_cols:
    X = pd.DataFrame({'dummy': np.zeros(len(df))})
    numeric_features = ['dummy']
    categorical_features = []
    forced_dummy = True
else:
    X = df[feature_cols].copy()
    numeric_features = []
    categorical_features = []
    for col in X.columns:
        series = X[col]
        numeric_series = pd.to_numeric(series, errors='coerce')
        if numeric_series.notna().mean() >= 0.8:
            numeric_features.append(col)
            X[col] = numeric_series
        else:
            categorical_features.append(col)
    X.replace([np.inf, -np.inf], np.nan, inplace=True)
    if not numeric_features and not categorical_features:
        X = pd.DataFrame({'dummy': np.zeros(len(df))})
        numeric_features = ['dummy']
        categorical_features = []
        forced_dummy = True

if task == 'classification':
    n_classes = y.nunique(dropna=True)
    if forced_dummy or n_classes < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    if forced_dummy:
        model = DummyRegressor(strategy='mean')
    else:
        model = LinearRegression()

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))
])
transformers = []
if numeric_features:
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    transformers.append(('cat', categorical_transformer, categorical_features))
preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')

model_pipeline = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])

n_samples = len(X)
if n_samples < 2:
    X_train, X_test, y_train, y_test = X.copy(), X.copy(), y.copy(), y.copy()
else:
    test_size = 0.2
    if n_samples < 5:
        test_size = 0.5
    stratify = y if task == 'classification' and y.nunique(dropna=True) > 1 else None
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=stratify
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=None
        )

assert len(X_train) > 0 and len(X_test) > 0

model_pipeline.fit(X_train, y_train)
y_pred = model_pipeline.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    mse = mean_squared_error(y_test, y_pred)
    accuracy = 1.0 / (1.0 + mse) if np.isfinite(mse) else 0.0
accuracy = float(np.clip(accuracy, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear/logistic or dummy estimators for CPU-friendly training.
# - Applied a minimal, reproducible preprocessing pipeline with imputation, scaling, and one-hot encoding.
# - For regression fallback, reported a bounded accuracy proxy as 1/(1+MSE) to keep scores in [0,1].