# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import glob
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def locate_csv():
    candidates = ["Iris.csv", "iris.csv", "data.csv", "dataset.csv", "train.csv"]
    for name in candidates:
        if os.path.isfile(name):
            return name
    csvs = glob.glob("*.csv")
    if csvs:
        return csvs[0]
    for root, _, files in os.walk("."):
        for f in files:
            if f.lower().endswith(".csv"):
                return os.path.join(root, f)
    return None

csv_path = locate_csv()
if csv_path is None:
    raise FileNotFoundError("CSV dataset not found")

df = pd.read_csv(csv_path)
df = df.dropna()

columns_lower = {col.lower(): col for col in df.columns}
target_col = None
for candidate in ["species", "target", "label", "class"]:
    if candidate in columns_lower:
        target_col = columns_lower[candidate]
        break
if target_col is None:
    target_col = df.columns[-1]

if "id" in columns_lower:
    df = df.drop(columns=[columns_lower["id"]])

X = df.drop(columns=[target_col])
y = df[target_col]

if y.dtype == object or str(y.dtype).startswith("category"):
    encoder = LabelEncoder()
    y = encoder.fit_transform(y)

unique_classes = pd.unique(y)
stratify = y if len(unique_classes) > 1 else None

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=stratify
)

model = Pipeline([
    ("scaler", StandardScaler()),
    ("classifier", LogisticRegression(max_iter=200, multi_class="auto", random_state=42))
])

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Used a lightweight logistic regression model with standardized features for efficient CPU training.
# Implemented a minimal preprocessing pipeline (drop ID, handle labels, scale features) for reproducibility.
# Selected simple train/test split and avoided heavy computation or deep learning to conserve energy.