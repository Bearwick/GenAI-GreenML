# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

RANDOM_STATE = 42
TEST_SIZE = 0.2
N_ESTIMATORS = 100


def load_data(path: str) -> pd.DataFrame:
    usecols = ["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]
    return pd.read_csv(path, usecols=usecols)


def prepare_features_labels(df: pd.DataFrame):
    le = LabelEncoder()
    y = le.fit_transform(df["Species"].to_numpy())
    X = df.drop(columns=["Species"]).to_numpy()
    return X, y


def train_and_evaluate(X, y) -> float:
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=TEST_SIZE,
        random_state=RANDOM_STATE,
        stratify=y,
    )

    model = RandomForestClassifier(
        n_estimators=N_ESTIMATORS,
        random_state=RANDOM_STATE,
        n_jobs=1,
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return accuracy_score(y_test, y_pred)


def main() -> None:
    df = load_data("Iris.csv")
    X, y = prepare_features_labels(df)
    accuracy = train_and_evaluate(X, y)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# Removed plotting/visualization and extra reporting (classification report, confusion matrix) to cut runtime and dependencies.
# Removed cross-validation to avoid redundant training runs and heavy compute while preserving the core train/test behavior.
# Read only required columns via usecols to reduce I/O, parsing work, and memory footprint; avoids loading/dropping "Id".
# Converted DataFrame columns to NumPy arrays for model training to reduce pandas overhead and data movement.
# Centralized constants and logic into small functions for readability and reproducibility (fixed random_state).
# Set n_jobs=1 to avoid uncontrolled parallel CPU usage/energy spikes and improve determinism across environments.