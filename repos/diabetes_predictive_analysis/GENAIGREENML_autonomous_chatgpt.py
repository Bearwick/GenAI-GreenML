# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _find_dataset_path() -> str:
    # Prefer common local filenames; fall back to first CSV in working directory
    candidates = [
        "diabetes.csv",
        "Diabetes.csv",
        "pima-indians-diabetes.csv",
        "pima_diabetes.csv",
        "dataset.csv",
        "data.csv",
        "train.csv",
    ]
    for c in candidates:
        if os.path.isfile(c):
            return c
    csvs = [f for f in os.listdir(".") if f.lower().endswith(".csv")]
    if not csvs:
        raise FileNotFoundError("No CSV dataset file found in current directory.")
    csvs.sort()
    return csvs[0]


def _load_data(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    expected = [
        "Pregnancies",
        "Glucose",
        "BloodPressure",
        "SkinThickness",
        "Insulin",
        "BMI",
        "DiabetesPedigreeFunction",
        "Age",
        "Outcome",
    ]
    missing = [c for c in expected if c not in df.columns]
    if missing:
        raise ValueError(f"Missing expected columns: {missing}")
    return df[expected].copy()


def _build_pipeline(feature_cols):
    # Lightweight numeric preprocessing + linear model for CPU efficiency
    numeric_preprocess = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[("num", numeric_preprocess, feature_cols)],
        remainder="drop",
        n_jobs=1,
    )

    # Logistic Regression is a small, fast, strong baseline for tabular binary classification
    clf = LogisticRegression(
        solver="liblinear",
        max_iter=200,
        random_state=42,
    )

    return Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])


def main():
    path = _find_dataset_path()
    df = _load_data(path)

    X = df.drop(columns=["Outcome"])
    y = df["Outcome"].astype(int)

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y,
    )

    pipeline = _build_pipeline(feature_cols=X.columns.tolist())
    pipeline.fit(X_train, y_train)

    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Chose LogisticRegression (linear, low-parameter) to minimize compute and memory while remaining effective on small tabular datasets.
# - Used a single CPU-friendly sklearn Pipeline + ColumnTransformer for reproducible preprocessing and to avoid redundant passes over data.
# - Median imputation is robust and cheap; StandardScaler improves convergence for linear models with minimal overhead.
# - Avoided deep learning, ensembles, embeddings, plotting, and model persistence to reduce energy use and runtime.