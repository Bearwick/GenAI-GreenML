# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor


warnings.filterwarnings("ignore")


DATASET_PATH = "diabetes.csv"
DATASET_HEADERS = [
    "Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin",
    "BMI", "DiabetesPedigreeFunction", "Age", "Outcome"
]


def _clean_columns(cols):
    cleaned = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        cleaned.append(c2)
    return cleaned


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _read_csv_robust(path):
    df = None
    # Attempt 1: default CSV
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(d):
        if d is None or d.empty:
            return True
        if d.shape[1] <= 1:
            return True
        # Heuristic: if many columns are "Unnamed" or a single giant column, parsing likely wrong
        unnamed_frac = np.mean([str(c).startswith("Unnamed:") for c in d.columns]) if len(d.columns) else 1.0
        if unnamed_frac > 0.5:
            return True
        return False

    if looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df = None

    if df is None:
        # Last resort: try python engine with automatic delimiter inference
        try:
            df = pd.read_csv(path, sep=None, engine="python")
        except Exception as e:
            raise RuntimeError(f"Failed to read dataset: {e}")

    df.columns = _clean_columns(df.columns)
    df = _drop_unnamed(df)
    return df


def _choose_target(df, expected_headers):
    # Prefer provided header "Outcome" if present
    if "Outcome" in df.columns:
        return "Outcome"

    # Otherwise try any matching expected header present
    for h in expected_headers[::-1]:
        if h in df.columns:
            return h

    # Otherwise choose a non-constant numeric column, prefer low cardinality (classification-like)
    numeric_cols = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() > 0:
            numeric_cols.append(c)
    if numeric_cols:
        best = None
        best_score = None
        for c in numeric_cols:
            s = pd.to_numeric(df[c], errors="coerce")
            s = s.replace([np.inf, -np.inf], np.nan).dropna()
            if s.empty:
                continue
            nunique = s.nunique(dropna=True)
            if nunique <= 1:
                continue
            # Prefer small nunique (potential classification), but not too tiny sample support
            score = (nunique, -len(s))
            if best is None or score < best_score:
                best = c
                best_score = score
        if best is not None:
            return best

    # Fallback: last column
    return df.columns[-1]


def _is_classification_target(y):
    # Decide classification if small number of unique values and values are integer-like
    y_nonan = y.replace([np.inf, -np.inf], np.nan).dropna()
    if y_nonan.empty:
        return False
    nunique = y_nonan.nunique(dropna=True)
    if nunique < 2:
        return False
    if nunique <= 20:
        # integer-like check
        y_num = pd.to_numeric(y_nonan, errors="coerce")
        if y_num.notna().all():
            frac_int = np.mean(np.isclose(y_num.values, np.round(y_num.values)))
            if frac_int > 0.98:
                return True
        # also allow object with small unique values
        return True
    return False


def _bounded_regression_score(y_true, y_pred):
    # Stable proxy in [0,1]: 1 / (1 + NRMSE), where NRMSE uses (std + eps)
    yt = np.asarray(y_true, dtype=float)
    yp = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(yt) & np.isfinite(yp)
    if mask.sum() == 0:
        return 0.0
    yt = yt[mask]
    yp = yp[mask]
    mse = np.mean((yt - yp) ** 2)
    rmse = np.sqrt(mse)
    denom = np.std(yt) + 1e-12
    nrmse = rmse / denom
    score = 1.0 / (1.0 + nrmse)
    if not np.isfinite(score):
        return 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    df = _read_csv_robust(DATASET_PATH)
    assert df is not None and not df.empty

    # Normalize columns again (defensive)
    df.columns = _clean_columns(df.columns)
    df = _drop_unnamed(df)
    assert not df.empty and df.shape[1] >= 1

    target_col = _choose_target(df, DATASET_HEADERS)
    if target_col not in df.columns:
        target_col = df.columns[-1]

    # Keep features as all columns except target; if empty, use all columns and let downstream handle
    feature_cols = [c for c in df.columns if c != target_col]
    if not feature_cols:
        feature_cols = [c for c in df.columns if c != target_col]
    if not feature_cols:
        feature_cols = [c for c in df.columns if c != target_col]

    X = df[feature_cols].copy() if feature_cols else df.drop(columns=[target_col], errors="ignore")
    y_raw = df[target_col].copy()

    # Determine target type; coerce numeric when possible
    y_num = pd.to_numeric(y_raw, errors="coerce")
    use_numeric_target = y_num.notna().sum() > 0

    # For classification, prefer using numeric y if it looks binary/integer-like; else use raw
    if use_numeric_target:
        y_for_type = y_num
    else:
        y_for_type = y_raw.astype(str)

    is_clf = _is_classification_target(y_for_type)

    # Prepare feature column types
    # Coerce potential numeric columns; keep objects as categorical
    X_clean = X.copy()
    for c in X_clean.columns:
        # attempt numeric coercion for all columns; if many NaNs result, keep original object column
        coerced = pd.to_numeric(X_clean[c], errors="coerce")
        non_na_ratio = float(coerced.notna().mean()) if len(coerced) else 0.0
        if non_na_ratio >= 0.6:
            X_clean[c] = coerced
        else:
            X_clean[c] = X_clean[c].astype(object)

    numeric_features = [c for c in X_clean.columns if pd.api.types.is_numeric_dtype(X_clean[c])]
    categorical_features = [c for c in X_clean.columns if c not in numeric_features]

    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    rng_state = 42

    if is_clf:
        # Build classification target y
        if use_numeric_target:
            y = y_num.replace([np.inf, -np.inf], np.nan)
            # drop rows where y is NaN
            valid = y.notna()
            X_final = X_clean.loc[valid].copy()
            y_final = y.loc[valid].copy()
            # cast to int if integer-like; otherwise keep as is
            y_vals = y_final.values
            if np.mean(np.isclose(y_vals, np.round(y_vals))) > 0.98:
                y_final = y_final.round().astype(int)
        else:
            y = y_raw.astype(str)
            valid = y.notna()
            X_final = X_clean.loc[valid].copy()
            y_final = y.loc[valid].copy()

        assert len(X_final) > 0 and len(y_final) > 0

        # If only 1 class, fallback to dummy classifier
        if y_final.nunique(dropna=True) < 2:
            X_train, X_test, y_train, y_test = train_test_split(
                X_final, y_final, test_size=0.2, random_state=rng_state
            )
            assert len(X_train) > 0 and len(X_test) > 0
            model = Pipeline(steps=[
                ("preprocess", preprocessor),
                ("clf", DummyClassifier(strategy="most_frequent")),
            ])
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            print(f"ACCURACY={accuracy:.6f}")
            return

        # Stratify when possible
        strat = y_final if y_final.nunique() >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X_final, y_final, test_size=0.2, random_state=rng_state, stratify=strat
        )
        assert len(X_train) > 0 and len(X_test) > 0

        # Lightweight linear model
        clf = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            C=1.0,
            random_state=rng_state,
        )

        model = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("clf", clf),
        ])

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Regression fallback
    if use_numeric_target:
        y = y_num.replace([np.inf, -np.inf], np.nan)
        valid = y.notna()
        X_final = X_clean.loc[valid].copy()
        y_final = y.loc[valid].astype(float).copy()
    else:
        # If target non-numeric, encode to category codes for a stable numeric regression target
        y_cat = y_raw.astype("category")
        y_final = pd.Series(y_cat.cat.codes, index=y_raw.index).astype(float)
        valid = y_final.notna()
        X_final = X_clean.loc[valid].copy()
        y_final = y_final.loc[valid].copy()

    assert len(X_final) > 0 and len(y_final) > 0

    X_train, X_test, y_train, y_test = train_test_split(
        X_final, y_final, test_size=0.2, random_state=rng_state
    )
    assert len(X_train) > 0 and len(X_test) > 0

    reg = Ridge(alpha=1.0, random_state=rng_state)

    model = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("reg", reg),
    ])

    # If y is constant, use dummy regressor
    if float(pd.Series(y_train).nunique(dropna=True)) < 2:
        model = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("reg", DummyRegressor(strategy="mean")),
        ])

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = _bounded_regression_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses robust CSV parsing with a lightweight heuristic + fallback separators to avoid costly manual debugging.
# - Chooses a CPU-friendly baseline: LogisticRegression (liblinear) for classification; Ridge for regression fallback.
# - Minimal preprocessing via ColumnTransformer: median imputation + standardization for numeric; one-hot for categoricals.
# - Coerces numerics defensively and avoids heavy feature engineering to reduce compute and memory.
# - Uses fixed random_state and a simple train/test split for reproducibility and low overhead.
# - If regression is needed, reports a bounded [0,1] proxy score: 1/(1+NRMSE) to keep "ACCURACY" stable and comparable.