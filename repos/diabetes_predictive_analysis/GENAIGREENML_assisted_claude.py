# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

data = pd.read_csv("diabetes.csv")

replace_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
data[replace_cols] = data[replace_cols].replace(0, np.nan)

data['Glucose'] = data['Glucose'].fillna(data['Glucose'].mean())
for col in ['BloodPressure', 'SkinThickness', 'Insulin', 'BMI']:
    data[col] = data[col].fillna(data[col].median())

x = data.drop('Outcome', axis=1).values
y = data['Outcome'].values

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train_scaled, y_train)
y_pred = knn.predict(x_test_scaled)

accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Removed unused imports (matplotlib, seaborn) to reduce load time and memory
# Removed data.copy() - work directly on the original dataframe to save memory
# Removed all plots, prints, and visualizations as required
# Removed redundant loop over k=1..15 with metric computation (not needed for final model)
# Removed cross_val_score loop over k=1..15 (unnecessary computation)
# Removed RandomizedSearchCV since original final model uses k=5 on scaled data
# Combined fit+transform into fit_transform for scaler to avoid redundant pass
# Used single fillna loop for median columns to reduce repetition
# Kept only essential pipeline: load, clean, split, scale, train k=5 KNN, predict, report accuracy