# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset - assumes standard Pima Indians Diabetes dataset
df = pd.read_csv("./dataset.csv")

# Separate features and target
X = df.drop("Outcome", axis=1).values
y = df["Outcome"].values

# Handle zero values that are likely missing (Glucose, BloodPressure, SkinThickness, Insulin, BMI)
# Replace zeros with NaN then impute with median - energy efficient approach
cols_with_zeros = [1, 2, 3, 4, 5]  # Glucose, BloodPressure, SkinThickness, Insulin, BMI
X = X.astype(float)
for col in cols_with_zeros:
    mask = X[:, col] == 0
    if mask.any():
        median_val = np.median(X[~mask, col])
        X[mask, col] = median_val

# Use StratifiedKFold for robust evaluation on small dataset
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

accuracies = []

for train_idx, test_idx in skf.split(X, y):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # StandardScaler is lightweight and effective for logistic regression
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Logistic Regression: lightweight, interpretable, CPU-friendly
    # liblinear solver is efficient for small datasets
    model = LogisticRegression(
        solver="liblinear",
        C=1.0,
        max_iter=200,
        random_state=42
    )
    model.fit(X_train_scaled, y_train)

    y_pred = model.predict(X_test_scaled)
    accuracies.append(accuracy_score(y_test, y_pred))

accuracy = np.mean(accuracies)

# Final model trained on all data for reporting
scaler_final = StandardScaler()
X_scaled = scaler_final.fit_transform(X)
model_final = LogisticRegression(solver="liblinear", C=1.0, max_iter=200, random_state=42)
model_final.fit(X_scaled, y)
y_pred_final = model_final.predict(X_scaled)
train_accuracy = accuracy_score(y, y_pred_final)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Used LogisticRegression with liblinear solver - extremely lightweight, ideal for small binary classification tasks
# 2. Replaced physiologically impossible zero values with median imputation - simple, no extra library needed
# 3. StandardScaler for feature normalization - minimal compute overhead, improves convergence
# 4. StratifiedKFold (5-fold) for reliable accuracy estimation on ~768 samples
# 5. No deep learning, no embeddings, no GPU required - entire pipeline runs in milliseconds on CPU
# 6. Minimal memory footprint: only numpy arrays and a single linear model in memory