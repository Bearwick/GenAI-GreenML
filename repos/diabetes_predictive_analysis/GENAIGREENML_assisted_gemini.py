# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.neighbors import KNeighborsClassifier

def load_data(filepath):
    try:
        df = pd.read_csv(filepath)
        if df.shape[1] <= 1:
            raise ValueError
    except Exception:
        df = pd.read_csv(filepath, sep=';', decimal=',')
    return df

df = load_data("diabetes.csv")

cols_to_fix = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[cols_to_fix] = df[cols_to_fix].replace(0, np.nan)

impute_values = {
    'Glucose': df['Glucose'].mean(),
    'BloodPressure': df['BloodPressure'].median(),
    'SkinThickness': df['SkinThickness'].median(),
    'Insulin': df['Insulin'].median(),
    'BMI': df['BMI'].median()
}
df.fillna(value=impute_values, inplace=True)

X = df.drop('Outcome', axis=1).values
y = df['Outcome'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

param_dist = {
    'n_neighbors': np.arange(1, 16),
    'weights': ['uniform', 'distance']
}

knn = KNeighborsClassifier()
random_search = RandomizedSearchCV(
    knn, 
    param_distributions=param_dist, 
    n_iter=10, 
    cv=5, 
    random_state=42,
    n_jobs=-1
)
random_search.fit(X_train, y_train)

best_model = random_search.best_estimator_
accuracy = best_model.score(X_test, y_test)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Removed redundant data.copy() to reduce memory usage and avoid unnecessary data movement.
# 2. Replaced iterative column-by-column fillna with a vectorized dictionary-based fillna for better performance.
# 3. Eliminated multiple redundant loops (1-15 k-range) that calculated metrics solely for visualization.
# 4. Removed all visualization code (matplotlib/seaborn) to minimize computational overhead and library dependencies.
# 5. Combined preprocessing steps into a single sequence to reduce intermediate object creation.
# 6. Optimized the hyperparameter search using n_jobs=-1 for parallel execution, reducing runtime.
# 7. Fixed random seeds (random_state) to ensure reproducibility and stable energy-efficient execution.
# 8. Implemented robust CSV parsing to ensure the script runs end-to-end without manual intervention.
# 9. Removed all intermediate logging and prints to focus purely on the required output.