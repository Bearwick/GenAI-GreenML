# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.neighbors import KNeighborsClassifier

def load_data(file_path):
    try:
        df = pd.read_csv(file_path)
        if df.shape[1] < 2:
            raise ValueError
    except (pd.errors.ParserError, ValueError):
        df = pd.read_csv(file_path, sep=';', decimal=',')
    return df

data = load_data("diabetes.csv")

target_col = 'Outcome'
impute_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

data[impute_cols] = data[impute_cols].replace(0, np.nan)

fill_values = {
    'Glucose': data['Glucose'].mean(),
    'BloodPressure': data['BloodPressure'].median(),
    'SkinThickness': data['SkinThickness'].median(),
    'Insulin': data['Insulin'].median(),
    'BMI': data['BMI'].median()
}
data.fillna(value=fill_values, inplace=True)

X = data.drop(target_col, axis=1).values
y = data[target_col].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

param_dist = {
    'n_neighbors': np.arange(1, 16),
    'weights': ['uniform', 'distance']
}

search = RandomizedSearchCV(
    KNeighborsClassifier(),
    param_dist,
    n_iter=10,
    cv=5,
    random_state=42
)
search.fit(X_train_scaled, y_train)

best_model = search.best_estimator_
accuracy = best_model.score(X_test_scaled, y_test)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Consolidated multiple redundant training loops and cross-validation steps into a single RandomizedSearchCV.
# 2. Replaced inefficient column-by-column fillna calls with a vectorized dictionary-based imputation.
# 3. Eliminated memory-intensive data.copy() operations by performing operations in-place where possible.
# 4. Removed all visualization libraries (matplotlib, seaborn) and plotting logic to reduce runtime and resource overhead.
# 5. Integrated robust CSV parsing with a fallback mechanism to handle different delimiters in a single execution path.
# 6. Optimized data scaling by using fit_transform() on the training set and transform() on the test set to avoid redundant fitting.
# 7. Reduced computational cost by ensuring KNN operates on scaled data, which improves convergence and distance calculation efficiency.
# 8. Removed all non-essential print statements and logging to minimize I/O operations.