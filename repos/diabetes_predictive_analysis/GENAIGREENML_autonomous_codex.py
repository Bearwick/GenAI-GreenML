# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re

DATASET_PATH = "diabetes.csv"
DATASET_HEADERS = ["Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "DiabetesPedigreeFunction", "Age", "Outcome"]

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    if df.shape[1] == 1:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > 1:
                df = df_alt
        except Exception:
            pass
    if df.shape[1] == 1:
        try:
            df_alt = pd.read_csv(path, sep=',')
            if df_alt.shape[1] > 1:
                df = df_alt
        except Exception:
            pass
    return df

df = read_csv_robust(DATASET_PATH)

def normalize_col(c):
    s = str(c).strip()
    s = re.sub(r'\s+', ' ', s)
    return s

df.columns = [normalize_col(c) for c in df.columns]
df = df.loc[:, [c for c in df.columns if not str(c).lower().startswith('unnamed')]]

assert df.shape[0] > 0

normalized_headers = [normalize_col(h) for h in DATASET_HEADERS]
col_map = {c.lower(): c for c in df.columns}
target = None
for h in reversed(normalized_headers):
    if h.lower() in col_map:
        target = col_map[h.lower()]
        break
if target is None:
    for name in ['outcome', 'target', 'label', 'class', 'y']:
        if name in col_map:
            target = col_map[name]
            break
if target is None:
    numeric_candidates = []
    for c in df.columns:
        coerced = pd.to_numeric(df[c], errors='coerce')
        if coerced.notna().mean() > 0.5 and coerced.nunique(dropna=True) > 1:
            numeric_candidates.append(c)
    if numeric_candidates:
        target = numeric_candidates[-1]
    else:
        target = df.columns[-1]

features = [c for c in df.columns if c != target]
if len(features) == 0:
    df["_dummy_feature"] = 0
    features = ["_dummy_feature"]

df.replace([np.inf, -np.inf], np.nan, inplace=True)

y_raw = df[target]
y_coerced = pd.to_numeric(y_raw, errors='coerce')
if y_coerced.notna().mean() >= 0.5:
    y = y_coerced
    y_is_numeric = True
else:
    y = y_raw.astype(str)
    y_is_numeric = False

mask = y.notna()
df = df.loc[mask].copy()
y = y.loc[mask]

numeric_features = []
categorical_features = []
for col in features:
    series = df[col]
    coerced = pd.to_numeric(series, errors='coerce')
    if coerced.notna().mean() >= 0.5:
        df[col] = coerced
        numeric_features.append(col)
    else:
        df[col] = series.astype(str)
        categorical_features.append(col)

X = df[features]
assert len(X) > 0

if y_is_numeric:
    n_unique = y.nunique(dropna=True)
    if n_unique <= 20 and n_unique / max(1, len(y)) <= 0.2:
        task = "classification"
    else:
        task = "regression"
else:
    task = "classification"

if task == "classification" and y.nunique(dropna=True) < 2:
    task = "regression"

if task == "classification":
    if y_is_numeric:
        y_round = y.round()
        if np.allclose(y.dropna(), y_round.dropna()):
            y = y_round.astype(int)
        else:
            y = pd.Series(pd.factorize(y)[0], index=y.index)
    else:
        y = pd.Series(pd.factorize(y)[0], index=y.index)
else:
    if not y_is_numeric:
        y = pd.Series(pd.factorize(y)[0], index=y.index).astype(float)

from sklearn.model_selection import train_test_split

stratify = y if task == "classification" and y.nunique(dropna=True) > 1 else None
try:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=stratify
    )
except Exception:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

assert len(X_train) > 0 and len(X_test) > 0

if task == "classification" and pd.Series(y_train).nunique() < 2:
    task = "regression"

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

transformers = []
if numeric_features:
    transformers.append(
        ("num", Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False))
        ]), numeric_features)
    )
if categorical_features:
    transformers.append(
        ("cat", Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("encoder", OneHotEncoder(handle_unknown="ignore"))
        ]), categorical_features)
    )

preprocessor = ColumnTransformer(transformers=transformers, remainder="drop")

if task == "classification":
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(max_iter=200, solver="liblinear")
else:
    from sklearn.linear_model import Ridge
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if task == "classification":
    from sklearn.metrics import accuracy_score
    accuracy = accuracy_score(y_test, y_pred)
else:
    from sklearn.metrics import mean_squared_error
    mse = mean_squared_error(y_test, y_pred)
    var = np.var(y_test)
    if var > 0:
        accuracy = 1.0 / (1.0 + mse / var)
    else:
        accuracy = 1.0 if mse == 0 else 0.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Used lightweight linear models with simple preprocessing to minimize CPU and energy usage.
# Applied a single ColumnTransformer pipeline for reproducible, low-overhead feature handling.
# Robust CSV parsing and column normalization reduce failure retries and wasted computation.
# Regression fallback accuracy uses 1/(1+mse/var) to keep the score bounded in [0,1].