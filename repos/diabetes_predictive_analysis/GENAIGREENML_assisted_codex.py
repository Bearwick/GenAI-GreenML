# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

data = pd.read_csv("diabetes.csv")

cols_zero = ["Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI"]
data[cols_zero] = data[cols_zero].replace(0, np.nan)

data["Glucose"] = data["Glucose"].fillna(data["Glucose"].mean())
median_cols = ["BloodPressure", "SkinThickness", "Insulin", "BMI"]
data[median_cols] = data[median_cols].fillna(data[median_cols].median())

X = data.drop(columns="Outcome").to_numpy()
y = data["Outcome"].to_numpy()

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

accuracy = knn.score(X_test_scaled, y_test)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Removed plotting, exploratory logging, and hyperparameter search that did not affect final predictions.
# Used vectorized zero-to-NaN replacement and imputation to avoid loops and redundant operations.
# Trained and evaluated a single deterministic model to reduce compute while preserving baseline behavior.
# Used direct model scoring to avoid extra prediction storage and minimize memory overhead.