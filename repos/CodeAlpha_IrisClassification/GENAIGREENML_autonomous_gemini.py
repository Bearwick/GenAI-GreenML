# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder

iris = load_iris()
df = pd.DataFrame(iris.data, columns=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'])
df['Species'] = [iris.target_names[i] for i in iris.target]
df['Id'] = range(1, len(df) + 1)
df = df[['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']]

df = df.drop(columns=['Id'])
X = df.drop(columns=['Species'])
y = df['Species']

encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

clf = DecisionTreeClassifier(max_depth=3, random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Model Selection: DecisionTreeClassifier was selected for its minimal computational complexity (O(n_features * n_samples * log n_samples)) compared to iterative models.
# 2. Green Coding: The model does not require feature normalization or scaling, which reduces the total number of floating-point operations (FLOPs) during preprocessing.
# 3. Energy Efficiency: No deep learning or GPU-dependent components were used; the training and inference processes are lightweight and CPU-optimized.
# 4. Memory Management: Irrelevant columns like 'Id' were dropped immediately to minimize the DataFrame's memory footprint.
# 5. Resource Constraint: The tree depth was limited to 3, which prevents overfitting while ensuring the smallest possible model size and fastest execution time.