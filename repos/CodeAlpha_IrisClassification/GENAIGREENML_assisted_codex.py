# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

DATASET_PATH = "Iris.csv"
DATASET_HEADERS = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]
SEED = 42

def _parsing_suspect(df, expected_headers):
    if df.empty:
        return True
    cols = [str(c).strip() for c in df.columns]
    if len(cols) == 1:
        return True
    lower_cols = [c.lower() for c in cols]
    expected_lower = [h.lower() for h in expected_headers]
    if len(set(lower_cols).intersection(expected_lower)) == 0:
        return True
    if any(";" in c for c in cols):
        return True
    return False

def load_dataset(path, expected_headers):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
        df.columns = [str(c).strip() for c in df.columns]
        return df
    if _parsing_suspect(df, expected_headers):
        try:
            df_alt = pd.read_csv(path, sep=";", decimal=",")
            if not df_alt.empty:
                df = df_alt
        except Exception:
            pass
    df.columns = [str(c).strip() for c in df.columns]
    return df

def resolve_column(df, expected_name):
    expected_lower = expected_name.lower()
    for col in df.columns:
        if str(col).lower() == expected_lower:
            return col
    for col in df.columns:
        if expected_lower in str(col).lower():
            return col
    return None

def prepare_data(df, expected_headers):
    target_col = resolve_column(df, expected_headers[-1])
    if target_col is None:
        raise ValueError("Target column not found")
    id_col = resolve_column(df, expected_headers[0])
    drop_cols = [c for c in (id_col, target_col) if c in df.columns]
    X = df.drop(columns=drop_cols)
    y = df[target_col]
    return X, y

def main():
    df = load_dataset(DATASET_PATH, DATASET_HEADERS)
    X, y = prepare_data(df, DATASET_HEADERS)
    y_encoded = LabelEncoder().fit_transform(y)
    X_values = X.to_numpy(copy=False)
    X_train, X_test, y_train, y_test = train_test_split(
        X_values, y_encoded, test_size=0.2, random_state=SEED, stratify=y_encoded
    )
    log_reg = LogisticRegression(max_iter=200, random_state=SEED)
    log_reg.fit(X_train, y_train)
    _ = log_reg.score(X_test, y_test)
    rf_model = RandomForestClassifier(n_estimators=100, random_state=SEED)
    rf_model.fit(X_train, y_train)
    accuracy = rf_model.score(X_test, y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed data inspection, reports, and visualization to eliminate unnecessary computation and I/O.
# - Implemented robust CSV parsing with delimiter/decimal fallback to avoid reprocessing failures.
# - Converted features to NumPy once and reused a single split for both models to minimize data movement.
# - Used estimator .score for accuracy to avoid extra prediction storage and kept fixed random states for reproducibility.