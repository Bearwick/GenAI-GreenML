# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

DATA_PATH = "C:/Users/Lalit Pathak/python2.0/Iris.csv"


def load_and_split(path):
    data = pd.read_csv(path, usecols=lambda c: c != "Id")
    if "Species" not in data.columns:
        raise ValueError("Species column not found in dataset.")
    y = data.pop("Species").to_numpy()
    X = data.to_numpy()
    y_encoded = LabelEncoder().fit_transform(y)
    return train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )


def train_models(X_train, y_train):
    LogisticRegression(max_iter=200).fit(X_train, y_train)
    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)
    return rf_model


def main():
    X_train, X_test, y_train, y_test = load_and_split(DATA_PATH)
    rf_model = train_models(X_train, y_train)
    accuracy = rf_model.score(X_test, y_test)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# Removed unused imports, reporting, and visualization to cut overhead.
# Loaded only necessary columns and extracted the label with pop to avoid extra copies.
# Converted features once to NumPy to prevent repeated DataFrame-to-array conversions.
# Used model.score for direct accuracy computation without extra prediction steps.
# Maintained fixed random_state for reproducible splits and model behavior.