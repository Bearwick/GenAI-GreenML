# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


def load_dataset(path: str) -> pd.DataFrame:
    return pd.read_csv(path)


def prepare_features_labels(df: pd.DataFrame):
    if "Id" in df.columns:
        df = df.drop(columns=["Id"])

    if "Species" not in df.columns:
        raise KeyError("'Species' column not found in dataset.")

    x = df.drop(columns=["Species"]).to_numpy(dtype="float32", copy=False)
    y = df["Species"].to_numpy(copy=False)

    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    return x, y_encoded


def train_and_evaluate(x, y, random_state: int = 42) -> float:
    x_train, x_test, y_train, y_test = train_test_split(
        x,
        y,
        test_size=0.2,
        random_state=random_state,
        stratify=y,
    )

    log_reg = LogisticRegression(max_iter=200, solver="lbfgs")
    log_reg.fit(x_train, y_train)
    _ = log_reg.predict(x_test)

    rf_model = RandomForestClassifier(
        n_estimators=100,
        random_state=random_state,
        n_jobs=-1,
    )
    rf_model.fit(x_train, y_train)
    y_pred = rf_model.predict(x_test)

    return accuracy_score(y_test, y_pred)


def main() -> None:
    data = load_dataset("C:/Users/Lalit Pathak/python2.0/Iris.csv")
    x, y = prepare_features_labels(data)
    accuracy = train_and_evaluate(x, y)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Removed unused imports (numpy, reports, confusion matrix, seaborn, matplotlib) to reduce load time and memory footprint.
# - Removed exploratory prints, visualizations, and any file/model output to avoid unnecessary I/O and rendering overhead.
# - Converted feature matrix to float32 NumPy array once (copy=False when possible) to reduce memory and speed up downstream computations.
# - Kept label encoding but avoided retaining extra intermediate structures; returned only what is needed for training.
# - Enabled RandomForest parallelism via n_jobs=-1 to reduce wall-clock time on multi-core CPUs while preserving model behavior.
# - Preserved original task and split strategy (stratified, random_state=42) for reproducible and stable results.
# - Avoided generating heavy classification reports and confusion matrices; computed only final accuracy as required.