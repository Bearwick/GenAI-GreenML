# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import sys
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


RANDOM_SEED = 42
DATASET_PATH = "Iris.csv"
DATASET_HEADERS = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]


def _read_csv_with_fallback(path: str) -> pd.DataFrame:
    try:
        df = pd.read_csv(path)
    except FileNotFoundError:
        raise

    def looks_wrong(d: pd.DataFrame) -> bool:
        if d is None or d.empty:
            return True
        if d.shape[1] == 1:
            return True
        if "Species" not in d.columns and "Species" in DATASET_HEADERS:
            return True
        if any(col not in d.columns for col in DATASET_HEADERS if col in ("Species", "Id")):
            return True
        return False

    if looks_wrong(df):
        df2 = pd.read_csv(path, sep=";", decimal=",")
        if not looks_wrong(df2):
            return df2
    return df


def _prepare_features_and_labels(df: pd.DataFrame):
    cols = list(df.columns)

    species_col = "Species" if "Species" in cols else next((c for c in cols if c.strip().lower() == "species"), None)
    if species_col is None:
        raise ValueError("Species column not found.")

    id_col = "Id" if "Id" in cols else next((c for c in cols if c.strip().lower() == "id"), None)

    y = df[species_col]
    feature_cols = [c for c in cols if c != species_col and c != id_col]

    X = df[feature_cols]
    X = X.apply(pd.to_numeric, errors="coerce")

    valid_mask = X.notna().all(axis=1) & y.notna()
    X = X.loc[valid_mask].astype(np.float32, copy=False)
    y = y.loc[valid_mask]

    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    return X, y_encoded


def main():
    np.random.seed(RANDOM_SEED)

    df = _read_csv_with_fallback(DATASET_PATH)

    X, y_encoded = _prepare_features_and_labels(df)

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y_encoded,
        test_size=0.2,
        random_state=RANDOM_SEED,
        stratify=y_encoded,
    )

    log_reg = LogisticRegression(max_iter=200, n_jobs=1, random_state=RANDOM_SEED)
    log_reg.fit(X_train, y_train)

    rf_model = RandomForestClassifier(
        n_estimators=100,
        random_state=RANDOM_SEED,
        n_jobs=1,
    )
    rf_model.fit(X_train, y_train)

    y_pred_rf = rf_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred_rf)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    try:
        main()
    except FileNotFoundError:
        sys.exit(1)

# Optimization Summary
# - Removed all non-essential outputs (head/info/reports/confusion matrix plots) to reduce I/O and rendering overhead.
# - Implemented robust CSV parsing with a lightweight heuristic and a single fallback read (sep=';', decimal=',') to avoid repeated work.
# - Dropped unused columns (Id) and inferred target/feature columns from df.columns to prevent schema assumptions and extra processing.
# - Converted features to float32 to reduce memory footprint and data movement while preserving model behavior for this dataset.
# - Filtered invalid rows once with a boolean mask to avoid repeated NaN handling and ensure consistent downstream computations.
# - Set fixed random seeds and constrained n_jobs=1 for reproducibility and to avoid energy spikes from uncontrolled parallelism.
# - Kept both original models trained to preserve evaluation intent, but computed/printed only the required final accuracy line to minimize overhead.