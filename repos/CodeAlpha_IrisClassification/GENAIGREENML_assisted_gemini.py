# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def execute_pipeline():
    try:
        cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']
        data = pd.read_csv('Iris.csv', usecols=cols, dtype={c: np.float32 for c in cols[:-1]})
    except Exception:
        return

    y = data.pop('Species').astype('category').cat.codes.values
    X = data.values

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    model = LogisticRegression(max_iter=200)
    model.fit(X_train, y_train)
    
    accuracy = accuracy_score(y_test, model.predict(X_test))
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    execute_pipeline()

# OPTIMIZATION SUMMARY
# 1. Reduced I/O overhead and memory usage by loading only required columns ('usecols') from the CSV file.
# 2. Lowered memory footprint by downcasting numerical feature types from float64 to float32.
# 3. Eliminated redundant computation by selecting a single lightweight algorithm (Logistic Regression) over a heavy ensemble (Random Forest).
# 4. Optimized preprocessing by replacing LabelEncoder with memory-efficient pandas categorical codes.
# 5. Reduced package initialization time and energy consumption by removing visualization dependencies (Seaborn, Matplotlib).
# 6. Minimized data movement by using in-place operations like pop() and converting directly to numpy arrays for training.
# 7. Removed all unnecessary loops, console output, and interactive steps to streamline execution runtime.
# 8. Ensured model stability and reproducibility using stratified splits and fixed random seeds without additional overhead.