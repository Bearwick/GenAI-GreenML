# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _read_csv_robust(path):
    df = None
    # Attempt 1: default CSV parse
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    # Heuristic "looks wrong": only 1 column with embedded separators, retry with ';' and decimal=','
    def looks_wrong(d):
        if d is None or d.empty:
            return True
        if d.shape[1] == 1:
            sample = d.iloc[:10, 0].astype(str)
            joined = " ".join(sample.tolist())
            if ";" in joined or "," in joined:
                return True
        return False

    if looks_wrong(df):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df2 is not None and not df2.empty and df2.shape[1] >= 2:
                df = df2
        except Exception:
            pass

    if df is None:
        raise FileNotFoundError(f"Could not read dataset at path: {path}")
    return df


def _choose_target(df, preferred_headers=None):
    cols = list(df.columns)
    if preferred_headers:
        pref = [c for c in preferred_headers if c in cols]
        if pref:
            # prefer last header as potential target (common in many datasets)
            if pref[-1] in cols:
                return pref[-1]
            return pref[0]

    # Prefer typical target-like names if present
    target_like = []
    for c in cols:
        cl = c.strip().lower()
        if cl in ("target", "label", "y", "class", "species", "outcome"):
            target_like.append(c)
    if target_like:
        return target_like[0]

    # Else choose a non-constant column with few unique values (classification-friendly) if possible
    best = None
    best_score = -1
    for c in cols:
        s = df[c]
        nun = s.nunique(dropna=True)
        if nun <= 1:
            continue
        # Prefer categorical/object, or low-cardinality numeric
        score = 0
        if s.dtype == "O":
            score += 3
        if nun <= 20:
            score += 2
        if nun <= max(2, int(0.2 * len(df))):
            score += 1
        if score > best_score:
            best_score = score
            best = c
    if best is not None:
        return best

    # Fallback: pick first column
    return cols[0] if cols else None


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    if y_true.size == 0:
        return 0.0
    denom = np.mean((y_true - np.mean(y_true)) ** 2)
    if not np.isfinite(denom) or denom <= 1e-12:
        # If variance is ~0, treat perfect prediction as 1 else 0
        err = np.mean((y_true - y_pred) ** 2)
        return 1.0 if err <= 1e-12 else 0.0
    mse = np.mean((y_true - y_pred) ** 2)
    r2 = 1.0 - (mse / denom)
    # Map to [0,1] for stable ACCURACY proxy
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


# Resolve dataset path robustly (environment variable -> common local names)
DATASET_HEADERS = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]
candidate_paths = []

env_path = os.environ.get("DATASET_PATH", "").strip()
if env_path:
    candidate_paths.append(env_path)

candidate_paths.extend(
    [
        "Iris.csv",
        "iris.csv",
        "data.csv",
        "dataset.csv",
        "train.csv",
        "./Iris.csv",
        "./iris.csv",
    ]
)

df = None
last_err = None
for p in candidate_paths:
    try:
        if os.path.exists(p):
            df = _read_csv_robust(p)
            break
    except Exception as e:
        last_err = e
        df = None

if df is None:
    # Try reading from current directory by picking first CSV if exists
    try:
        for fn in os.listdir("."):
            if fn.lower().endswith(".csv"):
                try:
                    df = _read_csv_robust(fn)
                    break
                except Exception as e:
                    last_err = e
                    df = None
        if df is None and last_err is not None:
            raise last_err
    except Exception:
        raise FileNotFoundError("No readable CSV dataset found. Set DATASET_PATH or place a CSV in the working directory.")

# Normalize columns and drop Unnamed columns
df.columns = _normalize_columns(df.columns)
df = df.loc[:, [c for c in df.columns if not str(c).strip().lower().startswith("unnamed:")]]

# Basic sanity
assert df is not None and isinstance(df, pd.DataFrame)
assert not df.empty, "Dataset is empty after loading."

# Choose target
target_col = _choose_target(df, preferred_headers=DATASET_HEADERS)
if target_col is None or target_col not in df.columns:
    # fallback: make a synthetic target (all zeros) to keep pipeline running
    target_col = "__target__"
    df[target_col] = 0

y_raw = df[target_col]
X_raw = df.drop(columns=[target_col])

# If an ID-like column exists, drop it as a feature (low signal, potentially leaky)
id_like = []
for c in X_raw.columns:
    cl = c.strip().lower()
    if cl in ("id", "index") or cl.endswith("_id") or cl.endswith("id"):
        id_like.append(c)
if id_like:
    X_raw = X_raw.drop(columns=id_like)

# Attempt to coerce numeric-looking object columns into numbers safely
X = X_raw.copy()
for c in X.columns:
    if X[c].dtype == "O":
        coerced = pd.to_numeric(X[c], errors="coerce")
        # If coercion yields some valid numbers, keep as numeric (avoid heavy parsing)
        if np.isfinite(coerced.to_numpy(dtype=float, na_value=np.nan)).sum() > 0:
            # But only switch if it's not purely missing after coercion
            if coerced.notna().sum() >= max(1, int(0.1 * len(coerced))):
                X[c] = coerced

# Identify column types
numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
categorical_features = [c for c in X.columns if c not in numeric_features]

# Prepare y and decide task
is_classification = False
y = y_raw

# If y is object or low-cardinality, treat as classification
try:
    nunique_y = y.nunique(dropna=True)
except Exception:
    nunique_y = 0

if y.dtype == "O":
    is_classification = nunique_y >= 2
else:
    # Numeric y: classification if it is integer-like and low cardinality
    y_num = pd.to_numeric(y, errors="coerce")
    if y_num.notna().sum() > 0:
        if nunique_y >= 2 and nunique_y <= 20:
            # check integer-likeness
            vals = y_num.dropna().to_numpy()
            if vals.size > 0 and np.all(np.isclose(vals, np.round(vals))):
                is_classification = True

# Build preprocessing
numeric_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ]
)
categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ]
)

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Split defensively
# For stratify, only use if classification and y has >=2 classes with enough samples
stratify = None
if is_classification:
    try:
        vc = y.value_counts(dropna=True)
        if (vc.min() >= 2) and (vc.size >= 2):
            stratify = y
    except Exception:
        stratify = None

# Drop rows with missing target
mask = pd.notna(y)
X = X.loc[mask].copy()
y = y.loc[mask].copy()

assert len(X) > 0, "No rows left after dropping missing target."

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=stratify.loc[mask] if stratify is not None else None,
)

assert len(X_train) > 0 and len(X_test) > 0, "Train/test split produced empty set."

accuracy = 0.0

if is_classification:
    # If after split there is only one class in train, fallback to trivial baseline
    try:
        train_classes = pd.Series(y_train).nunique(dropna=True)
    except Exception:
        train_classes = 0

    if train_classes < 2:
        # Predict the most frequent label seen in training
        try:
            majority = pd.Series(y_train).mode(dropna=True)
            pred_label = majority.iloc[0] if len(majority) else y_train.iloc[0]
        except Exception:
            pred_label = y_train.iloc[0]
        y_pred = np.full(shape=(len(y_test),), fill_value=pred_label, dtype=object)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Lightweight linear model; good CPU baseline for small tabular data
        clf = LogisticRegression(
            max_iter=200,
            solver="lbfgs",
            n_jobs=1,
            multi_class="auto",
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Regression path: coerce y to numeric
    y_train_num = pd.to_numeric(y_train, errors="coerce")
    y_test_num = pd.to_numeric(y_test, errors="coerce")
    # Align to non-missing
    train_mask = y_train_num.notna()
    test_mask = y_test_num.notna()
    if train_mask.sum() == 0 or test_mask.sum() == 0:
        accuracy = 0.0
    else:
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train.loc[train_mask], y_train_num.loc[train_mask])
        preds = model.predict(X_test.loc[test_mask])
        accuracy = _bounded_regression_score(y_test_num.loc[test_mask].to_numpy(), preds)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Uses a lightweight CPU-friendly baseline: LogisticRegression for classification, Ridge for regression fallback (small memory/compute).
# - Employs a single sklearn Pipeline + ColumnTransformer to avoid redundant preprocessing and ensure reproducibility.
# - Robust CSV loading: retries with ';' separator and ',' decimal if default parsing appears incorrect.
# - Defensive schema handling: normalizes column names, drops 'Unnamed:*', drops ID-like columns, and selects a target safely if expected missing.
# - Safe numeric coercion and missing handling via SimpleImputer; avoids mean/median on object dtypes by type-detecting first.
# - Keeps stdout minimal as required; prints only the final ACCURACY line.
# - Regression "accuracy" proxy: bounded score in [0,1] computed from R2 mapped to [0,1] to remain stable and comparable.