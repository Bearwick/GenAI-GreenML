# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import glob
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _find_dataset_path() -> str:
    candidates = []
    candidates.extend(glob.glob("*.csv"))
    candidates.extend(glob.glob("data/*.csv"))
    candidates.extend(glob.glob("dataset/*.csv"))
    candidates.extend(glob.glob("input/*.csv"))
    candidates.extend(glob.glob("**/*.csv", recursive=True))

    best = None
    best_score = -1
    required = {"SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"}

    for p in candidates:
        try:
            df_head = pd.read_csv(p, nrows=5)
        except Exception:
            continue
        cols = set(df_head.columns.astype(str))
        score = len(required.intersection(cols))
        if score > best_score:
            best_score = score
            best = p
        if score == len(required):
            return p

    if best is None:
        raise FileNotFoundError("No suitable CSV dataset found with required headers.")
    return best


def main() -> None:
    path = _find_dataset_path()
    df = pd.read_csv(path)

    required = ["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns: {missing}")

    X = df[["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm"]].copy()
    y = df["Species"].astype(str).copy()

    le = LabelEncoder()
    y_enc = le.fit_transform(y)

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y_enc,
        test_size=0.2,
        random_state=42,
        stratify=y_enc if len(np.unique(y_enc)) > 1 else None,
    )

    pipeline = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
            ("clf", LogisticRegression(solver="lbfgs", max_iter=200, n_jobs=1)),
        ]
    )

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Used a compact, classical model (multinomial Logistic Regression) suitable for small tabular data; avoids deep learning.
# - CPU-efficient preprocessing pipeline (median imputation + standard scaling) ensures reproducibility with minimal overhead.
# - Limited iterations and single-threaded execution (n_jobs=1) to reduce energy use and avoid unnecessary parallelism overhead.
# - No plotting, interactivity, model serialization, or verbose logging to keep runtime and I/O minimal.
# - Simple train/test split with fixed random_state for deterministic results.