# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import glob
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def load_dataset():
    csv_files = glob.glob("**/*.csv", recursive=True)
    required = {"SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"}
    for file in csv_files:
        try:
            df = pd.read_csv(file)
        except Exception:
            continue
        if required.issubset(set(df.columns)):
            return df
    raise FileNotFoundError("Dataset with required columns not found.")

data = load_dataset()
if "Id" in data.columns:
    data = data.drop(columns=["Id"])
data = data.dropna()

X = data.drop(columns=["Species"])
y = data["Species"]

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

model = Pipeline([
    ("scaler", StandardScaler()),
    ("classifier", LogisticRegression(solver="liblinear", max_iter=200))
])

model.fit(X_train, y_train)
predictions = model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# - Used a simple logistic regression model suitable for small datasets and efficient CPU execution.
# - StandardScaler ensures stable optimization without heavy computation.
# - Pipeline keeps preprocessing reproducible and avoids unnecessary complexity.