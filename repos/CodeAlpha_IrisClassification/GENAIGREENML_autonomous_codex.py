# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier

warnings.filterwarnings("ignore")

DATASET_PATH = "Iris.csv"
DATASET_HEADERS = "Id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species"

def load_dataset(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            df = pd.read_csv(path, sep=';', decimal=',')
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    return df

def normalize_columns(cols):
    return [re.sub(r'\s+', ' ', str(c).strip()) for c in cols]

def is_id_like(col, series):
    lc = col.lower()
    if lc == 'id' or lc.endswith('id') or lc.startswith('id') or lc.endswith('_id') or lc.startswith('id_'):
        if series.nunique(dropna=True) == len(series):
            return True
    return False

def select_target_column(df, expected_headers):
    candidates = []
    for col in df.columns:
        lc = col.lower()
        if any(k in lc for k in ['target', 'label', 'class', 'species']):
            if not is_id_like(col, df[col]):
                candidates.append(col)
    for col in candidates:
        if df[col].nunique(dropna=True) >= 2:
            return col
    for h in expected_headers[::-1]:
        if h in df.columns and df[h].nunique(dropna=True) >= 2 and not is_id_like(h, df[h]):
            return h
    numeric_candidates = []
    for col in df.columns:
        if is_id_like(col, df[col]):
            continue
        series_num = pd.to_numeric(df[col], errors='coerce')
        if series_num.notna().mean() >= 0.9 and series_num.nunique(dropna=True) >= 2:
            var = series_num.var()
            numeric_candidates.append((col, var if pd.notna(var) else -np.inf))
    if numeric_candidates:
        numeric_candidates.sort(key=lambda x: x[1])
        return numeric_candidates[-1][0]
    for col in reversed(df.columns):
        if df[col].nunique(dropna=True) >= 2 and not is_id_like(col, df[col]):
            return col
    return df.columns[-1]

df = load_dataset(DATASET_PATH)
df.columns = normalize_columns(df.columns)
df = df.loc[:, ~df.columns.str.contains(r'^Unnamed', case=False, regex=True)]

expected_headers = [h for h in normalize_columns(DATASET_HEADERS.split(',')) if h]
lower_actual = {c.lower(): c for c in df.columns}
rename_map = {}
for h in expected_headers:
    lc = h.lower()
    if lc in lower_actual:
        actual = lower_actual[lc]
        if actual != h and h not in df.columns:
            rename_map[actual] = h
if rename_map:
    df = df.rename(columns=rename_map)

assert df.shape[0] > 0 and df.shape[1] > 0

target_col = select_target_column(df, expected_headers)
df = df.dropna(subset=[target_col])

id_cols = []
for col in df.columns:
    if col == target_col:
        continue
    if is_id_like(col, df[col]):
        id_cols.append(col)
if id_cols:
    df = df.drop(columns=id_cols)

feature_cols = [c for c in df.columns if c != target_col]
if len(feature_cols) == 0:
    df['constant'] = 0.0
    feature_cols = ['constant']

X = df[feature_cols].copy()
y_raw = df[target_col].copy()

numeric_features = []
categorical_features = []
cols_to_drop = []
for col in list(X.columns):
    series = X[col]
    if series.notna().sum() == 0:
        cols_to_drop.append(col)
        continue
    converted = pd.to_numeric(series, errors='coerce')
    non_na_ratio = converted.notna().mean()
    if pd.api.types.is_numeric_dtype(series) or non_na_ratio >= 0.9:
        if converted.notna().sum() == 0:
            cols_to_drop.append(col)
        else:
            X[col] = converted
            numeric_features.append(col)
    else:
        categorical_features.append(col)
if cols_to_drop:
    X = X.drop(columns=cols_to_drop)
    numeric_features = [c for c in numeric_features if c not in cols_to_drop]
    categorical_features = [c for c in categorical_features if c not in cols_to_drop]

if X.shape[1] == 0:
    X['constant'] = 0.0
    numeric_features = ['constant']
    categorical_features = []

X_full = X.copy()
y_raw_full = y_raw.copy()

y_numeric = pd.to_numeric(y_raw, errors='coerce')
numeric_ratio = y_numeric.notna().mean()
num_unique = y_raw.nunique(dropna=True)

is_int_like = False
if y_numeric.notna().sum() > 0:
    is_int_like = np.allclose(y_numeric.dropna(), np.round(y_numeric.dropna()))

if y_raw.dtype == object or str(y_raw.dtype).startswith('category'):
    task = 'classification'
elif numeric_ratio >= 0.9:
    if num_unique <= 20 and is_int_like:
        task = 'classification'
    else:
        task = 'regression'
else:
    task = 'classification'

if task == 'classification' and num_unique < 2:
    task = 'trivial'

if task == 'regression':
    y = y_numeric
    mask = y.notna()
    X = X_full.loc[mask].copy()
    y = y.loc[mask]
    if len(y) < 2:
        task = 'classification'
        X = X_full.copy()
        y = y_raw_full.copy()
else:
    X = X_full.copy()
    y = y_raw_full.copy()

X = X.replace([np.inf, -np.inf], np.nan)

assert len(X) > 0

n_samples = len(X)
if n_samples < 2:
    task = 'trivial'
    X_train = X
    X_test = X
    y_train = y
    y_test = y
else:
    test_size = max(0.2, 1.0 / n_samples)
    stratify = None
    if task == 'classification':
        value_counts = y.value_counts()
        if len(value_counts) > 1 and value_counts.min() >= 2:
            stratify = y
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=stratify)
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42)

assert len(X_train) > 0 and len(X_test) > 0

if task == 'classification':
    unique_train = pd.Series(y_train).nunique(dropna=True)
    if unique_train < 2:
        task = 'trivial'

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler(with_mean=False))
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

transformers = []
if len(numeric_features) > 0:
    transformers.append(('num', numeric_transformer, numeric_features))
if len(categorical_features) > 0:
    transformers.append(('cat', categorical_transformer, categorical_features))
if len(transformers) == 0:
    transformers = [('num', numeric_transformer, X.columns.tolist())]

preprocess = ColumnTransformer(transformers=transformers, remainder='drop')

if task == 'classification':
    model = LogisticRegression(max_iter=200, solver='liblinear')
elif task == 'regression':
    model = Ridge(alpha=1.0)
else:
    model = DummyClassifier(strategy='most_frequent')

clf = Pipeline(steps=[('preprocess', preprocess), ('model', model)])

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if task == 'regression':
    r2 = r2_score(y_test, y_pred)
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))
else:
    accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Lightweight linear models (LogisticRegression/Ridge/DummyClassifier) minimize CPU use and are suitable for small datasets.
# - ColumnTransformer with simple imputation, scaling, and one-hot encoding provides reproducible preprocessing without heavy feature engineering.
# - Regression fallback maps R^2 to a bounded (r2+1)/2 score in [0,1] for consistent ACCURACY reporting.