# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os, json, re
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.naive_bayes import MultinomialNB
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, r2_score
from sklearn.feature_extraction.text import TfidfVectorizer

DATASET_PATH = "data/input.json"

def load_dataset(path):
    df = None
    if os.path.exists(path):
        if path.lower().endswith(".json"):
            try:
                df = pd.read_json(path)
            except Exception:
                df = None
            if isinstance(df, pd.Series):
                df = df.to_frame()
            if df is not None and (df.shape[0] == 0 or df.shape[1] == 0):
                df = None
            if df is None:
                try:
                    df = pd.read_json(path, lines=True)
                except Exception:
                    df = None
            if df is None:
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                    if isinstance(data, dict) and len(data) == 1:
                        only_val = list(data.values())[0]
                        if isinstance(only_val, (list, dict)):
                            data = only_val
                    df = pd.json_normalize(data)
                except Exception:
                    df = None
        if df is None:
            try:
                df = pd.read_csv(path)
                if df.shape[1] == 1:
                    df_alt = pd.read_csv(path, sep=";", decimal=",")
                    if df_alt.shape[1] > df.shape[1]:
                        df = df_alt
            except Exception:
                try:
                    df = pd.read_csv(path, sep=";", decimal=",")
                except Exception:
                    df = None
    if df is None:
        df = pd.DataFrame()
    if df is not None and not df.empty and df.shape[1] == 1:
        sample = df.iloc[0, 0]
        if isinstance(sample, (dict, list)):
            try:
                df = pd.json_normalize(df.iloc[:, 0])
            except Exception:
                pass
    if not isinstance(df, pd.DataFrame):
        df = pd.DataFrame(df)
    return df

def clean_columns(df):
    df = df.copy()
    df.columns = [re.sub(r"\s+", " ", str(c).strip()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.match(r"^Unnamed")]
    if df.columns.duplicated().any():
        new_cols = []
        counts = {}
        for c in df.columns:
            if c in counts:
                counts[c] += 1
                new_cols.append(f"{c}_{counts[c]}")
            else:
                counts[c] = 0
                new_cols.append(c)
        df.columns = new_cols
    return df

def select_target(df):
    cols = [c for c in df.columns if not df[c].isna().all()]
    if not cols:
        return None
    non_constant = [c for c in cols if df[c].nunique(dropna=True) > 1]
    if not non_constant:
        non_constant = cols
    keywords = ["target", "label", "class", "y", "intent", "output"]
    for c in non_constant:
        name = str(c).lower()
        for kw in keywords:
            if name == kw or name.endswith(kw):
                return c
    n_rows = len(df)
    numeric_like = set()
    for c in non_constant:
        if pd.api.types.is_numeric_dtype(df[c]):
            numeric_like.add(c)
        else:
            coerced = pd.to_numeric(df[c], errors="coerce")
            if coerced.notna().sum() >= max(1, int(0.8 * n_rows)):
                numeric_like.add(c)
    for c in non_constant:
        if c in numeric_like:
            continue
        nunique = df[c].nunique(dropna=True)
        if nunique > 1 and nunique <= max(20, n_rows // 2 if n_rows > 0 else 1):
            return c
    for c in non_constant:
        if c in numeric_like:
            if pd.to_numeric(df[c], errors="coerce").nunique(dropna=True) > 1:
                return c
    return non_constant[0]

def determine_problem_type(y):
    if pd.api.types.is_bool_dtype(y):
        return "classification"
    if pd.api.types.is_object_dtype(y) or pd.api.types.is_categorical_dtype(y):
        y_num = pd.to_numeric(y, errors="coerce")
        if y_num.notna().sum() >= max(1, int(0.8 * len(y))) and y_num.nunique(dropna=True) > 20:
            return "regression"
        return "classification"
    y_num = pd.to_numeric(y, errors="coerce")
    nunique = y_num.nunique(dropna=True)
    if nunique <= 20 and nunique < max(2, len(y_num) / 2):
        if (y_num.dropna() % 1 == 0).all():
            return "classification"
    return "regression"

def extract_text_column(x):
    if isinstance(x, pd.DataFrame):
        s = x.iloc[:, 0]
    elif isinstance(x, pd.Series):
        s = x
    else:
        arr = np.array(x)
        if arr.ndim == 1:
            s = pd.Series(arr)
        else:
            s = pd.Series(arr[:, 0])
    return s.astype(str)

df = load_dataset(DATASET_PATH)
df = clean_columns(df)
if df is None or df.empty:
    df = pd.DataFrame({"__dummy__": [0], "__target__": [0]})
for c in df.columns:
    if df[c].apply(lambda v: isinstance(v, (dict, list))).any():
        df[c] = df[c].apply(lambda v: json.dumps(v) if isinstance(v, (dict, list)) else v)
target_col = select_target(df)
if target_col is None or target_col not in df.columns:
    df["__target__"] = 0
    target_col = "__target__"
features = [c for c in df.columns if c != target_col]
if not features:
    df["__dummy__"] = 0.0
    features = ["__dummy__"]
y_raw = df[target_col]
problem_type = determine_problem_type(y_raw)
if problem_type == "regression":
    y_num = pd.to_numeric(y_raw, errors="coerce")
    mask = y_num.notna()
    X = df.loc[mask, features].copy()
    y = y_num.loc[mask]
else:
    mask = y_raw.notna()
    X = df.loc[mask, features].copy()
    y = y_raw.loc[mask].astype(str)
if len(X) == 0:
    X = pd.DataFrame({"__dummy__": [0.0]})
    y = pd.Series([0.0])
    problem_type = "regression"
    features = ["__dummy__"]
assert len(X) > 0

numeric_cols = []
for c in features:
    if c not in X.columns:
        continue
    if pd.api.types.is_numeric_dtype(X[c]):
        X[c] = pd.to_numeric(X[c], errors="coerce")
        numeric_cols.append(c)
    else:
        coerced = pd.to_numeric(X[c], errors="coerce")
        if coerced.notna().sum() >= max(1, int(0.8 * len(X))):
            X[c] = coerced
            numeric_cols.append(c)
for c in numeric_cols:
    X[c] = X[c].replace([np.inf, -np.inf], np.nan)
cat_cols = [c for c in features if c in X.columns and c not in numeric_cols]
text_cols = []
cat_cols_final = []
for c in cat_cols:
    col_str = X[c].fillna("").astype(str)
    avg_len = col_str.str.len().mean() if len(col_str) > 0 else 0
    nunique = X[c].nunique(dropna=True)
    if avg_len > 30 or (nunique > 50 and avg_len > 10):
        text_cols.append(c)
    else:
        cat_cols_final.append(c)
    X[c] = col_str
text_feature_col = None
if text_cols:
    X["__text__"] = X[text_cols].fillna("").astype(str).agg(" ".join, axis=1)
    X["__text__"] = X["__text__"].fillna("")
    X = X.drop(columns=text_cols)
    text_feature_col = "__text__"
if text_feature_col is not None:
    if X[text_feature_col].astype(str).str.strip().eq("").all():
        X = X.drop(columns=[text_feature_col])
        text_feature_col = None
numeric_cols = [c for c in numeric_cols if c in X.columns and not X[c].isna().all()]
cat_cols_final = [c for c in cat_cols_final if c in X.columns and not X[c].isna().all()]
if not numeric_cols and not cat_cols_final and text_feature_col is None:
    X["__dummy__"] = 0.0
    numeric_cols = ["__dummy__"]

transformers = []
if numeric_cols:
    numeric_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler(with_mean=False))])
    transformers.append(("num", numeric_transformer, numeric_cols))
if cat_cols_final:
    cat_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="most_frequent")), ("onehot", OneHotEncoder(handle_unknown="ignore"))])
    transformers.append(("cat", cat_transformer, cat_cols_final))
if text_feature_col is not None:
    text_transformer = Pipeline(steps=[("selector", FunctionTransformer(extract_text_column, validate=False)), ("tfidf", TfidfVectorizer(max_features=1000))])
    transformers.append(("text", text_transformer, [text_feature_col]))
preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

n_samples = len(X)
if n_samples < 2:
    X_train = X_test = X
    y_train = y_test = y
else:
    test_size = max(1, int(round(0.2 * n_samples)))
    if n_samples - test_size < 1:
        test_size = n_samples - 1
    test_ratio = test_size / n_samples if n_samples > 0 else 0.5
    stratify = None
    if problem_type == "classification" and y.nunique() >= 2:
        class_counts = y.value_counts()
        if class_counts.min() >= 2:
            stratify = y
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=42, stratify=stratify)
if len(X_train) == 0 or len(X_test) == 0:
    X_train = X_test = X
    y_train = y_test = y
assert len(X_train) > 0 and len(X_test) > 0

if problem_type == "classification":
    class_count = y.nunique()
    if class_count < 2 or len(X) < 2 or y_train.nunique() < 2:
        model = DummyClassifier(strategy="most_frequent")
    else:
        if text_feature_col is not None and not numeric_cols and not cat_cols_final:
            model = MultinomialNB()
        else:
            model = LogisticRegression(max_iter=200, solver="liblinear", random_state=42)
else:
    if len(y_train) < 2:
        model = DummyRegressor(strategy="mean")
    else:
        model = Ridge(alpha=1.0, random_state=42)

model_pipeline = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])
model_pipeline.fit(X_train, y_train)
y_pred = model_pipeline.predict(X_test)

if problem_type == "classification":
    accuracy = accuracy_score(y_test, y_pred)
else:
    y_test_num = pd.to_numeric(pd.Series(y_test), errors="coerce")
    y_pred_num = pd.to_numeric(pd.Series(y_pred), errors="coerce")
    if len(y_test_num) < 2 or y_test_num.isna().all():
        accuracy = 0.0
        if len(y_test_num) > 0 and np.allclose(y_test_num.fillna(0).to_numpy(), y_pred_num.fillna(0).to_numpy()):
            accuracy = 1.0
    else:
        try:
            score = r2_score(y_test_num, y_pred_num)
        except Exception:
            score = np.nan
        if np.isnan(score):
            diff = np.abs(y_test_num - y_pred_num)
            mae = np.nanmean(diff)
            denom = np.nanmean(np.abs(y_test_num - np.nanmean(y_test_num)))
            if denom == 0 or np.isnan(denom):
                score = 0.0
            else:
                score = 1.0 - mae / denom
        accuracy = float(np.clip((score + 1.0) / 2.0, 0.0, 1.0))
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear/Dummy models and limited TF-IDF dimensionality for CPU efficiency.
# - Employed a compact preprocessing pipeline with simple imputers and encoders for robustness.
# - Applied a bounded R2-to-accuracy proxy for regression to keep output in [0,1].