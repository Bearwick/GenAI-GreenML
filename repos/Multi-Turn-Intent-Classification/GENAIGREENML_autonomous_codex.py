# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import json
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.base import BaseEstimator, TransformerMixin

warnings.filterwarnings("ignore")

class TextCombiner(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        if isinstance(X, pd.DataFrame):
            df = X
        else:
            df = pd.DataFrame(X)
        combined = df.fillna("").astype(str).agg(" ".join, axis=1)
        return combined

def load_df(path):
    df = None
    try:
        df = pd.read_json(path)
    except Exception:
        df = None
    if isinstance(df, pd.Series):
        df = df.to_frame()
    if df is None or df.empty:
        try:
            df = pd.read_json(path, lines=True)
        except Exception:
            df = None
    if isinstance(df, pd.Series):
        df = df.to_frame()
    if df is None or df.empty:
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, dict):
                list_data = None
                for v in data.values():
                    if isinstance(v, list):
                        list_data = v
                        break
                if list_data is not None:
                    data = list_data
            df = pd.DataFrame(data)
        except Exception:
            df = None
    if df is None or df.empty:
        try:
            df = pd.read_csv(path)
            if df.shape[1] == 1:
                df_alt = pd.read_csv(path, sep=";", decimal=",")
                if df_alt.shape[1] > 1:
                    df = df_alt
        except Exception:
            try:
                df = pd.read_csv(path, sep=";", decimal=",")
            except Exception:
                df = pd.DataFrame()
    return df

def clean_columns(df):
    cols = []
    for c in df.columns:
        if isinstance(c, str):
            c2 = re.sub(r"\s+", " ", c.strip())
        else:
            c2 = str(c)
        cols.append(c2)
    df.columns = cols
    df = df.loc[:, [c for c in df.columns if not re.match(r"^Unnamed: ?\d+$", c)]]
    return df

def infer_numeric_cols(df, threshold=0.8):
    numeric_cols = []
    for col in df.columns:
        series = df[col]
        if pd.api.types.is_numeric_dtype(series):
            numeric_cols.append(col)
        else:
            converted = pd.to_numeric(series, errors="coerce")
            non_na_ratio = converted.notna().mean()
            if non_na_ratio > threshold:
                df[col] = converted
                numeric_cols.append(col)
    return numeric_cols

def safe_nunique(series):
    try:
        return series.nunique(dropna=True)
    except Exception:
        return series.apply(lambda v: json.dumps(v, ensure_ascii=False) if isinstance(v, (dict, list)) else str(v)).nunique(dropna=True)

def select_target(df, numeric_cols):
    candidate_keys = ["target", "label", "intent", "class", "y", "output", "category"]
    for c in df.columns:
        cname = str(c).lower()
        for key in candidate_keys:
            if key == cname or cname.endswith(key) or key in cname:
                if safe_nunique(df[c]) > 1:
                    return c
    for c in numeric_cols:
        if c in df.columns and safe_nunique(df[c]) > 1:
            return c
    for c in df.columns:
        if safe_nunique(df[c]) > 1:
            return c
    return df.columns[0]

def determine_task(y):
    n_unique = safe_nunique(y)
    n_samples = len(y)
    is_numeric = pd.api.types.is_numeric_dtype(y)
    if is_numeric:
        unique_ratio = n_unique / max(1, n_samples)
        if n_unique <= 20 or unique_ratio <= 0.2:
            return "classification"
        else:
            return "regression"
    else:
        return "classification"

def main():
    path = "data/input.json"
    df = load_df(path)
    df = clean_columns(df)
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    if df is None or df.empty:
        df = pd.DataFrame({"__dummy__": [0, 1]})
    numeric_cols = infer_numeric_cols(df)
    df = df.loc[:, df.notna().any()].copy()
    if df.shape[1] == 0:
        df["__dummy__"] = 0
        numeric_cols = ["__dummy__"]
    numeric_cols = [c for c in numeric_cols if c in df.columns]
    assert df.shape[0] > 0

    target = select_target(df, numeric_cols)
    if target not in df.columns:
        target = df.columns[0]
    df = df[df[target].notna()].copy()
    if df.empty:
        df = pd.DataFrame({"__dummy__": [0, 1]})
        target = "__dummy__"
    y = df[target]
    X = df.drop(columns=[target])

    if y.dtype == "object" or isinstance(y.iloc[0], (dict, list)):
        y = y.apply(lambda v: json.dumps(v, ensure_ascii=False) if isinstance(v, (dict, list)) else v)
        y = y.astype(str)

    task = determine_task(y)

    if task == "regression":
        y_numeric = pd.to_numeric(y, errors="coerce")
        mask = y_numeric.notna()
        if mask.sum() == 0:
            task = "classification"
            y = df[target].apply(lambda v: json.dumps(v, ensure_ascii=False) if isinstance(v, (dict, list)) else v).astype(str)
            X = df.drop(columns=[target])
        else:
            X = X.loc[mask]
            y = y_numeric.loc[mask]

    if len(y) < 2:
        X = pd.concat([X, X], ignore_index=True)
        y = pd.concat([y, y], ignore_index=True)

    numeric_cols = []
    for col in X.columns:
        series = X[col]
        if pd.api.types.is_numeric_dtype(series):
            numeric_cols.append(col)
        else:
            converted = pd.to_numeric(series, errors="coerce")
            non_na_ratio = converted.notna().mean()
            if non_na_ratio > 0.8:
                X[col] = converted
                numeric_cols.append(col)

    X.replace([np.inf, -np.inf], np.nan, inplace=True)
    non_all_na_cols = [c for c in X.columns if X[c].notna().any()]
    X = X[non_all_na_cols]
    numeric_cols = [c for c in numeric_cols if c in X.columns]

    if X.shape[1] == 0:
        X = pd.DataFrame({"__dummy__": np.ones(len(y))})
        numeric_cols = ["__dummy__"]

    object_cols = [c for c in X.columns if c not in numeric_cols]

    text_cols = []
    categorical_cols = []
    for c in object_cols:
        series_str = X[c].fillna("").astype(str)
        avg_len = series_str.map(len).mean()
        unique_ratio = series_str.nunique(dropna=True) / max(1, len(series_str))
        if avg_len > 30 or unique_ratio > 0.5:
            text_cols.append(c)
        else:
            categorical_cols.append(c)

    if text_cols:
        combined = X[text_cols].fillna("").astype(str).agg(" ".join, axis=1)
        if combined.str.strip().eq("").all():
            text_cols = []

    transformers = []
    if numeric_cols:
        numeric_transformer = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False))
        ])
        transformers.append(("num", numeric_transformer, list(numeric_cols)))
    if categorical_cols:
        categorical_transformer = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=True, dtype=np.float32))
        ])
        transformers.append(("cat", categorical_transformer, list(categorical_cols)))
    if text_cols:
        text_transformer = Pipeline(steps=[
            ("combine", TextCombiner()),
            ("tfidf", TfidfVectorizer(max_features=1000))
        ])
        transformers.append(("text", text_transformer, list(text_cols)))

    if not transformers:
        X = pd.DataFrame({"__dummy__": np.ones(len(y))})
        numeric_cols = ["__dummy__"]
        numeric_transformer = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False))
        ])
        transformers = [("num", numeric_transformer, list(numeric_cols))]

    preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    if task == "classification":
        n_classes = safe_nunique(y)
        if n_classes < 2:
            model = DummyClassifier(strategy="most_frequent")
        else:
            model = LogisticRegression(max_iter=200, solver="liblinear", n_jobs=1)
    else:
        if safe_nunique(y) < 2:
            model = DummyRegressor(strategy="mean")
        else:
            model = Ridge(alpha=1.0)

    clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

    n_samples = len(y)
    test_size = 0.2 if n_samples >= 5 else 0.5
    stratify = None
    if task == "classification" and safe_nunique(y) >= 2:
        vc = y.value_counts()
        if vc.min() >= 2:
            stratify = y
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=stratify)
    assert len(y_train) > 0 and len(y_test) > 0

    if task == "classification" and safe_nunique(y_train) < 2:
        model = DummyClassifier(strategy="most_frequent")
        clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    if task == "classification":
        accuracy = accuracy_score(y_test, y_pred)
    else:
        r2 = r2_score(y_test, y_pred)
        if not np.isfinite(r2):
            r2 = 0.0
        accuracy = max(0.0, min(1.0, float(r2)))

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Used lightweight linear models with sparse encodings to remain CPU- and energy-efficient.
# - Limited text vectorization dimensionality and applied simple imputers/scalers for fast preprocessing.
# - Regression accuracy uses a clipped R2 score to stay in [0,1] while remaining stable.