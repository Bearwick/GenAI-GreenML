# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import json
import csv
import random
from transformers import pipeline, set_seed, logging as hf_logging

def set_reproducible(seed=42):
    os.environ["PYTHONHASHSEED"] = str(seed)
    os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
    random.seed(seed)
    try:
        import numpy as np
        np.random.seed(seed)
    except Exception:
        pass
    try:
        import torch
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    except Exception:
        pass
    try:
        set_seed(seed)
    except Exception:
        pass

class IntentDetector:
    def __init__(self):
        self.intent_options = (
            "Book Appointment",
            "Product Inquiry",
            "Pricing Negotiation",
            "Support Request",
            "Follow-Up"
        )
        self.intent_pipeline = pipeline(
            task="zero-shot-classification",
            model="cross-encoder/nli-distilroberta-base",
        )

    def classify_batch(self, dialogues):
        if not dialogues:
            return []
        outputs = self.intent_pipeline(dialogues, self.intent_options)
        if isinstance(outputs, dict):
            outputs = [outputs]
        return [out["labels"][0] for out in outputs]

def create_conversation(messages, max_messages=None):
    if not messages:
        return ""
    if max_messages is not None:
        messages = messages[-max_messages:]
    lines = []
    for m in messages:
        if isinstance(m, dict):
            sender = m.get("sender", "")
            text = m.get("text", "")
            lines.append(f"{sender.capitalize()}: {text}")
        else:
            lines.append(str(m))
    return "\n".join(lines)

def load_conversations(path):
    with open(path, "r", encoding="utf-8") as infile:
        data = json.load(infile)
    headers = None
    entries = []
    if isinstance(data, dict):
        headers = data.get("DATASET_HEADERS") or data.get("headers")
        if "data" in data:
            entries = data.get("data", [])
        elif "conversations" in data:
            entries = data.get("conversations", [])
        elif "items" in data:
            entries = data.get("items", [])
        elif all(isinstance(v, dict) for v in data.values()):
            entries = list(data.values())
    elif isinstance(data, list):
        entries = data
    if headers and entries and not isinstance(entries[0], dict):
        mapped = []
        for row in entries:
            if isinstance(row, dict):
                mapped.append(row)
            elif isinstance(row, (list, tuple)):
                mapped.append(dict(zip(headers, row)))
        entries = mapped
    return entries, headers

def infer_label_key(entries, headers=None):
    keys = []
    if headers:
        keys = [k for k in headers if isinstance(k, str)]
    elif entries and isinstance(entries[0], dict):
        keys = list(entries[0].keys())
    if not keys:
        return None
    for key in keys:
        kl = key.lower()
        if "predicted" in kl or "rationale" in kl:
            continue
        if "intent" in kl or "label" in kl or "class" in kl:
            return key
    return None

def infer_conv_id_key(entries, headers=None):
    keys = []
    if headers:
        keys = [k for k in headers if isinstance(k, str)]
    elif entries and isinstance(entries[0], dict):
        keys = list(entries[0].keys())
    for key in keys:
        kl = key.lower()
        if "conversation" in kl and "id" in kl:
            return key
    return None

def find_messages_list(entry):
    if not isinstance(entry, dict):
        return None
    for key, val in entry.items():
        if not isinstance(val, list):
            continue
        kl = key.lower()
        if "message" in kl or "turn" in kl or "dialogue" in kl or "conversation" in kl:
            return val
    for val in entry.values():
        if isinstance(val, list) and val:
            first = val[0]
            if isinstance(first, dict) and ("text" in first or "message" in first):
                return val
            if isinstance(first, str):
                return val
    for val in entry.values():
        if isinstance(val, list):
            return val
    return None

def extract_dialogue(entry):
    messages = find_messages_list(entry)
    if messages is not None:
        return create_conversation(messages)
    for key, val in entry.items():
        if isinstance(val, str):
            kl = key.lower()
            if kl in ("text", "conversation", "dialogue", "message"):
                return val
    return ""

def predict_intents(input_file, json_output, csv_output, batch_size=8):
    entries, headers = load_conversations(input_file)
    label_key = infer_label_key(entries, headers)
    conv_id_key = infer_conv_id_key(entries, headers)
    detector = IntentDetector()
    output_data = []
    batch_texts = []
    batch_meta = []
    correct = 0
    total = 0
    append_output = output_data.append
    for entry in entries:
        dialogue = extract_dialogue(entry)
        conv_id = entry.get(conv_id_key) if conv_id_key else None
        label = entry.get(label_key) if label_key else None
        batch_texts.append(dialogue)
        batch_meta.append((conv_id, label))
        if len(batch_texts) >= batch_size:
            preds = detector.classify_batch(batch_texts)
            for (cid, lab), pred in zip(batch_meta, preds):
                pred_lower = pred.lower()
                rationale = f"Based on the conversation, the customer is likely interested in '{pred_lower}'."
                append_output({
                    "conversation_id": cid,
                    "predicted_intent": pred,
                    "rationale": rationale
                })
                if lab is not None:
                    total += 1
                    if str(lab).strip().lower() == pred_lower:
                        correct += 1
            batch_texts.clear()
            batch_meta.clear()
    if batch_texts:
        preds = detector.classify_batch(batch_texts)
        for (cid, lab), pred in zip(batch_meta, preds):
            pred_lower = pred.lower()
            rationale = f"Based on the conversation, the customer is likely interested in '{pred_lower}'."
            append_output({
                "conversation_id": cid,
                "predicted_intent": pred,
                "rationale": rationale
            })
            if lab is not None:
                total += 1
                if str(lab).strip().lower() == pred_lower:
                    correct += 1
    os.makedirs(os.path.dirname(json_output) or ".", exist_ok=True)
    with open(json_output, "w", encoding="utf-8") as json_file:
        json.dump(output_data, json_file, indent=2)
    with open(csv_output, "w", newline="", encoding="utf-8") as csv_file:
        fieldnames = ["conversation_id", "predicted_intent", "rationale"]
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(output_data)
    accuracy = correct / total if total else 0.0
    print(f"ACCURACY={accuracy:.6f}")

def main():
    predict_intents(
        input_file="data/input.json",
        json_output="data/output/predictions.json",
        csv_output="data/output/predictions.csv"
    )

if __name__ == "__main__":
    hf_logging.set_verbosity_error()
    set_reproducible(42)
    main()

# Optimization Summary
# - Batched inference reduces repeated pipeline overhead and improves throughput.
# - Removed unused preprocessing and imports to lower memory and startup cost.
# - Derived keys from input schema to avoid unnecessary assumptions and data handling.
# - Reused buffers and local bindings to minimize allocations inside loops.
# - Fixed random seeds and disabled tokenizer parallelism for reproducible, efficient runs.