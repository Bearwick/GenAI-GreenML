# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import json
import csv
from typing import List, Optional, Any
from transformers import pipeline, set_seed

set_seed(42)

class IntentDetector:
    def __init__(self):
        self.intent_options = [
            "Book Appointment",
            "Product Inquiry",
            "Pricing Negotiation",
            "Support Request",
            "Follow-Up"
        ]
        self.intent_pipeline = pipeline(
            task="zero-shot-classification",
            model="cross-encoder/nli-distilroberta-base"
        )

    def classify_batch(self, dialogues: List[str]) -> List[dict]:
        if not dialogues:
            return []
        results = self.intent_pipeline(dialogues, self.intent_options)
        if isinstance(results, dict):
            results = [results]
        template = "Based on the conversation, the customer is likely interested in '{intent}'."
        output = []
        for res in results:
            top_intent = res["labels"][0]
            output.append({
                "predicted_intent": top_intent,
                "rationale": template.format(intent=top_intent.lower())
            })
        return output

intent_model = IntentDetector()

def create_conversation(messages: List[dict], max_messages: Optional[int] = None) -> str:
    if not messages:
        return ""
    if max_messages is not None:
        messages = messages[-max_messages:]
    return "\n".join(
        f"{str(m.get('sender', '')).capitalize()}: {str(m.get('text', ''))}"
        for m in messages
    )

def load_conversations(path: str) -> List[dict]:
    with open(path, "r") as infile:
        data = json.load(infile)
    if isinstance(data, dict):
        for key in ("conversations", "data", "items"):
            value = data.get(key)
            if isinstance(value, list):
                return value
        if "messages" in data:
            return [data]
    return data if isinstance(data, list) else []

def detect_label_key(conversations: List[dict]) -> Optional[str]:
    if not conversations:
        return None
    keys = set()
    for entry in conversations:
        if isinstance(entry, dict):
            keys.update(entry.keys())
    if not keys:
        return None
    def priority(key: str) -> tuple:
        lk = key.lower()
        if "pred" in lk:
            return (9, key)
        if "true" in lk or "gold" in lk:
            return (0, key)
        if "label" in lk:
            return (1, key)
        if "intent" in lk:
            return (2, key)
        return (3, key)
    candidates = [k for k in keys if ("intent" in k.lower() or "label" in k.lower()) and "pred" not in k.lower()]
    if candidates:
        return sorted(candidates, key=priority)[0]
    remaining = [k for k in keys if k not in {"conversation_id", "messages"}]
    if len(remaining) == 1:
        return remaining[0]
    return None

def process_batch(ids: List[Any], results: List[dict], labels: Optional[List[Any]], output_data: List[dict]) -> tuple:
    correct = 0
    total = 0
    if labels is None:
        for cid, res in zip(ids, results):
            output_data.append({
                "conversation_id": cid,
                "predicted_intent": res["predicted_intent"],
                "rationale": res["rationale"]
            })
    else:
        for cid, res, lbl in zip(ids, results, labels):
            output_data.append({
                "conversation_id": cid,
                "predicted_intent": res["predicted_intent"],
                "rationale": res["rationale"]
            })
            if lbl is not None:
                total += 1
                if str(res["predicted_intent"]).lower() == str(lbl).lower():
                    correct += 1
    return correct, total

def write_outputs(output_data: List[dict], json_output: str, csv_output: str) -> None:
    with open(json_output, "w") as json_file:
        json.dump(output_data, json_file, indent=2)
    with open(csv_output, "w", newline="") as csv_file:
        fieldnames = ["conversation_id", "predicted_intent", "rationale"]
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(output_data)

def predict_intents(input_file: str, json_output: str, csv_output: str, batch_size: int = 8) -> float:
    conversations = load_conversations(input_file)
    label_key = detect_label_key(conversations)
    output_data = []
    correct = 0
    total = 0
    batch_dialogues = []
    batch_ids = []
    batch_labels = [] if label_key else None
    for entry in conversations:
        if not isinstance(entry, dict):
            continue
        batch_ids.append(entry.get("conversation_id"))
        batch_dialogues.append(create_conversation(entry.get("messages", [])))
        if batch_labels is not None:
            batch_labels.append(entry.get(label_key))
        if len(batch_dialogues) >= batch_size:
            results = intent_model.classify_batch(batch_dialogues)
            batch_correct, batch_total = process_batch(batch_ids, results, batch_labels, output_data)
            correct += batch_correct
            total += batch_total
            batch_dialogues.clear()
            batch_ids.clear()
            if batch_labels is not None:
                batch_labels.clear()
    if batch_dialogues:
        results = intent_model.classify_batch(batch_dialogues)
        batch_correct, batch_total = process_batch(batch_ids, results, batch_labels, output_data)
        correct += batch_correct
        total += batch_total
    write_outputs(output_data, json_output, csv_output)
    return correct / total if total else 0.0

if __name__ == "__main__":
    os.makedirs("data/output", exist_ok=True)
    accuracy = predict_intents(
        input_file="data/input.json",
        json_output="data/output/predictions.json",
        csv_output="data/output/predictions.csv"
    )
    print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Removed unused preprocessing and imports to reduce runtime and memory footprint.
# - Batched inference to lower pipeline call overhead and improve throughput.
# - Incremental batch processing minimizes intermediate data storage.
# - Added deterministic seeding for reproducible, stable results.