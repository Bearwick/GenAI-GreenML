# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import json
import csv
from transformers import pipeline, set_seed

os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
SEED = 42
set_seed(SEED)


class IntentDetector:
    def __init__(self, intent_options=None):
        self.intent_options = intent_options if intent_options is not None else [
            "Book Appointment",
            "Product Inquiry",
            "Pricing Negotiation",
            "Support Request",
            "Follow-Up"
        ]
        self.intent_pipeline = pipeline(
            task="zero-shot-classification",
            model="cross-encoder/nli-distilroberta-base"
        )

    def classify_batch(self, dialogues):
        return self.intent_pipeline(dialogues, self.intent_options) if dialogues else []


def create_conversation(messages, max_messages=None):
    if max_messages is not None:
        messages = messages[-max_messages:]
    return "\n".join(
        f"{(m.get('sender') or '').capitalize()}: {m.get('text') or ''}"
        for m in messages
    )


def detect_label_key(entries):
    for entry in entries:
        if isinstance(entry, dict):
            for key in entry.keys():
                if isinstance(key, str):
                    key_lower = key.lower()
                    if "intent" in key_lower and key_lower not in ("predicted_intent", "rationale"):
                        return key
    return None


def predict_intents(input_file, json_output, csv_output, model):
    with open(input_file, "r") as infile:
        conversations = json.load(infile)

    formatted_texts = [create_conversation(entry.get("messages", [])) for entry in conversations]
    classifications = model.classify_batch(formatted_texts)

    label_key = detect_label_key(conversations)
    correct = 0
    total = 0

    output_data = []
    for entry, classification in zip(conversations, classifications):
        labels = classification.get("labels") if isinstance(classification, dict) else []
        top_intent = labels[0] if labels else None
        top_intent_lower = top_intent.lower() if isinstance(top_intent, str) else ""
        explanation = f"Based on the conversation, the customer is likely interested in '{top_intent_lower}'."
        output_record = {
            "conversation_id": entry.get("conversation_id"),
            "predicted_intent": top_intent,
            "rationale": explanation
        }
        output_data.append(output_record)

        if label_key is not None:
            true_label = entry.get(label_key)
            if true_label is not None:
                total += 1
                if str(true_label).strip().lower() == top_intent_lower:
                    correct += 1

    for path in (json_output, csv_output):
        dir_path = os.path.dirname(path)
        if dir_path:
            os.makedirs(dir_path, exist_ok=True)

    with open(json_output, "w") as json_file:
        json.dump(output_data, json_file, indent=2)

    with open(csv_output, "w", newline="") as csv_file:
        fieldnames = ["conversation_id", "predicted_intent", "rationale"]
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(output_data)

    return correct / total if total else 0.0


def main():
    intent_model = IntentDetector()
    accuracy = predict_intents(
        input_file="data/input.json",
        json_output="data/output/predictions.json",
        csv_output="data/output/predictions.csv",
        model=intent_model
    )
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Batched zero-shot classification to reduce repeated model invocation overhead.
# - Used generator-based joining to avoid intermediate message lists.
# - Removed unused preprocessing and dependencies to cut import and runtime overhead.
# - Computed accuracy during output creation to avoid extra passes and storage.
# - Fixed random seed and disabled tokenizer parallelism for reproducibility and reduced CPU contention.