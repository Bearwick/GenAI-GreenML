# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import json
import csv
import re
from collections import Counter
from typing import List, Dict, Any, Tuple

import numpy as np


def clean_and_lowercase(text: str) -> str:
    text = "" if text is None else str(text)
    text = re.sub(r"[\U00010000-\U0010ffff]", "", text)  # remove most emoji / non-BMP symbols
    cleaned = text.lower()
    cleaned = re.sub(r"[^\w\s]", " ", cleaned)
    cleaned = re.sub(r"\s+", " ", cleaned)
    return cleaned.strip()


def create_conversation(messages: List[dict], max_messages: int = None) -> str:
    if not isinstance(messages, list):
        messages = []
    if max_messages is not None and max_messages > 0:
        messages = messages[-max_messages:]
    formatted_lines = []
    for m in messages:
        if not isinstance(m, dict):
            continue
        sender = str(m.get("sender", "")).strip().capitalize()
        text = str(m.get("text", "")).strip()
        if sender or text:
            formatted_lines.append(f"{sender}: {text}".strip())
    return "\n".join(formatted_lines).strip()


def _safe_read_json(path: str) -> Any:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def _normalize_headers(headers: List[str]) -> List[str]:
    out = []
    for h in headers:
        h2 = "" if h is None else str(h)
        h2 = re.sub(r"\s+", " ", h2.strip())
        out.append(h2)
    return out


def _extract_text_label_pairs(conversations: Any) -> List[Tuple[str, str]]:
    pairs = []

    def is_nonempty_str(x: Any) -> bool:
        return isinstance(x, str) and x.strip() != ""

    if isinstance(conversations, dict):
        candidates = []
        for k in ["conversations", "data", "items", "records"]:
            v = conversations.get(k)
            if isinstance(v, list):
                candidates = v
                break
        conversations_list = candidates if candidates else []
    elif isinstance(conversations, list):
        conversations_list = conversations
    else:
        conversations_list = []

    for entry in conversations_list:
        if not isinstance(entry, dict):
            continue
        messages = entry.get("messages", [])
        text = create_conversation(messages)
        if not text:
            for key in ["text", "dialogue", "conversation", "content"]:
                if is_nonempty_str(entry.get(key)):
                    text = str(entry.get(key))
                    break

        label = None
        for lk in [
            "intent",
            "label",
            "target",
            "y",
            "gold_intent",
            "true_intent",
            "category",
            "class",
            "predicted_intent",
        ]:
            if is_nonempty_str(entry.get(lk)):
                label = str(entry.get(lk)).strip()
                break

        if text and label:
            pairs.append((text, label))
    return pairs


class TinyMultinomialNB:
    def __init__(self, alpha: float = 1.0):
        self.alpha = float(alpha)
        self.vocab_ = {}
        self.class_to_idx_ = {}
        self.idx_to_class_ = []
        self.class_log_prior_ = None
        self.feature_log_prob_ = None
        self.fitted_ = False

    def _tokenize(self, text: str) -> List[str]:
        t = clean_and_lowercase(text)
        if not t:
            return []
        return t.split()

    def fit(self, texts: List[str], y: List[str]) -> "TinyMultinomialNB":
        y = [str(v) for v in y]
        classes = sorted(list(set(y)))
        self.class_to_idx_ = {c: i for i, c in enumerate(classes)}
        self.idx_to_class_ = classes
        n_classes = len(classes)

        df = Counter()
        for txt in texts:
            toks = set(self._tokenize(txt))
            df.update(toks)

        min_df = 2 if len(texts) >= 20 else 1
        vocab_tokens = [tok for tok, c in df.items() if c >= min_df]
        vocab_tokens = sorted(vocab_tokens)
        self.vocab_ = {tok: i for i, tok in enumerate(vocab_tokens)}
        n_features = len(self.vocab_)

        class_count = np.zeros(n_classes, dtype=np.float64)
        feature_count = np.zeros((n_classes, n_features), dtype=np.float64)

        for txt, label in zip(texts, y):
            ci = self.class_to_idx_[label]
            class_count[ci] += 1.0
            toks = self._tokenize(txt)
            for tok in toks:
                j = self.vocab_.get(tok)
                if j is not None:
                    feature_count[ci, j] += 1.0

        total = class_count.sum()
        self.class_log_prior_ = np.log((class_count + 1e-12) / (total + 1e-12))

        smoothed_fc = feature_count + self.alpha
        smoothed_cc = smoothed_fc.sum(axis=1, keepdims=True)
        self.feature_log_prob_ = np.log(smoothed_fc) - np.log(smoothed_cc)

        self.fitted_ = True
        return self

    def predict(self, texts: List[str]) -> List[str]:
        if not self.fitted_:
            raise RuntimeError("Model not fitted.")
        preds = []
        for txt in texts:
            toks = self._tokenize(txt)
            logp = self.class_log_prior_.copy()
            for tok in toks:
                j = self.vocab_.get(tok)
                if j is not None:
                    logp += self.feature_log_prob_[:, j]
            preds.append(self.idx_to_class_[int(np.argmax(logp))])
        return preds

    def predict_proba(self, texts: List[str]) -> List[Dict[str, float]]:
        if not self.fitted_:
            raise RuntimeError("Model not fitted.")
        out = []
        for txt in texts:
            toks = self._tokenize(txt)
            logp = self.class_log_prior_.copy()
            for tok in toks:
                j = self.vocab_.get(tok)
                if j is not None:
                    logp += self.feature_log_prob_[:, j]
            m = np.max(logp)
            p = np.exp(logp - m)
            p = p / (p.sum() + 1e-12)
            out.append({c: float(p[i]) for i, c in enumerate(self.idx_to_class_)})
        return out


def _train_test_split_indices(n: int, test_size: float = 0.2, random_state: int = 42) -> Tuple[np.ndarray, np.ndarray]:
    rng = np.random.RandomState(random_state)
    idx = np.arange(n)
    rng.shuffle(idx)
    n_test = int(round(n * test_size))
    n_test = max(1, min(n - 1, n_test)) if n >= 2 else 0
    test_idx = idx[:n_test]
    train_idx = idx[n_test:]
    return train_idx, test_idx


def _accuracy(y_true: List[str], y_pred: List[str]) -> float:
    if len(y_true) == 0:
        return 0.0
    correct = sum(1 for a, b in zip(y_true, y_pred) if a == b)
    return float(correct) / float(len(y_true))


class IntentDetector:
    def __init__(self):
        self.intent_options = [
            "Book Appointment",
            "Product Inquiry",
            "Pricing Negotiation",
            "Support Request",
            "Follow-Up",
        ]
        self.model = TinyMultinomialNB(alpha=1.0)
        self.fallback_priors = {k: 1.0 / len(self.intent_options) for k in self.intent_options}
        self.trained = False

    def fit_if_possible(self, dataset_path: str) -> float:
        data = _safe_read_json(dataset_path)
        pairs = _extract_text_label_pairs(data)

        if len(pairs) < 4:
            self.trained = False
            return 0.0

        texts = [p[0] for p in pairs]
        labels = [p[1] for p in pairs]
        label_set = sorted(set(labels))
        if len(label_set) < 2:
            self.trained = False
            return 0.0

        train_idx, test_idx = _train_test_split_indices(len(texts), test_size=0.2, random_state=42)
        assert len(train_idx) > 0 and len(test_idx) > 0

        X_train = [texts[i] for i in train_idx.tolist()]
        y_train = [labels[i] for i in train_idx.tolist()]
        X_test = [texts[i] for i in test_idx.tolist()]
        y_test = [labels[i] for i in test_idx.tolist()]

        self.model.fit(X_train, y_train)
        self.trained = True

        y_pred = self.model.predict(X_test)
        acc = _accuracy(y_test, y_pred)
        return acc

    def classify_intent(self, dialogue: str) -> dict:
        dialogue = "" if dialogue is None else str(dialogue)

        if self.trained:
            pred = self.model.predict([dialogue])[0]
        else:
            pred = self.intent_options[0]

        explanation = f"Based on the conversation, the customer is likely interested in '{str(pred).lower()}'."
        return {"predicted_intent": pred, "rationale": explanation}


def predict_intents(input_file: str, json_output: str, csv_output: str) -> float:
    intent_model = IntentDetector()
    accuracy = intent_model.fit_if_possible(input_file)

    conversations = _safe_read_json(input_file)

    if isinstance(conversations, dict):
        conv_list = None
        for k in ["conversations", "data", "items", "records"]:
            v = conversations.get(k)
            if isinstance(v, list):
                conv_list = v
                break
        conv_list = conv_list if conv_list is not None else []
    elif isinstance(conversations, list):
        conv_list = conversations
    else:
        conv_list = []

    output_data = []
    for entry in conv_list:
        if not isinstance(entry, dict):
            continue
        conv_id = entry.get("conversation_id", entry.get("id", None))
        formatted_text = create_conversation(entry.get("messages", []))
        if not formatted_text:
            formatted_text = str(entry.get("text", entry.get("dialogue", "")) or "")
        intent_result = intent_model.classify_intent(formatted_text)
        output_record = {
            "conversation_id": conv_id,
            "predicted_intent": intent_result["predicted_intent"],
            "rationale": intent_result["rationale"],
        }
        output_data.append(output_record)

    os.makedirs(os.path.dirname(json_output), exist_ok=True)
    with open(json_output, "w", encoding="utf-8") as json_file:
        json.dump(output_data, json_file, indent=2, ensure_ascii=False)

    with open(csv_output, "w", newline="", encoding="utf-8") as csv_file:
        fieldnames = _normalize_headers(["conversation_id", "predicted_intent", "rationale"])
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        for r in output_data:
            writer.writerow(r)

    return float(accuracy)


def _main():
    os.makedirs("data/output", exist_ok=True)
    input_path = "data/input.json"
    json_output = "data/output/predictions.json"
    csv_output = "data/output/predictions.csv"

    if not os.path.exists(input_path):
        os.makedirs("data", exist_ok=True)
        with open(input_path, "w", encoding="utf-8") as f:
            json.dump([], f)

    accuracy = predict_intents(input_path, json_output, csv_output)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    _main()

# Optimization Summary
# - Replaced transformer zero-shot inference (high compute/memory, slow on CPU) with a tiny Multinomial Naive Bayes bag-of-words classifier.
# - Uses simple whitespace tokenization + lowercase + punctuation stripping to minimize preprocessing cost and avoid heavy NLP dependencies.
# - Trains only if labels exist in the provided JSON; otherwise falls back to a constant baseline to keep end-to-end execution robust.
# - Implements a fixed random_state split for reproducibility; accuracy computed on a held-out test set when possible.
# - Keeps I/O simple and avoids verbose stdout; only prints ACCURACY at the end as required.