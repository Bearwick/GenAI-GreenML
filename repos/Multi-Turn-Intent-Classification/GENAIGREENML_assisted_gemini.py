# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import os
import json
import csv
try:
    import torch
    DEVICE = 0 if torch.cuda.is_available() else -1
except ImportError:
    DEVICE = -1
from transformers import pipeline

class IntentDetector:
    def __init__(self, model_name="cross-encoder/nli-distilroberta-base"):
        self.intent_options = [
            "Book Appointment",
            "Product Inquiry",
            "Pricing Negotiation",
            "Support Request",
            "Follow-Up"
        ]
        self.intent_pipeline = pipeline(
            task="zero-shot-classification",
            model=model_name,
            device=DEVICE
        )

    def classify_batch(self, sequences):
        return self.intent_pipeline(sequences, self.intent_options, batch_size=16)

def create_conversation(messages):
    return "\n".join(f"{m.get('sender', '').capitalize()}: {m.get('text', '')}" for m in messages)

def run_workflow():
    input_file = "data/input.json"
    json_output = "data/output/predictions.json"
    csv_output = "data/output/predictions.csv"
    
    if not os.path.exists(input_file):
        return

    try:
        with open(input_file, 'r') as f:
            conversations = json.load(f)
    except (json.JSONDecodeError, FileNotFoundError):
        return

    if not conversations:
        print(f"ACCURACY={0.0:.6f}")
        return

    detector = IntentDetector()
    dialogues = [create_conversation(c.get('messages', [])) for c in conversations]
    
    predictions = detector.classify_batch(dialogues)
    
    output_records = []
    correct_count = 0
    
    first_entry = conversations[0]
    label_key = next((k for k in ['intent', 'label', 'ground_truth'] if k in first_entry), None)
    id_key = next((k for k in ['conversation_id', 'id'] if k in first_entry), 'conversation_id')

    for entry, result in zip(conversations, predictions):
        top_intent = result["labels"][0]
        record = {
            "conversation_id": entry.get(id_key),
            "predicted_intent": top_intent,
            "rationale": f"Based on the conversation, the customer is likely interested in '{top_intent.lower()}'."
        }
        output_records.append(record)
        
        if label_key and entry.get(label_key) == top_intent:
            correct_count += 1

    os.makedirs(os.path.dirname(json_output), exist_ok=True)
    
    with open(json_output, 'w') as f:
        json.dump(output_records, f, indent=2)
        
    with open(csv_output, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=["conversation_id", "predicted_intent", "rationale"])
        writer.writeheader()
        writer.writerows(output_records)

    accuracy = correct_count / len(conversations) if label_key else 0.0
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_workflow()

# Optimization Summary
# 1. Batch Processing: Implemented batch inference in the HuggingFace pipeline, which significantly reduces the energy cost per sample by optimizing GPU/CPU utilization and reducing forward pass overhead.
# 2. Hardware Acceleration: Added automatic device detection to utilize CUDA/GPU when available, reducing total runtime and overall energy consumption.
# 3. Redundant Logic Removal: Eliminated unused preprocessing functions (regex and emoji cleaning) that were defined but not utilized in the main execution path.
# 4. Memory Footprint Reduction: Replaced manual loops with list comprehensions for faster string processing and avoided creating large intermediate data structures.
# 5. I/O Efficiency: Optimized file operations by aggregating all results into a single list before performing bulk writes to JSON and CSV.
# 6. Simplified Workflow: Streamlined the class structure to separate model initialization from business logic, improving execution flow.