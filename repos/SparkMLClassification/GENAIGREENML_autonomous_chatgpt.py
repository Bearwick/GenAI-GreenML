# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "diabetes.csv"
RANDOM_STATE = 42


def _read_csv_robust(path: str) -> pd.DataFrame:
    # Try default parsing first
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(d: pd.DataFrame) -> bool:
        if d is None or d.empty:
            return True
        # If only one column and it contains separators, parsing likely wrong
        if d.shape[1] == 1:
            col0 = d.columns[0]
            sample = d[col0].astype(str).head(10).tolist()
            if any((";" in s) or ("," in s) for s in sample):
                return True
        return False

    if looks_wrong(df):
        # Retry with european CSV conventions
        df = pd.read_csv(path, sep=";", decimal=",")

    return df


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols = []
    for c in df.columns:
        c2 = str(c).strip()
        c2 = re.sub(r"\s+", " ", c2)
        cols.append(c2)
    df = df.copy()
    df.columns = cols

    # Drop unnamed index-like columns
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", c)]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _pick_target(df: pd.DataFrame, preferred: str = "Outcome") -> str:
    if preferred in df.columns:
        return preferred

    # Prefer a non-constant numeric column as target
    numeric_candidates = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        nunique = s.nunique(dropna=True)
        if nunique >= 2:
            numeric_candidates.append((c, nunique, s.isna().mean()))
    if numeric_candidates:
        # Prefer fewer missing values; then more unique values
        numeric_candidates.sort(key=lambda x: (x[2], -x[1]))
        return numeric_candidates[0][0]

    # Fallback: last column
    return df.columns[-1]


def _coerce_object_numerics(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    for c in df.columns:
        if df[c].dtype == "object":
            s = df[c].astype(str)
            # If most values look numeric, coerce
            sample = s.dropna().head(200)
            if len(sample) > 0:
                numeric_like = sample.str.match(r"^\s*[-+]?(\d+(\.\d*)?|\.\d+)\s*$").mean()
                if numeric_like >= 0.8:
                    df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _build_preprocess(df: pd.DataFrame, feature_cols: list) -> tuple[ColumnTransformer, list, list]:
    X = df[feature_cols].copy()

    numeric_features = [c for c in feature_cols if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in feature_cols if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    return preprocessor, numeric_features, categorical_features


def _safe_accuracy_proxy_regression(y_true, y_pred) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    var = np.var(y_true)
    if var <= 1e-12:
        return 1.0 if np.mean((y_true - y_pred) ** 2) <= 1e-12 else 0.0
    mse = np.mean((y_true - y_pred) ** 2)
    # 1/(1+MSE/Var) yields bounded [0,1], equals 1 when perfect, decreases smoothly
    score = 1.0 / (1.0 + (mse / (var + 1e-12)))
    return float(np.clip(score, 0.0, 1.0))


def main():
    df = _read_csv_robust(DATASET_PATH)
    df = _normalize_columns(df)
    df = _coerce_object_numerics(df)

    # Replace inf with NaN safely
    df = df.replace([np.inf, -np.inf], np.nan)

    assert df is not None and not df.empty, "Dataset is empty after loading."

    target_col = _pick_target(df, preferred="Outcome")

    # Ensure target exists and handle missingness
    y_raw = df[target_col]
    feature_cols = [c for c in df.columns if c != target_col]
    if not feature_cols:
        # Minimal fallback: create a dummy feature
        df = df.copy()
        df["_dummy_feature_"] = 0
        feature_cols = ["_dummy_feature_"]

    # Drop rows where target is missing
    mask_target = ~pd.isna(y_raw)
    df = df.loc[mask_target].copy()
    y_raw = df[target_col]

    assert not df.empty, "No rows with non-missing target."

    # Decide task: classification if few discrete classes
    y_num = pd.to_numeric(y_raw, errors="coerce")
    unique_non_na = pd.Series(y_raw).dropna().unique()
    nunique = len(unique_non_na)

    # Classification heuristic: integer-like and small unique count
    classification = False
    if nunique >= 2:
        if nunique <= 20:
            if y_num.notna().mean() >= 0.9:
                # If values are close to integers, treat as classification
                vals = y_num.dropna().values
                if vals.size > 0:
                    classification = np.mean(np.isclose(vals, np.round(vals), atol=1e-8)) >= 0.98
            else:
                # Non-numeric labels with limited unique values -> classification
                classification = True

    X = df[feature_cols].copy()

    preprocessor, num_feats, cat_feats = _build_preprocess(df, feature_cols)

    if classification:
        # Encode y to integers stably
        y = pd.Series(y_raw).astype("category").cat.codes.to_numpy()
        # If <2 classes after encoding, fallback to regression proxy
        if np.unique(y).size < 2:
            classification = False
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
            )
            assert len(y_train) > 0 and len(y_test) > 0, "Train/test split failed."

            clf = Pipeline(
                steps=[
                    ("preprocess", preprocessor),
                    ("model", LogisticRegression(solver="liblinear", max_iter=200, random_state=RANDOM_STATE)),
                ]
            )
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

    # Regression fallback
    y = pd.to_numeric(y_raw, errors="coerce")
    # Drop rows where y is NaN after coercion
    mask = y.notna()
    X = X.loc[mask].copy()
    y = y.loc[mask].to_numpy(dtype=float)

    assert X.shape[0] > 1, "Not enough samples for regression."

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE
    )
    assert len(y_train) > 0 and len(y_test) > 0, "Train/test split failed."

    reg = Pipeline(
        steps=[
            ("preprocess", preprocessor),
            ("model", Ridge(alpha=1.0, random_state=RANDOM_STATE)),
        ]
    )
    reg.fit(X_train, y_train)
    y_pred = reg.predict(X_test)
    accuracy = _safe_accuracy_proxy_regression(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight CPU-friendly models: LogisticRegression (liblinear) for classification; Ridge for regression fallback.
# - Robust CSV parsing with a fallback for ';' separators and ',' decimals to avoid expensive manual intervention.
# - Column normalization and dropping 'Unnamed' columns prevents wasted computation on index artifacts.
# - Coerces numeric-like object columns to numeric to reduce one-hot expansion and improve efficiency.
# - ColumnTransformer + Pipeline ensures reproducible preprocessing and avoids recomputing transforms separately.
# - SimpleImputer + StandardScaler for numeric and OneHotEncoder(handle_unknown='ignore') for categoricals: minimal, robust feature handling.
# - Regression fallback uses a bounded accuracy proxy in [0,1]: 1/(1+MSE/Var), stable and cheap to compute.