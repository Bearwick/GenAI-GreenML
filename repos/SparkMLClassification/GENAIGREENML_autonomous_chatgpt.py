# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "diabetes.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _read_csv_robust(path):
    # Attempt 1: default parser
    try:
        df1 = pd.read_csv(path)
    except Exception:
        df1 = None

    # Attempt 2: common European formatting
    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
    except Exception:
        df2 = None

    # Pick the "better" parse: more columns and more rows; fallback safely
    candidates = [d for d in [df1, df2] if isinstance(d, pd.DataFrame)]
    if not candidates:
        return pd.DataFrame()

    def score(d):
        # Favor non-trivial shapes
        return (d.shape[1], d.shape[0])

    return max(candidates, key=score)


def _drop_unnamed(df):
    if df.empty:
        return df
    keep_cols = [c for c in df.columns if not re.match(r"^Unnamed:\s*\d+$", str(c))]
    return df[keep_cols]


def _choose_target(df, expected_headers=None):
    cols = list(df.columns)
    expected_headers = expected_headers or []
    expected_set = {c.strip() for c in expected_headers if isinstance(c, str)}

    # Prefer standard target names if present
    for cand in ["Outcome", "target", "Target", "label", "Label", "y", "Y"]:
        if cand in cols:
            return cand

    # If expected headers include one that exists, and looks like a target
    for cand in expected_set:
        if cand in cols and cand.lower() in {"outcome", "target", "label"}:
            return cand

    # Otherwise pick a numeric, non-constant column
    numeric_candidates = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        nunq = int(s.nunique(dropna=True))
        if nunq >= 2:
            numeric_candidates.append((nunq, c))
    if numeric_candidates:
        # choose column with few unique values first (often a class label), else any
        numeric_candidates.sort(key=lambda x: (x[0], x[1]))
        return numeric_candidates[0][1]

    # Fallback: last column if exists
    return cols[-1] if cols else None


def _is_classification_target(y):
    # Decide classification if small number of discrete values
    y_num = pd.to_numeric(y, errors="coerce")
    y_clean = y_num.replace([np.inf, -np.inf], np.nan).dropna()
    if y_clean.empty:
        return False
    nunq = int(y_clean.nunique())
    if nunq < 2:
        return False
    # Heuristic: typical labels small cardinality
    if nunq <= 20:
        # If values are close to integers, it's likely class labels
        vals = y_clean.values
        if np.all(np.isclose(vals, np.round(vals), atol=1e-8)):
            return True
        # Also allow binary floats like 0.0/1.0
        if nunq <= 5:
            return True
    return False


df = _read_csv_robust(DATASET_PATH)
if df is None or not isinstance(df, pd.DataFrame):
    df = pd.DataFrame()

# Normalize headers, drop junk unnamed columns
df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)

assert not df.empty, "Dataset is empty or failed to load."

# If the file is mis-parsed into 1 column (e.g., comma-separated read as single string),
# try to split it defensively.
if df.shape[1] == 1:
    only_col = df.columns[0]
    if df[only_col].dtype == object:
        sample = df[only_col].astype(str).head(5).tolist()
        # If values contain commas, attempt split
        if any("," in x for x in sample):
            split_df = df[only_col].astype(str).str.split(",", expand=True)
            # If headers were embedded, attempt to use first row as header when it matches known names
            split_df.columns = [f"col_{i}" for i in range(split_df.shape[1])]
            df = split_df

# Expected headers (if present in prompt); used only as guidance
EXPECTED_HEADERS = [
    "Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin",
    "BMI", "DiabetesPedigreeFunction", "Age", "Outcome"
]

target_col = _choose_target(df, expected_headers=EXPECTED_HEADERS)
assert target_col is not None, "Could not determine a target column."

# Ensure we have at least one feature column
feature_cols = [c for c in df.columns if c != target_col]
if not feature_cols:
    # Create a constant feature to keep pipeline valid
    df["_constant_feature"] = 1.0
    feature_cols = ["_constant_feature"]

# Build X, y
X = df[feature_cols].copy()
y_raw = df[target_col].copy()

# Coerce numeric where possible for y; keep y_raw for classification label handling
y_num = pd.to_numeric(y_raw, errors="coerce")
y_num = y_num.replace([np.inf, -np.inf], np.nan)

# Determine task type
is_clf = _is_classification_target(y_raw)

# Identify feature types robustly: attempt numeric coercion per column
numeric_features = []
categorical_features = []
for c in feature_cols:
    s_num = pd.to_numeric(X[c], errors="coerce")
    valid_ratio = float(s_num.notna().mean()) if len(s_num) else 0.0
    if valid_ratio >= 0.7:
        # Treat as numeric if mostly parseable
        numeric_features.append(c)
        X[c] = s_num
    else:
        categorical_features.append(c)
        X[c] = X[c].astype("string")

# Clean infs in numeric features
for c in numeric_features:
    X[c] = X[c].replace([np.inf, -np.inf], np.nan)

# For classification, build labels; otherwise use numeric y
if is_clf:
    y = pd.to_numeric(y_raw, errors="coerce").replace([np.inf, -np.inf], np.nan)
    # Impute missing labels with mode (if all missing, fallback later)
    if y.notna().any():
        mode_val = y.dropna().mode()
        fill_val = float(mode_val.iloc[0]) if not mode_val.empty else float(y.dropna().iloc[0])
        y = y.fillna(fill_val)
    else:
        y = pd.Series(np.zeros(len(X), dtype=float))
else:
    # Regression path
    y = y_num
    if y.notna().any():
        y = y.fillna(y.median())
    else:
        y = pd.Series(np.zeros(len(X), dtype=float))

# Final sanity: drop rows where all features are missing (after coercion) to avoid degenerate fit
if numeric_features:
    all_num_missing = X[numeric_features].isna().all(axis=1)
else:
    all_num_missing = pd.Series(False, index=X.index)

if categorical_features:
    all_cat_missing = X[categorical_features].isna().all(axis=1)
else:
    all_cat_missing = pd.Series(True, index=X.index)

drop_mask = all_num_missing & all_cat_missing
if drop_mask.any():
    X = X.loc[~drop_mask].copy()
    y = y.loc[X.index].copy()

assert len(X) > 1, "Not enough samples after preprocessing."

# If classification target collapses to <2 classes, fallback to regression-like scoring
if is_clf:
    y_unique = pd.Series(y).nunique(dropna=True)
    if y_unique < 2:
        is_clf = False

# Preprocessors
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=True, with_std=True)),
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Models (lightweight, CPU-friendly)
if is_clf:
    model = LogisticRegression(
        solver="liblinear",  # efficient for small/medium, CPU-friendly
        max_iter=300,
        random_state=RANDOM_STATE,
    )
else:
    model = Ridge(alpha=1.0, random_state=RANDOM_STATE)

pipe = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", model),
])

# Train/test split
if is_clf:
    # Try stratification when possible
    y_for_split = pd.Series(y)
    stratify = y_for_split if y_for_split.nunique() >= 2 else None
else:
    stratify = None

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)

if is_clf:
    # Ensure integer-like labels for accuracy
    y_test_i = np.round(pd.to_numeric(pd.Series(y_test), errors="coerce")).astype(int).values
    y_pred_i = np.round(pd.to_numeric(pd.Series(y_pred), errors="coerce")).astype(int).values
    accuracy = float(accuracy_score(y_test_i, y_pred_i))
else:
    # Regression "accuracy" proxy in [0,1]: 1/(1+NRMSE), stable and bounded
    y_t = pd.to_numeric(pd.Series(y_test), errors="coerce").astype(float).values
    y_p = pd.to_numeric(pd.Series(y_pred), errors="coerce").astype(float).values
    y_t = np.nan_to_num(y_t, nan=0.0, posinf=0.0, neginf=0.0)
    y_p = np.nan_to_num(y_p, nan=0.0, posinf=0.0, neginf=0.0)
    rmse = float(np.sqrt(np.mean((y_t - y_p) ** 2))) if len(y_t) else 0.0
    scale = float(np.std(y_t)) if len(y_t) else 0.0
    nrmse = rmse / (scale + 1e-12)
    accuracy = float(1.0 / (1.0 + nrmse))

print(f”ACCURACY={accuracy:.6f}”)

# Optimization Summary
# - Used lightweight sklearn Pipeline + ColumnTransformer to avoid redundant preprocessing and ensure reproducibility.
# - Robust CSV loading with fallback separators/decimals to reduce manual intervention and re-runs.
# - Simple models (LogisticRegression liblinear / Ridge) chosen for CPU efficiency and strong baselines on tabular data.
# - Minimal feature engineering: median/mode imputation, standard scaling for linear model stability, one-hot encoding for categoricals.
# - Defensive schema handling: normalized headers, dropped Unnamed columns, inferred target/feature types without hard assumptions.
# - Regression fallback uses bounded proxy accuracy = 1/(1+NRMSE) to keep output stable in [0,1] with minimal compute.