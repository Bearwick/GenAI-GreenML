# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col
from pyspark.ml.feature import Imputer, StandardScaler, VectorAssembler, ChiSqSelector
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
import numpy as np

def diabetes():
    spark = SparkSession.builder \
        .master("local[*]") \
        .appName("DiabetesClassification") \
        .getOrCreate()
    spark.sparkContext.setLogLevel("ERROR")

    try:
        raw_data = spark.read.csv("diabetes.csv", header=True, inferSchema=True)
        if len(raw_data.columns) <= 1:
            raw_data = spark.read.csv("diabetes.csv", header=True, inferSchema=True, sep=";")
    except Exception:
        raw_data = spark.read.csv("diabetes.csv", header=True, inferSchema=True, sep=";")

    zero_replace_cols = ["Glucose", "BloodPressure", "SkinThickness", "BMI", "Insulin"]
    for c in zero_replace_cols:
        raw_data = raw_data.withColumn(c, when(col(c) == 0, np.nan).otherwise(col(c)))

    imputer = Imputer(inputCols=zero_replace_cols, outputCols=zero_replace_cols)
    raw_data = imputer.fit(raw_data).transform(raw_data)

    feature_cols = [c for c in raw_data.columns if c != "Outcome"]
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    raw_data = assembler.transform(raw_data)

    scaler = StandardScaler(inputCol="features", outputCol="Scaled_features")
    raw_data = scaler.fit(raw_data).transform(raw_data)

    train, test = raw_data.randomSplit([0.8, 0.2], seed=12345)
    train.cache()
    test.cache()

    dataset_size = float(train.count())
    numPositives = train.where("Outcome == 1").count()
    numNegatives = dataset_size - numPositives
    balancing_ratio = numNegatives / dataset_size

    train = train.withColumn("classWeights",
                             when(col("Outcome") == 1, balancing_ratio).otherwise(1 - balancing_ratio))

    css = ChiSqSelector(featuresCol="Scaled_features", outputCol="Aspect", labelCol="Outcome", fpr=0.05)
    css_model = css.fit(train)
    train = css_model.transform(train)
    test = css_model.transform(test)

    lr = LogisticRegression(labelCol="Outcome", featuresCol="Aspect", weightCol="classWeights", maxIter=10)
    model = lr.fit(train)
    predict_test = model.transform(test)

    evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction", labelCol="Outcome")
    accuracy = evaluator.evaluate(predict_test)

    print(f"ACCURACY={accuracy:.6f}")

    spark.stop()

def main():
    diabetes()

if __name__ == "__main__":
    main()

# Optimization Summary
# 1. Removed all print/show/logging statements to avoid unnecessary I/O and computation.
# 2. Removed all matplotlib/plot imports and visualization code.
# 3. Removed redundant imports (unused modules like LinearRegression, RandomForestClassifier, etc.).
# 4. Reused the ChiSqSelector model fitted on train for transforming test, instead of fitting twice (eliminates redundant computation).
# 5. Cached train and test DataFrames to avoid recomputation during multiple actions.
# 6. Replaced individual withColumn calls for zero replacement with a loop to reduce code but same Spark plan