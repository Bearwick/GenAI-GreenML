# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col, count, lit
from pyspark.ml.feature import Imputer, VectorAssembler, StandardScaler, ChiSqSelector
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml import Pipeline
import numpy as np

def run_diabetes_pipeline():
    spark = SparkSession.builder \
        .appName("GreenDiabetes") \
        .config("spark.sql.shuffle.partitions", "2") \
        .getOrCreate()
    spark.sparkContext.setLogLevel("ERROR")

    try:
        data = spark.read.csv("diabetes.csv", header=True, inferSchema=True)
    except Exception:
        data = spark.read.csv("diabetes.csv", header=True, inferSchema=True, sep=';', decimal=',')

    target_col = "Outcome"
    feature_cols = [c for c in data.columns if c != target_col]
    impute_cols = ["Glucose", "BloodPressure", "SkinThickness", "BMI", "Insulin"]

    for c in impute_cols:
        data = data.withColumn(c, when(col(c) == 0, np.nan).otherwise(col(c)))

    train, test = data.randomSplit([0.8, 0.2], seed=12345)

    stats = train.agg(
        count("*").alias("total"),
        count(when(col(target_col) == 1, 1)).alias("pos")
    ).collect()[0]
    
    total = float(stats["total"])
    pos = float(stats["pos"])
    neg = total - pos
    balancing_ratio = neg / total

    train = train.withColumn("classWeights", when(col(target_col) == 1, balancing_ratio).otherwise(1.0 - balancing_ratio))

    imputer = Imputer(inputCols=impute_cols, outputCols=impute_cols)
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="raw_features")
    scaler = StandardScaler(inputCol="raw_features", outputCol="scaled_features")
    selector = ChiSqSelector(featuresCol="scaled_features", outputCol="selected_features", labelCol=target_col, fpr=0.05)
    lr = LogisticRegression(labelCol=target_col, featuresCol="selected_features", weightCol="classWeights", maxIter=10)

    pipeline = Pipeline(stages=[imputer, assembler, scaler, selector, lr])
    model = pipeline.fit(train)
    predictions = model.transform(test)

    evaluator = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol="prediction", metricName="accuracy")
    accuracy = evaluator.evaluate(predictions)
    
    print(f"ACCURACY={accuracy:.6f}")
    spark.stop()

if __name__ == "__main__":
    run_diabetes_pipeline()

# Optimization Summary
# 1. Pipeline integration: Combined Imputer, Assembler, Scaler, Selector, and Model into a single ML Pipeline to minimize redundant data passes and transformations.
# 2. Reduced Actions: Replaced multiple .count() and .show() calls with a single .agg() to calculate dataset statistics in one pass.
# 3. Efficiency in Feature Selection: Changed ChiSqSelector to fit once on training data instead of refitting on test data, preventing data leakage and reducing compute.
# 4. Resource Management: Set spark.sql.shuffle.partitions to 2 to optimize for small datasets and reduce scheduling overhead on local execution.
# 5. Minimized Data Movement: Removed all intermediate .show(), .print(), and logging calls that trigger expensive Spark actions and I/O.
# 6. Optimized Imputation: Applied NaN transformations in a tighter loop before entering the pipeline to ensure schema consistency.
# 7. Memory Footprint: Eliminated unnecessary intermediate DataFrame assignments and metadata calculations.
# 8. Deterministic Execution: Fixed the random seed in randomSplit and used consistent pipeline stages for reproducibility.