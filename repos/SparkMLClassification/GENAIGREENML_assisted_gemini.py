# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col
from pyspark.ml.feature import Imputer, StandardScaler, VectorAssembler, ChiSqSelector
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

def main():
    spark = SparkSession.builder \
        .appName("DiabetesClassification") \
        .config("spark.sql.shuffle.partitions", "1") \
        .config("spark.ui.enabled", "false") \
        .getOrCreate()

    try:
        raw_data = spark.read.csv("diabetes.csv", header=True, inferSchema=True)
        if "Outcome" not in raw_data.columns:
            raise ValueError
    except Exception:
        raw_data = spark.read.option("sep", ";").option("decimal", ",").csv("diabetes.csv", header=True, inferSchema=True)

    cols_to_fix = ["Glucose", "BloodPressure", "SkinThickness", "BMI", "Insulin"]
    for c in cols_to_fix:
        raw_data = raw_data.withColumn(c, when(col(c) == 0, float('nan')).otherwise(col(c)))

    imputer = Imputer(inputCols=cols_to_fix, outputCols=cols_to_fix)
    raw_data = imputer.fit(raw_data).transform(raw_data)

    feature_cols = [c for c in raw_data.columns if c != "Outcome"]
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    raw_data = assembler.transform(raw_data)

    scaler = StandardScaler(inputCol="features", outputCol="Scaled_features")
    raw_data = scaler.fit(raw_data).transform(raw_data)

    raw_data.cache()

    train, test = raw_data.randomSplit([0.8, 0.2], seed=12345)

    counts = train.groupBy("Outcome").count().collect()
    num_pos = next((row['count'] for row in counts if row['Outcome'] == 1), 0)
    total = train.count()
    balancing_ratio = (total - num_pos) / float(total)

    train = train.withColumn("classWeights", when(col("Outcome") == 1, balancing_ratio).otherwise(1 - balancing_ratio))

    css = ChiSqSelector(featuresCol='Scaled_features', outputCol='Aspect', labelCol='Outcome', fpr=0.05)
    css_model = css.fit(train)
    train = css_model.transform(train)
    test = css_model.transform(test)

    lr = LogisticRegression(labelCol="Outcome", featuresCol="Aspect", weightCol="classWeights", maxIter=10)
    model = lr.fit(train)
    predictions = model.transform(test)

    evaluator = MulticlassClassificationEvaluator(labelCol="Outcome", predictionCol="prediction", metricName="accuracy")
    accuracy = evaluator.evaluate(predictions)

    print(f"ACCURACY={accuracy:.6f}")
    
    spark.stop()

if __name__ == "__main__":
    main()

# Optimization Summary
# 1. Configured Spark with 1 partition and disabled UI to reduce execution and resource overhead for a small dataset.
# 2. Replaced multiple .count() calls with a single groupBy and collect to minimize distributed computation passes.
# 3. Chained DataFrame transformations and used loops for column processing to reduce intermediate object creation.
# 4. Fitted ChiSqSelector once on the training data and applied it to both train and test to avoid redundant fitting and prevent data leakage.
# 5. Cached the processed DataFrame before entering the iterative training and evaluation phase to save re-computation.
# 6. Replaced np.nan with native float('nan') to eliminate unnecessary heavy library dependencies.
# 7. Implemented a robust CSV loading fallback to ensure environment-independent execution without manual adjustment.
# 8. Eliminated all redundant show(), print(), and logging actions to reduce I/O and runtime.
# 9. Consolidated imports and removed unused regression modules to minimize memory footprint.