# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.ml.feature import Imputer, VectorAssembler, StandardScaler, ChiSqSelector
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator


DATASET_HEADERS = [
    "Pregnancies",
    "Glucose",
    "BloodPressure",
    "SkinThickness",
    "Insulin",
    "BMI",
    "DiabetesPedigreeFunction",
    "Age",
    "Outcome",
]

RANDOM_SEED = 12345


def _read_csv_with_fallback(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 1 or (df.shape[1] != len(DATASET_HEADERS) and set(DATASET_HEADERS).issuperset(df.columns) is False):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _to_spark_with_schema_alignment(spark: SparkSession, pdf: pd.DataFrame):
    pdf = pdf.copy()

    if "Outcome" not in pdf.columns and len(pdf.columns) == len(DATASET_HEADERS):
        pdf.columns = DATASET_HEADERS

    missing = [c for c in DATASET_HEADERS if c not in pdf.columns]
    if missing:
        raise ValueError(f"Missing required columns: {missing}. Found columns: {list(pdf.columns)}")

    pdf = pdf[DATASET_HEADERS]

    for c in DATASET_HEADERS:
        pdf[c] = pd.to_numeric(pdf[c], errors="coerce")

    return spark.createDataFrame(pdf)


def _replace_zeros_with_null(df, cols):
    out = df
    for c in cols:
        out = out.withColumn(c, F.when(F.col(c) == F.lit(0), F.lit(None)).otherwise(F.col(c)))
    return out


def diabetes():
    spark = (
        SparkSession.builder.appName("diabetes_green")
        .config("spark.sql.shuffle.partitions", "8")
        .config("spark.default.parallelism", "8")
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("ERROR")

    csv_path = os.environ.get("DIABETES_CSV_PATH", "diabetes.csv")
    pdf = _read_csv_with_fallback(csv_path)
    raw_data = _to_spark_with_schema_alignment(spark, pdf)

    zero_to_null_cols = ["Glucose", "BloodPressure", "SkinThickness", "BMI", "Insulin"]
    raw_data = _replace_zeros_with_null(raw_data, zero_to_null_cols)

    imputer = Imputer(inputCols=zero_to_null_cols, outputCols=zero_to_null_cols)
    raw_data = imputer.fit(raw_data).transform(raw_data)

    feature_cols = [c for c in raw_data.columns if c != "Outcome"]
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    raw_data = assembler.transform(raw_data)

    scaler = StandardScaler(inputCol="features", outputCol="Scaled_features", withMean=False, withStd=True)
    raw_data = scaler.fit(raw_data).transform(raw_data)

    train, test = raw_data.randomSplit([0.8, 0.2], seed=RANDOM_SEED)

    train = train.select("Outcome", "Scaled_features").cache()
    test = test.select("Outcome", "Scaled_features").cache()
    _ = train.count()
    _ = test.count()

    dataset_size = float(train.count())
    numPositives = float(train.where(F.col("Outcome") == 1).count())
    numNegatives = dataset_size - numPositives
    balancing_ratio = (numNegatives / dataset_size) if dataset_size else 0.0

    train = train.withColumn(
        "classWeights",
        F.when(F.col("Outcome") == 1, F.lit(balancing_ratio)).otherwise(F.lit(1.0 - balancing_ratio)),
    )

    selector = ChiSqSelector(
        featuresCol="Scaled_features",
        outputCol="Aspect",
        labelCol="Outcome",
        fpr=0.05,
    )
    selector_model = selector.fit(train)
    train = selector_model.transform(train)
    test = selector_model.transform(test)

    lr = LogisticRegression(
        labelCol="Outcome",
        featuresCol="Aspect",
        weightCol="classWeights",
        maxIter=10,
    )
    model = lr.fit(train)
    predict_test = model.transform(test)

    evaluator = MulticlassClassificationEvaluator(
        labelCol="Outcome",
        predictionCol="prediction",
        metricName="accuracy",
    )
    accuracy = evaluator.evaluate(predict_test)
    print(f"ACCURACY={accuracy:.6f}")

    train.unpersist()
    test.unpersist()
    spark.stop()


def main():
    diabetes()


if __name__ == "__main__":
    main()

# Optimization Summary
# - Switched CSV ingestion to a pandas-based robust fallback reader (default then sep=';' & decimal=',') to handle parsing issues deterministically before Spark work starts.
# - Aligned schema using DATASET_HEADERS and df.columns, reordering columns and coercing to numeric once in pandas to avoid repeated Spark schema inference and extra passes.
# - Removed unused imports, plotting, and all non-required prints to reduce overhead and side effects.
# - Replaced multiple ChiSqSelector fits (train and test) with a single fitted selector model from the training set and reused it on test, reducing redundant computation and ensuring consistent feature selection.
# - Reduced unnecessary actions: removed show/select debugging actions; only materialized cached train/test once to prevent repeated recomputation during counts and fitting.
# - Limited Spark parallelism/shuffle partitions to smaller defaults to reduce cluster-side overhead on small datasets, lowering energy use while preserving results.
# - Ensured reproducibility via a fixed random seed for the train/test split and stable preprocessing steps.