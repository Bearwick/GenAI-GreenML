# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.ml import Pipeline
from pyspark.ml.feature import Imputer, VectorAssembler, StandardScaler, ChiSqSelector
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator


DATASET_PATH = "diabetes.csv"
DATASET_HEADERS = [
    "Pregnancies",
    "Glucose",
    "BloodPressure",
    "SkinThickness",
    "Insulin",
    "BMI",
    "DiabetesPedigreeFunction",
    "Age",
    "Outcome",
]
SEED = 12345


def _read_csv_with_fallback(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] == 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _ensure_schema(df: pd.DataFrame) -> pd.DataFrame:
    if df.shape[1] == len(DATASET_HEADERS) and list(df.columns) != DATASET_HEADERS:
        df = df.copy()
        df.columns = DATASET_HEADERS
        return df
    if len(df.columns) == 1 and isinstance(df.columns[0], str):
        possible_split = df.columns[0].split(",")
        if len(possible_split) == len(DATASET_HEADERS):
            df = df.copy()
            df.columns = [c.strip() for c in possible_split]
    return df


def _build_spark_session() -> SparkSession:
    return (
        SparkSession.builder.appName("diabetes_green_refactor")
        .config("spark.sql.shuffle.partitions", "8")
        .getOrCreate()
    )


def _to_spark_df(spark: SparkSession, pdf: pd.DataFrame):
    for c in pdf.columns:
        if c not in ("Outcome",):
            pdf[c] = pd.to_numeric(pdf[c], errors="coerce")
    if "Outcome" in pdf.columns:
        pdf["Outcome"] = pd.to_numeric(pdf["Outcome"], errors="coerce").astype("Int64")
    return spark.createDataFrame(pdf)


def _derive_columns(sdf) -> tuple[list[str], str]:
    cols = list(sdf.columns)
    label_col = "Outcome" if "Outcome" in cols else cols[-1]
    feature_cols = [c for c in cols if c != label_col]
    return feature_cols, label_col


def diabetes() -> float:
    spark = _build_spark_session()
    spark.sparkContext.setLogLevel("ERROR")

    if not os.path.exists(DATASET_PATH):
        raise FileNotFoundError(f"Dataset not found at path: {DATASET_PATH}")

    pdf = _ensure_schema(_read_csv_with_fallback(DATASET_PATH))
    sdf = _to_spark_df(spark, pdf)

    feature_cols, label_col = _derive_columns(sdf)

    zero_to_nan_candidates = [c for c in ["Glucose", "BloodPressure", "SkinThickness", "BMI", "Insulin"] if c in sdf.columns]
    for c in zero_to_nan_candidates:
        sdf = sdf.withColumn(c, F.when(F.col(c) == F.lit(0), F.lit(None)).otherwise(F.col(c)))

    if zero_to_nan_candidates:
        imputer = Imputer(inputCols=zero_to_nan_candidates, outputCols=zero_to_nan_candidates)
        sdf = imputer.fit(sdf).transform(sdf)

    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features", handleInvalid="keep")
    scaler = StandardScaler(inputCol="features", outputCol="Scaled_features", withMean=False, withStd=True)
    selector = ChiSqSelector(featuresCol="Scaled_features", outputCol="Aspect", labelCol=label_col, fpr=0.05)

    sdf = Pipeline(stages=[assembler, scaler, selector]).fit(sdf).transform(sdf).select(
        F.col(label_col).alias("Outcome"), "Aspect"
    )

    train, test = sdf.randomSplit([0.8, 0.2], seed=SEED)

    agg = train.agg(
        F.count(F.lit(1)).alias("n"),
        F.sum(F.col("Outcome").cast("double")).alias("pos"),
    ).collect()[0]
    n = float(agg["n"])
    pos = float(agg["pos"]) if agg["pos"] is not None else 0.0
    neg = n - pos
    balancing_ratio = (neg / n) if n > 0 else 0.5

    train = train.withColumn(
        "classWeights",
        F.when(F.col("Outcome") == F.lit(1), F.lit(balancing_ratio)).otherwise(F.lit(1.0 - balancing_ratio)),
    ).select("Outcome", "Aspect", "classWeights")

    lr = LogisticRegression(
        labelCol="Outcome",
        featuresCol="Aspect",
        weightCol="classWeights",
        maxIter=10,
        tol=1e-6,
        standardization=False,
    )
    lr_model = lr.fit(train)

    predictions = lr_model.transform(test).select("Outcome", "prediction")

    evaluator = MulticlassClassificationEvaluator(labelCol="Outcome", predictionCol="prediction", metricName="accuracy")
    accuracy = float(evaluator.evaluate(predictions))

    spark.stop()
    return accuracy


def main():
    accuracy = diabetes()
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced multiple Spark CSV inference/actions with a single pandas CSV read (with delimiter/decimal fallback) and one Spark DataFrame creation to reduce Spark-side parsing overhead and I/O.
# - Removed all show()/prints/plots and extra transformations that only triggered actions; this avoids expensive Spark jobs and data movement.
# - Combined feature assembly, scaling, and chi-square selection into a single Pipeline and applied once (fit on full data) to avoid redundant model fitting and repeated passes.
# - Computed class balance statistics in one aggregation (count + sum) instead of multiple count actions, reducing Spark job count.
# - Reduced shuffle overhead by setting spark.sql.shuffle.partitions to a small fixed value suitable for this dataset.
# - Selected only required columns before training/inference to lower memory footprint and serialization costs.
# - Ensured reproducibility by fixing the randomSplit seed and avoiding nondeterministic side effects.