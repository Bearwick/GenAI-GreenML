# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import random
import warnings
import numpy as np
import pandas as pd
from sklearn.feature_selection import SelectFpr, chi2
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.exceptions import ConvergenceWarning

SEED = 12345
DATASET_PATH = "diabetes.csv"
DATASET_HEADERS = [
    "Pregnancies",
    "Glucose",
    "BloodPressure",
    "SkinThickness",
    "Insulin",
    "BMI",
    "DiabetesPedigreeFunction",
    "Age",
    "Outcome",
]
ZERO_AS_MISSING = {"Glucose", "BloodPressure", "SkinThickness", "BMI", "Insulin"}

random.seed(SEED)
np.random.seed(SEED)
warnings.filterwarnings("ignore", category=ConvergenceWarning)


def parsing_wrong(df, expected_headers):
    if df.shape[1] == 1:
        return True
    expected = {h.strip().lower() for h in expected_headers}
    actual = {c.strip().lower() for c in df.columns}
    common = expected & actual
    return len(common) < max(1, len(expected_headers) // 2)


def read_csv_with_fallback(path, expected_headers):
    df = pd.read_csv(path)
    if parsing_wrong(df, expected_headers):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def normalize_columns(df, expected_headers):
    col_map = {c.strip().lower(): c for c in df.columns}
    rename = {}
    for h in expected_headers:
        key = h.strip().lower()
        if key in col_map and col_map[key] != h:
            rename[col_map[key]] = h
    if rename:
        df = df.rename(columns=rename)
    return df


def prepare_data(path, expected_headers):
    df = read_csv_with_fallback(path, expected_headers)
    df = normalize_columns(df, expected_headers)
    available = [h for h in expected_headers if h in df.columns]
    if available:
        df = df.loc[:, available].copy()
    else:
        df = df.copy()
    df = df.apply(pd.to_numeric, errors="coerce")
    label_col = "Outcome" if "Outcome" in df.columns else df.columns[-1]
    df = df[df[label_col].notna()].copy()
    zero_cols = [c for c in ZERO_AS_MISSING if c in df.columns]
    if zero_cols:
        df.loc[:, zero_cols] = df[zero_cols].replace(0, np.nan)
        df.loc[:, zero_cols] = df[zero_cols].fillna(df[zero_cols].mean())
    feature_cols = [c for c in df.columns if c != label_col]
    if df[feature_cols].isnull().any().any():
        df.loc[:, feature_cols] = df[feature_cols].fillna(df[feature_cols].mean())
    X = df[feature_cols].to_numpy(dtype=float)
    y = df[label_col].to_numpy(dtype=int)
    return X, y


def create_logistic_regression(max_iter, seed):
    try:
        return LogisticRegression(
            penalty="none", solver="lbfgs", max_iter=max_iter, random_state=seed
        )
    except Exception:
        return LogisticRegression(
            C=1e6, solver="lbfgs", max_iter=max_iter, random_state=seed
        )


def train_and_evaluate(X, y, seed):
    std = X.std(axis=0, ddof=0)
    std[std == 0] = 1.0
    X_scaled = X / std
    rng = np.random.RandomState(seed)
    mask = rng.rand(X_scaled.shape[0]) < 0.8
    X_train, X_test = X_scaled[mask], X_scaled[~mask]
    y_train, y_test = y[mask], y[~mask]
    if y_train.size == 0 or y_test.size == 0:
        return 0.0
    dataset_size = y_train.size
    num_pos = np.sum(y_train == 1)
    num_neg = dataset_size - num_pos
    balancing_ratio = num_neg / dataset_size
    sample_weight = np.where(y_train == 1, balancing_ratio, 1.0 - balancing_ratio)
    selector = SelectFpr(score_func=chi2, alpha=0.05)
    X_train_sel = selector.fit_transform(X_train, y_train)
    X_test_sel = selector.transform(X_test)
    if X_train_sel.shape[1] == 0:
        X_train_sel, X_test_sel = X_train, X_test
    model = create_logistic_regression(max_iter=10, seed=seed)
    model.fit(X_train_sel, y_train, sample_weight=sample_weight)
    y_pred = model.predict(X_test_sel)
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy


def main():
    X, y = prepare_data(DATASET_PATH, DATASET_HEADERS)
    accuracy = train_and_evaluate(X, y, SEED)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced Spark workflow with pandas/NumPy and scikit-learn to reduce distributed overhead for a small dataset.
# - Used vectorized imputation and scaling to minimize Python-level loops and data movement.
# - Applied a single feature selection fit and reused it for testing to avoid redundant computation.
# - Implemented deterministic random splitting with fixed seeds for reproducibility.
# - Removed all intermediate outputs and warnings to reduce unnecessary runtime work.