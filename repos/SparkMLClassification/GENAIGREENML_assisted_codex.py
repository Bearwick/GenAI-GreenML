# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")
os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
os.environ.setdefault("NUMEXPR_NUM_THREADS", "1")

import warnings
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import chi2
from sklearn.exceptions import ConvergenceWarning

warnings.filterwarnings("ignore", category=ConvergenceWarning)

DATASET_PATH = "diabetes.csv"
DATASET_HEADERS = "Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age,Outcome"
RANDOM_SEED = 12345


def _needs_fallback(df, headers):
    if df.shape[1] != len(headers):
        return True
    expected = set(headers)
    if expected.isdisjoint(df.columns):
        return True
    sample = df.head(10)
    numeric = sample.apply(pd.to_numeric, errors="coerce")
    return numeric.isna().mean().mean() > 0.4


def read_dataset(path, headers_str):
    headers = [h.strip() for h in headers_str.split(",") if h.strip()]
    df = pd.read_csv(path)
    df.columns = [str(c).strip() for c in df.columns]
    if _needs_fallback(df, headers):
        df = pd.read_csv(path, sep=";", decimal=",")
        df.columns = [str(c).strip() for c in df.columns]
    if df.shape[1] == len(headers) and not set(headers).issubset(df.columns):
        df.columns = headers
    ordered = [c for c in headers if c in df.columns]
    if ordered:
        df = df[ordered]
    df = df.apply(pd.to_numeric, errors="coerce")
    return df, headers


def prepare_arrays(df, headers):
    label_col = None
    for col in df.columns:
        if col.lower() == "outcome":
            label_col = col
            break
    if label_col is None:
        label_col = "Outcome" if "Outcome" in df.columns else df.columns[-1]
    feature_cols = [col for col in df.columns if col != label_col]
    zero_missing_set = {h for h in headers if h in {"Glucose", "BloodPressure", "SkinThickness", "BMI", "Insulin"}}
    zero_missing_cols = [col for col in feature_cols if col in zero_missing_set]
    if zero_missing_cols:
        df[zero_missing_cols] = df[zero_missing_cols].replace(0, np.nan)
    X = df[feature_cols].to_numpy(dtype=float)
    y = df[label_col].to_numpy(dtype=float)
    valid_mask = ~np.isnan(y)
    if not np.all(valid_mask):
        X = X[valid_mask]
        y = y[valid_mask]
    y = y.astype(int)
    return X, y


def impute_and_scale(X):
    with np.errstate(invalid="ignore"):
        col_means = np.nanmean(X, axis=0)
    col_means = np.where(np.isnan(col_means), 0.0, col_means)
    nan_mask = np.isnan(X)
    if nan_mask.any():
        X[nan_mask] = np.take(col_means, np.where(nan_mask)[1])
    std = X.std(axis=0)
    std[std == 0] = 1.0
    X /= std
    return X


def split_data(X, y, train_ratio=0.8, seed=RANDOM_SEED):
    rng = np.random.default_rng(seed)
    mask = rng.random(len(X)) < train_ratio
    if mask.all() or (~mask).all():
        indices = rng.permutation(len(X))
        test_size = max(1, int(len(X) * (1 - train_ratio)))
        test_idx = indices[:test_size]
        train_idx = indices[test_size:]
        return X[train_idx], X[test_idx], y[train_idx], y[test_idx]
    return X[mask], X[~mask], y[mask], y[~mask]


def compute_sample_weights(y):
    n = y.size
    if n == 0:
        return np.array([], dtype=float)
    num_pos = np.sum(y == 1)
    num_neg = n - num_pos
    balancing_ratio = num_neg / n
    weight_pos = balancing_ratio
    weight_neg = 1 - balancing_ratio
    return np.where(y == 1, weight_pos, weight_neg)


def select_features_chi2(X_train, y_train, X_test, alpha=0.05):
    _, p_values = chi2(X_train, y_train)
    selected = np.where(p_values < alpha)[0]
    if selected.size == 0:
        return X_train, X_test
    return X_train[:, selected], X_test[:, selected]


def main():
    np.random.seed(RANDOM_SEED)
    df, headers = read_dataset(DATASET_PATH, DATASET_HEADERS)
    X, y = prepare_arrays(df, headers)
    X = impute_and_scale(X)
    X_train, X_test, y_train, y_test = split_data(X, y, train_ratio=0.8, seed=RANDOM_SEED)
    sample_weight = compute_sample_weights(y_train)
    X_train_sel, X_test_sel = select_features_chi2(X_train, y_train, X_test, alpha=0.05)
    model = LogisticRegression(max_iter=10, solver="lbfgs", C=1e6, random_state=RANDOM_SEED)
    model.fit(X_train_sel, y_train, sample_weight=sample_weight)
    y_pred = model.predict(X_test_sel)
    accuracy = float(np.mean(y_pred == y_test))
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced the Spark pipeline with a local pandas/NumPy/Scikit-learn workflow to avoid distributed overhead for a small dataset.
# - Performed mean imputation and scaling in-place with NumPy to reduce intermediate allocations and data movement.
# - Computed chi-square feature selection once on the training data and reused indices for the test set to avoid redundant fitting.
# - Used vectorized NumPy operations for splitting, weighting, and accuracy computation with fixed seeds for deterministic results.
# - Limited BLAS thread usage and suppressed non-critical warnings to reduce unnecessary runtime overhead.