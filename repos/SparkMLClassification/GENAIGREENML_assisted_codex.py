# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd
import warnings
from sklearn.exceptions import ConvergenceWarning
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFpr, chi2
from sklearn.linear_model import LogisticRegression

warnings.filterwarnings("ignore", category=ConvergenceWarning)

RANDOM_SEED = 12345
DATASET_PATH = "diabetes.csv"
DATASET_HEADERS = [
    "Pregnancies",
    "Glucose",
    "BloodPressure",
    "SkinThickness",
    "Insulin",
    "BMI",
    "DiabetesPedigreeFunction",
    "Age",
    "Outcome",
]

np.random.seed(RANDOM_SEED)


def load_dataset(path, headers):
    def read_csv(**kwargs):
        try:
            return pd.read_csv(path, **kwargs)
        except Exception:
            return pd.DataFrame()

    def clean(df):
        if df.empty:
            return df
        df = df.loc[:, ~df.columns.astype(str).str.startswith("Unnamed")]
        df.columns = [str(c).strip() for c in df.columns]
        return df

    def looks_wrong(df):
        if df.empty:
            return True
        cols = df.columns.tolist()
        if len(cols) == 1:
            return True
        if len(cols) != len(headers) and not set(headers).issubset(cols):
            return True
        return False

    df = clean(read_csv())
    if looks_wrong(df):
        df_alt = clean(read_csv(sep=";", decimal=","))
        if not looks_wrong(df_alt):
            df = df_alt
    if df.shape[1] == len(headers) and not set(headers).issubset(df.columns):
        df.columns = headers
    if set(headers).issubset(df.columns):
        df = df[headers]
    return df


def prepare_data(df, headers):
    df.columns = [str(c).strip() for c in df.columns]
    df = df.apply(pd.to_numeric, errors="coerce")
    label_col = next((c for c in df.columns if c.lower() == "outcome"), None)
    if label_col is None:
        label_col = headers[-1] if headers and headers[-1] in df.columns else df.columns[-1]
    zero_cols = [
        c
        for c in ["Glucose", "BloodPressure", "SkinThickness", "BMI", "Insulin"]
        if c in df.columns
    ]
    if zero_cols:
        df[zero_cols] = df[zero_cols].replace(0, np.nan)
        df[zero_cols] = df[zero_cols].fillna(df[zero_cols].mean())
    feature_cols = [c for c in df.columns if c != label_col]
    X = df[feature_cols].to_numpy(dtype=np.float64)
    y = df[label_col].to_numpy(dtype=np.int64)
    return X, y


def train_and_evaluate(X, y, seed):
    rng = np.random.RandomState(seed)
    mask = rng.rand(len(y)) < 0.8
    scaler = StandardScaler(with_mean=False)
    X_scaled = scaler.fit_transform(X)
    X_train = X_scaled[mask]
    X_test = X_scaled[~mask]
    y_train = y[mask]
    y_test = y[~mask]
    if y_train.size == 0 or y_test.size == 0:
        split_idx = int(0.8 * len(y))
        X_train = X_scaled[:split_idx]
        X_test = X_scaled[split_idx:]
        y_train = y[:split_idx]
        y_test = y[split_idx:]
    dataset_size = y_train.size
    if dataset_size == 0:
        return 0.0
    num_pos = np.sum(y_train == 1)
    num_neg = dataset_size - num_pos
    balance_ratio = num_neg / dataset_size
    weights = np.where(y_train == 1, balance_ratio, 1.0 - balance_ratio)
    selector = SelectFpr(chi2, alpha=0.05)
    X_train_sel = selector.fit_transform(X_train, y_train)
    X_test_sel = selector.transform(X_test)
    if X_train_sel.shape[1] == 0:
        X_train_sel = X_train
        X_test_sel = X_test
    lr = LogisticRegression(max_iter=10, solver="lbfgs", C=1e6, random_state=seed)
    lr.fit(X_train_sel, y_train, sample_weight=weights)
    y_pred = lr.predict(X_test_sel)
    accuracy = float(np.mean(y_pred == y_test)) if y_test.size else 0.0
    return accuracy


def main():
    df = load_dataset(DATASET_PATH, DATASET_HEADERS)
    X, y = prepare_data(df, DATASET_HEADERS)
    accuracy = train_and_evaluate(X, y, RANDOM_SEED)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()


# Optimization Summary
# - Replaced the distributed Spark pipeline with a lightweight pandas/sklearn workflow to reduce startup and data movement overhead.
# - Applied vectorized zero-to-NaN replacement and mean imputation to avoid iterative transformations.
# - Scaled the full dataset once and reused a fixed-seed boolean mask for deterministic, low-cost splitting.
# - Fit the feature selector a single time and reused it for test data to remove redundant fitting.
# - Limited computation to required predictions and the final accuracy output only.