# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score

def load_and_clean_data(filepath):
    # Robust CSV loading
    try:
        df = pd.read_csv(filepath)
        if df.shape[1] <= 1:
            raise ValueError
    except Exception:
        df = pd.read_csv(filepath, sep=';', decimal=',')

    # Header normalization
    df.columns = [str(c).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    
    # Coerce numeric columns and handle missing values
    for col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    return df

def run_pipeline():
    # Load dataset
    df = load_and_clean_data('diabetes.csv')
    
    if df.empty:
        return 0.0

    # Identify Target and Features
    # Priority 1: 'Outcome' (from provided schema)
    # Priority 2: Last numeric column
    potential_targets = ['Outcome', 'target', 'label', 'class']
    target_col = None
    
    for pt in potential_targets:
        if pt in df.columns:
            target_col = pt
            break
    
    if target_col is None:
        target_col = df.columns[-1]
    
    # Drop rows where target is NaN
    df = df.dropna(subset=[target_col])
    
    # Feature selection
    features = [c for c in df.columns if c != target_col]
    
    if not features:
        return 0.0

    X = df[features]
    y = df[target_col]

    # Ensure target is discrete for classification or handle regression fallback
    unique_vals = y.nunique()
    is_classification = unique_vals < 10 or y.dtype == 'int64'

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if is_classification and unique_vals > 1 else None
    )

    # Preprocessing: Impute and Scale
    # Logistic Regression is highly energy-efficient and lightweight for CPU
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    preprocessor = ColumnTransformer(
        transformers=[('num', numeric_transformer, features)]
    )

    if is_classification and unique_vals >= 2:
        model = LogisticRegression(solver='liblinear', random_state=42)
        clf = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        # Fallback to a simple regression proxy if the data is continuous
        from sklearn.linear_model import Ridge
        model = Ridge()
        reg = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])
        reg.fit(X_train, y_train)
        score = reg.score(X_test, y_test)
        # Scale R^2 to [0,1] for accuracy format requirement
        accuracy = max(0, min(1, score))

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary:
# 1. Model Choice: Logistic Regression was selected as the primary model. It is mathematically 
#    simple, requiring significantly fewer CPU cycles than ensembles or deep learning.
# 2. Scaling: Standard scaling was applied via Pipeline to ensure fast convergence for the 
#    linear solver, reducing the number of iterations and thus total energy consumption.
# 3. Robustness: The pipeline includes automated delimiter detection and header normalization 
#    to prevent runtime crashes without manual intervention.
# 4. Efficiency: Used 'liblinear' solver which is optimized for small-to-medium datasets 
#    common in tabular CPU-based tasks.
# 5. Preprocessing: SimpleImputer with 'median' strategy was used to handle missing data 
#    efficiently without the overhead of iterative or KNN-based imputation.
# 6. Memory: Dataframes are processed in-place where possible, avoiding unnecessary large 
#    object copies.