# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score
from sklearn.dummy import DummyClassifier

warnings.filterwarnings("ignore")

dataset_path = "diabetes.csv"
dataset_headers = ["Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "DiabetesPedigreeFunction", "Age", "Outcome"]

def read_csv_robust(path):
    df_local = pd.DataFrame()
    try:
        df_local = pd.read_csv(path)
    except Exception:
        df_local = pd.DataFrame()
    if df_local.empty or df_local.shape[1] <= 1 or any(";" in str(c) for c in df_local.columns):
        try:
            df_local = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            pass
    return df_local

df = read_csv_robust(dataset_path)

if not df.empty and df.shape[1] == len(dataset_headers):
    cols_generic = True
    for c in df.columns:
        if isinstance(c, str):
            c_str = c.strip().lower()
            if c_str and not c_str.startswith("unnamed") and not c_str.replace(".", "", 1).isdigit():
                cols_generic = False
                break
        elif not isinstance(c, (int, np.integer)):
            cols_generic = False
            break
    if cols_generic:
        df.columns = dataset_headers

def normalize_columns(cols):
    new_cols = []
    for c in cols:
        if not isinstance(c, str):
            c = str(c)
        c = c.strip()
        c = " ".join(c.split())
        new_cols.append(c)
    return new_cols

if not df.empty:
    df.columns = normalize_columns(df.columns)
    df = df.loc[:, [c for c in df.columns if not c.lower().startswith("unnamed")]]

assert len(df) > 0

lower_map = {c.lower(): c for c in df.columns}
preferred_targets = ["outcome", "target", "label", "y", "class"]
target_col = None
for pt in preferred_targets:
    if pt in lower_map:
        target_col = lower_map[pt]
        break
if target_col is None:
    numeric_candidates = []
    for col in df.columns:
        s_num = pd.to_numeric(df[col].astype(str).str.replace(",", ".", regex=False), errors="coerce")
        if s_num.notna().sum() > 0 and s_num.nunique(dropna=True) > 1:
            numeric_candidates.append(col)
    if numeric_candidates:
        target_col = numeric_candidates[-1]
    else:
        target_col = df.columns[-1]

df = df.copy()
y_raw = df[target_col]
y_num = pd.to_numeric(y_raw.astype(str).str.replace(",", ".", regex=False), errors="coerce")
if y_raw.dtype.kind in "biufc" or (len(y_raw) > 0 and y_num.notna().sum() / len(y_raw) >= 0.8):
    is_numeric_target = True
    df[target_col] = y_num
else:
    is_numeric_target = False
    y_obj = y_raw.astype(str)
    y_obj[y_raw.isna()] = np.nan
    df[target_col] = y_obj

df = df.replace([np.inf, -np.inf], np.nan)
df = df.dropna(subset=[target_col])
assert len(df) > 0

y = df[target_col]
X = df.drop(columns=[target_col])

if X.shape[1] == 0:
    X = pd.DataFrame({"dummy_feature": np.zeros(len(df))}, index=df.index)
    numeric_features = ["dummy_feature"]
    categorical_features = []
else:
    feature_cols = list(X.columns)
    numeric_features = []
    categorical_features = []
    X_converted = pd.DataFrame(index=X.index)
    for col in feature_cols:
        s = X[col]
        s_num = pd.to_numeric(s.astype(str).str.replace(",", ".", regex=False), errors="coerce")
        non_na = s_num.notna().sum()
        if s.dtype.kind in "biufc" or (non_na > 0 and non_na / len(s) >= 0.8):
            if non_na > 0:
                X_converted[col] = s_num
                numeric_features.append(col)
        else:
            s_obj = s.astype(str)
            s_obj[s.isna()] = np.nan
            if s_obj.notna().sum() > 0:
                X_converted[col] = s_obj
                categorical_features.append(col)
    if X_converted.shape[1] == 0:
        X_converted = pd.DataFrame({"dummy_feature": np.zeros(len(df))}, index=df.index)
        numeric_features = ["dummy_feature"]
        categorical_features = []
    X = X_converted

X = X.replace([np.inf, -np.inf], np.nan)
cols_to_drop = [c for c in X.columns if X[c].isna().all()]
if cols_to_drop:
    X = X.drop(columns=cols_to_drop)
    numeric_features = [c for c in numeric_features if c in X.columns]
    categorical_features = [c for c in categorical_features if c in X.columns]
if X.shape[1] == 0:
    X = pd.DataFrame({"dummy_feature": np.zeros(len(df))}, index=df.index)
    numeric_features = ["dummy_feature"]
    categorical_features = []

n_samples = len(X)
if n_samples < 2:
    accuracy = 1.0
    print(f"ACCURACY={accuracy:.6f}")
    raise SystemExit

n_unique = y.nunique(dropna=True)
if is_numeric_target:
    if n_unique <= 20 and (n_unique / max(1, n_samples)) <= 0.2:
        problem_type = "classification"
    else:
        problem_type = "regression"
else:
    problem_type = "classification"
if problem_type == "classification" and n_unique < 2:
    problem_type = "regression"

if problem_type == "regression" and not pd.api.types.is_numeric_dtype(y):
    y_numeric = pd.to_numeric(pd.Series(y).astype(str).str.replace(",", ".", regex=False), errors="coerce")
    mask = y_numeric.notna()
    y = y_numeric[mask]
    X = X.loc[mask]
    n_samples = len(X)
    if n_samples < 2:
        accuracy = 1.0
        print(f"ACCURACY={accuracy:.6f}")
        raise SystemExit

test_count = max(1, int(round(n_samples * 0.2)))
if n_samples - test_count < 1:
    test_count = max(1, n_samples // 2)
if n_samples - test_count < 1:
    test_count = 1
test_size = test_count

stratify = None
if problem_type == "classification" and y.nunique() >= 2:
    class_counts = y.value_counts()
    if class_counts.min() >= 2 and test_size >= y.nunique():
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=42, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0

try:
    ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=True)
except TypeError:
    ohe = OneHotEncoder(handle_unknown="ignore", sparse=True)

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False))
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", ohe)
])

transformers = []
if numeric_features:
    transformers.append(("num", numeric_transformer, numeric_features))
if categorical_features:
    transformers.append(("cat", categorical_transformer, categorical_features))
if not transformers:
    transformers.append(("num", numeric_transformer, list(X.columns)))

preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.0)

if problem_type == "classification":
    if y_train.nunique() < 2:
        model = DummyClassifier(strategy="most_frequent")
    else:
        if y_train.nunique() <= 2:
            model = LogisticRegression(max_iter=200, solver="liblinear")
        else:
            model = LogisticRegression(max_iter=200, solver="lbfgs", multi_class="auto")
else:
    model = LinearRegression()

pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", model)
])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if problem_type == "classification":
    accuracy = accuracy_score(y_test, y_pred)
else:
    y_true = pd.to_numeric(pd.Series(y_test), errors="coerce").to_numpy()
    y_pred_num = pd.to_numeric(pd.Series(y_pred), errors="coerce").to_numpy()
    mask = np.isfinite(y_true) & np.isfinite(y_pred_num)
    if mask.sum() == 0:
        accuracy = 0.0
    else:
        y_true = y_true[mask]
        y_pred_num = y_pred_num[mask]
        mae = np.mean(np.abs(y_true - y_pred_num))
        y_range = np.nanmax(y_true) - np.nanmin(y_true)
        if not np.isfinite(y_range) or y_range == 0:
            accuracy = 1.0 if mae == 0 else 0.0
        else:
            accuracy = 1.0 - mae / y_range
            accuracy = float(np.clip(accuracy, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Used lightweight linear/logistic models with simple imputation and scaling for CPU efficiency.
# Applied robust schema/type inference with numeric coercion and minimal one-hot encoding.
# For regression fallback, reported a normalized MAE-based accuracy proxy bounded in [0,1].