# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import re
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, r2_score
from sklearn.linear_model import LogisticRegression, Ridge

warnings.filterwarnings("ignore")

DATASET_PATH = "diabetes.csv"
EXPECTED_HEADERS = ["Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "DiabetesPedigreeFunction", "Age", "Outcome"]

def read_csv_robust(path, header="infer"):
    try:
        df = pd.read_csv(path, header=header)
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",", header=header)
    if df.shape[1] == 1:
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",", header=header)
            if df2.shape[1] > 1:
                df = df2
        except Exception:
            pass
    return df

def normalize_columns(df):
    new_cols = []
    for c in df.columns:
        c_str = str(c).strip()
        c_str = re.sub(r"\s+", " ", c_str)
        new_cols.append(c_str)
    df.columns = new_cols
    df = df.loc[:, [c for c in df.columns if not str(c).startswith("Unnamed")]]
    return df

def choose_target(df):
    lower_map = {str(c).lower(): c for c in df.columns}
    for key in ["outcome", "target", "label", "class"]:
        if key in lower_map:
            return lower_map[key]
    numeric_cols = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() > 0:
            numeric_cols.append((c, s.nunique(dropna=True)))
    for c, nunq in reversed(numeric_cols):
        if nunq > 1:
            return c
    for c in reversed(df.columns):
        if df[c].nunique(dropna=True) > 1:
            return c
    return df.columns[-1]

df = read_csv_robust(DATASET_PATH, header="infer")
if df.shape[1] == len(EXPECTED_HEADERS):
    cols = [str(c).strip() for c in df.columns]
    if all(pd.notna(pd.to_numeric(c, errors="coerce")) for c in cols):
        df = read_csv_robust(DATASET_PATH, header=None)
        df.columns = EXPECTED_HEADERS
df = normalize_columns(df)
df = df.dropna(how="all")
assert df.shape[0] > 0 and df.shape[1] > 0

target = choose_target(df)
if target not in df.columns:
    target = df.columns[-1]

y_raw = df[target]
y_numeric = pd.to_numeric(y_raw, errors="coerce").replace([np.inf, -np.inf], np.nan)
if y_raw.dtype == "object" or y_raw.dtype.name == "category":
    mask = y_raw.notna()
else:
    mask = y_numeric.notna() if y_numeric.notna().sum() > 0 else y_raw.notna()
if mask.sum() == 0:
    mask = np.ones(len(df), dtype=bool)
df = df.loc[mask].copy()
y_raw = df[target].copy()
if y_raw.isna().any():
    if y_raw.dtype == "object" or y_raw.dtype.name == "category":
        y_raw = y_raw.fillna("missing")
    else:
        y_raw = y_raw.fillna(0)
y_numeric = pd.to_numeric(y_raw, errors="coerce").replace([np.inf, -np.inf], np.nan)
if y_numeric.isna().any():
    y_numeric = y_numeric.fillna(0)
X = df.drop(columns=[target])

if y_raw.dtype == "object" or y_raw.dtype.name == "category":
    is_classification = True
    y = y_raw.astype(str)
else:
    num_unique = y_numeric.nunique(dropna=True)
    threshold = max(2, min(20, int(0.1 * len(y_numeric)) if len(y_numeric) > 0 else 2))
    is_classification = num_unique <= threshold
    y = y_numeric
if is_classification:
    if pd.Series(y).nunique(dropna=True) < 2:
        is_classification = False
        y = y_numeric

if X.shape[1] == 0:
    X = pd.DataFrame({"constant": np.ones(len(df))})
    numeric_cols = ["constant"]
    categorical_cols = []
else:
    numeric_cols = []
    categorical_cols = []
    drop_cols = []
    for c in X.columns:
        s = X[c]
        s_num = pd.to_numeric(s, errors="coerce").replace([np.inf, -np.inf], np.nan)
        if s_num.notna().sum() >= max(1, int(0.5 * len(s_num))):
            if s_num.notna().sum() == 0:
                drop_cols.append(c)
            else:
                X[c] = s_num
                numeric_cols.append(c)
        else:
            if s.isna().all():
                drop_cols.append(c)
            else:
                categorical_cols.append(c)
    if drop_cols:
        X = X.drop(columns=drop_cols)
        numeric_cols = [c for c in numeric_cols if c not in drop_cols]
        categorical_cols = [c for c in categorical_cols if c not in drop_cols]
    if X.shape[1] == 0:
        X = pd.DataFrame({"constant": np.ones(len(df))})
        numeric_cols = ["constant"]
        categorical_cols = []

transformers = []
if numeric_cols:
    num_pipe = Pipeline(steps=[("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler())])
    transformers.append(("num", num_pipe, numeric_cols))
if categorical_cols:
    cat_pipe = Pipeline(steps=[("imputer", SimpleImputer(strategy="most_frequent")), ("onehot", OneHotEncoder(handle_unknown="ignore"))])
    transformers.append(("cat", cat_pipe, categorical_cols))
preprocess = ColumnTransformer(transformers=transformers, remainder="drop")

if is_classification:
    model = LogisticRegression(max_iter=200, solver="liblinear")
else:
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])

n_samples = len(X)
if n_samples < 2:
    X_train, X_test, y_train, y_test = X.copy(), X.copy(), y.copy(), y.copy()
else:
    test_size = max(1, int(round(0.2 * n_samples)))
    if n_samples - test_size < 1:
        test_size = max(1, n_samples // 2)
    stratify = None
    if is_classification:
        y_series = pd.Series(y)
        class_counts = y_series.value_counts()
        if class_counts.min() >= 2:
            n_classes = class_counts.shape[0]
            if test_size >= n_classes:
                stratify = y_series
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )
assert len(X_train) > 0 and len(X_test) > 0

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if is_classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    if len(y_test) < 2:
        r2 = 0.0
    else:
        r2 = r2_score(y_test, y_pred)
        if np.isnan(r2):
            r2 = 0.0
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# Used lightweight linear models (logistic regression or ridge) to minimize CPU usage.
# Preprocessing uses simple imputation, scaling, and one-hot encoding in a reproducible pipeline.
# Regression fallback reports a bounded accuracy proxy derived from (R2+1)/2 for stability.