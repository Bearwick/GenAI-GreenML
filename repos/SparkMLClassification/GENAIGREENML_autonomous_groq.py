# Generated by generate_llm_code.py
# LLM: groq
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset
data = pd.read_csv("diabetes.csv")

# Define feature columns and target
feature_cols = [
    "Pregnancies", "Glucose", "BloodPressure", "SkinThickness",
    "Insulin", "BMI", "DiabetesPedigreeFunction", "Age"
]
target_col = "Outcome"

# Replace zeros with NaN for specific columns that can have missing values
missing_cols = ["Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI"]
data[missing_cols] = data[missing_cols].replace(0, np.nan)

# Impute missing values with column means
imputer = SimpleImputer(strategy="mean")
data[missing_cols] = imputer.fit_transform(data[missing_cols])

# Standardize features
scaler = StandardScaler()
X = scaler.fit_transform(data[feature_cols])
y = data[target_col].values

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Logistic regression with class weight to handle imbalance
model = LogisticRegression(
    solver="liblinear",
    max_iter=200,
    class_weight="balanced",
    random_state=42
)
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Justification:
# - Uses pandas and scikit-learn (CPUâ€‘friendly) instead of heavy Spark frameworks.
# - SimpleImputer + StandardScaler are lightweight preprocessing steps.
# - LogisticRegression with liblinear solver is efficient for small tabular data.
# - class_weight='balanced' addresses class imbalance without needing extra balancing logic.
# - The pipeline avoids deep learning or large embeddings, ensuring minimal energy consumption.