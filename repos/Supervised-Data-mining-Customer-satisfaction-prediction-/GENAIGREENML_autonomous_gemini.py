# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import accuracy_score

data = pd.read_csv("EireJet.csv")
data = data.dropna()

data['Gender'] = data['Gender'].map({'Female': 1, 'Male': 0})
data['Frequent Flyer'] = data['Frequent Flyer'].map({'Yes': 1, 'No': 0})
data['Type of Travel'] = data['Type of Travel'].map({'Personal Travel': 1, 'Business travel': 0})
data['Class'] = data['Class'].map({'Eco': 0, 'Eco Plus': 1, 'Business': 2})
data['satisfaction'] = data['satisfaction'].map({'neutral or dissatisfied': 0, 'satisfied': 1})

X = data.drop('satisfaction', axis=1)
y = data['satisfaction']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)

model = HistGradientBoostingClassifier(
    max_iter=100,
    max_depth=10,
    random_state=1,
    early_stopping=True,
    validation_fraction=0.1,
    n_iter_no_change=5
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

"""
DESIGN JUSTIFICATION FOR ENERGY EFFICIENCY:
1. Model Choice: Used HistGradientBoostingClassifier instead of standard GradientBoosting or RandomForest. 
   It uses histogram-based binning of continuous features, which drastically reduces the number of split 
   points the algorithm needs to evaluate, lowering CPU cycles and memory usage.
2. Training Efficiency: Enabled early_stopping to prevent unnecessary iterations once the model 
   converges, saving energy by truncating the training process.
3. Preprocessing: Implemented manual label mapping rather than One-Hot Encoding to keep the 
   feature dimensionality low, which reduces the computational footprint during both training and inference.
4. Data Handling: Avoided SMOTE (Synthetic Minority Over-sampling) as it increases the training set size 
   and computational load; boosted trees generally handle the slight class imbalance present in 
   satisfaction datasets efficiently without augmentation.
5. Hardware Optimization: The solution is designed for CPU execution, avoiding the energy overhead 
   associated with initializing and transferring data to/from a GPU.
"""