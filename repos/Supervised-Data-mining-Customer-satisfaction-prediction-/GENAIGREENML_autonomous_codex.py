# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os, glob, re, warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")

def find_csv():
    candidates = []
    env_path = os.environ.get("DATASET_PATH")
    if env_path and os.path.isfile(env_path):
        candidates.append(env_path)
    for name in ["EireJet.csv", "data.csv", "dataset.csv", "train.csv"]:
        if os.path.isfile(name):
            candidates.append(name)
    if not candidates:
        files = sorted(glob.glob("*.csv"))
        if files:
            candidates.append(files[0])
    return candidates[0] if candidates else None

def read_csv_flexible(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
        return df
    if df.shape[1] == 1:
        sample = ""
        if len(df.columns) > 0:
            sample += str(df.columns[0])
        if len(df) > 0:
            sample += str(df.iloc[0, 0])
        if ";" in sample:
            try:
                df = pd.read_csv(path, sep=";", decimal=",")
            except Exception:
                pass
    return df

csv_path = find_csv()
if csv_path is None:
    raise FileNotFoundError("No CSV file found.")

df = read_csv_flexible(csv_path)

df.columns = [re.sub(r"\s+", " ", str(c).strip()) for c in df.columns]
df = df.loc[:, ~df.columns.str.match(r"^Unnamed")]

df.replace([np.inf, -np.inf], np.nan, inplace=True)

assert df.shape[0] > 0

col_lower = {c.lower(): c for c in df.columns}
target_col = None
for cand in ["satisfaction", "target", "label", "class", "outcome", "y"]:
    if cand in col_lower:
        target_col = col_lower[cand]
        break
if target_col is None:
    for col in df.columns:
        series_num = pd.to_numeric(df[col], errors="coerce")
        if series_num.notna().sum() > 0 and series_num.nunique(dropna=True) > 1:
            target_col = col
            df[col] = series_num
            break
if target_col is None:
    target_col = df.columns[-1]

y_raw = df[target_col]
y_numeric = pd.to_numeric(y_raw, errors="coerce")
if y_numeric.notna().mean() >= 0.9:
    y_series = y_numeric
    y_is_numeric = True
else:
    y_series = y_raw
    y_is_numeric = False

mask = y_series.notna()
df = df.loc[mask].reset_index(drop=True)
y_series = y_series.loc[mask].reset_index(drop=True)
assert df.shape[0] > 0

X = df.drop(columns=[target_col])
if X.shape[1] == 0:
    X = pd.DataFrame({"dummy": np.zeros(len(df))})

is_classification = False
if not y_is_numeric:
    is_classification = True
else:
    unique_vals = pd.unique(y_series.dropna())
    if len(unique_vals) <= 20 and np.all(np.isclose(unique_vals, np.round(unique_vals))):
        is_classification = True

if is_classification:
    if not y_is_numeric:
        le = LabelEncoder()
        y_processed = pd.Series(le.fit_transform(y_series.astype(str)))
    else:
        y_processed = pd.Series(np.round(y_series).astype(int))
else:
    y_processed = pd.Series(pd.to_numeric(y_series, errors="coerce"))

X = X.copy()
numeric_cols = []
categorical_cols = []
n_rows = len(X)
for col in X.columns:
    col_data = X[col]
    if pd.api.types.is_numeric_dtype(col_data):
        X[col] = pd.to_numeric(col_data, errors="coerce")
        numeric_cols.append(col)
    else:
        converted = pd.to_numeric(col_data, errors="coerce")
        non_na = converted.notna().sum()
        if n_rows > 0 and non_na / n_rows >= 0.8:
            X[col] = converted
            numeric_cols.append(col)
        else:
            categorical_cols.append(col)

all_nan_cols = [col for col in X.columns if X[col].isna().all()]
if all_nan_cols:
    X = X.drop(columns=all_nan_cols)
    numeric_cols = [c for c in numeric_cols if c not in all_nan_cols]
    categorical_cols = [c for c in categorical_cols if c not in all_nan_cols]

if X.shape[1] == 0:
    X = pd.DataFrame({"dummy": np.zeros(len(df))})
    numeric_cols = ["dummy"]
    categorical_cols = []

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

try:
    ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=True)
except TypeError:
    ohe = OneHotEncoder(handle_unknown="ignore", sparse=True)

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", ohe)
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_cols),
        ("cat", categorical_transformer, categorical_cols)
    ],
    remainder="drop"
)

if is_classification:
    unique_classes = np.unique(y_processed)
    if len(unique_classes) < 2:
        estimator = DummyClassifier(strategy="most_frequent")
    else:
        estimator = LogisticRegression(max_iter=200, solver="liblinear")
else:
    if y_processed.nunique() < 2:
        estimator = DummyRegressor(strategy="mean")
    else:
        estimator = Ridge(alpha=1.0, random_state=42)

model = Pipeline(steps=[("preprocess", preprocess), ("model", estimator)])

n_samples = len(X)
if n_samples < 2:
    X_train, X_test = X.copy(), X.copy()
    y_train, y_test = y_processed.copy(), y_processed.copy()
else:
    test_size = 0.2 if n_samples >= 5 else 0.5
    stratify = None
    if is_classification and len(np.unique(y_processed)) >= 2:
        class_counts = pd.Series(y_processed).value_counts()
        if class_counts.min() >= 2:
            stratify = y_processed
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_processed, test_size=test_size, random_state=42, stratify=stratify
    )

assert len(X_train) > 0 and len(X_test) > 0

model.fit(X_train, y_train)
preds = model.predict(X_test)

if is_classification:
    if len(np.unique(y_processed)) < 2:
        accuracy = 1.0
    else:
        accuracy = accuracy_score(y_test, preds)
else:
    if np.nanstd(y_test) == 0:
        accuracy = 1.0
    else:
        r2 = r2_score(y_test, preds)
        r2_clipped = max(-1.0, min(1.0, r2))
        accuracy = (r2_clipped + 1.0) / 2.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Lightweight LogisticRegression/Ridge (or dummy fallback) keeps computation CPU-friendly and energy efficient.
# - ColumnTransformer with simple imputation, scaling, and one-hot encoding provides reproducible preprocessing.
# - Robust CSV parsing and dynamic target/feature selection handle unknown schemas without heavy processing.
# - Regression accuracy uses clipped R2 mapped to [0,1] as a stable accuracy proxy.