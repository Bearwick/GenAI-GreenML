# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

def load_data(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
        return df
    if df.shape[1] == 1:
        try:
            with open(path, 'r', encoding='utf-8') as f:
                first_line = f.readline()
            if ';' in first_line:
                df = pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            try:
                df = pd.read_csv(path, sep=';', decimal=',')
            except Exception:
                pass
    return df

data_path = "EireJet (1).csv"
df = load_data(data_path)

df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
df = df.loc[:, ~df.columns.str.contains(r'^Unnamed', case=False, na=False)]

cols_lower = {c.lower(): c for c in df.columns}
if 'satisfaction' in cols_lower:
    target_col = cols_lower['satisfaction']
else:
    numeric_candidates = []
    for c in df.columns:
        ser = pd.to_numeric(df[c], errors='coerce')
        if ser.notna().sum() > 0 and ser.nunique(dropna=True) > 1:
            numeric_candidates.append(c)
    if numeric_candidates:
        target_col = numeric_candidates[0]
    else:
        target_col = df.columns[-1]

df = df.copy()
y_raw = df[target_col]
mask = y_raw.notna()
df = df.loc[mask].copy()
y_raw = y_raw.loc[mask]

numeric_target = False
y_numeric = None
if pd.api.types.is_numeric_dtype(y_raw):
    numeric_target = True
    y_numeric = y_raw
else:
    coerced_y = pd.to_numeric(y_raw, errors='coerce')
    if coerced_y.notna().mean() > 0.9:
        numeric_target = True
        y_numeric = coerced_y

if numeric_target:
    y_numeric = pd.to_numeric(y_numeric, errors='coerce')
    y_numeric = y_numeric.replace([np.inf, -np.inf], np.nan)
    mask = y_numeric.notna()
    df = df.loc[mask].copy()
    y_numeric = y_numeric.loc[mask]
    unique_vals = y_numeric.nunique(dropna=True)
    if unique_vals <= 20 and np.all(np.isclose((y_numeric.dropna() % 1), 0)):
        task = 'classification'
        y_final = y_numeric
    else:
        task = 'regression'
        y_final = y_numeric
else:
    task = 'classification'
    y_final = y_raw.astype(str)

assert df.shape[0] > 0

feature_cols = [c for c in df.columns if c != target_col]

if len(feature_cols) == 0:
    df["_constant_feature"] = 0
    feature_cols = ["_constant_feature"]

numeric_features = []
categorical_features = []
valid_features = []
for col in feature_cols:
    series = df[col]
    if series.notna().sum() == 0:
        continue
    if pd.api.types.is_numeric_dtype(series):
        df[col] = pd.to_numeric(series, errors='coerce')
        numeric_features.append(col)
        valid_features.append(col)
    else:
        coerced = pd.to_numeric(series, errors='coerce')
        if coerced.notna().mean() > 0.8:
            df[col] = coerced
            numeric_features.append(col)
            valid_features.append(col)
        else:
            categorical_features.append(col)
            valid_features.append(col)

feature_cols = valid_features

if len(feature_cols) == 0:
    df["_constant_feature"] = 0
    feature_cols = ["_constant_feature"]
    numeric_features = ["_constant_feature"]
    categorical_features = []

if numeric_features:
    df[numeric_features] = df[numeric_features].replace([np.inf, -np.inf], np.nan)

X = df[feature_cols]

if task == 'classification':
    le = LabelEncoder()
    y_encoded = le.fit_transform(y_final.astype(str))
else:
    y_encoded = pd.to_numeric(y_final, errors='coerce').to_numpy()

n_samples = len(X)
if n_samples < 2:
    X = pd.concat([X, X], ignore_index=True)
    y_encoded = np.concatenate([y_encoded, y_encoded])
    n_samples = len(X)

test_size = int(round(0.2 * n_samples))
if test_size < 1:
    test_size = 1
if n_samples - test_size < 1:
    test_size = n_samples - 1

stratify = None
if task == 'classification':
    y_series = pd.Series(y_encoded)
    n_classes = y_series.nunique()
    if n_classes > 1:
        min_count = y_series.value_counts().min()
        if min_count >= 2 and test_size >= n_classes and (n_samples - test_size) >= n_classes:
            stratify = y_series

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y_encoded,
    test_size=test_size,
    random_state=42,
    stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0

transformers = []
if numeric_features:
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler(with_mean=False))
    ])
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
    ])
    transformers.append(('cat', categorical_transformer, categorical_features))

preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')

if task == 'classification':
    if len(np.unique(y_encoded)) < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear', n_jobs=1, random_state=42)
else:
    if len(np.unique(y_encoded)) < 2:
        model = DummyRegressor(strategy='mean')
    else:
        model = Ridge(alpha=1.0)

clf = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    if np.isnan(r2):
        r2 = 0.0
    accuracy = max(0.0, min(1.0, r2))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# Used lightweight linear models with sparse one-hot encoding to keep CPU usage low.
# Implemented robust column normalization, coercion, and simple imputers to avoid heavy preprocessing.
# Applied a single fixed train/test split and bounded R2 as an accuracy proxy for regression fallback.