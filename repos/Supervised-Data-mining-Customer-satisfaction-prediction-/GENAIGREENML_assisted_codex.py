# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import random
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE

warnings.filterwarnings("ignore")

DATASET_PATH = "EireJet (1).csv"
DATASET_HEADERS = "Gender,Frequent Flyer,Age,Type of Travel,Class,Flight Distance,Inflight wifi service,Departure/Arrival time convenient,Ease of Online booking,Gate location,Food and drink,Online boarding,Seat comfort,Inflight entertainment,On-board service,Leg room service,Baggage handling,Checkin service,Inflight service,Cleanliness,Departure Delay in Minutes,Arrival Delay in Minutes,satisfaction"

def normalize(text):
    return text.strip().lower()

def resolve_path(path):
    if os.path.exists(path):
        return path
    alt = path.replace(" (1)", "")
    if os.path.exists(alt):
        return alt
    return path

def parsing_looks_wrong(df, expected_headers):
    if df is None:
        return True
    if df.shape[1] == 1 and len(expected_headers) > 1:
        return True
    expected_norm = {normalize(h) for h in expected_headers}
    df_norm = {normalize(c) for c in df.columns}
    return len(df_norm & expected_norm) < max(1, len(expected_norm) // 2)

def load_dataset(path, expected_headers):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if parsing_looks_wrong(df, expected_headers):
        df = pd.read_csv(path, sep=';', decimal=',')
    df.columns = [c.strip() for c in df.columns]
    return df

def align_columns(df, expected_headers):
    norm_cols = {normalize(c): c for c in df.columns}
    norm_expected = [normalize(h) for h in expected_headers]
    if all(ne in norm_cols for ne in norm_expected):
        df = df[[norm_cols[ne] for ne in norm_expected]]
    return df

def preprocess(df, expected_headers):
    df = df.dropna(axis=0, how='any')
    mappings = {
        "Gender": {"Female": 1, "Male": 0},
        "Frequent Flyer": {"Yes": 1, "No": 0},
        "Type of Travel": {"Personal Travel": 1, "Business travel": 0},
        "Class": {"Eco": 0, "Eco Plus": 1, "Business": 2},
        "satisfaction": {"neutral or dissatisfied": 0, "satisfied": 1},
    }
    norm_cols = {normalize(c): c for c in df.columns}
    for key, mapping in mappings.items():
        col = norm_cols.get(normalize(key))
        if col is not None:
            df[col] = df[col].map(mapping)
    target_norm = normalize(expected_headers[-1])
    target_col = norm_cols.get(target_norm, df.columns[-1])
    y = df.pop(target_col)
    X = df
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    return X_scaled, y

def train_and_evaluate(X_scaled, y):
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=100
    )
    smote = SMOTE(random_state=101)
    X_res, y_res = smote.fit_resample(X_train, y_train)
    model = GradientBoostingClassifier(
        n_estimators=200, max_depth=9, max_leaf_nodes=32, random_state=1
    )
    model.fit(X_res, y_res)
    y_pred = model.predict(X_test)
    return accuracy_score(y_test, y_pred)

def main():
    np.random.seed(42)
    random.seed(42)
    headers = [h.strip() for h in DATASET_HEADERS.split(",")]
    path = resolve_path(DATASET_PATH)
    df = load_dataset(path, headers)
    df = align_columns(df, headers)
    X_scaled, y = preprocess(df, headers)
    accuracy = train_and_evaluate(X_scaled, y)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed grid searches, repeated splits, and unused model trainings to cut redundant computation.
# - Consolidated preprocessing with a single pass and in-place target extraction to reduce data copies.
# - Added robust CSV parsing fallback to avoid mis-parsing and rework.
# - Limited imports to only required components, reducing load time and memory.
# - Fixed random seeds for deterministic, reproducible results without extra overhead.