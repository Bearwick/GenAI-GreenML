# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
from typing import Tuple

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split
from sklearn.preprocessing import StandardScaler

try:
    from imblearn.over_sampling import SMOTE
except Exception as e:
    raise ImportError(
        "This script requires imbalanced-learn (imblearn) for SMOTE, as in the original code."
    ) from e


SEED = 100
SMOTE_SEED = 101
MODEL_SEED = 1


def _set_reproducible_seeds(seed: int) -> None:
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)


def _read_csv_with_fallback(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _encode_categoricals(df: pd.DataFrame, dataset_headers: Tuple[str, ...]) -> pd.DataFrame:
    colset = set(df.columns)
    mapping = {
        "Gender": {"Female": 1, "Male": 0},
        "Frequent Flyer": {"Yes": 1, "No": 0},
        "Type of Travel": {"Personal Travel": 1, "Business travel": 0},
        "Class": {"Eco": 0, "Eco Plus": 1, "Business": 2},
        "satisfaction": {"neutral or dissatisfied": 0, "satisfied": 1},
    }

    for col in ("Gender", "Frequent Flyer", "Type of Travel", "Class", "satisfaction"):
        if col in colset:
            df[col] = df[col].map(mapping[col])

    expected_cols = [c for c in dataset_headers if c in df.columns]
    if expected_cols:
        df = df.loc[:, expected_cols]
    return df


def _prepare_data(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
    df = df.dropna(axis=0, how="any")

    if "satisfaction" not in df.columns:
        raise KeyError("Target column 'satisfaction' not found in the loaded dataset.")

    y = df["satisfaction"].to_numpy()
    x_df = df.drop(columns=["satisfaction"])
    x = x_df.to_numpy()

    scaler = StandardScaler()
    x_scaled = scaler.fit_transform(x)
    return x_scaled, y


def _train_evaluate_random_forest(x_scaled: np.ndarray, y: np.ndarray) -> float:
    x_train, x_test, y_train, y_test = train_test_split(
        x_scaled,
        y,
        test_size=0.3,
        random_state=SEED,
        stratify=y if len(np.unique(y)) > 1 else None,
    )

    smote = SMOTE(random_state=SMOTE_SEED)
    x_train_os, y_train_os = smote.fit_resample(x_train, y_train)

    rfc = RandomForestClassifier(
        criterion="entropy",
        max_features="sqrt",
        random_state=MODEL_SEED,
        n_jobs=-1,
    )
    param_grid = {"n_estimators": [50, 100, 150, 200, 250, 300]}
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)

    grid = GridSearchCV(
        estimator=rfc,
        param_grid=param_grid,
        scoring="precision",
        cv=cv,
        n_jobs=-1,
    )
    grid.fit(x_train_os, y_train_os)

    best_n = int(grid.best_params_.get("n_estimators", 150))

    final_model = RandomForestClassifier(
        n_estimators=best_n,
        criterion="entropy",
        max_features="sqrt",
        random_state=MODEL_SEED,
        n_jobs=-1,
    )
    final_model.fit(x_train_os, y_train_os)

    y_pred = final_model.predict(x_test)
    return float(accuracy_score(y_test, y_pred))


def main() -> None:
    _set_reproducible_seeds(SEED)

    dataset_headers = (
        "Gender",
        "Frequent Flyer",
        "Age",
        "Type of Travel",
        "Class",
        "Flight Distance",
        "Inflight wifi service",
        "Departure/Arrival time convenient",
        "Ease of Online booking",
        "Gate location",
        "Food and drink",
        "Online boarding",
        "Seat comfort",
        "Inflight entertainment",
        "On-board service",
        "Leg room service",
        "Baggage handling",
        "Checkin service",
        "Inflight service",
        "Cleanliness",
        "Departure Delay in Minutes",
        "Arrival Delay in Minutes",
        "satisfaction",
    )

    df = _read_csv_with_fallback("EireJet.csv")
    df = _encode_categoricals(df, dataset_headers)
    x_scaled, y = _prepare_data(df)

    accuracy = _train_evaluate_random_forest(x_scaled, y)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed redundant train/test split and repeated preprocessing; split once and reuse results to cut compute and data movement.
# - Dropped heavy/unused imports (plotting, extra models, verbose metrics) to reduce overhead and startup time.
# - Replaced repeated pandas Series-based reporting/feature importance computations with a single accuracy evaluation to match required output and avoid extra work.
# - Used vectorized numpy/pandas conversions once (to_numpy) to minimize intermediate structures and memory churn.
# - Enabled parallelism where safe (n_jobs=-1) for RandomForest and GridSearchCV to reduce wall-clock time.
# - Reduced nondeterminism by setting fixed seeds and using StratifiedKFold(shuffle=True, random_state=...) for stable CV splits.
# - Made CSV ingestion robust with a fallback delimiter/decimal parse to prevent costly downstream failures/retries.
# - Kept the same ML task and pipeline intent (encoding -> scaling -> SMOTE -> CV-tuned RandomForest -> evaluation) while removing side effects and all original logging/plots.