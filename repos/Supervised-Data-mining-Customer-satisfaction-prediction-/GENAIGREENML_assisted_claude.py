# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import GradientBoostingClassifier

try:
    EJdata = pd.read_csv("EireJet (1).csv")
    if EJdata.shape[1] < 3:
        EJdata = pd.read_csv("EireJet (1).csv", sep=';', decimal=',')
except Exception:
    EJdata = pd.read_csv("EireJet (1).csv", sep=';', decimal=',')

EJdata = EJdata.dropna(how='any', axis=0)

gender_col = [c for c in EJdata.columns if 'gender' in c.lower()][0]
ff_col = [c for c in EJdata.columns if 'frequent' in c.lower()][0]
travel_col = [c for c in EJdata.columns if 'type' in c.lower()][0]
class_col = [c for c in EJdata.columns if c.lower() == 'class'][0]
target_col = [c for c in EJdata.columns if 'satisfaction' in c.lower()][0]

EJdata[gender_col] = EJdata[gender_col].map({'Female': 1, 'Male': 0})
EJdata[ff_col] = EJdata[ff_col].map({'Yes': 1, 'No': 0})
EJdata[travel_col] = EJdata[travel_col].map({'Personal Travel': 1, 'Business travel': 0})
EJdata[class_col] = EJdata[class_col].map({'Eco': 0, 'Eco Plus': 1, 'Business': 2})
EJdata[target_col] = EJdata[target_col].map({'neutral or dissatisfied': 0, 'satisfied': 1})

X = EJdata.drop(target_col, axis=1)
Y = EJdata[target_col]

feature_scaler = StandardScaler()
X_scaled = feature_scaler.fit_transform(X)

X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.3, random_state=100)

smote = SMOTE(random_state=101)
X_train, Y_train = smote.fit_resample(X_train, Y_train)

gb = GradientBoostingClassifier(n_estimators=200, max_depth=9, max_leaf_nodes=32, random_state=1)
gb.fit(X_train, Y_train)

Y_pred = gb.predict(X_test)
accuracy = metrics.accuracy_score(Y_test, Y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Removed all print statements, plots, and visualizations to eliminate I/O overhead.
# Removed redundant duplicate train_test_split call that was performed twice identically.
# Removed GridSearchCV for all three models (RandomForest, AdaBoost, GradientBoosting) since
#   the original code ultimately used fixed hyperparameters for final model training; grid search
#   was expensive computation whose results were already known and hardcoded.
# Kept only the best-performing model (GradientBoosting with tuned params) instead of training
#   all three models (RandomForest, AdaBoost, GradientBoosting), reducing compute by ~3x.
# Replaced deprecated fit_sample with fit_resample for SMOTE compatibility.
# Removed unused imports (matplotlib, seaborn, tree, GridSearchCV, RandomForest, AdaBoost).
# Used robust CSV parsing with fallback for separator/decimal handling.
# Used dynamic column name detection from actual dataframe columns instead of hardcoded names.
# Set all random seeds as in original to ensure reproducibility.
# Removed all artifact saving and intermediate data