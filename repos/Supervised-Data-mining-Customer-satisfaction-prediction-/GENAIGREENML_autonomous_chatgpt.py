# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

#!/usr/bin/env python
# coding: utf-8

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler


warnings.filterwarnings("ignore")


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df):
    keep = []
    for c in df.columns:
        if not re.match(r"^Unnamed:\s*\d+$", str(c)):
            keep.append(c)
    return df[keep]


def _read_csv_robust(path):
    # try default CSV parsing; if schema looks wrong, retry common European format
    df1 = pd.read_csv(path)
    df1.columns = _normalize_columns(df1.columns)
    df1 = _drop_unnamed(df1)
    if df1.shape[1] <= 1:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        df2.columns = _normalize_columns(df2.columns)
        df2 = _drop_unnamed(df2)
        return df2
    return df1


def _choose_target(df, dataset_headers=None):
    # prefer known target name if present, else choose a reasonable fallback
    cols_lower = {c.lower(): c for c in df.columns}
    if "satisfaction" in cols_lower:
        return cols_lower["satisfaction"]

    if dataset_headers is not None:
        for h in dataset_headers:
            hl = str(h).strip().lower()
            if hl in cols_lower:
                cand = cols_lower[hl]
                if cand.lower() in ("target", "label", "y"):
                    return cand

    # fallback: pick a non-constant numeric column if possible, else any non-constant column
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    for c in num_cols:
        s = df[c]
        s2 = pd.to_numeric(s, errors="coerce")
        if s2.nunique(dropna=True) > 1:
            return c

    for c in df.columns:
        if df[c].nunique(dropna=True) > 1:
            return c

    return df.columns[-1]


def _is_classification_target(y):
    # heuristic: classification if object/bool or few unique integer-like values
    if y.dtype == bool or y.dtype == object:
        return True
    y_num = pd.to_numeric(y, errors="coerce")
    uniq = y_num.dropna().unique()
    if len(uniq) == 0:
        return False
    if len(uniq) <= 20:
        # if values are integer-like, treat as classification
        if np.all(np.isclose(uniq, np.round(uniq))):
            return True
    return False


def _bounded_regression_score(y_true, y_pred):
    # stable proxy in [0,1] from normalized MAE:
    # score = 1 - MAE / (MAD + eps), clipped to [0,1]
    yt = np.asarray(y_true, dtype=float)
    yp = np.asarray(y_pred, dtype=float)
    mae = np.mean(np.abs(yt - yp))
    mad = np.mean(np.abs(yt - np.mean(yt)))
    score = 1.0 - (mae / (mad + 1e-12))
    return float(np.clip(score, 0.0, 1.0))


def main():
    dataset_headers = [
        "Gender", "Frequent Flyer", "Age", "Type of Travel", "Class", "Flight Distance",
        "Inflight wifi service", "Departure/Arrival time convenient", "Ease of Online booking",
        "Gate location", "Food and drink", "Online boarding", "Seat comfort",
        "Inflight entertainment", "On-board service", "Leg room service", "Baggage handling",
        "Checkin service", "Inflight service", "Cleanliness", "Departure Delay in Minutes",
        "Arrival Delay in Minutes", "satisfaction"
    ]

    # Locate CSV in a robust way
    candidate_paths = []
    env_path = os.environ.get("DATASET_PATH")
    if env_path:
        candidate_paths.append(env_path)
    candidate_paths += ["EireJet.csv", "data.csv", "dataset.csv", "train.csv", "EJdata.csv"]

    csv_path = None
    for p in candidate_paths:
        if p and os.path.exists(p) and os.path.isfile(p):
            csv_path = p
            break
    if csv_path is None:
        # last resort: pick first csv in current directory
        for fn in os.listdir("."):
            if fn.lower().endswith(".csv") and os.path.isfile(fn):
                csv_path = fn
                break
    if csv_path is None:
        raise FileNotFoundError("No CSV dataset found.")

    df = _read_csv_robust(csv_path)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Basic cleanup for robust numeric ops
    df = df.replace([np.inf, -np.inf], np.nan)

    assert df.shape[0] > 0 and df.shape[1] > 0, "Dataset is empty after loading/cleanup."

    target_col = _choose_target(df, dataset_headers=dataset_headers)

    # If target missing (extreme edge), fall back to last column
    if target_col not in df.columns:
        target_col = df.columns[-1]

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # If features ended up empty, create a constant feature to keep pipeline runnable
    if X.shape[1] == 0:
        X = pd.DataFrame({"__bias__": np.ones(len(df), dtype=float)})

    # Identify column types defensively (do not assume schema)
    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = [c for c in X.columns if c not in numeric_cols]

    # Attempt to coerce numeric-like strings in "numeric" expectation columns into numeric
    # but only where current dtype is object and coercion yields enough non-NaN values
    for c in categorical_cols[:]:
        s = X[c]
        coerced = pd.to_numeric(s, errors="coerce")
        if coerced.notna().mean() >= 0.8:
            X[c] = coerced
            numeric_cols.append(c)
            categorical_cols.remove(c)

    # Recompute after coercion
    numeric_cols = list(dict.fromkeys([c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]))
    categorical_cols = [c for c in X.columns if c not in numeric_cols]

    # Decide task type and construct y
    is_clf = _is_classification_target(y_raw)

    if is_clf:
        # preserve original labels; handle missing by imputing most frequent
        y_df = pd.DataFrame({"y": y_raw})
        y_proc = SimpleImputer(strategy="most_frequent").fit_transform(y_df).ravel()

        # if still degenerate, fallback to regression
        if pd.Series(y_proc).nunique(dropna=True) < 2:
            is_clf = False
            y_proc = pd.to_numeric(y_raw, errors="coerce")
    else:
        y_proc = pd.to_numeric(y_raw, errors="coerce")

    # Drop rows with missing y for regression, keep for classification (imputed above)
    if not is_clf:
        mask = pd.Series(y_proc).notna().values
        X = X.loc[mask].reset_index(drop=True)
        y_proc = np.asarray(y_proc)[mask]

    assert len(y_proc) > 1 and X.shape[0] == len(y_proc), "No usable samples after target processing."

    # Preprocessing pipeline
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3
    )

    # Split
    if is_clf:
        y_series = pd.Series(y_proc)
        stratify = y_series if y_series.nunique(dropna=True) >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_proc, test_size=0.25, random_state=42, stratify=stratify
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_proc, test_size=0.25, random_state=42
        )

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Train/test split failed."

    if is_clf:
        # Lightweight linear baseline; saga handles sparse OHE efficiently
        model = LogisticRegression(
            solver="saga",
            penalty="l2",
            C=1.0,
            max_iter=300,
            n_jobs=1,
            random_state=42
        )
        clf = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # CPU-friendly regression fallback; outputs bounded score as "accuracy proxy"
        model = Ridge(alpha=1.0, random_state=42)
        reg = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ])
        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        accuracy = _bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses a lightweight linear model (LogisticRegression with saga) as the default classifier for CPU efficiency and fast convergence on one-hot encoded sparse features.
# - Avoids energy-heavy ensembles/boosting/SMOTE/grid search; uses a single fixed train/test split with random_state for reproducibility and low compute.
# - ColumnTransformer + Pipeline ensures one-pass, reproducible preprocessing (median imputation + standard scaling for numeric; most-frequent + one-hot for categoricals).
# - Robust schema handling: normalizes headers, drops Unnamed columns, coercion of numeric-like strings, and selects target defensively (prefers 'satisfaction' if present).
# - Regression fallback uses Ridge and reports a bounded [0,1] "accuracy proxy" defined as 1 - MAE/(MAD+eps), clipped to [0,1], to remain stable across scales.