# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer, make_column_selector as selector
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "EireJet (1).csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c = str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        out.append(c)
    return out


def _read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d):
        if d is None or d.empty:
            return True
        if d.shape[1] <= 1:
            return True
        # heuristic: single wide column with many separators implies wrong delimiter
        if d.shape[1] == 1:
            s = d.iloc[:, 0].astype(str).head(20).str.contains(";").mean()
            if s > 0.3:
                return True
        return False

    if _looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            pass

    if df is None:
        raise RuntimeError("Failed to read dataset.")

    df.columns = _normalize_columns(df.columns)

    # Drop unnamed index-like columns
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+", c)]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")

    return df


def _coerce_numeric_inplace(df, cols):
    for c in cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")


def _pick_target(df, preferred_name="satisfaction"):
    cols_lc = {c.lower(): c for c in df.columns}
    if preferred_name.lower() in cols_lc:
        return cols_lc[preferred_name.lower()]

    # Prefer non-constant object column with low cardinality (classification-like)
    obj_cols = [c for c in df.columns if df[c].dtype == "object"]
    best = None
    best_score = -1
    for c in obj_cols:
        nun = df[c].nunique(dropna=True)
        if nun >= 2 and nun <= max(20, int(0.2 * max(1, len(df)))):
            score = 1000 - nun  # fewer classes better
            if score > best_score:
                best, best_score = c, score
    if best is not None:
        return best

    # Otherwise pick a numeric non-constant column (regression-like)
    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    for c in num_cols:
        if df[c].nunique(dropna=True) >= 2:
            return c

    # Fallback to first column
    return df.columns[0]


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mse = np.mean((y_true - y_pred) ** 2)
    var = np.var(y_true)
    if not np.isfinite(mse):
        return 0.0
    if var <= 1e-12 or not np.isfinite(var):
        return 0.0
    score = 1.0 - (mse / (var + 1e-12))
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


df = _read_csv_robust(DATASET_PATH)

assert df is not None and isinstance(df, pd.DataFrame)
assert df.shape[0] > 0 and df.shape[1] > 0

# Normalize obvious numeric columns if present; keep schema-agnostic
possible_numeric = [
    "Age",
    "Flight Distance",
    "Departure Delay in Minutes",
    "Arrival Delay in Minutes",
]
for name in possible_numeric:
    # match case-insensitively
    for c in df.columns:
        if c.lower() == name.lower():
            _coerce_numeric_inplace(df, [c])

# Replace inf with NaN to avoid pipeline surprises
df = df.replace([np.inf, -np.inf], np.nan)

target_col = _pick_target(df, preferred_name="satisfaction")

y_raw = df[target_col]
X = df.drop(columns=[target_col], errors="ignore")

# If X becomes empty, use all columns except an arbitrary one
if X.shape[1] == 0:
    # pick a different target if possible
    alt_cols = [c for c in df.columns if c != target_col]
    if alt_cols:
        X = df[alt_cols]
    else:
        X = df.copy()

assert X.shape[0] > 0

# Decide classification vs regression robustly
task = "classification"
y_series = y_raw.copy()

# If numeric and many unique values -> likely regression
if pd.api.types.is_numeric_dtype(y_series):
    nun = pd.Series(y_series).nunique(dropna=True)
    if nun > 20:
        task = "regression"
else:
    # object: but if too many unique values, fallback to regression by encoding numeric if possible
    nun = pd.Series(y_series).nunique(dropna=True)
    if nun > max(50, int(0.5 * max(1, len(y_series)))):
        # Try coercing to numeric, else keep classification
        y_num = pd.to_numeric(y_series, errors="coerce")
        if y_num.notna().sum() >= max(10, int(0.5 * len(y_series))):
            y_series = y_num
            task = "regression"

# Minimal cleaning for y
if task == "classification":
    y_series = y_series.astype("object")
    # Ensure at least two classes; else fallback to regression with numeric coercion if possible
    if y_series.nunique(dropna=True) < 2:
        y_num = pd.to_numeric(y_series, errors="coerce")
        if y_num.notna().sum() >= 2 and y_num.nunique(dropna=True) >= 2:
            y_series = y_num
            task = "regression"
else:
    y_series = pd.to_numeric(y_series, errors="coerce")

# Drop rows where y is missing (avoids leaking NaNs through split)
mask = pd.Series(y_series).notna()
X = X.loc[mask]
y_series = pd.Series(y_series).loc[mask]

assert len(X) > 1 and len(y_series) > 1

# Build preprocessing
numeric_features = selector(dtype_include=np.number)
categorical_features = selector(dtype_exclude=np.number)

numeric_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False)),
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ]
)

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Split
stratify = None
if task == "classification" and y_series.nunique(dropna=True) >= 2:
    # Avoid stratify errors with very small classes
    counts = y_series.value_counts(dropna=True)
    if (counts.min() >= 2) and (len(y_series) >= 10):
        stratify = y_series

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y_series,
    test_size=0.2,
    random_state=RANDOM_STATE,
    stratify=stratify,
)

assert len(X_train) > 0 and len(X_test) > 0

# Model selection (lightweight)
if task == "classification":
    # Logistic regression: strong baseline, efficient on CPU, works with sparse one-hot
    model = LogisticRegression(
        solver="liblinear",
        max_iter=200,
        C=1.0,
        random_state=RANDOM_STATE,
    )
    clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Ridge regression: stable linear baseline, efficient on CPU
    model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
    reg = Pipeline(steps=[("preprocess", preprocess), ("model", model)])
    reg.fit(X_train, y_train)
    y_pred = reg.predict(X_test)
    accuracy = _bounded_regression_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) to minimize CPU cycles and memory vs. tree ensembles.
# - Employed ColumnTransformer + Pipeline to ensure single-pass, reproducible preprocessing without redundant dataframe copies.
# - OneHotEncoder with sparse output keeps high-cardinality categorical expansions memory-efficient on CPU.
# - SimpleImputer + numeric coercion + inf/NaN handling provides robust execution under unknown/dirty schemas.
# - Regression fallback uses a bounded 1 - MSE/Var proxy clipped to [0,1] to keep "ACCURACY" stable and comparable across runs.