# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE

def load_data(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] < 2:
            raise ValueError
    except:
        df = pd.read_csv(path, sep=';', decimal=',')
    return df

df = load_data("EireJet (1).csv")
df.dropna(axis=0, how='any', inplace=True)

mapping = {
    'Gender': {'Female': 1, 'Male': 0},
    'Frequent Flyer': {'Yes': 1, 'No': 0},
    'Type of Travel': {'Personal Travel': 1, 'Business travel': 0},
    'Class': {'Eco': 0, 'Eco Plus': 1, 'Business': 2},
    'satisfaction': {'neutral or dissatisfied': 0, 'satisfied': 1}
}

for col, m in mapping.items():
    if col in df.columns:
        df[col] = df[col].map(m)

X = df.drop('satisfaction', axis=1)
y = df['satisfaction']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X).astype(np.float32)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=100)

smote = SMOTE(random_state=101)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

rf_grid = {'n_estimators': [50, 100, 150, 200, 250, 300]}
rfc_search = GridSearchCV(
    estimator=RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=1),
    param_grid=rf_grid, scoring='precision', cv=5, n_jobs=-1
)
rfc_search.fit(X_train_res, y_train_res)

ada_grid = {'n_estimators': [30, 35, 40, 45, 50, 55, 60]}
ada_search = GridSearchCV(
    estimator=AdaBoostClassifier(random_state=1),
    param_grid=ada_grid, scoring='precision', cv=5, n_jobs=-1
)
ada_search.fit(X_train_res, y_train_res)

gb_grid = {
    'n_estimators': [100, 150, 200],
    'max_depth': [9, 10, 11, 12],
    'max_leaf_nodes': [8, 12, 16, 20, 24, 28, 32]
}
gb_search = GridSearchCV(
    estimator=GradientBoostingClassifier(random_state=1),
    param_grid=gb_grid, scoring='precision', cv=5, n_jobs=-1
)
gb_search.fit(X_train_res, y_train_res)

final_model = GradientBoostingClassifier(n_estimators=200, max_depth=9, max_leaf_nodes=32, random_state=1)
final_model.fit(X_train_res, y_train_res)
y_pred = final_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary:
# 1. Minimized redundant computation by consolidating categorical mappings and removing duplicate train_test_split calls.
# 2. Reduced memory footprint by casting scaled features to float32 and using inplace data cleaning.
# 3. Accelerated hyperparameter tuning by enabling parallel processing (n_jobs=-1) in GridSearchCV.
# 4. Removed all visualization, logging, and statistical overhead to focus computational resources on the ML pipeline.
# 5. Implemented a robust data loading fallback mechanism to handle different CSV formats efficiently.
# 6. Streamlined preprocessing by combining feature encoding and target mapping into a single loop.
# 7. Preserved original behavior by maintaining consistent random seeds and training sequences for SMOTE and model fitting.