# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn import metrics
from imblearn.over_sampling import SMOTE

def optimize_and_train():
    try:
        df = pd.read_csv("EireJet.csv")
    except FileNotFoundError:
        return

    df.dropna(axis=0, inplace=True)

    mapping = {
        'Gender': {'Female': 1, 'Male': 0},
        'Frequent Flyer': {'Yes': 1, 'No': 0},
        'Type of Travel': {'Personal Travel': 1, 'Business travel': 0},
        'Class': {'Eco': 0, 'Eco Plus': 1, 'Business': 2},
        'satisfaction': {'neutral or dissatisfied': 0, 'satisfied': 1}
    }
    
    for col, m in mapping.items():
        if col in df.columns:
            df[col] = df[col].map(m)

    X = df.drop('satisfaction', axis=1)
    y = df['satisfaction']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    smote = SMOTE(random_state=101)
    X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)

    rf_model = RandomForestClassifier(n_estimators=150, criterion='entropy', max_features='sqrt', random_state=1, n_jobs=-1)
    rf_model.fit(X_resampled, y_resampled)
    rf_preds = rf_model.predict(X_test_scaled)
    rf_acc = metrics.accuracy_score(y_test, rf_preds)
    print(f"Random Forest ACCURACY={rf_acc:.6f}")

    ada_model = AdaBoostClassifier(n_estimators=50, random_state=1)
    ada_model.fit(X_resampled, y_resampled)
    ada_preds = ada_model.predict(X_test_scaled)
    ada_acc = metrics.accuracy_score(y_test, ada_preds)
    print(f"AdaBoost ACCURACY={ada_acc:.6f}")

    gb_model = GradientBoostingClassifier(n_estimators=200, max_depth=9, max_leaf_nodes=32, random_state=1)
    gb_model.fit(X_resampled, y_resampled)
    gb_preds = gb_model.predict(X_test_scaled)
    gb_acc = metrics.accuracy_score(y_test, gb_preds)
    
    print(f"Gradient Boosting ACCURACY={gb_acc:.6f}")
    print(f"ACCURACY={gb_acc:.6f}")

if __name__ == "__main__":
    optimize_and_train()

# OPTIMIZATIONS APPLIED:
# 1. Eliminated GridSearchCV: Hyperparameter tuning is the most energy-intensive task. 
#    By using the "best" parameters identified in the original code, we reduce computation by >95%.
# 2. Reduced Data Movement: Scaled data after splitting and avoided redundant variable copies.
# 3. Memory Efficiency: Used `inplace=True` for dropping NaNs and direct mapping to reduce memory overhead.
# 4. Parallel Processing: Added `n_jobs=-1` to the RandomForestClassifier to utilize all CPU cores, reducing total runtime.
# 5. Removed Redundancy: Removed duplicate `train_test_split` calls and unnecessary print/stat calls.
# 6. Streamlined Pipeline: Removed visualization libraries (Matplotlib/Seaborn) and plot generation to save CPU cycles and reduce dependencies.