# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE

def run_satisfaction_prediction():
    data_path = "EireJet (1).csv"
    try:
        df = pd.read_csv(data_path)
        if df.shape[1] < 10:
            raise ValueError
    except:
        df = pd.read_csv(data_path, sep=';', decimal=',')

    df.dropna(axis=0, how='any', inplace=True)

    mapping = {
        'Gender': {'Female': 1, 'Male': 0},
        'Frequent Flyer': {'Yes': 1, 'No': 0},
        'Type of Travel': {'Personal Travel': 1, 'Business travel': 0},
        'Class': {'Eco': 0, 'Eco Plus': 1, 'Business': 2},
        'satisfaction': {'neutral or dissatisfied': 0, 'satisfied': 1}
    }

    for col, map_dict in mapping.items():
        if col in df.columns:
            df[col] = df[col].map(map_dict)

    X = df.drop('satisfaction', axis=1)
    y = df['satisfaction']

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=100
    )

    smote = SMOTE(random_state=101)
    X_train, y_train = smote.fit_resample(X_train, y_train)

    rf_model = RandomForestClassifier(
        n_estimators=150, 
        criterion='entropy', 
        max_features='sqrt', 
        random_state=1, 
        n_jobs=-1
    )
    rf_model.fit(X_train, y_train)

    ada_model = AdaBoostClassifier(
        n_estimators=50, 
        random_state=1
    )
    ada_model.fit(X_train, y_train)

    gb_model = GradientBoostingClassifier(
        n_estimators=200, 
        max_depth=9, 
        max_leaf_nodes=32, 
        random_state=1
    )
    gb_model.fit(X_train, y_train)

    y_pred = gb_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_satisfaction_prediction()

# Optimization Summary
# 1. Removed redundant train_test_split call to save computational time and memory.
# 2. Eliminated GridSearchCV for all models as the optimal parameters were already identified in the original workflow, significantly reducing energy consumption.
# 3. Enabled n_jobs=-1 in RandomForestClassifier to parallelize training and reduce execution runtime.
# 4. Streamlined categorical feature mapping using a centralized dictionary and loop to minimize redundant data transformations.
# 5. Removed visualization libraries (matplotlib, seaborn) and plotting code to reduce the memory footprint and CPU overhead.
# 6. Replaced deprecated fit_sample with fit_resample from imblearn for better compatibility and efficiency.
# 7. Implemented robust CSV parsing with fallbacks to ensure end-to-end execution regardless of delimiter variation.
# 8. Removed all non-essential logging, printing, and descriptive statistics to minimize I/O operations and runtime.