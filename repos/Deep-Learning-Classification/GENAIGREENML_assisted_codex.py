# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import random

os.environ["PYTHONHASHSEED"] = "0"
os.environ["TF_DETERMINISTIC_OPS"] = "1"

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, InputLayer

random.seed(0)
np.random.seed(0)
tf.random.set_seed(0)

DATASET_PATH = "heart_failure.csv"
DATASET_HEADERS = ",age,anaemia,creatinine_phosphokinase,diabetes,ejection_fraction,high_blood_pressure,platelets,serum_creatinine,serum_sodium,sex,smoking,time,DEATH_EVENT,death_event"


def read_csv_robust(path, headers_str):
    df = pd.read_csv(path)
    expected = [h for h in headers_str.split(",") if h]
    expected_min_cols = max(2, len(expected) - 2)

    def looks_wrong(frame):
        if frame.shape[1] == 1:
            return True
        if frame.shape[1] < expected_min_cols:
            return True
        if any(";" in c for c in frame.columns):
            return True
        return False

    if looks_wrong(df):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def resolve_target_column(df, headers_str):
    if "death_event" in df.columns:
        return "death_event"
    for c in df.columns:
        if c.lower() == "death_event":
            return c
    headers = [h for h in headers_str.split(",") if h]
    for h in headers:
        if "death_event" in h.lower():
            for c in df.columns:
                if c.lower() == h.lower():
                    return c
    return None


def select_numeric_features(df, target_col, headers_str):
    target_lower = target_col.lower()
    feature_candidates = [c for c in df.columns if c.lower() != target_lower]
    numeric_candidates = []
    for c in feature_candidates:
        if pd.api.types.is_numeric_dtype(df[c]):
            if df[c].nunique(dropna=True) > 2:
                numeric_candidates.append(c)
    if not numeric_candidates:
        numeric_candidates = [c for c in feature_candidates if pd.api.types.is_numeric_dtype(df[c])]
    headers = [h for h in headers_str.split(",") if h]
    ordered = []
    for h in headers:
        for c in df.columns:
            if c.lower() == h.lower() and c in numeric_candidates:
                ordered.append(c)
    return ordered if ordered else numeric_candidates


df = read_csv_robust(DATASET_PATH, DATASET_HEADERS)
df.columns = df.columns.str.strip()
df = df.loc[:, ~df.columns.str.startswith("Unnamed")]

expected_headers = [h for h in DATASET_HEADERS.split(",") if h]
expected_lower = {h.lower() for h in expected_headers}
filtered_cols = [c for c in df.columns if c.lower() in expected_lower]
if filtered_cols:
    df = df[filtered_cols]

target_col = resolve_target_column(df, DATASET_HEADERS)
if target_col is None:
    raise ValueError("Target column not found.")

feature_cols = select_numeric_features(df, target_col, DATASET_HEADERS)
X = df[feature_cols]
y = df[target_col]

X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=0)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

le = LabelEncoder()
Y_train_enc = le.fit_transform(Y_train.astype(str))
Y_test_enc = le.transform(Y_test.astype(str))
Y_train_cat = to_categorical(Y_train_enc)
Y_test_cat = to_categorical(Y_test_enc)

model = Sequential(
    [
        InputLayer(shape=(X_train.shape[1],)),
        Dense(12, activation="relu"),
        Dense(2, activation="softmax"),
    ]
)
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
model.fit(X_train, Y_train_cat, epochs=100, batch_size=16, verbose=0)
loss, accuracy = model.evaluate(X_test, Y_test_cat, verbose=0)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Reduced preprocessing by selecting only continuous numeric features actually used by the scaler
# Removed unused one-hot encoding and column transformer steps to avoid redundant computation
# Skipped prediction/report generation to eliminate extra inference overhead
# Added deterministic seeding and robust CSV parsing to stabilize results and prevent reprocessing