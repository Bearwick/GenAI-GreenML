# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline

# Load dataset
df = pd.read_csv("dataset.csv")

# Handle duplicate target columns - use 'DEATH_EVENT' as primary target
# 'death_event' and 'DEATH_EVENT' appear to be duplicates
if 'death_event' in df.columns and 'DEATH_EVENT' in df.columns:
    target_col = 'DEATH_EVENT'
    drop_cols = ['death_event']
else:
    target_col = 'DEATH_EVENT' if 'DEATH_EVENT' in df.columns else 'death_event'
    drop_cols = []

# Drop unnamed index column if present
if df.columns[0] == '' or df.columns[0] == 'Unnamed: 0':
    drop_cols.append(df.columns[0])

# Separate features and target
y = df[target_col].values
X = df.drop(columns=[target_col] + drop_cols, errors='ignore')

# Drop any remaining non-feature columns
if 'death_event' in X.columns:
    X = X.drop(columns=['death_event'])

# Handle missing values if any
X = X.fillna(X.median())

# Feature selection: use all clinical features, they are already meaningful and small-dimensional
feature_names = X.columns.tolist()
X = X.values

# Stratified train/test split for final evaluation
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# StandardScaler for normalization - lightweight and effective
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Lightweight ensemble of small models - energy efficient, no deep learning needed
# Logistic Regression: very fast, interpretable baseline
lr = LogisticRegression(
    C=0.5, max_iter=500, solver='lbfgs', random_state=42
)

# Random Forest: small number of trees, limited depth for efficiency
rf = RandomForestClassifier(
    n_estimators=100, max_depth=6, min_samples_split=5,
    min_samples_leaf=3, random_state=42, n_jobs=-1
)

# Gradient Boosting: small and shallow trees, few estimators
gb = GradientBoostingClassifier(
    n_estimators=80, max_depth=3, learning_rate=0.1,
    subsample=0.8, min_samples_split=5, random_state=42
)

# Soft voting ensemble for robustness
ensemble = VotingClassifier(
    estimators=[('lr', lr), ('rf', rf), ('gb', gb)],
    voting='soft'
)

# Cross-validation on training set to verify generalization
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(ensemble, X_train_scaled, y_train, cv=cv, scoring='accuracy')

# Train final model on full training set
ensemble.fit(X_train_scaled, y_train)

# Evaluate on held-out test set
y_pred = ensemble.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Dataset is small (299 samples, 12 features) - no need for deep learning or GPU
# 2. Used StandardScaler for normalization - O(n)