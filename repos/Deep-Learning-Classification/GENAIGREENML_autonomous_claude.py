# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Robust CSV loading
try:
    df = pd.read_csv("heart_failure.csv")
    if df.shape[1] < 3:
        raise ValueError("Too few columns, try alternative separator")
except Exception:
    df = pd.read_csv("heart_failure.csv", sep=";", decimal=",")

# Strip and normalize column names
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)

# Drop unnamed index-like columns
drop_cols = [c for c in df.columns if c.lower().startswith('unnamed')]
df = df.drop(columns=drop_cols, errors='ignore')

# Also drop the first column if it looks like a row index (all unique integers matching row numbers)
if df.columns[0] == '' or (df.iloc[:, 0].nunique() == len(df) and pd.api.types.is_numeric_dtype(df.iloc[:, 0])):
    df = df.iloc[:, 1:]

# Identify target column
# Prefer 'death_event' or 'DEATH_EVENT' as target
target_col = None
for candidate in ['death_event', 'DEATH_EVENT']:
    if candidate in df.columns:
        target_col = candidate
        break

if target_col is None:
    # Fallback: pick last column
    target_col = df.columns[-1]

# If there are duplicate target columns (DEATH_EVENT and death_event), keep only one as target and drop the other from features
duplicate_targets = [c for c in ['death_event', 'DEATH_EVENT'] if c in df.columns and c != target_col]
df = df.drop(columns=duplicate_targets, errors='ignore')

# Map target to numeric if needed
target_series = df[target_col].copy()
if target_series.dtype == object:
    # Try mapping yes/no
    mapping = {'yes': 1, 'no': 0, 'Yes': 1, 'No': 0, 'YES': 1, 'NO': 0}
    target_series = target_series.map(mapping)
    if target_series.isna().any():
        # Fallback: label encode
        from sklearn.preprocessing import LabelEncoder
        le = LabelEncoder()
        target_series = pd.Series(le.fit_transform(df[target_col].astype(str)), index=df.index)

target_series = pd.to_numeric(target_series, errors='coerce')
df[target_col] = target_series

# Drop rows where target is NaN
df = df.dropna(subset=[target_col])

# Separate features and target
feature_cols = [c for c in df.columns if c != target_col]
X = df[list(feature_cols)].copy()
y = df[target_col].copy()

# Identify numeric and categorical columns
# First, try to coerce all columns and see which ones are mostly numeric
cat_cols = []
num_cols = []

for col in X.columns:
    if X[col].dtype == object:
        # Check if it can be converted to numeric
        converted = pd.to_numeric(X[col], errors='coerce')
        non_null_ratio = converted.notna().sum() / len(converted) if len(converted) > 0 else 0
        if non_null_ratio > 0.8:
            X[col] = converted
            num_cols.append(col)
        else:
            cat_cols.append(col)
    else:
        X[col] = pd.to_numeric(X[col], errors='coerce')
        num_cols.append(col)

# Handle NaN/inf in numeric columns
for col in num_cols:
    X[col