# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _load_dataset() -> pd.DataFrame:
    # Energy-efficient I/O: prefer small local CSV reads; avoid expensive discovery or downloads
    candidates = [
        "data.csv",
        "dataset.csv",
        "heart_failure_clinical_records_dataset.csv",
        "heart_failure_clinical_records.csv",
        "train.csv",
        "input.csv",
    ]

    env_path = os.environ.get("DATASET_PATH", "").strip()
    if env_path:
        candidates.insert(0, env_path)

    for p in candidates:
        if p and os.path.isfile(p):
            return pd.read_csv(p)

    # Minimal directory scan fallback (bounded): only check current directory
    for fn in os.listdir("."):
        if fn.lower().endswith(".csv") and os.path.isfile(fn):
            return pd.read_csv(fn)

    raise FileNotFoundError("No CSV dataset file found. Set DATASET_PATH or place CSV in working directory.")


def _clean_headers(df: pd.DataFrame) -> pd.DataFrame:
    # Handle accidental unnamed index column from CSV export (common with leading comma in header)
    if df.columns.size > 0:
        first = str(df.columns[0]).strip().lower()
        if first in ("", "unnamed: 0", "index"):
            df = df.drop(columns=[df.columns[0]])
    # Normalize column names for robustness
    df = df.rename(columns={c: str(c).strip() for c in df.columns})
    return df


def _select_target(df: pd.DataFrame) -> str:
    # Prefer canonical target; handle duplicate target columns with different casing
    candidates = ["DEATH_EVENT", "death_event"]
    for c in candidates:
        if c in df.columns:
            return c
    raise KeyError("Target column not found. Expected one of: DEATH_EVENT, death_event")


def main() -> None:
    df = _load_dataset()
    df = _clean_headers(df)

    target_col = _select_target(df)

    # If both targets exist, avoid leakage by dropping the other
    other_target = "death_event" if target_col == "DEATH_EVENT" else "DEATH_EVENT"
    drop_cols = [c for c in [other_target] if c in df.columns]
    if drop_cols:
        df = df.drop(columns=drop_cols)

    # Coerce to numeric where possible (lightweight); errors become NaN for imputation
    for c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="ignore")

    X = df.drop(columns=[target_col])
    y = pd.to_numeric(df[target_col], errors="coerce")

    # Remove rows with missing target (rare)
    valid = ~y.isna()
    X = X.loc[valid].copy()
    y = y.loc[valid].astype(int)

    # Use a fixed seed for reproducibility
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if y.nunique() > 1 else None
    )

    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    # CPU-efficient preprocessing:
    # - Median imputation for numeric (robust, O(n))
    # - Most frequent for categorical, with low-cost ordinal encoding via pandas factorization inside FunctionTransformer avoided
    # - StandardScaler helps LogisticRegression converge quickly with fewer iterations
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    # Keep categorical handling lightweight: impute then one-hot with sparse output
    # Note: OneHotEncoder is fairly efficient; dataset is small. Use sparse to reduce memory.
    from sklearn.preprocessing import OneHotEncoder

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
        n_jobs=None,  # Avoid parallel overhead for small datasets
    )

    # Energy-efficient model choice:
    # LogisticRegression with liblinear is strong for small tabular data, fast on CPU, and low memory.
    # Limit iterations for efficiency; scaling improves convergence reliability.
    clf = LogisticRegression(
        solver="liblinear",
        penalty="l2",
        C=1.0,
        max_iter=200,
        random_state=42,
    )

    model = Pipeline(steps=[("preprocess", preprocessor), ("clf", clf)])

    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Used a simple, well-regularized LogisticRegression model (liblinear) to keep CPU cost low while maintaining solid accuracy for small tabular datasets.
# - Employed a compact preprocessing pipeline with median/mode imputation and standard scaling to improve convergence, reducing required iterations/compute.
# - One-hot encoding is sparse to minimize memory usage; no deep learning, embeddings, or large ensembles were used.
# - Avoided parallelism (n_jobs=None) to prevent overhead on small datasets and to improve energy efficiency on typical CPUs.
# - Reproducibility ensured via fixed random_state and deterministic train/test split.