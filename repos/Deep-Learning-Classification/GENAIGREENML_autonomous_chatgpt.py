# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    # Heuristic: if single column, likely wrong delimiter
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [(" ".join(str(c).strip().split())) for c in df.columns]
    df = df.loc[:, ~df.columns.astype(str).str.match(r"^Unnamed:\s*\d+$", na=False)]
    return df


def _to_binary_series(s: pd.Series):
    if s is None:
        return None
    ss = s.copy()

    # Try numeric first
    num = pd.to_numeric(ss, errors="coerce")
    unique_num = pd.Series(num.dropna().unique())
    if len(unique_num) > 0 and unique_num.nunique() <= 2:
        vals = sorted(unique_num.unique().tolist())
        if len(vals) == 1:
            return None
        mapping = {vals[0]: 0, vals[-1]: 1}
        return num.map(mapping)

    # String mapping
    st = ss.astype(str).str.strip().str.lower()
    st = st.replace(
        {
            "yes": "1",
            "y": "1",
            "true": "1",
            "t": "1",
            "no": "0",
            "n": "0",
            "false": "0",
            "f": "0",
        }
    )
    num2 = pd.to_numeric(st, errors="coerce")
    uniq2 = pd.Series(num2.dropna().unique())
    if len(uniq2) > 0 and uniq2.nunique() <= 2:
        vals2 = sorted(uniq2.unique().tolist())
        if len(vals2) == 1:
            return None
        mapping2 = {vals2[0]: 0, vals2[-1]: 1}
        return num2.map(mapping2)

    return None


def _choose_target(df: pd.DataFrame):
    cols_lower = {c.lower(): c for c in df.columns}

    preferred = []
    for name in ["death_event", "death event", "deathevent"]:
        if name in cols_lower:
            preferred.append(cols_lower[name])
    if "DEATH_EVENT" in df.columns:
        preferred.insert(0, "DEATH_EVENT")

    # Prefer binary classification if possible
    for cand in preferred:
        s = df[cand]
        b = _to_binary_series(s)
        if b is not None and b.dropna().nunique() >= 2:
            return cand, "classification"

    # Otherwise choose any non-constant numeric column as regression target
    numeric_cols = []
    for c in df.columns:
        num = pd.to_numeric(df[c], errors="coerce")
        if num.notna().sum() > 0 and num.nunique(dropna=True) >= 2:
            numeric_cols.append(c)

    # Avoid ID-like columns if possible
    id_like = set()
    for c in df.columns:
        cl = c.lower()
        if cl in {"id", "index"} or cl.endswith("id") or cl.startswith("id"):
            id_like.add(c)

    candidates = [c for c in numeric_cols if c not in id_like] or numeric_cols
    if candidates:
        return candidates[-1], "regression"

    # Last resort: pick last column
    return df.columns[-1], "regression"


def _safe_accuracy_proxy_r2(y_true, y_pred) -> float:
    yt = np.asarray(y_true, dtype=float)
    yp = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(yt) & np.isfinite(yp)
    yt = yt[mask]
    yp = yp[mask]
    if yt.size == 0:
        return 0.0
    ss_res = float(np.sum((yt - yp) ** 2))
    ss_tot = float(np.sum((yt - float(np.mean(yt))) ** 2))
    if ss_tot <= 0.0:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    # Map to [0,1] to satisfy "accuracy" scalar printing
    acc = max(0.0, min(1.0, 0.5 * (r2 + 1.0)))
    return float(acc)


def main():
    df = _read_csv_robust("heart_failure.csv")
    df = _normalize_columns(df)

    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col, task = _choose_target(df)

    # Split X/y
    y_raw = df[target_col]
    X = df.drop(columns=[target_col])

    # Drop fully empty columns
    X = X.dropna(axis=1, how="all")
    assert X.shape[1] > 0

    # Identify column types with defensive coercion
    numeric_features = []
    categorical_features = []
    for c in X.columns:
        num = pd.to_numeric(X[c], errors="coerce")
        non_na = num.notna().sum()
        if non_na > 0 and non_na >= int(0.7 * len(X)):
            numeric_features.append(c)
        else:
            categorical_features.append(c)

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, list(numeric_features)),
            ("cat", categorical_transformer, list(categorical_features)),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    random_state = 42

    if task == "classification":
        y_bin = _to_binary_series(y_raw)
        if y_bin is None:
            task = "regression"
        else:
            y = y_bin
            valid = y.notna()
            X2 = X.loc[valid]
            y2 = y.loc[valid].astype(int)
            if y2.nunique() < 2 or len(y2) < 10:
                task = "regression"
            else:
                X_train, X_test, y_train, y_test = train_test_split(
                    X2, y2, test_size=0.2, random_state=random_state, stratify=y2
                )
                assert len(X_train) > 0 and len(X_test) > 0

                clf = Pipeline(
                    steps=[
                        ("preprocess", preprocessor),
                        ("model", LogisticRegression(max_iter=300, solver="liblinear")),
                    ]
                )
                clf.fit(X_train, y_train)
                y_pred = clf.predict(X_test)
                accuracy = float(accuracy_score(y_test, y_pred))
                print(f"ACCURACY={accuracy:.6f}")
                return

    # Regression fallback path
    y_num = pd.to_numeric(y_raw, errors="coerce")
    valid = y_num.notna() & np.isfinite(y_num)
    X2 = X.loc[valid]
    y2 = y_num.loc[valid].astype(float)

    # If still too small, use trivial baseline
    if len(y2) < 5:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    X_train, X_test, y_train, y_test = train_test_split(
        X2, y2, test_size=0.2, random_state=random_state
    )
    assert len(X_train) > 0 and len(X_test) > 0

    reg = Pipeline(
        steps=[
            ("preprocess", preprocessor),
            ("model", Ridge(alpha=1.0, random_state=random_state)),
        ]
    )
    reg.fit(X_train, y_train)
    y_pred = reg.predict(X_test)
    accuracy = _safe_accuracy_proxy_r2(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) for CPU-friendly training and low energy use.
# - Employed sklearn Pipeline + ColumnTransformer to avoid redundant preprocessing and ensure reproducibility.
# - Robust CSV parsing fallback (default, then ';' + decimal ',') and column normalization/dropping "Unnamed" reduces brittle I/O failures.
# - Automatic target selection favors binary DEATH_EVENT/death_event when available; otherwise falls back to a numeric regression target.
# - Numeric coercion with errors='coerce' and simple imputers (median/mode) provide stable handling of missing/dirty values without heavy feature engineering.
# - OneHotEncoder with sparse output keeps memory and compute lower for categorical features.
# - Regression fallback reports a bounded proxy accuracy in [0,1] via 0.5*(R2+1) clipped, ensuring a stable single metric output.