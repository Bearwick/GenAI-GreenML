# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, InputLayer
from tensorflow.keras.utils import to_categorical
import tensorflow as tf


SEED = 0


def set_reproducibility(seed: int = SEED) -> None:
    os.environ["PYTHONHASHSEED"] = str(seed)
    os.environ["TF_DETERMINISTIC_OPS"] = "1"
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    try:
        tf.config.experimental.enable_op_determinism()
    except Exception:
        pass


def read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 2:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def resolve_target_column(df: pd.DataFrame, headers: str) -> str:
    possible = [c for c in df.columns if c in ("death_event", "DEATH_EVENT")]
    if possible:
        if "death_event" in possible:
            return "death_event"
        return possible[0]

    header_tokens = [h.strip() for h in headers.split(",") if h.strip()]
    for cand in ("death_event", "DEATH_EVENT"):
        if cand in header_tokens and cand in df.columns:
            return cand

    raise ValueError("Target column not found (expected 'death_event' or 'DEATH_EVENT').")


def build_feature_frame(df: pd.DataFrame, target_col: str, headers: str) -> tuple[pd.DataFrame, pd.Series]:
    header_tokens = [h.strip() for h in headers.split(",") if h.strip()]
    candidate_features = [h for h in header_tokens if h not in (target_col, "DEATH_EVENT", "death_event")]
    feature_cols = [c for c in candidate_features if c in df.columns]

    if not feature_cols:
        feature_cols = [c for c in df.columns if c != target_col]

    x = df[feature_cols].copy()
    y = df[target_col]
    return x, y


def main() -> None:
    set_reproducibility(SEED)

    dataset_path = "heart_failure.csv"
    dataset_headers = ",age,anaemia,creatinine_phosphokinase,diabetes,ejection_fraction,high_blood_pressure,platelets,serum_creatinine,serum_sodium,sex,smoking,time,DEATH_EVENT,death_event"

    data = read_csv_robust(dataset_path)
    target_col = resolve_target_column(data, dataset_headers)
    x, y = build_feature_frame(data, target_col, dataset_headers)

    x = pd.get_dummies(x, dummy_na=False)

    X_train, X_test, Y_train, Y_test = train_test_split(
        x, y, test_size=0.3, random_state=SEED
    )

    numeric_candidates = [
        "age",
        "creatinine_phosphokinase",
        "ejection_fraction",
        "platelets",
        "serum_creatinine",
        "serum_sodium",
        "time",
    ]
    numeric_cols = [c for c in numeric_candidates if c in X_train.columns]

    if numeric_cols:
        ct = ColumnTransformer(
            transformers=[("numeric", StandardScaler(), numeric_cols)],
            remainder="passthrough",
            sparse_threshold=0.0,
        )
        X_train = ct.fit_transform(X_train)
        X_test = ct.transform(X_test)
    else:
        X_train = X_train.to_numpy(dtype=np.float32, copy=False)
        X_test = X_test.to_numpy(dtype=np.float32, copy=False)

    le = LabelEncoder()
    y_train_enc = le.fit_transform(Y_train.astype(str))
    y_test_enc = le.transform(Y_test.astype(str))
    Y_train_cat = to_categorical(y_train_enc)
    Y_test_cat = to_categorical(y_test_enc)

    model = Sequential(
        [
            InputLayer(shape=(X_train.shape[1],)),
            Dense(12, activation="relu"),
            Dense(2, activation="softmax"),
        ]
    )
    model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

    model.fit(X_train, Y_train_cat, epochs=100, batch_size=16, verbose=0)
    _, accuracy = model.evaluate(X_test, Y_test_cat, verbose=0)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed non-essential prints and the classification report to reduce I/O and post-processing overhead; kept required final accuracy print only.
# - Added robust CSV parsing fallback (default read_csv, then retry with sep=';' and decimal=',') to avoid wasted runs due to mis-parsing.
# - Derived target/feature columns from provided headers and df.columns to prevent schema assumptions and avoid errors/re-runs.
# - Used ColumnTransformer with remainder='passthrough' to avoid dropping non-scaled one-hot columns (preserves intended behavior while reducing manual column handling).
# - Avoided extra intermediate arrays by transforming labels once, then one-hot encoding; used copy=False where safe to reduce memory movement.
# - Enforced reproducibility via fixed seeds and deterministic TensorFlow settings to stabilize results and avoid repeated experimentation runs.
# - Set training verbose=0 to reduce runtime overhead from console logging.