# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

possible_files = [
    "dataset.csv",
    "data.csv",
    "train.csv",
    "heart_failure_clinical_records_dataset.csv",
]
data_file = None
for file in possible_files:
    if os.path.isfile(file):
        data_file = file
        break
if data_file is None:
    csv_files = [f for f in os.listdir(".") if f.lower().endswith(".csv")]
    if csv_files:
        data_file = csv_files[0]
    else:
        raise FileNotFoundError("No CSV dataset found.")

df = pd.read_csv(data_file)

drop_cols = [c for c in df.columns if c.startswith("Unnamed") or c.strip() == ""]
if drop_cols:
    df = df.drop(columns=drop_cols)

if "DEATH_EVENT" in df.columns:
    target = "DEATH_EVENT"
elif "death_event" in df.columns:
    target = "death_event"
else:
    target = df.columns[-1]

for c in ["DEATH_EVENT", "death_event"]:
    if c in df.columns and c != target:
        df = df.drop(columns=[c])

df = df.apply(pd.to_numeric, errors="coerce")
df = df[df[target].notna()]

X = df.drop(columns=[target])
y = df[target]

class_counts = y.value_counts()
stratify = y if len(class_counts) > 1 and class_counts.min() >= 2 else None

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=stratify
)

pipeline = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler()),
        ("classifier", LogisticRegression(max_iter=200, solver="liblinear")),
    ]
)

pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Chose logistic regression for a lightweight linear model suitable for small tabular data.
# Used median imputation and standard scaling in a compact pipeline for reproducibility with minimal overhead.
# Avoided complex models, plots, and disk outputs to keep CPU and energy usage low.