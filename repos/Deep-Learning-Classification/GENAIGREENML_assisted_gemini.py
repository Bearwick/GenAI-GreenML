# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, InputLayer
from tensorflow.keras.utils import to_categorical

tf.keras.utils.set_random_seed(0)

try:
    data = pd.read_csv('heart_failure.csv')
    if data.shape[1] <= 1:
        data = pd.read_csv('heart_failure.csv', sep=';', decimal=',')
except Exception:
    data = pd.read_csv('heart_failure.csv', sep=';', decimal=',')

target_col = 'death_event'
if target_col not in data.columns:
    for col in data.columns:
        if col.lower() == 'death_event':
            target_col = col
            break

numeric_features = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']
X = data[numeric_features].values.astype('float32')
y_mapped = data[target_col].astype(str).str.lower().map({'no': 0, '0': 0, '0.0': 0, 'yes': 1, '1': 1, '1.0': 1}).values
Y = to_categorical(y_mapped, num_classes=2)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = Sequential([
    InputLayer(shape=(X_train.shape[1],)),
    Dense(12, activation='relu'),
    Dense(2, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, Y_train, epochs=100, batch_size=16, verbose=0)

_, accuracy = model.evaluate(X_test, Y_test, verbose=0)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Removed pd.get_dummies and categorical column selection as they were discarded by the ColumnTransformer in the original design.
# 2. Replaced the high-level ColumnTransformer with a direct StandardScaler on the relevant feature matrix to minimize object overhead.
# 3. Streamlined target preprocessing by replacing LabelEncoder with a fast dictionary map for binary classification labels.
# 4. Enforced float32 data types for features to reduce memory consumption and accelerate computation on modern CPUs/GPUs.
# 5. Minimized I/O energy consumption and runtime by setting verbose=0 in training, evaluation, and prediction stages.
# 6. Eliminated redundant prediction and argmax steps used for the classification report, utilizing model.evaluate for accuracy directly.
# 7. Implemented robust CSV parsing with automated fallback for different separators and decimals to ensure consistent data loading.
# 8. Set global random seeds (NumPy and TensorFlow) to ensure reproducibility and stable performance metrics.