# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
import tensorflow as tf

tf.random.set_seed(0)
np.random.seed(0)

data = pd.read_csv('heart_failure.csv')

y = data['death_event'].values
feature_cols = ['age','anaemia','creatinine_phosphokinase','diabetes','ejection_fraction',
                'high_blood_pressure','platelets','serum_creatinine','serum_sodium','sex','smoking','time']
x = data[feature_cols].values

X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=0)

numeric_cols = [0, 2, 4, 6, 7, 8, 11]
ct = ColumnTransformer([("numeric", StandardScaler(), numeric_cols)], remainder='passthrough')
X_train = ct.fit_transform(X_train).astype(np.float32)
X_test = ct.transform(X_test).astype(np.float32)

Y_train_cat = tf.keras.utils.to_categorical(Y_train, num_classes=2)
Y_test_cat = tf.keras.utils.to_categorical(Y_test, num_classes=2)

model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(12, activation='relu'),
    tf.keras.layers.Dense(2, activation='softmax')
])
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train, Y_train_cat, epochs=100, batch_size=16, verbose=0)

loss, accuracy = model.evaluate(X_test, Y_test_cat, verbose=0)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Removed redundant LabelEncoder since death_event is already numeric (0/1).
# 2. Used numpy arrays directly instead of pandas DataFrames to reduce memory and overhead.
# 3. Used float32 instead of float64 to halve memory usage and speed up computation.
# 4. Removed pd.get_dummies since all features are already numeric, avoiding unnecessary computation.
# 5. Set verbose=0 for training to reduce I/O overhead from console output.
# 6. Removed all plots, prints, saves, and classification report to reduce unnecessary computation.
# 7. Set random seeds for reproducibility.
# 8. Used column indices instead of column names in ColumnTransformer for efficiency with numpy arrays.