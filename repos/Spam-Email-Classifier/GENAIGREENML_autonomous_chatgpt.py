# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, FunctionTransformer
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


RANDOM_STATE = 42


def _read_csv_robust(path: str) -> pd.DataFrame:
    # Try default CSV parsing first; if it yields a single wide column, retry with common European settings.
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.DataFrame()

    def looks_wrong(d: pd.DataFrame) -> bool:
        if d is None or d.empty:
            return True
        if d.shape[1] == 1:
            col0 = str(d.columns[0])
            if ";" in col0:
                return True
            sample = d.iloc[:10, 0].astype(str).tolist()
            if any(";" in s for s in sample):
                return True
        return False

    if looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            pass

    return df


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    cols = []
    for c in df.columns:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        cols.append(c2)
    df.columns = cols
    drop_cols = [c for c in df.columns if str(c).startswith("Unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _pick_target_and_features(df: pd.DataFrame, dataset_headers=None):
    cols = list(df.columns)

    # Try to use provided headers if available
    target = None
    if dataset_headers:
        for cand in dataset_headers:
            if cand in cols:
                if cand.lower() in ("category", "label", "target", "y", "class"):
                    target = cand
                    break
        if target is None and "Category" in cols:
            target = "Category"

    # If still unknown, choose a likely label column: low cardinality object/categorical
    if target is None:
        obj_cols = [c for c in cols if df[c].dtype == "object"]
        best = None
        best_card = None
        for c in obj_cols:
            nun = df[c].nunique(dropna=True)
            if nun >= 2:
                if best is None or nun < best_card:
                    best = c
                    best_card = nun
        if best is not None:
            target = best

    # If still none, choose a numeric non-constant column
    if target is None:
        numeric_candidates = []
        for c in cols:
            s = pd.to_numeric(df[c], errors="coerce")
            nun = s.nunique(dropna=True)
            if nun >= 2:
                numeric_candidates.append((c, nun))
        if numeric_candidates:
            target = sorted(numeric_candidates, key=lambda x: x[1])[0][0]

    if target is None:
        # Last resort: first column
        target = cols[0]

    feature_cols = [c for c in cols if c != target]
    if not feature_cols:
        feature_cols = [target]  # allow a degenerate path

    return target, feature_cols


def _is_text_series(s: pd.Series) -> bool:
    if s.dtype != "object":
        return False
    non_null = s.dropna()
    if non_null.empty:
        return False
    sample = non_null.astype(str).head(200)
    avg_len = sample.map(len).mean()
    # Heuristic: text tends to be longer than short categorical codes
    return avg_len >= 15


def _coerce_numeric(df: pd.DataFrame, cols):
    out = df.copy()
    for c in cols:
        out[c] = pd.to_numeric(out[c], errors="coerce")
    return out


def _bounded_regression_score(y_true, y_pred) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    if not np.isfinite(ss_tot) or ss_tot <= 0:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    # Map to [0,1] for a stable "accuracy" proxy
    score = (r2 + 1.0) / 2.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    df = _read_csv_robust("spam.csv")
    df = _normalize_columns(df)

    # If file includes extra unnamed columns, keep the meaningful ones.
    assert df is not None and not df.empty, "Dataset is empty after load."

    dataset_headers = ["Category", "Message"]
    target_col, feature_cols = _pick_target_and_features(df, dataset_headers=dataset_headers)

    # Ensure target exists
    if target_col not in df.columns:
        target_col = df.columns[0]
        feature_cols = [c for c in df.columns if c != target_col] or [target_col]

    y_raw = df[target_col]

    # Build feature type lists robustly
    X = df[feature_cols].copy()

    text_cols = [c for c in feature_cols if _is_text_series(df[c])]
    non_text_cols = [c for c in feature_cols if c not in text_cols]

    # Determine task type (classification if target looks categorical or few unique)
    y_obj = (y_raw.dtype == "object")
    y_nunique = y_raw.nunique(dropna=True)

    classification = y_obj or (y_nunique <= 20 and y_nunique >= 2)

    # Split with stratify when feasible for classification
    stratify = None
    if classification:
        y_str = y_raw.astype(str).fillna("NA")
        if y_str.nunique() >= 2 and y_str.nunique() <= max(2, int(0.5 * len(y_str))):
            stratify = y_str

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y_raw,
        test_size=0.2,
        random_state=RANDOM_STATE,
        stratify=stratify
    )

    assert len(X_train) > 0 and len(X_test) > 0, "Train/test split produced empty set."

    # If classification but only one class in train, fallback to regression-like or trivial baseline.
    accuracy = 0.0

    if classification:
        y_train_str = y_train.astype(str).fillna("NA")
        y_test_str = y_test.astype(str).fillna("NA")

        if y_train_str.nunique() < 2:
            # Trivial baseline: always predict the only seen class
            const_label = y_train_str.iloc[0]
            y_pred = np.array([const_label] * len(y_test_str), dtype=object)
            accuracy = float(accuracy_score(y_test_str, y_pred))
        else:
            # Preprocessor: TF-IDF for text, OneHot for other categoricals, passthrough numeric (after coercion+impute)
            transformers = []

            if text_cols:
                # Use lightweight TF-IDF settings suitable for SMS-like text; avoid heavy ngrams
                text_transformer = Pipeline(steps=[
                    ("imputer", SimpleImputer(strategy="constant", fill_value="")),
                    ("to_str", FunctionTransformer(lambda a: a.astype(str).ravel(), feature_names_out="one-to-one")),
                    ("tfidf", TfidfVectorizer(
                        lowercase=True,
                        stop_words=None,
                        max_features=20000,
                        min_df=2,
                        norm="l2"
                    ))
                ])
                transformers.append(("text", text_transformer, text_cols[0] if len(text_cols) == 1 else text_cols))

            # Separate numeric vs categorical among non-text
            num_cols = []
            cat_cols = []
            for c in non_text_cols:
                if pd.api.types.is_numeric_dtype(df[c]):
                    num_cols.append(c)
                else:
                    cat_cols.append(c)

            if num_cols:
                num_transformer = Pipeline(steps=[
                    ("to_num", FunctionTransformer(
                        lambda d: _coerce_numeric(pd.DataFrame(d, columns=num_cols), num_cols).values,
                        feature_names_out="one-to-one"
                    )),
                    ("imputer", SimpleImputer(strategy="median"))
                ])
                transformers.append(("num", num_transformer, num_cols))

            if cat_cols:
                cat_transformer = Pipeline(steps=[
                    ("imputer", SimpleImputer(strategy="most_frequent")),
                    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True))
                ])
                transformers.append(("cat", cat_transformer, cat_cols))

            if not transformers:
                # Degenerate: no usable features; trivial majority class
                majority = y_train_str.value_counts(dropna=False).idxmax()
                y_pred = np.array([majority] * len(y_test_str), dtype=object)
                accuracy = float(accuracy_score(y_test_str, y_pred))
            else:
                preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

                # Prefer MultinomialNB when text is present; otherwise use a small linear model.
                if text_cols:
                    clf = MultinomialNB(alpha=1.0)
                else:
                    clf = LogisticRegression(
                        solver="liblinear",
                        max_iter=300,
                        C=1.0
                    )

                model = Pipeline(steps=[
                    ("preprocess", preprocessor),
                    ("model", clf)
                ])

                model.fit(X_train, y_train_str)
                y_pred = model.predict(X_test)
                accuracy = float(accuracy_score(y_test_str, y_pred))
    else:
        # Regression path
        y_train_num = pd.to_numeric(y_train, errors="coerce")
        y_test_num = pd.to_numeric(y_test, errors="coerce")

        # Drop rows with missing target in train/test
        train_mask = np.isfinite(y_train_num.values)
        test_mask = np.isfinite(y_test_num.values)

        X_train2 = X_train.loc[train_mask]
        y_train2 = y_train_num.loc[train_mask]
        X_test2 = X_test.loc[test_mask]
        y_test2 = y_test_num.loc[test_mask]

        if len(X_train2) == 0 or len(X_test2) == 0 or y_train2.nunique() < 2:
            # Trivial baseline: predict mean; score becomes 0.5 in bounded mapping when r2=0
            pred = np.full(len(y_test2), float(y_train2.mean()) if len(y_train2) else 0.0)
            accuracy = _bounded_regression_score(y_test2.values, pred)
        else:
            transformers = []

            if text_cols:
                text_transformer = Pipeline(steps=[
                    ("imputer", SimpleImputer(strategy="constant", fill_value="")),
                    ("to_str", FunctionTransformer(lambda a: a.astype(str).ravel(), feature_names_out="one-to-one")),
                    ("tfidf", TfidfVectorizer(
                        lowercase=True,
                        max_features=20000,
                        min_df=2,
                        norm="l2"
                    ))
                ])
                transformers.append(("text", text_transformer, text_cols[0] if len(text_cols) == 1 else text_cols))

            num_cols = []
            cat_cols = []
            for c in non_text_cols:
                if pd.api.types.is_numeric_dtype(df[c]):
                    num_cols.append(c)
                else:
                    cat_cols.append(c)

            if num_cols:
                num_transformer = Pipeline(steps=[
                    ("to_num", FunctionTransformer(
                        lambda d: _coerce_numeric(pd.DataFrame(d, columns=num_cols), num_cols).values,
                        feature_names_out="one-to-one"
                    )),
                    ("imputer", SimpleImputer(strategy="median"))
                ])
                transformers.append(("num", num_transformer, num_cols))

            if cat_cols:
                cat_transformer = Pipeline(steps=[
                    ("imputer", SimpleImputer(strategy="most_frequent")),
                    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True))
                ])
                transformers.append(("cat", cat_transformer, cat_cols))

            preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

            reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)

            model = Pipeline(steps=[
                ("preprocess", preprocessor),
                ("model", reg)
            ])

            model.fit(X_train2, y_train2.values)
            pred = model.predict(X_test2)
            accuracy = _bounded_regression_score(y_test2.values, pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses robust CSV parsing (default then ;/, fallback) and column normalization to avoid re-runs/hand fixes.
# - Lightweight text model: TF-IDF (bounded max_features, min_df=2, unigrams) + MultinomialNB for high accuracy per CPU watt.
# - Avoids heavy ensembles/deep learning; LogisticRegression(liblinear) only when no text; Ridge for regression fallback.
# - ColumnTransformer+Pipeline ensures single-pass preprocessing, reproducibility, and minimal redundant computation.
# - Defensive schema handling: auto target selection, numeric coercion, safe imputers, and trivial baselines for degenerate cases.
# - Regression "accuracy" proxy maps R² to [0,1] via (R²+1)/2 and clipping for stability and comparability.