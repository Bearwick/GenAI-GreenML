# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, r2_score
from sklearn.base import BaseEstimator, TransformerMixin


def _read_csv_robust(path: str) -> pd.DataFrame:
    # Try default parsing
    try:
        df0 = pd.read_csv(path)
    except Exception:
        df0 = pd.DataFrame()

    def _looks_wrong(d: pd.DataFrame) -> bool:
        if d is None or d.empty:
            return True
        # If single column and it contains separators, likely wrong delimiter
        if d.shape[1] == 1:
            col0 = d.columns[0]
            sample = d[col0].astype(str).head(10).str.cat(sep=" ")
            if (";" in sample) or ("," in sample and sample.count(",") > 20):
                return True
        return False

    if _looks_wrong(df0):
        try:
            df1 = pd.read_csv(path, sep=";", decimal=",")
            if not df1.empty and df1.shape[1] >= 2:
                return df1
        except Exception:
            pass

    return df0


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols = []
    for c in df.columns:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        cols.append(c2)
    df.columns = cols
    # Drop unnamed columns
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", c)]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


class TextCleaner(BaseEstimator, TransformerMixin):
    def __init__(self, lowercase: bool = True):
        self.lowercase = lowercase

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        # X is expected to be a 2D array-like with 1 column
        if isinstance(X, pd.DataFrame):
            s = X.iloc[:, 0]
        else:
            arr = np.asarray(X)
            s = pd.Series(arr.ravel())
        s = s.astype(str).fillna("")
        if self.lowercase:
            s = s.str.lower()
        # Lightweight normalization; avoids heavy NLP dependencies
        s = s.str.replace(r"[^a-z0-9\s]+", " ", regex=True)
        s = s.str.replace(r"\s+", " ", regex=True).str.strip()
        return s.to_numpy()


def _pick_target_and_features(df: pd.DataFrame):
    cols = list(df.columns)

    # Prefer typical target names if present
    preferred_targets = ["Category", "Target", "Label", "Class", "y"]
    target = None
    for t in preferred_targets:
        if t in cols:
            target = t
            break

    # Otherwise pick a non-constant column (prefer low-cardinality non-numeric for classification)
    if target is None:
        candidate_scores = []
        for c in cols:
            nun = df[c].nunique(dropna=True)
            if nun <= 1:
                continue
            # Prefer object/categorical with limited cardinality
            if df[c].dtype == "object" or str(df[c].dtype).startswith("category"):
                score = (0, nun)  # best
            else:
                score = (1, nun)  # next
            candidate_scores.append((score, c))
        if candidate_scores:
            candidate_scores.sort(key=lambda x: x[0])
            target = candidate_scores[0][1]
        else:
            target = cols[0] if cols else None

    feature_cols = [c for c in cols if c != target]
    return target, feature_cols


def _safe_accuracy_proxy_regression(y_true, y_pred) -> float:
    # Map R^2 to [0, 1] for a stable "accuracy" proxy
    r2 = r2_score(y_true, y_pred)
    if not np.isfinite(r2):
        r2 = 0.0
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


df = _read_csv_robust("spam.csv")
df = _normalize_columns(df)

# Basic cleanup: ensure non-empty
df = df.copy()
assert df.shape[0] > 0 and df.shape[1] > 0, "Dataset is empty after loading."

target_col, feature_cols = _pick_target_and_features(df)

# If no features exist, create a trivial feature from index
if not feature_cols:
    df["_idx_feature"] = np.arange(len(df), dtype=np.int64)
    feature_cols = ["_idx_feature"]

# Prepare X, y
y_raw = df[target_col].copy()
X = df[feature_cols].copy()

# Determine text columns: prefer a "Message"/"Text" column if present; else any object column
preferred_text = ["Message", "Text", "Email", "Body", "Content"]
text_cols = [c for c in preferred_text if c in X.columns]
if not text_cols:
    text_cols = [c for c in X.columns if X[c].dtype == "object"]
text_cols = text_cols[:1]  # keep lightweight: use at most one text column

# Numeric and categorical columns excluding chosen text
other_cols = [c for c in X.columns if c not in text_cols]
numeric_cols = []
categorical_cols = []
for c in other_cols:
    # Coerce to numeric for numeric candidates; keep original if not numeric
    coerced = pd.to_numeric(X[c], errors="coerce")
    numeric_ratio = np.isfinite(coerced.to_numpy()).mean() if len(coerced) else 0.0
    if numeric_ratio >= 0.9:
        X[c] = coerced
        numeric_cols.append(c)
    else:
        categorical_cols.append(c)

# Drop all-NaN numeric columns
numeric_cols = [c for c in numeric_cols if np.isfinite(X[c].to_numpy()).any()]
# For categorical, ensure string type
for c in categorical_cols:
    X[c] = X[c].astype(str).replace({"nan": ""})

# Decide task type
is_classification = True
y_for_task = y_raw.copy()

if y_for_task.dtype != "object" and not str(y_for_task.dtype).startswith("category"):
    y_num = pd.to_numeric(y_for_task, errors="coerce")
    # If mostly numeric and high cardinality, consider regression
    nun = pd.Series(y_for_task).nunique(dropna=True)
    if np.isfinite(y_num).mean() >= 0.9 and nun > 20:
        is_classification = False
        y_for_task = y_num
else:
    # Classification; keep as string labels (LogReg handles via internal encoding)
    y_for_task = y_for_task.astype(str).fillna("")

# Remove rows with missing target for the chosen task
if is_classification:
    mask = y_for_task.astype(str).str.len() > 0
else:
    mask = np.isfinite(pd.to_numeric(y_for_task, errors="coerce").to_numpy())
X = X.loc[mask].reset_index(drop=True)
y_for_task = y_for_task.loc[mask].reset_index(drop=True)

assert len(X) > 1, "Not enough samples after preprocessing."

# Train/test split with fixed random_state
stratify = None
if is_classification:
    # If too many rare labels, skip stratification
    vc = y_for_task.value_counts()
    if (vc.min() if len(vc) else 0) >= 2 and y_for_task.nunique() >= 2:
        stratify = y_for_task

X_train, X_test, y_train, y_test = train_test_split(
    X, y_for_task, test_size=0.2, random_state=42, stratify=stratify
)
assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

# Handle degenerate classification targets
if is_classification and pd.Series(y_train).nunique() < 2:
    # Fallback to a trivial baseline predictor: always predict the only class
    only_class = pd.Series(y_train).iloc[0]
    y_pred = np.array([only_class] * len(y_test), dtype=object)
    accuracy = float(accuracy_score(y_test, y_pred))
else:
    transformers = []

    if text_cols:
        text_pipe = Pipeline(
            steps=[
                ("clean", TextCleaner(lowercase=True)),
                ("vect", CountVectorizer(stop_words="english", max_features=2000)),
            ]
        )
        transformers.append(("text", text_pipe, text_cols))

    if numeric_cols:
        num_pipe = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="median")),
            ]
        )
        transformers.append(("num", num_pipe, numeric_cols))

    if categorical_cols:
        cat_pipe = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
            ]
        )
        transformers.append(("cat", cat_pipe, categorical_cols))

    # If no transformers (edge case), use a passthrough on first column coerced to string
    if not transformers:
        fallback_col = [X_train.columns[0]]
        text_pipe = Pipeline(
            steps=[
                ("clean", TextCleaner(lowercase=True)),
                ("vect", CountVectorizer(stop_words="english", max_features=2000)),
            ]
        )
        transformers.append(("text", text_pipe, fallback_col))

    preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    if is_classification:
        model = LogisticRegression(max_iter=200, solver="liblinear")
        pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        from sklearn.linear_model import Ridge

        model = Ridge(alpha=1.0, random_state=42)
        pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = _safe_accuracy_proxy_regression(np.asarray(y_test, dtype=float), np.asarray(y_pred, dtype=float))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Uses CPU-friendly linear models (LogisticRegression/Ridge) with small iteration limits; avoids heavy ensembles/deep learning.
# - Lightweight text normalization + CountVectorizer (max_features=2000) to bound memory/compute for bag-of-words.
# - ColumnTransformer/Pipeline ensures single-pass, reproducible preprocessing without redundant computation.
# - Robust CSV loading with delimiter/decimal fallback; column-name normalization; drops "Unnamed" columns.
# - Defensive schema handling: auto-selects target/features; supports missing/degenerate targets with safe trivial baseline.
# - For regression fallback, reports an "accuracy" proxy by mapping R^2 to [0,1] via (R^2+1)/2 for stability.