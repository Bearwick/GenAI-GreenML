# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import LabelEncoder
from nltk.stem import PorterStemmer

RANDOM_SEED = 42

try:
    df = pd.read_csv("spam.csv", encoding='utf-8')
except (UnicodeDecodeError, Exception):
    df = pd.read_csv("spam.csv", sep=';', decimal=',', encoding='latin-1')

expected_cols = ["Category", "Message"]
if not all(col in df.columns for col in expected_cols):
    df = df.iloc[:, :2]
    df.columns = expected_cols

df = df[expected_cols].dropna()

ps = PorterStemmer()

def convert(text):
    return " ".join([ps.stem(word) for word in str(text).split(" ")])

df["Message"] = df["Message"].apply(convert)

le = LabelEncoder()
df["Category"] = le.fit_transform(df["Category"])

X_train, X_test, y_train, y_test = train_test_split(
    df["Message"], 
    df["Category"], 
    test_size=0.2, 
    random_state=RANDOM_SEED
)

cv = CountVectorizer(stop_words="english")
X_train_count = cv.fit_transform(X_train)
X_test_count = cv.transform(X_test)

model = MultinomialNB()
model.fit(X_train_count, y_train)

accuracy = model.score(X_test_count, y_test)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Improved computational efficiency by replacing manual loops with list comprehensions in the stemming function.
# - Reduced memory footprint by filtering the dataframe to essential columns and dropping null values immediately after loading.
# - Enhanced robustness and avoided runtime failures with a multi-stage CSV parsing fallback mechanism (handling encodings and delimiters).
# - Ensured reproducibility by implementing a fixed random seed for data splitting.
# - Eliminated redundant computations and overhead by removing interactive functions, printing, and display settings.
# - Optimized data flow by transforming only the necessary feature sets for training and evaluation.