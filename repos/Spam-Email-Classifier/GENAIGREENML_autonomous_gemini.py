# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
import sys
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline

def load_data(filepath):
    """Robust CSV loader with fallback mechanisms."""
    try:
        # Attempt standard read
        df = pd.read_csv(filepath, encoding='utf-8')
    except (UnicodeDecodeError, Exception):
        try:
            # Common fallback for spam datasets
            df = pd.read_csv(filepath, encoding='latin-1', sep=',', decimal='.')
        except Exception:
            try:
                df = pd.read_csv(filepath, sep=';', decimal=',', encoding='utf-8')
            except Exception:
                return pd.DataFrame()

    # Normalize column names: strip, single space, remove Unnamed
    df.columns = [str(c).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    df.columns = [" ".join(str(c).split()) for c in df.columns]
    return df

def get_xy(df):
    """Dynamically identifies target and feature columns."""
    cols = list(df.columns)
    if len(cols) < 2:
        return None, None
    
    # Priority 1: Use Dataset Headers if matched
    target_col = None
    text_col = None
    
    for c in cols:
        if c.lower() in ['category', 'label', 'target', 'class']:
            target_col = c
        if c.lower() in ['message', 'text', 'body', 'sms', 'content']:
            text_col = c
            
    # Priority 2: Fallback to position
    if not target_col:
        target_col = cols[0]
    if not text_col:
        text_col = cols[1] if len(cols) > 1 else cols[0]
        
    y = df[target_col].astype(str)
    X = df[text_col].astype(str).fillna('')
    return X, y

def main():
    filepath = 'spam.csv'
    df = load_data(filepath)
    
    if df.empty:
        # Trivial fallback for missing file/parse error to ensure end-to-end run
        print(f"ACCURACY={0.000000:.6f}")
        return

    X, y = get_xy(df)
    
    if X is None or len(np.unique(y)) < 2:
        # Graceful exit for insufficient data classes
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Energy-efficient choice: CountVectorizer (binary) + MultinomialNB
    # We avoid heavy NLP libraries (NLTK/Spacy) to save CPU cycles
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Pipeline handles vectorization and classification efficiently
    # CountVectorizer uses an internal fast C-based tokenizer
    pipeline = Pipeline([
        ('vectorizer', CountVectorizer(
            stop_words='english', 
            lowercase=True, 
            min_df=2, 
            max_features=5000
        )),
        ('classifier', MultinomialNB())
    ])

    # Fit and Predict
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary:
# 1. Model Selection: Used Multinomial Naive Bayes, which has O(n) training complexity 
#    and minimal memory footprint, making it ideal for CPU-only, energy-efficient tasks.
# 2. Feature Engineering: Utilized CountVectorizer with a max_features limit and min_df
#    to reduce the dimensionality of the sparse matrix, lowering memory usage and heat 
#    generation during computation.
# 3. Preprocessing: Avoided computationally expensive Lemmatization or Stemming. Standard
#    tokenization and stop-word removal provide high accuracy for spam tasks at a 
#    fraction of the CPU cost.
# 4. Data Handling: Implemented robust CSV parsing with encoding detection to prevent
#    runtime failures on diverse environments.
# 5. Pipeline Efficiency: Used sklearn.pipeline.Pipeline to minimize overhead and 
#    ensure the preprocessing logic is not redundantly computed.
# 6. Evaluation: Fixed random_state and stratified splits ensure reproducibility without
#    expensive cross-validation loops.