# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load dataset - 784 pixels suggests 28x28 image classification (like MNIST/Fashion-MNIST)
df = pd.read_csv("dataset.csv")

# Separate features and labels
y = df["label"].values
X = df.drop(columns=["label"]).values.astype(np.float32)

# Normalize pixel values to [0, 1] - lightweight and effective for pixel data
X = X / 255.0

# Train/test split with stratification to preserve class distribution
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# PCA for dimensionality reduction - reduces 784 features significantly
# This is key for energy efficiency: fewer features = faster training
# Retaining 95% variance typically keeps enough info for good accuracy
pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# SVM with RBF kernel - strong performer on image classification tasks
# For small-to-medium datasets, SVM often matches or beats neural networks
# Using reasonable C and gamma for good generalization
clf = SVC(kernel="rbf", C=10.0, gamma="scale", random_state=42, cache_size=500)
clf.fit(X_train_pca, y_train)

# Predict and evaluate
y_pred = clf.predict(X_test_pca)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Pixel normalization (/ 255.0): simple, no fitting required, prevents numerical issues
# 2. PCA with 95% variance retention: drastically reduces dimensionality from 784 to ~150-200
#    components, cutting SVM training time significantly while preserving discriminative info
# 3. SVM with RBF kernel: ideal for medium-scale image classification on CPU;
#    achieves high accuracy without deep learning overhead
# 4. C=10.0 with gamma="scale": slightly regularized SVM that generalizes well
# 5. cache_size=500 MB: speeds up SVM kernel computations within reasonable memory
# 6. No deep learning used - entire pipeline runs efficiently on CPU
# 7. Stratified split ensures balanced evaluation across all digit/class categories