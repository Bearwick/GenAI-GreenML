# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import glob
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MaxAbsScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _find_dataset_path() -> str:
    candidates = []
    candidates.extend(glob.glob("*.csv"))
    candidates.extend(glob.glob("data/*.csv"))
    candidates.extend(glob.glob("dataset/*.csv"))
    candidates.extend(glob.glob("input/*.csv"))

    for p in candidates:
        base = os.path.basename(p).lower()
        if any(k in base for k in ("train", "mnist", "digit", "digits", "fashion")):
            return p

    if candidates:
        return candidates[0]

    raise FileNotFoundError("No CSV dataset found in common locations.")


def _load_dataset(path: str) -> tuple[np.ndarray, np.ndarray]:
    df = pd.read_csv(path)
    if "label" not in df.columns:
        raise ValueError("Expected a 'label' column in the dataset.")

    y = df["label"].to_numpy()
    X = df.drop(columns=["label"]).to_numpy()

    if X.ndim != 2 or X.shape[1] != 784:
        raise ValueError(f"Expected 784 pixel features; got shape {X.shape}.")

    return X, y


def main() -> None:
    dataset_path = _find_dataset_path()
    X, y = _load_dataset(dataset_path)

    # Energy/perf: use float32 to reduce memory bandwidth and CPU cache pressure
    X = X.astype(np.float32, copy=False)

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y if len(np.unique(y)) > 1 else None,
    )

    model = Pipeline(
        steps=[
            # Energy/perf: MaxAbsScaler is lightweight and preserves sparsity/scale well for pixel ranges
            ("scaler", MaxAbsScaler()),
            # Energy/perf: Logistic regression is a compact linear model; "saga" is efficient for multiclass and large sparse-ish features
            ("clf", LogisticRegression(solver="saga", max_iter=200, n_jobs=1)),
        ]
    )

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Chose a linear classifier (multinomial logistic regression) for 28x28 pixel classification: small model, fast CPU inference, no deep learning.
# - Used float32 features to cut memory usage and improve cache efficiency.
# - Applied MaxAbsScaler in a reproducible Pipeline to normalize pixel magnitudes with minimal compute overhead.
# - Used the SAGA solver with limited iterations for good speed/accuracy trade-off on high-dimensional data; constrained to n_jobs=1 for predictable CPU use.
# - Avoided expensive feature engineering, augmentations, ensembles, plotting, and model serialization to minimize compute and I/O.