# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


RANDOM_STATE = 42
DATASET_PATH = "MNIST_train.csv"


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _robust_read_csv(path):
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", engine="python")
    return df


def _drop_unnamed(df):
    bad = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if bad:
        df = df.drop(columns=bad, errors="ignore")
    return df


def _pick_target(df):
    cols_lower = {c.lower(): c for c in df.columns}
    if "label" in cols_lower:
        return cols_lower["label"]

    nunique = df.nunique(dropna=True)
    numeric_cols = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().mean() > 0.8:
            numeric_cols.append(c)

    candidates = [c for c in numeric_cols if nunique.get(c, 0) > 1]
    if candidates:
        candidates_sorted = sorted(candidates, key=lambda c: nunique.get(c, 0), reverse=True)
        return candidates_sorted[0]

    candidates = [c for c in df.columns if nunique.get(c, 0) > 1]
    if candidates:
        return candidates[0]

    return df.columns[0]


def _infer_task(y):
    if y.dtype == "O":
        return "classification"
    y_num = pd.to_numeric(y, errors="coerce")
    uniq = pd.unique(y_num.dropna())
    if len(uniq) == 0:
        return "regression"
    if len(uniq) <= 20 and np.all(np.isclose(uniq, np.round(uniq))):
        return "classification"
    return "regression"


def _safe_dense(x):
    if hasattr(x, "toarray"):
        return x.toarray()
    return x


def _regression_accuracy_proxy(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    denom = np.sum((y_true - np.mean(y_true)) ** 2)
    if denom <= 0:
        return 0.0
    r2 = 1.0 - (np.sum((y_true - y_pred) ** 2) / denom)
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    if not os.path.exists(DATASET_PATH):
        raise FileNotFoundError(DATASET_PATH)

    df = _robust_read_csv(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    assert df.shape[0] > 0 and df.shape[1] > 0, "Empty dataset after read."

    target_col = _pick_target(df)
    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    for c in X.columns:
        if X[c].dtype == "O":
            X[c] = X[c].astype(str).replace({"nan": np.nan, "None": np.nan})
    y = y_raw.copy()

    task = _infer_task(y)

    numeric_cols = []
    categorical_cols = []
    for c in X.columns:
        s = pd.to_numeric(X[c], errors="coerce")
        if s.notna().mean() > 0.8:
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)

    if not numeric_cols and not categorical_cols:
        X = pd.DataFrame({"__bias__": np.ones(len(df), dtype=float)})
        numeric_cols = ["__bias__"]
        categorical_cols = []

    numeric_transformer = Pipeline(
        steps=[
            ("to_numeric", FunctionTransformer(lambda d: d.apply(pd.to_numeric, errors="coerce"), validate=False)),
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if task == "classification":
        y_num = pd.to_numeric(y, errors="coerce")
        if y_num.notna().mean() > 0.9:
            y = y_num.astype("Int64").astype("float").astype(int)
        else:
            y = y.astype(str)

        uniq = pd.unique(pd.Series(y).dropna())
        if len(uniq) < 2:
            task = "regression"
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
            )
            assert len(X_train) > 0 and len(X_test) > 0, "Empty split."

            clf = LogisticRegression(
                solver="saga",
                penalty="l2",
                C=1.0,
                max_iter=200,
                tol=1e-3,
                n_jobs=1,
                verbose=0,
                multi_class="auto",
            )

            model = Pipeline(steps=[("preprocess", preprocessor), ("clf", clf)])
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

    y_num = pd.to_numeric(y, errors="coerce")
    if y_num.isna().all():
        y_num = pd.Series(np.zeros(len(y), dtype=float))
    else:
        y_num = y_num.fillna(y_num.median())

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_num, test_size=0.2, random_state=RANDOM_STATE
    )
    assert len(X_train) > 0 and len(X_test) > 0, "Empty split."

    reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
    model = Pipeline(steps=[("preprocess", preprocessor), ("reg", reg)])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = _regression_accuracy_proxy(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Robust CSV loading with delimiter/decimal fallback avoids manual intervention and reruns cheaply.
# - Column normalization + dropping 'Unnamed:' prevents accidental feature bloat.
# - Lightweight preprocessing via ColumnTransformer avoids duplicated work and is CPU-friendly.
# - Numeric coercion uses pandas.to_numeric(errors='coerce') and median imputation to handle dirty schemas safely.
# - Sparse OneHotEncoder (only if needed) keeps memory/CPU low; MNIST is numeric so it stays mostly numeric.
# - LogisticRegression(saga) chosen as a small, strong baseline for high-dimensional sparse/dense data; n_jobs=1 to limit CPU power draw.
# - Modest max_iter/tol provide good-enough convergence while reducing compute.
# - Regression fallback uses Ridge and reports a bounded accuracy proxy: ACCURACY = clip((R2+1)/2, 0, 1).