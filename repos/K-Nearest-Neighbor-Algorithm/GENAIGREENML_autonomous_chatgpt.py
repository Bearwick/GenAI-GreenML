# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import SGDClassifier, SGDRegressor
from sklearn.metrics import accuracy_score, r2_score


DATASET_PATH = "MNIST_train.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    df = pd.read_csv(path)
    # Heuristic: if only one column, likely wrong separator; retry with ';' and decimal ','
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _pick_target(df):
    # Prefer common target names first
    preferred = ["label", "target", "y", "class"]
    lower_map = {c.lower(): c for c in df.columns}
    for p in preferred:
        if p in lower_map:
            return lower_map[p]

    # Otherwise choose a non-constant numeric column if possible
    numeric_candidates = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        nunique = s.nunique(dropna=True)
        if nunique >= 2:
            numeric_candidates.append((nunique, c))
    if numeric_candidates:
        # Heuristic: choose column with smallest unique count >=2 (often class labels)
        numeric_candidates.sort(key=lambda x: x[0])
        return numeric_candidates[0][1]

    # Fallback: last column
    return df.columns[-1]


def _is_classification_target(y):
    # Decide classification if y has relatively few unique values (typical for labels) and is integer-like
    y_num = pd.to_numeric(y, errors="coerce")
    y_non_na = y_num.dropna()
    if y_non_na.empty:
        return False

    nunique = y_non_na.nunique()
    if nunique < 2:
        return False

    # integer-like check (allow floats that are whole numbers)
    frac = np.abs(y_non_na - np.round(y_non_na))
    int_like = (frac < 1e-9).mean() > 0.99

    # For MNIST-like: 10 classes; keep generous threshold for unknown datasets
    if int_like and nunique <= 50:
        return True

    # If object dtype with limited categories, classification
    if y.dtype == object and y.nunique(dropna=True) <= 50:
        return True

    return False


def _bounded_regression_score(y_true, y_pred):
    # Convert R^2 to [0,1] proxy: max(0, R2) clipped to 1.
    r2 = r2_score(y_true, y_pred)
    if not np.isfinite(r2):
        return 0.0
    return float(np.clip(max(0.0, r2), 0.0, 1.0))


def main():
    if not os.path.exists(DATASET_PATH):
        # Minimal viable run even if file missing
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    assert df.shape[0] > 0 and df.shape[1] > 0, "Empty dataset after load/cleanup."

    target_col = _pick_target(df)
    if target_col not in df.columns:
        target_col = df.columns[-1]

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # If features end up empty, keep at least one column (excluding target) if possible
    if X.shape[1] == 0 and df.shape[1] > 1:
        X = df.drop(columns=[target_col], errors="ignore")
    assert X.shape[1] > 0, "No features available."

    # Identify columns by dtype
    numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_cols = [c for c in X.columns if c not in numeric_cols]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    is_clf = _is_classification_target(y_raw)

    if is_clf:
        y = pd.to_numeric(y_raw, errors="coerce")
        valid = y.notna()
        X2 = X.loc[valid].copy()
        y2 = y.loc[valid].astype(int)

        # Guard against empty after coercion
        assert len(y2) > 0, "No valid target values after coercion."

        # If after cleaning there is only one class, fallback to regression-like path
        if y2.nunique() < 2:
            is_clf = False
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                X2, y2, test_size=0.2, random_state=RANDOM_STATE, stratify=y2
            )
            assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Train/test split invalid."

            clf = SGDClassifier(
                loss="log_loss",
                penalty="l2",
                alpha=1e-4,
                max_iter=30,
                tol=1e-3,
                random_state=RANDOM_STATE,
                n_jobs=1,
            )

            model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

    # Regression fallback path
    y = pd.to_numeric(y_raw, errors="coerce")
    valid = y.notna()
    X2 = X.loc[valid].copy()
    y2 = y.loc[valid].astype(float)
    assert len(y2) > 0, "No valid target values after coercion."

    X_train, X_test, y_train, y_test = train_test_split(
        X2, y2, test_size=0.2, random_state=RANDOM_STATE
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Train/test split invalid."

    reg = SGDRegressor(
        loss="squared_error",
        penalty="l2",
        alpha=1e-4,
        max_iter=1000,
        tol=1e-3,
        random_state=RANDOM_STATE,
    )

    model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = _bounded_regression_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used a linear SGDClassifier (log-loss) for MNIST-like multiclass: fast, CPU-friendly, good baseline accuracy without heavy ensembles.
# - Kept preprocessing in a single Pipeline/ColumnTransformer for reproducibility and to avoid repeated work.
# - StandardScaler(with_mean=False) supports sparse matrices and avoids densifying (saves memory/energy).
# - OneHotEncoder only used if non-numeric columns exist; MNIST pixels remain numeric and bypass encoding.
# - Added robust CSV parsing fallback (separator/decimal), column normalization, and dropping "Unnamed" columns.
# - Defensive target selection: prefer typical label names; otherwise choose a non-constant numeric column to keep the run end-to-end.
# - Regression fallback computes a bounded [0,1] score from R^2 (clip(max(0,R2),0,1)) and prints as ACCURACY for stability.