# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import os

def load_robust_csv(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise Exception
    except:
        df = pd.read_csv(path, sep=';', decimal=',')
    return df

def main():
    train_path = "MNIST_train.csv"
    test_path = "MNIST_test.csv"
    
    if not os.path.exists(train_path):
        return

    df_train = load_robust_csv(train_path)
    
    label_col = df_train.columns[0]
    X_train = df_train.drop(columns=[label_col]).values.astype(np.float32)
    y_train = df_train[label_col].values

    if os.path.exists(test_path):
        df_test = load_robust_csv(test_path)
        X_test = df_test.drop(columns=[label_col]).values.astype(np.float32)
        y_test = df_test[label_col].values
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42
        )

    knn = KNeighborsClassifier(
        n_neighbors=7, 
        weights='distance', 
        algorithm='auto', 
        n_jobs=-1
    )
    knn.fit(X_train, y_train)
    
    accuracy = knn.score(X_test, y_test)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# 1. Replaced manual CSV parsing loops with pandas.read_csv for significant I/O speedup and reduced memory overhead.
# 2. Replaced manual O(N*K) KNN implementation with scikit-learn's optimized KNeighborsClassifier, which uses spatial trees (KD-Tree/Ball Tree) for sub-linear search time.
# 3. Vectorized distance calculations using NumPy-backed operations, eliminating Python-level nested loops and math.pow calls.
# 4. Implemented n_jobs=-1 to utilize multi-core parallelism, reducing wall-clock runtime and total energy consumption.
# 5. Used float32 for feature arrays to reduce memory footprint compared to Python's default arbitrary-precision integers/floats.
# 6. Optimized preprocessing by removing redundant type checks and manual list manipulations.
# 7. Added robust CSV loading to handle different delimiters and decimals without redundant trials.
# 8. Eliminated unnecessary intermediate data structures (like distance lists) that were re-allocated in every iteration.
# 9. Avoided redundant prints and calculations within the prediction loop to minimize CPU cycles.
# 10. Used scikit-learn's .score() method for efficient batch evaluation instead of per-sample loop predictions.