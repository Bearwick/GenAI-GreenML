# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import numpy as np

def load_data(file_path):
    data = np.loadtxt(file_path, delimiter=',', skiprows=1, dtype=np.float32)
    return data[:, 1:], data[:, 0].astype(np.int32)

def predict_knn(x_train, y_train, x_query, k):
    diff = x_train - x_query
    dist_sq = np.einsum('ij,ij->i', diff, diff)
    distances = np.sqrt(dist_sq)
    
    nearest_idx = np.argpartition(distances, k)[:k]
    k_distances = distances[nearest_idx]
    k_labels = y_train[nearest_idx]
    
    weights = 1.0 / (k_distances + 1e-10)
    unique_labels = np.unique(k_labels)
    
    best_label = -1
    max_weight = -1.0
    
    for label in unique_labels:
        current_weight = np.sum(weights[k_labels == label])
        if current_weight > max_weight:
            max_weight = current_weight
            best_label = label
            
    return best_label

def main():
    train_x, train_y = load_data("MNIST_train.csv")
    test_x, test_y = load_data("MNIST_test.csv")
    
    k = 7
    correct_predictions = 0
    total_samples = len(test_y)
    
    for i in range(total_samples):
        prediction = predict_knn(train_x, train_y, test_x[i], k)
        if prediction == test_y[i]:
            correct_predictions += 1
            
    accuracy = (correct_predictions / total_samples) * 100
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# 1. Replaced manual CSV parsing loops with NumPy's vectorized loadtxt, significantly reducing data loading time and energy.
# 2. Implemented vectorization using NumPy for distance calculations, replacing O(N*D) Python loops with optimized C-level operations.
# 3. Used np.einsum for squared Euclidean distance calculation to minimize intermediate memory allocations and improve cache efficiency.
# 4. Replaced full sorting O(N log N) with np.argpartition O(N) to find the k-nearest neighbors, reducing computational complexity.
# 5. Utilized float32 instead of default float64 for feature data, halving the memory footprint and improving processing speed.
# 6. Optimized memory movement by processing test samples in a loop rather than creating a massive distance matrix, preventing memory exhaustion.
# 7. Removed redundant data checks and unnecessary intermediate list structures (like neighborClasses and neighborDistances lists) to lower overhead.
# 8. Eliminated expensive math.pow and math.sqrt calls within nested Python loops, substituting them with high-performance NumPy array operations.