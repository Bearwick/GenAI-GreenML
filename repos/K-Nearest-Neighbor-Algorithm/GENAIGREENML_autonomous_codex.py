# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import Ridge
from sklearn.dummy import DummyClassifier
from sklearn.metrics import accuracy_score, mean_squared_error

warnings.filterwarnings("ignore")

DATASET_PATH = "MNIST_train.csv"
expected_headers = ["label"] + [f"pixel{i}" for i in range(784)]

def read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is None:
        df = pd.read_csv(path, sep=';', decimal=',')
    else:
        if df.shape[1] <= 1:
            try:
                df_alt = pd.read_csv(path, sep=';', decimal=',')
                if df_alt.shape[1] > df.shape[1]:
                    df = df_alt
            except Exception:
                pass
    return df

def normalize_col(c):
    c = str(c).strip()
    c = re.sub(r'\s+', ' ', c)
    return c

def columns_look_numeric(cols):
    cnt = 0
    total = len(cols)
    for c in cols:
        if re.match(r'^-?\d+(\.\d+)?$', str(c)):
            cnt += 1
    return cnt / max(1, total) > 0.8

def ensure_features(X, y, numeric_cols):
    non_all_nan_cols = [c for c in X.columns if X[c].notna().any()]
    X = X[non_all_nan_cols]
    feature_cols = list(X.columns)
    if len(feature_cols) == 0:
        X = pd.DataFrame({'dummy_feature': np.zeros(len(y))}, index=y.index)
        feature_cols = ['dummy_feature']
        if 'dummy_feature' not in numeric_cols:
            numeric_cols.append('dummy_feature')
    return X, feature_cols, numeric_cols

df = read_csv_robust(DATASET_PATH)

df.columns = [normalize_col(c) for c in df.columns]
df = df.loc[:, [c for c in df.columns if not str(c).lower().startswith('unnamed')]]
df = df.dropna(axis=1, how='all')

if expected_headers and df.shape[1] == len(expected_headers):
    if columns_look_numeric(df.columns):
        df.columns = expected_headers

df = df.replace([np.inf, -np.inf], np.nan)

assert df.shape[0] > 0 and df.shape[1] > 0

numeric_cols = []
categorical_cols = []
n_rows = len(df)
for c in df.columns:
    s = df[c]
    s_num = pd.to_numeric(s, errors='coerce')
    if s_num.notna().sum() >= max(1, int(0.5 * n_rows)):
        df[c] = s_num
        numeric_cols.append(c)
    else:
        df[c] = s.astype(str)
        categorical_cols.append(c)

preferred = []
for name in ['label', 'target', 'y', 'class']:
    if name in expected_headers and name not in preferred:
        preferred.append(name)
for name in ['label', 'target', 'y', 'class']:
    if name not in preferred:
        preferred.append(name)

target_col = None
for name in preferred:
    if name in df.columns:
        target_col = name
        break
if target_col is None:
    for c in numeric_cols:
        if df[c].nunique(dropna=True) > 1:
            target_col = c
            break
if target_col is None:
    target_col = df.columns[0]

feature_cols = [c for c in df.columns if c != target_col]
if len(feature_cols) == 0:
    df['dummy_feature'] = 0
    feature_cols = ['dummy_feature']
    if 'dummy_feature' not in numeric_cols:
        numeric_cols.append('dummy_feature')

X = df[feature_cols]
y = df[target_col]
mask = y.notna()
X = X.loc[mask]
y = y.loc[mask]
X, feature_cols, numeric_cols = ensure_features(X, y, numeric_cols)

assert X.shape[0] > 0

unique_vals = y.nunique(dropna=True)
if y.dtype == object:
    task = 'classification'
else:
    if unique_vals <= max(20, int(0.05 * len(y))):
        task = 'classification'
    else:
        task = 'regression'

if task == 'regression':
    y = pd.to_numeric(y, errors='coerce')
    mask = y.notna()
    X = X.loc[mask]
    y = y.loc[mask]
    X, feature_cols, numeric_cols = ensure_features(X, y, numeric_cols)

assert X.shape[0] > 0

numeric_features = [c for c in feature_cols if c in numeric_cols]
categorical_features = [c for c in feature_cols if c not in numeric_features]

test_size = 0.2 if X.shape[0] > 10 else 0.5
stratify = None
if task == 'classification' and unique_vals >= 2:
    vc = y.value_counts()
    if vc.min() >= 2:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=42, stratify=stratify
)

assert X_train.shape[0] > 0 and X_test.shape[0] > 0

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', MinMaxScaler())
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

transformers = []
if numeric_features:
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    transformers.append(('cat', categorical_transformer, categorical_features))
if not transformers:
    transformers = [('pass', 'passthrough', feature_cols)]

preprocessor = ColumnTransformer(transformers=transformers, sparse_threshold=0.3)

if task == 'classification':
    if unique_vals < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = MultinomialNB()
    is_classification = True
else:
    model = Ridge(alpha=1.0)
    is_classification = False

pipe = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)

if is_classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    mse = mean_squared_error(y_test, y_pred)
    var = np.var(y_test)
    if var == 0:
        accuracy = 1.0
    else:
        accuracy = 1 - mse / (var + 1e-9)
        accuracy = max(0.0, min(1.0, accuracy))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Used lightweight MultinomialNB for classification and Ridge regression as a simple CPU-friendly fallback.
# ColumnTransformer with imputation and MinMax scaling keeps preprocessing reproducible and ensures non-negative features for NB.
# Robust schema handling and minimal feature engineering avoid heavy computation while maintaining end-to-end reliability.
# Regression accuracy is a bounded R2-like score mapped to [0,1] for consistent ACCURACY reporting.