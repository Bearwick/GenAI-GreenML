# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import glob
import numpy as np
import pandas as pd
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score
from sklearn.exceptions import ConvergenceWarning
import warnings

warnings.filterwarnings("ignore", category=ConvergenceWarning)

def find_dataset_file():
    csv_files = sorted(glob.glob("*.csv"))
    for file in csv_files:
        try:
            cols = pd.read_csv(file, nrows=1).columns
        except Exception:
            continue
        if "label" in cols:
            return file
    raise FileNotFoundError("No dataset file with 'label' column found")

file_path = find_dataset_file()
data = pd.read_csv(file_path, dtype=np.uint8)
y = data["label"].astype(np.int64).to_numpy()
X = data.drop(columns=["label"]).to_numpy(dtype=np.float32)
X /= 255.0
del data

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

ver_parts = sklearn.__version__.split(".")
ver_tuple = (int(ver_parts[0]), int(ver_parts[1]))
loss_name = "log_loss" if ver_tuple >= (1, 1) else "log"

model = SGDClassifier(
    loss=loss_name,
    max_iter=1000,
    tol=1e-3,
    random_state=42,
    n_jobs=1
)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# - Used a linear SGD classifier to keep computation lightweight for high-dimensional pixel data.
# - Normalized pixel values to [0,1] using float32 to reduce memory and improve convergence.
# - Employed a deterministic train/test split with stratification for reproducible evaluation.