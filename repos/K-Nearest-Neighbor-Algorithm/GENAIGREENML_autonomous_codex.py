# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import re
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import SGDClassifier, SGDRegressor
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")
RANDOM_STATE = 42

def load_csv(file_path):
    try:
        df = pd.read_csv(file_path)
    except Exception:
        df = pd.read_csv(file_path, sep=';', decimal=',')
        return df
    if df.shape[1] <= 1:
        try:
            df = pd.read_csv(file_path, sep=';', decimal=',')
        except Exception:
            pass
    return df

def normalize_columns(df):
    cols = []
    for c in df.columns:
        if isinstance(c, str):
            c_clean = re.sub(r'\s+', ' ', c.strip())
        else:
            c_clean = str(c)
        cols.append(c_clean)
    df.columns = cols
    df = df.loc[:, ~df.columns.str.match(r'^Unnamed')]
    return df

def clean_dataframe(df):
    df = normalize_columns(df)
    df = df.loc[:, df.notna().any()]
    df = df.replace([np.inf, -np.inf], np.nan)
    for col in df.columns:
        if df[col].dtype == object:
            series = df[col].astype(str).str.strip()
            series = series.replace({'': np.nan, 'nan': np.nan, 'None': np.nan, 'NA': np.nan, 'NaN': np.nan})
            coerced = pd.to_numeric(series.str.replace(',', '.', regex=False), errors='coerce')
            if coerced.notna().mean() > 0.8:
                df[col] = coerced
            else:
                df[col] = series
    return df

def infer_target_column(df):
    if df is None or df.empty:
        return None
    cols = list(df.columns)
    lower_map = {c.lower(): c for c in cols}
    for key in ['label', 'target', 'class', 'y', 'output']:
        if key in lower_map:
            return lower_map[key]
    n = len(df)
    candidate = None
    for col in cols:
        nun = df[col].nunique(dropna=True)
        if nun < 2:
            continue
        if nun <= max(20, int(n * 0.05)):
            if candidate is None or nun < candidate[0]:
                candidate = (nun, col)
    if candidate:
        return candidate[1]
    for col in reversed(cols):
        if df[col].nunique(dropna=True) > 1:
            return col
    return cols[-1]

def is_classification(y):
    if y.dtype == object or str(y.dtype).startswith('category'):
        return True
    unique = y.nunique(dropna=True)
    if unique <= max(20, int(0.05 * len(y))):
        return True
    return False

csv_files = [f for f in os.listdir('.') if f.lower().endswith('.csv')]
assert len(csv_files) > 0

train_candidates = [f for f in csv_files if 'train' in f.lower()]
test_candidates = [f for f in csv_files if 'test' in f.lower()]

df_train = None
df_test = None
df_main = None
use_separate_test = False

if train_candidates and test_candidates:
    train_file = max(train_candidates, key=lambda f: os.path.getsize(f))
    test_file = max(test_candidates, key=lambda f: os.path.getsize(f))
    df_train = clean_dataframe(load_csv(train_file))
    df_test = clean_dataframe(load_csv(test_file))
    target_col = infer_target_column(df_train)
    if target_col is None or target_col not in df_train.columns:
        target_col = df_train.columns[-1]
    df_train = df_train.loc[df_train[target_col].notna()]
    if df_test is not None and target_col in df_test.columns:
        df_test = df_test.loc[df_test[target_col].notna()]
        if not df_test.empty and not df_train.empty:
            use_separate_test = True
            X_train = df_train.drop(columns=[target_col])
            y_train = df_train[target_col]
            X_test = df_test.drop(columns=[target_col])
            y_test = df_test[target_col]
            X_test = X_test.reindex(columns=X_train.columns, fill_value=np.nan)
    if not use_separate_test:
        df_main = df_train
else:
    main_file = max(csv_files, key=lambda f: os.path.getsize(f))
    df_main = clean_dataframe(load_csv(main_file))

if not use_separate_test:
    assert df_main is not None and not df_main.empty
    target_col = infer_target_column(df_main)
    if target_col is None or target_col not in df_main.columns:
        target_col = df_main.columns[-1]
    df_main = df_main.loc[df_main[target_col].notna()]
    X = df_main.drop(columns=[target_col])
    y = df_main[target_col]
    problem_guess = 'classification' if is_classification(y) else 'regression'
    stratify = y if problem_guess == 'classification' and y.nunique(dropna=True) > 1 else None
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=None
        )

assert X_train is not None and X_test is not None
assert len(X_train) > 0 and len(X_test) > 0

problem_type = 'classification' if is_classification(y_train) else 'regression'

if problem_type == 'regression':
    y_train = pd.to_numeric(y_train, errors='coerce')
    y_test = pd.to_numeric(y_test, errors='coerce')
    train_mask = y_train.notna()
    test_mask = y_test.notna()
    X_train = X_train.loc[train_mask]
    y_train = y_train.loc[train_mask]
    X_test = X_test.loc[test_mask]
    y_test = y_test.loc[test_mask]

assert len(X_train) > 0 and len(X_test) > 0

if X_train.shape[1] == 0:
    X_train = pd.DataFrame({'const': np.ones(len(X_train))})
    X_test = pd.DataFrame({'const': np.ones(len(X_test))})

numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = [c for c in X_train.columns if c not in numeric_features]

transformers = []
if numeric_features:
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler(with_mean=False))
    ])
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    transformers.append(('cat', categorical_transformer, categorical_features))

if transformers:
    preprocessor = ColumnTransformer(transformers=transformers)
else:
    preprocessor = 'passthrough'

if problem_type == 'classification':
    if y_train.nunique(dropna=True) < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = SGDClassifier(loss='log_loss', max_iter=300, tol=1e-3, random_state=RANDOM_STATE)
else:
    if y_train.nunique(dropna=True) < 2:
        model = DummyRegressor(strategy='mean')
    else:
        model = SGDRegressor(max_iter=300, tol=1e-3, random_state=RANDOM_STATE)

pipeline = Pipeline(steps=[
    ('preprocess', preprocessor),
    ('model', model)
])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if problem_type == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred) if len(y_test) > 1 else 0.0
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# - Used lightweight SGD linear models with limited iterations for CPU-efficient training on high-dimensional data.
# - Applied a single Pipeline with imputation and scaling/one-hot encoding to keep preprocessing reproducible and compact.
# - Regression accuracy is reported as a bounded proxy derived from R^2: (r2+1)/2 clamped to [0,1] for stability.