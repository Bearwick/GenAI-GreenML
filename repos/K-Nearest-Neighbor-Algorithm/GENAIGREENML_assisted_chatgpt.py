# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd


SEED = 42
TRAIN_FILE = "MNIST_train.csv"
TEST_FILE = "MNIST_test.csv"
K = 7

DATASET_HEADERS = [
    "label",
    *[f"pixel{i}" for i in range(784)],
]


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 2 or ("label" not in df.columns and df.iloc[:, 0].dtype == object):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _coerce_mnist_schema(df: pd.DataFrame) -> pd.DataFrame:
    cols = list(df.columns)
    if "label" not in cols:
        cols = DATASET_HEADERS[: df.shape[1]]
        df = df.copy()
        df.columns = cols

    df = df.apply(pd.to_numeric, errors="coerce")
    df = df.dropna(axis=0, how="any")
    return df


def _to_xy(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray]:
    label_col = "label" if "label" in df.columns else df.columns[0]
    y = df[label_col].to_numpy(dtype=np.int32, copy=False)

    feature_cols = [c for c in df.columns if c != label_col]
    X = df[feature_cols].to_numpy(dtype=np.float32, copy=False)
    return X, y


def _knn_predict_weighted_inverse_distance(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    k: int,
) -> np.ndarray:
    n_test = X_test.shape[0]
    preds = np.empty(n_test, dtype=y_train.dtype)

    classes = np.unique(y_train)
    class_to_idx = {c: i for i, c in enumerate(classes)}
    y_train_idx = np.fromiter((class_to_idx[v] for v in y_train), count=y_train.size, dtype=np.int32)

    for i in range(n_test):
        x = X_test[i]

        d2 = np.einsum("ij,ij->i", X_train - x, X_train - x, optimize=True)

        nn_idx = np.argpartition(d2, k - 1)[:k]
        nn_d2 = d2[nn_idx]
        nn_cls = y_train_idx[nn_idx]

        d = np.sqrt(nn_d2, dtype=np.float32)
        with np.errstate(divide="ignore", invalid="ignore"):
            w = 1.0 / d

        vote = np.zeros(classes.size, dtype=np.float32)
        np.add.at(vote, nn_cls, w)

        preds[i] = classes[int(vote.argmax())]

    return preds


def main() -> None:
    np.random.seed(SEED)

    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):
        raise FileNotFoundError("Required MNIST_train.csv and MNIST_test.csv files not found in the working directory.")

    train_df = _coerce_mnist_schema(_read_csv_robust(TRAIN_FILE))
    test_df = _coerce_mnist_schema(_read_csv_robust(TEST_FILE))

    X_train, y_train = _to_xy(train_df)
    X_test, y_test = _to_xy(test_df)

    y_pred = _knn_predict_weighted_inverse_distance(X_train, y_train, X_test, K)

    accuracy = (y_pred == y_test).mean() * 100.0
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced manual line-by-line CSV parsing with vectorized pandas read + numeric coercion to reduce Python-loop overhead and data movement.
# - Implemented robust CSV parsing fallback (default -> sep=';' and decimal=',') to handle common delimiter/decimal variants without manual edits.
# - Converted features to NumPy float32 and labels to int32 to cut memory footprint and improve cache efficiency while preserving numeric behavior.
# - Removed redundant header-row checks by dropping non-numeric rows via to_numeric(..., errors='coerce') + dropna, preserving intended skipping of header strings.
# - Optimized distance computation using NumPy einsum for fast squared L2 distances and avoided per-feature Python loops.
# - Used argpartition to select k nearest neighbors in linear time (no full sort) to reduce runtime and energy per test sample.
# - Vectorized vote accumulation with np.add.at and reused a compact class index mapping to avoid repeated dict operations per neighbor.
# - Set a fixed random seed for reproducibility (even though the algorithm is deterministic) and ensured deterministic operations where feasible.