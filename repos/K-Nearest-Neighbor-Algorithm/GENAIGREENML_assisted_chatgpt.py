# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import csv
import heapq
import math


def load_mnist_csv(path):
    with open(path, newline="") as f:
        reader = csv.reader(f)
        next(reader, None)
        y = []
        X = []
        for row in reader:
            y.append(int(row[0]))
            X.append([int(v) for v in row[1:]])
    return y, X


def knn_predict_one(train_y, train_X, x, k):
    heap = []
    for label, row in zip(train_y, train_X):
        d2 = 0
        for a, b in zip(row, x):
            diff = a - b
            d2 += diff * diff
        if d2 == 0:
            return label
        if len(heap) < k:
            heapq.heappush(heap, (-d2, label))
        else:
            if d2 < -heap[0][0]:
                heapq.heapreplace(heap, (-d2, label))

    votes = {}
    for neg_d2, label in heap:
        d2 = -neg_d2
        w = 1.0 / math.sqrt(d2)
        votes[label] = votes.get(label, 0.0) + w

    best_label = None
    best_weight = -1.0
    for label, weight in votes.items():
        if weight > best_weight:
            best_weight = weight
            best_label = label
    return best_label


def main():
    k = 7
    train_y, train_X = load_mnist_csv("MNIST_train.csv")
    test_y, test_X = load_mnist_csv("MNIST_test.csv")

    correct = 0
    total = 0
    for y_true, x in zip(test_y, test_X):
        y_pred = knn_predict_one(train_y, train_X, x, k)
        total += 1
        correct += int(y_pred == y_true)

    accuracy = (correct / total) * 100.0 if total else 0.0
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Replaced readlines()+string slicing/splitting with csv.reader streaming to reduce memory use and parsing overhead.
# - Split dataset into (labels, features) arrays once to avoid repeated isinstance checks and repeated indexing of row[0].
# - Computed squared Euclidean distance directly with integer arithmetic (diff*diff) to avoid math.pow and avoid sqrt per training sample.
# - Maintained top-k neighbors with a fixed-size heap (heapq) to avoid repeated max()/index()/list deletions, reducing time and allocations.
# - Added early return on exact match (distance==0) to skip unnecessary computation while preserving KNN behavior for identical samples.
# - Performed sqrt only for the final k neighbors when computing 1/d weights, matching original weighted voting with far fewer sqrt calls.
# - Removed storage of desired/computed class lists and computed accuracy in a single pass to reduce memory footprint and data movement.
# - Kept k as a constant and avoided any plotting, interactive input, verbose printing, or model saving to minimize runtime work.