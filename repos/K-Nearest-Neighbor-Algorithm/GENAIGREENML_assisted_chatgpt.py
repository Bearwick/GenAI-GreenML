# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd


SEED = 42
TRAIN_PATH = "MNIST_train.csv"
TEST_PATH = "MNIST_test.csv"
K = 7


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 2:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _infer_label_and_pixels(df: pd.DataFrame) -> tuple[str, list[str]]:
    headers = [h.strip() for h in "label," + ",".join([f"pixel{i}" for i in range(784)]).split(",")]
    cols = list(df.columns)

    label_col = "label" if "label" in cols else cols[0]

    pixel_cols = [c for c in cols if c != label_col and str(c).startswith("pixel")]
    if not pixel_cols:
        pixel_cols = [c for c in cols if c != label_col]

    expected_pixels = [h for h in headers if h.startswith("pixel")]
    expected_set = set(expected_pixels)
    pixel_cols_known = [c for c in pixel_cols if c in expected_set]
    if pixel_cols_known:
        pixel_cols = pixel_cols_known

    def _pix_key(name: str) -> int:
        s = str(name)
        if s.startswith("pixel"):
            try:
                return int(s[5:])
            except ValueError:
                return 10**9
        return 10**9

    pixel_cols = sorted(pixel_cols, key=_pix_key)
    return label_col, pixel_cols


def _to_numpy(df: pd.DataFrame, label_col: str, pixel_cols: list[str]) -> tuple[np.ndarray, np.ndarray]:
    y = pd.to_numeric(df[label_col], errors="coerce").to_numpy()
    x_df = df[pixel_cols].apply(pd.to_numeric, errors="coerce")
    X = x_df.to_numpy(dtype=np.float32, copy=False)
    mask = ~np.isnan(y)
    y = y[mask].astype(np.int64, copy=False)
    X = X[mask]
    X = np.nan_to_num(X, copy=False)
    return X, y


def knn_predict_all(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int) -> np.ndarray:
    preds = np.empty(X_test.shape[0], dtype=y_train.dtype)

    X_train = np.ascontiguousarray(X_train, dtype=np.float32)
    X_test = np.ascontiguousarray(X_test, dtype=np.float32)
    y_train = np.ascontiguousarray(y_train)

    for i in range(X_test.shape[0]):
        x = X_test[i]
        diff = X_train - x
        dist_sq = np.einsum("ij,ij->i", diff, diff, optimize=True)

        nn_idx = np.argpartition(dist_sq, k - 1)[:k]
        nn_dist = np.sqrt(dist_sq[nn_idx], dtype=np.float32)

        eps = 1e-12
        weights = 1.0 / np.maximum(nn_dist, eps)

        nn_labels = y_train[nn_idx]
        classes, inv = np.unique(nn_labels, return_inverse=True)
        class_weights = np.bincount(inv, weights=weights, minlength=classes.shape[0])
        preds[i] = classes[int(np.argmax(class_weights))]

    return preds


def main() -> None:
    os.environ.setdefault("PYTHONHASHSEED", str(SEED))
    np.random.seed(SEED)

    train_df = _read_csv_robust(TRAIN_PATH)
    test_df = _read_csv_robust(TEST_PATH)

    label_col_train, pixel_cols_train = _infer_label_and_pixels(train_df)
    label_col_test, pixel_cols_test = _infer_label_and_pixels(test_df)

    pixel_set_train = set(pixel_cols_train)
    pixel_cols = [c for c in pixel_cols_test if c in pixel_set_train] or pixel_cols_train

    X_train, y_train = _to_numpy(train_df, label_col_train, pixel_cols)
    X_test, y_test = _to_numpy(test_df, label_col_test, pixel_cols)

    y_pred = knn_predict_all(X_train, y_train, X_test, K)
    accuracy = float((y_pred == y_test).mean() * 100.0)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced manual line-by-line CSV parsing with pandas and a delimiter/decimal fallback to reduce Python-level overhead and improve I/O efficiency.
# - Converted data once to contiguous NumPy arrays (float32 for pixels, int64 for labels) to minimize memory footprint and speed up numerical operations.
# - Eliminated redundant header-row checks by filtering non-numeric labels via vectorized coercion/masking.
# - Replaced per-feature Python loops for Euclidean distance with vectorized BLAS-friendly computations (einsum) to reduce runtime and energy use.
# - Used argpartition to select k nearest neighbors in O(n) time per test sample instead of maintaining Python lists with repeated max/index operations.
# - Avoided storing per-sample desired/computed lists; computed accuracy directly from vectorized comparison to reduce memory and data movement.
# - Ensured reproducibility with fixed seeds and PYTHONHASHSEED without adding extra heavy dependencies or side effects.