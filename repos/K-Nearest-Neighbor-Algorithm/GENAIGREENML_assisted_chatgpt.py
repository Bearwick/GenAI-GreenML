# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd


SEED = 42
K = 7
TRAIN_PATH = "MNIST_train.csv"
TEST_PATH = "MNIST_test.csv"
DATASET_HEADERS = [
    "label",
    *[f"pixel{i}" for i in range(784)],
]


def _read_csv_robust(path: str, expected_min_cols: int) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] < expected_min_cols:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _coerce_and_select(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray]:
    colset = set(df.columns)
    label_col = "label" if "label" in colset else df.columns[0]
    feature_cols = [c for c in DATASET_HEADERS[1:] if c in colset]
    if not feature_cols:
        feature_cols = [c for c in df.columns if c != label_col]

    needed = [label_col] + feature_cols
    df = df.loc[:, needed]

    for c in needed:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    df = df.dropna(subset=needed)

    y = df[label_col].astype(np.int64, copy=False).to_numpy()
    X = df[feature_cols].astype(np.float32, copy=False).to_numpy()
    return X, y


def _knn_predict_weighted_inv_distance(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int) -> np.ndarray:
    n_test = X_test.shape[0]
    preds = np.empty(n_test, dtype=np.int64)
    classes = np.unique(y_train)

    for i in range(n_test):
        diff = X_train - X_test[i]
        dist = np.sqrt(np.einsum("ij,ij->i", diff, diff, optimize=True))
        k_eff = min(k, dist.shape[0])
        idx = np.argpartition(dist, k_eff - 1)[:k_eff]

        d = dist[idx]
        yk = y_train[idx]

        inv = 1.0 / np.maximum(d, np.finfo(np.float32).tiny)

        best_class = classes[0]
        best_weight = -1.0
        for c in classes:
            w = inv[yk == c].sum()
            if w > best_weight:
                best_weight = w
                best_class = c
        preds[i] = best_class

    return preds


def main() -> None:
    os.environ.setdefault("PYTHONHASHSEED", str(SEED))
    np.random.seed(SEED)

    train_df = _read_csv_robust(TRAIN_PATH, expected_min_cols=2)
    test_df = _read_csv_robust(TEST_PATH, expected_min_cols=2)

    X_train, y_train = _coerce_and_select(train_df)
    X_test, y_test = _coerce_and_select(test_df)

    y_pred = _knn_predict_weighted_inv_distance(X_train, y_train, X_test, K)

    accuracy = (y_pred == y_test).mean() * 100.0
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced manual CSV line parsing with pandas vectorized I/O and numeric coercion to reduce Python-level loops and overhead.
# - Implemented robust CSV parsing fallback (default read_csv, then retry with ';' separator and ',' decimal) to avoid mis-parsing without extra passes.
# - Avoided storing full training/test rows as Python lists; converted directly to compact NumPy arrays (float32 for features, int64 for labels) to reduce memory footprint and data movement.
# - Vectorized Euclidean distance computation per test sample using einsum, reducing redundant per-feature math.pow calls and speeding up distance calculations.
# - Used argpartition to select k nearest neighbors in linear time instead of maintaining and scanning Python lists of distances.
# - Computed accuracy directly via NumPy boolean mean, removing extra loops and intermediate lists while preserving the same evaluation intent.
# - Set fixed seeds (PYTHONHASHSEED, NumPy seed) to improve reproducibility and stabilize results.