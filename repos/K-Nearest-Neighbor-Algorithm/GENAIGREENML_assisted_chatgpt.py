# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd


SEED = 42
K = 7
TRAIN_PATH = "MNIST_train.csv"
TEST_PATH = "MNIST_test.csv"
DATASET_HEADERS = [
    "label",
    *[f"pixel{i}" for i in range(784)],
]


def _read_csv_robust(path: str, expected_min_cols: int) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] < expected_min_cols:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _coerce_schema(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray]:
    cols = list(df.columns)

    label_col = "label" if "label" in df.columns else cols[0]
    feature_cols = [c for c in cols if c != label_col]

    expected_features = [c for c in DATASET_HEADERS if c != "label"]
    if set(expected_features).issubset(df.columns):
        feature_cols = expected_features

    y = pd.to_numeric(df[label_col], errors="coerce").to_numpy()
    X = df[feature_cols].apply(pd.to_numeric, errors="coerce").to_numpy()

    valid = np.isfinite(y) & np.all(np.isfinite(X), axis=1)
    y = y[valid].astype(np.int64, copy=False)
    X = X[valid]

    return X, y


def _knn_predict_weighted_inverse_distance(
    X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int
) -> np.ndarray:
    X_train_f = X_train.astype(np.float32, copy=False)
    X_test_f = X_test.astype(np.float32, copy=False)

    y_pred = np.empty(X_test_f.shape[0], dtype=np.int64)
    y_unique = np.unique(y_train)

    for i in range(X_test_f.shape[0]):
        x = X_test_f[i]
        diffs = X_train_f - x
        d2 = np.einsum("ij,ij->i", diffs, diffs)

        if k < d2.size:
            nn_idx = np.argpartition(d2, k - 1)[:k]
        else:
            nn_idx = np.arange(d2.size)

        nn_d2 = d2[nn_idx]
        nn_y = y_train[nn_idx]

        nn_d = np.sqrt(nn_d2, dtype=np.float32)
        weights = 1.0 / nn_d

        class_weights = {int(c): 0.0 for c in y_unique}
        for cls, w in zip(nn_y, weights):
            class_weights[int(cls)] += float(w)

        best_class = None
        best_weight = None
        for cls, w in class_weights.items():
            if best_weight is None or w > best_weight:
                best_weight = w
                best_class = cls

        y_pred[i] = int(best_class)

    return y_pred


def main() -> None:
    np.random.seed(SEED)

    if not os.path.exists(TRAIN_PATH) or not os.path.exists(TEST_PATH):
        raise FileNotFoundError("Required dataset files not found in the working directory.")

    train_df = _read_csv_robust(TRAIN_PATH, expected_min_cols=2)
    test_df = _read_csv_robust(TEST_PATH, expected_min_cols=2)

    X_train, y_train = _coerce_schema(train_df)
    X_test, y_test = _coerce_schema(test_df)

    y_pred = _knn_predict_weighted_inverse_distance(X_train, y_train, X_test, K)

    accuracy = float(np.mean(y_pred == y_test) * 100.0)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced manual CSV parsing with pandas and added a delimiter/decimal fallback to avoid mis-parsing and rework.
# - Converted data directly to NumPy arrays and filtered invalid rows once, eliminating repeated type checks and per-element loops.
# - Used vectorized distance computation (einsum) and partial selection (argpartition) to avoid O(n log n) sorting and redundant scans.
# - Computed squared distances first and applied sqrt only for the selected k neighbors to reduce expensive operations.
# - Minimized memory movement by using float32 views (copy=False) where possible and avoiding intermediate Python lists.
# - Ensured reproducibility by fixing a global seed and removing interactive inputs, plots, and non-required logging.