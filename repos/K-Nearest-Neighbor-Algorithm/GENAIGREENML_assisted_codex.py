# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import csv
import math

def iter_csv_data(file_name):
    with open(file_name, newline="") as f:
        reader = csv.reader(f)
        next(reader, None)
        for row in reader:
            it = map(int, row)
            label = next(it)
            yield label, list(it)

def load_training_data(file_name):
    labels = []
    features = []
    for label, feats in iter_csv_data(file_name):
        labels.append(label)
        features.append(feats)
    return labels, features

def knn_predict(train_labels, train_features, test_vec, k, sentinel_sq):
    neighbor_distances = [sentinel_sq] * k
    neighbor_classes = [None] * k
    tv = test_vec
    rng = range(1, k)
    for label, train_vec in zip(train_labels, train_features):
        max_idx = 0
        max_dist = neighbor_distances[0]
        for idx in rng:
            d = neighbor_distances[idx]
            if d > max_dist:
                max_dist = d
                max_idx = idx
        dist_sq = 0
        for a, b in zip(tv, train_vec):
            diff = a - b
            dist_sq += diff * diff
            if dist_sq > max_dist:
                break
        if dist_sq > max_dist:
            continue
        neighbor_distances.pop(max_idx)
        neighbor_distances.append(dist_sq)
        neighbor_classes.pop(max_idx)
        neighbor_classes.append(label)
    class_votes = {}
    sqrt = math.sqrt
    for cls, dist_sq in zip(neighbor_classes, neighbor_distances):
        weight = 1.0 / sqrt(dist_sq)
        class_votes[cls] = class_votes.get(cls, 0.0) + weight
    best_class = None
    best_weight = -1.0
    for cls, weight in class_votes.items():
        if weight > best_weight:
            best_weight = weight
            best_class = cls
    return best_class

def main():
    training_file = "MNIST_train.csv"
    test_file = "MNIST_test.csv"
    k = 7
    sentinel_sq = 20000 * 20000
    train_labels, train_features = load_training_data(training_file)
    correct = 0
    total = 0
    predict = knn_predict
    for label, features in iter_csv_data(test_file):
        if predict(train_labels, train_features, features, k, sentinel_sq) == label:
            correct += 1
        total += 1
    accuracy = (correct / total) * 100 if total else 0.0
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Streamed test samples and computed accuracy incrementally to avoid storing predictions and reduce memory usage.
# - Skipped CSV headers and parsed rows with iterators to minimize preprocessing overhead and extra list copies.
# - Used squared distances with early abandonment and applied sqrt only for final neighbors to cut redundant math.
# - Consolidated neighbor-maximum search into a single pass and removed repeated max computations in voting.
# - Reduced unnecessary data movement by separating labels/features and using lightweight loops.