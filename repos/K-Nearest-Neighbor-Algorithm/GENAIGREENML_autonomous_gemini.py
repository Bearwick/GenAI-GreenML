# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

data = pd.read_csv('data.csv')
X = data.drop('label', axis=1).values.astype('float32')
y = data['label'].values
X /= 255.0
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LogisticRegression(
    multi_class='multinomial',
    solver='lbfgs',
    max_iter=100,
    tol=0.1
)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Memory Efficiency: Pixel data is converted to float32 to reduce the memory footprint by 50% compared to default float64, speeding up CPU operations.
# 2. Computationally Light Preprocessing: Simple division by 255.0 is used for normalization, avoiding the overhead of calculating dataset-wide statistics required for StandardScaling.
# 3. Efficient Model Choice: Logistic Regression with the L-BFGS solver is used as a lightweight alternative to deep learning, offering high interpretability and low energy consumption.
# 4. Energy-Aware Convergence: The tolerance parameter (tol) is set to 0.1 and max_iter to 100 to stop training early once the model stabilizes, preventing wasted CPU cycles on marginal gains.
# 5. Simplified Pipeline: The design avoids high-overhead components like PCA or complex ensembles, ensuring the solution runs quickly on standard hardware without specialized accelerators.