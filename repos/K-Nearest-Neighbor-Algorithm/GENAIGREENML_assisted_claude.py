# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
from sklearn.neighbors import KNeighborsClassifier

def main():
    train = np.loadtxt("MNIST_train.csv", delimiter=",", skiprows=1, dtype=np.float32)
    test = np.loadtxt("MNIST_test.csv", delimiter=",", skiprows=1, dtype=np.float32)

    X_train = train[:, 1:]
    y_train = train[:, 0].astype(np.int32)
    X_test = test[:, 1:]
    y_test = test[:, 0].astype(np.int32)

    k = 7
    knn = KNeighborsClassifier(
        n_neighbors=k,
        weights="distance",
        algorithm="ball_tree",
        n_jobs=-1,
    )
    knn.fit(X_train, y_train)
    predictions = knn.predict(X_test)

    correct = np.sum(predictions == y_test)
    total = len(y_test)
    accuracy = (correct / total) * 100

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# 1. Replaced manual CSV parsing with numpy loadtxt for fast I/O and minimal memory overhead.
# 2. Replaced hand-coded KNN with sklearn KNeighborsClassifier using ball_tree for efficient neighbor search.
# 3. Used weights="distance" to replicate the original 1/distance weighted voting.
# 4. Used n_jobs=-1 to parallelize distance computations across CPU cores.
# 5. Used float32 dtype to reduce memory footprint compared to float64.
# 6. Eliminated all redundant loops, intermediate lists, and per-sample Python-level distance calculations.
# 7. Vectorized accuracy computation with numpy instead of Python loop.