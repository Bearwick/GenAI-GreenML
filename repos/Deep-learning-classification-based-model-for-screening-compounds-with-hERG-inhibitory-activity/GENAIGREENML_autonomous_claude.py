# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import r2_score
from sklearn.pipeline import Pipeline

# Load dataset
df = pd.read_csv("data.csv")

# Separate features and target
target_col = "Activity_value"
feature_cols = [c for c in df.columns if c != target_col]

X = df[feature_cols].copy()
y = df[target_col].copy()

# Handle missing values with median imputation (lightweight, robust)
X = X.fillna(X.median())

# Remove rows where target is missing
mask = y.notna()
X = X[mask]
y = y[mask]

# Train/test split with fixed random state for reproducibility
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Pipeline: StandardScaler + lightweight GradientBoostingRegressor
# GBR chosen for strong performance on small tabular datasets with low computational cost
pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("model", GradientBoostingRegressor(
        n_estimators=200,
        max_depth=4,
        learning_rate=0.05,
        subsample=0.8,
        min_samples_split=5,
        min_samples_leaf=3,
        random_state=42,
    ))
])

# Fit the model
pipeline.fit(X_train, y_train)

# Evaluate on test set using R² as accuracy metric for regression
y_pred = pipeline.predict(X_test)
accuracy = r2_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Used GradientBoostingRegressor: excellent for small tabular datasets, CPU-friendly,
#    no GPU or deep learning required.
# 2. StandardScaler for normalization: lightweight O(n) preprocessing.
# 3. Median imputation: robust to outliers, zero overhead.
# 4. Kept n_estimators=200 and max_depth=4 to balance accuracy vs compute cost.
# 5. Subsample=0.8 reduces training time via stochastic gradient boosting.
# 6. No unnecessary embeddings, neural networks, or heavy feature engineering.
# 7. R² score used as the accuracy metric since Activity_value is continuous (regression task).