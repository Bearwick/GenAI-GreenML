# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

data = pd.read_csv('data.csv')

X = data[['a_acc', 'a_don', 'b_rotN', 'density', 'logP(o/w)', 'logS', 'rings', 'Weight']]
y = data['Activity_value']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = RandomForestClassifier(
    n_estimators=50, 
    max_depth=10, 
    n_jobs=-1, 
    random_state=42,
    class_weight='balanced'
)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Model Choice: RandomForestClassifier was selected for its high performance on small-to-medium tabular datasets without the massive energy overhead of training deep neural networks.
# 2. Complexity Control: The number of trees was limited to 50 and the maximum depth to 10 to significantly reduce the number of floating-point operations and memory footprint.
# 3. Efficiency: Utilizing n_jobs=-1 ensures that the model training utilizes available CPU cores in parallel, reducing the total active computation time and energy consumption.
# 4. Preprocessing: StandardScaler was used to normalize feature scales, which improves the decision-making efficiency of the tree splits and ensures consistency across different units (e.g., Weight vs density).
# 5. Hardware Constraints: The solution is designed to run purely on CPU, avoiding the high carbon footprint associated with GPU/TPU initialization and data transfer for small-scale ML tasks.