# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def load_dataset() -> pd.DataFrame:
    # Prefer local common filenames; fallback to reading from stdin
    candidates = [
        "data.csv",
        "dataset.csv",
        "train.csv",
        "input.csv",
        "Data.csv",
        "Dataset.csv",
        "Train.csv",
    ]
    for p in candidates:
        if os.path.exists(p):
            return pd.read_csv(p)
    # stdin fallback (e.g., platform piping a CSV)
    return pd.read_csv(os.sys.stdin)


def main() -> None:
    df = load_dataset()

    expected_cols = [
        "a_acc",
        "a_don",
        "b_rotN",
        "density",
        "logP(o/w)",
        "logS",
        "rings",
        "Weight",
        "Activity_value",
    ]

    # Keep only expected columns if extras exist; this makes pipeline deterministic
    missing = [c for c in expected_cols if c not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns: {missing}")

    df = df[expected_cols].copy()

    X = df.drop(columns=["Activity_value"])
    y = df["Activity_value"]

    # If target is continuous, binarize by median to enable lightweight classification
    # (classification accuracy is requested; this provides a deterministic conversion)
    if not (pd.api.types.is_integer_dtype(y) or pd.api.types.is_bool_dtype(y)):
        y_num = pd.to_numeric(y, errors="coerce")
        if y_num.isna().any():
            raise ValueError("Target column contains non-numeric values that cannot be coerced.")
        thr = float(y_num.median())
        y = (y_num >= thr).astype(int)

    numeric_features = list(X.columns)

    preprocess = ColumnTransformer(
        transformers=[
            (
                "num",
                Pipeline(
                    steps=[
                        ("imputer", SimpleImputer(strategy="median")),
                        ("scaler", StandardScaler(with_mean=True, with_std=True)),
                    ]
                ),
                numeric_features,
            )
        ],
        remainder="drop",
        sparse_threshold=0.0,
    )

    # Small, CPU-efficient model with strong baseline performance for tabular data
    clf = LogisticRegression(
        solver="lbfgs",
        max_iter=300,
        n_jobs=None,
    )

    model = Pipeline(
        steps=[
            ("preprocess", preprocess),
            ("model", clf),
        ]
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y if y.nunique() > 1 else None,
    )

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Uses a lightweight linear model (LogisticRegression) suitable for small tabular datasets; avoids deep learning to reduce compute/energy.
# - Employs a deterministic, minimal preprocessing pipeline: median imputation + standard scaling (fast, CPU-friendly).
# - Uses ColumnTransformer/Pipeline for reproducibility and to prevent data leakage (fit preprocessing only on train split).
# - Limits work by selecting only required columns and dropping extras to keep memory footprint low.
# - Keeps training single-process (n_jobs=None) for predictable CPU usage; lbfgs is efficient for small-to-medium problems.