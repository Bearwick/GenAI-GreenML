# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "herg_train_activity.csv"


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        normed.append(c2)
    return normed


def _read_csv_robust(path):
    # Attempt 1: default
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d):
        if d is None or d.empty:
            return True
        # If header parsing failed, it often results in a single wide column
        if d.shape[1] == 1:
            return True
        return False

    if _looks_wrong(df):
        # Attempt 2: European CSV conventions
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            df = df2
        except Exception:
            pass

    if df is None:
        raise RuntimeError("Could not read dataset.")

    df.columns = _normalize_columns(df.columns)
    # Drop "Unnamed" columns from index artifacts
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", c)]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")

    return df


def _coerce_numeric_inplace(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")


def _choose_target(df, preferred_headers=None):
    preferred_headers = preferred_headers or []
    # Normalize preferred names similarly to df columns for matching robustness
    pref = [_normalize_columns([x])[0] for x in preferred_headers]

    # First: exact match preference
    for t in pref:
        if t in df.columns:
            return t

    # Second: heuristic keywords
    lowered = {c: c.lower() for c in df.columns}
    for key in ["target", "label", "class", "activity", "y"]:
        for c in df.columns:
            if key in lowered[c]:
                return c

    # Third: pick a numeric non-constant column (prefer last)
    candidates = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        nun = s.nunique(dropna=True)
        if nun is not None and nun >= 2:
            # score by being more "label-like": low cardinality is often classification
            candidates.append((min(nun, 1000), c))
    if candidates:
        candidates.sort(key=lambda x: (x[0], df.columns.get_loc(x[1]) != (len(df.columns) - 1)))
        return candidates[0][1]

    # Fallback: last column
    return df.columns[-1]


def _is_classification_target(y_series):
    y = y_series.dropna()
    if y.empty:
        return False
    # If object -> treat as classification
    if y.dtype == "object" or str(y.dtype).startswith("string"):
        return True
    # If numeric: small number of unique values suggests classification
    nun = y.nunique(dropna=True)
    if nun <= 20:
        # Also ensure values look like discrete classes (near integers)
        vals = pd.to_numeric(y, errors="coerce").dropna().values
        if vals.size == 0:
            return False
        if np.all(np.isfinite(vals)) and np.mean(np.abs(vals - np.round(vals))) < 1e-6:
            return True
        # binary floats like 0.0/1.0
        if nun <= 2:
            return True
    return False


def _safe_accuracy_proxy_regression(y_true, y_pred):
    # bounded in [0,1] using R2 mapped: acc = max(0, min(1, (r2+1)/2))
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() < 2:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    ss_res = np.sum((yt - yp) ** 2)
    ss_tot = np.sum((yt - np.mean(yt)) ** 2)
    if ss_tot <= 0:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    acc = (r2 + 1.0) / 2.0
    if not np.isfinite(acc):
        return 0.0
    return float(np.clip(acc, 0.0, 1.0))


def main():
    df = _read_csv_robust(DATASET_PATH)

    assert df is not None and not df.empty, "Dataset is empty after reading."

    expected_headers = [
        "a_acc", "a_don", "b_rotN", "density", "logP(o/w)", "logS", "rings", "Weight", "Activity_value"
    ]
    target_col = _choose_target(df, preferred_headers=["Activity_value", "activity_value", "Activity", "label", "target"])

    # Separate X/y
    y_raw = df[target_col].copy()
    X = df.drop(columns=[target_col], errors="ignore").copy()

    # Normalize column names again for safety
    X.columns = _normalize_columns(X.columns)
    y_name = _normalize_columns([target_col])[0]
    y_raw.name = y_name

    # Identify numeric and categorical columns based on pandas dtypes
    numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_cols = [c for c in X.columns if c not in numeric_cols]

    # Coerce numeric-like object columns to numeric where possible (cheap heuristic)
    # If many object columns, only attempt those with limited unique values to reduce work
    for c in list(categorical_cols):
        s = X[c]
        if s.dtype == "object":
            nun = s.nunique(dropna=True)
            if nun is not None and nun <= max(50, int(0.05 * len(s))):
                coerced = pd.to_numeric(s, errors="coerce")
                # Promote to numeric if it meaningfully converts
                if coerced.notna().mean() > 0.8:
                    X[c] = coerced
                    numeric_cols.append(c)
                    categorical_cols.remove(c)

    # Ensure numeric columns are numeric (avoid mean/median on object)
    _coerce_numeric_inplace(X, numeric_cols)

    # Replace inf with NaN
    X.replace([np.inf, -np.inf], np.nan, inplace=True)

    # Decide task type
    is_clf = _is_classification_target(y_raw)

    # Prepare y
    if is_clf:
        # If numeric, round if near-integer to stabilize class labels
        if pd.api.types.is_numeric_dtype(y_raw):
            y_num = pd.to_numeric(y_raw, errors="coerce")
            if y_num.notna().any():
                y_raw = np.round(y_num).astype("Int64")
        y = y_raw.astype("category")
        # Remove rows with missing y
        mask = y.notna()
        X = X.loc[mask].copy()
        y = y.loc[mask].copy()

        # If still <2 classes, fallback to regression mode
        if y.nunique(dropna=True) < 2:
            is_clf = False
            y = pd.to_numeric(y_raw, errors="coerce")
            mask = y.notna()
            X = X.loc[mask].copy()
            y = y.loc[mask].copy()
    else:
        y = pd.to_numeric(y_raw, errors="coerce")
        mask = y.notna()
        X = X.loc[mask].copy()
        y = y.loc[mask].copy()

    assert len(X) > 2 and len(y) == len(X), "Not enough valid rows after preprocessing."

    # Recompute column types after row filtering
    numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_cols = [c for c in X.columns if c not in numeric_cols]

    # Lightweight preprocessing
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Train/test split
    if is_clf:
        # Use stratify when possible and safe
        strat = y if y.nunique(dropna=True) >= 2 and y.value_counts().min() >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=strat
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

        model = LogisticRegression(
            solver="liblinear",  # CPU-friendly for small/medium sparse data
            max_iter=200,
            class_weight="balanced" if y_train.nunique() == 2 else None,
        )

        clf = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ])

        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        # Regression fallback
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

        model = Ridge(alpha=1.0, random_state=42)

        reg = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ])

        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        accuracy = _safe_accuracy_proxy_regression(y_test.values, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight, CPU-friendly models: LogisticRegression (liblinear) for classification and Ridge for regression fallback.
# - ColumnTransformer + Pipeline ensures reproducible preprocessing and avoids repeated transformations.
# - Minimal feature engineering: median imputation + standardization for numeric; most_frequent + one-hot for categoricals.
# - Robust schema handling: normalizes headers, drops Unnamed columns, selects target defensively, coerces numerics, handles NaN/inf.
# - Robust CSV parsing: retries with sep=';' and decimal=',' when default parsing likely fails.
# - Regression fallback emits bounded [0,1] accuracy proxy via (R2+1)/2 clipped, ensuring stable ACCURACY reporting.