# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
from typing import Tuple

import numpy as np
import pandas as pd
import tensorflow as tf
from keras.layers import BatchNormalization, Dense
from keras.models import Sequential
from numpy.random import seed as np_seed
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler

DATASET_HEADERS = [
    "a_acc",
    "a_don",
    "b_rotN",
    "density",
    "logP(o/w)",
    "logS",
    "rings",
    "Weight",
    "Activity_value",
]


def set_reproducibility(seed_value: int = 1) -> None:
    os.environ["PYTHONHASHSEED"] = str(seed_value)
    random.seed(seed_value)
    np.random.seed(100)
    np_seed(0)
    tf.random.set_seed(seed_value)
    try:
        tf.config.experimental.enable_op_determinism()
    except Exception:
        pass


def robust_read_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def align_and_split_features(
    df: pd.DataFrame, target_col: str, expected_headers: list
) -> Tuple[np.ndarray, np.ndarray, list]:
    cols = list(df.columns)
    if target_col not in cols:
        raise ValueError(f"Target column '{target_col}' not found in data columns: {cols}")

    expected_features = [c for c in expected_headers if c != target_col]
    feature_cols = [c for c in expected_features if c in cols and c != target_col]
    if not feature_cols:
        feature_cols = [c for c in cols if c != target_col]

    X = df.loc[:, feature_cols].to_numpy(dtype=np.float32, copy=False)
    y = df.loc[:, [target_col]].to_numpy(dtype=np.float32, copy=False)
    return X, y, feature_cols


def scale_with_train_fit(X_train: np.ndarray, X_test: np.ndarray) -> Tuple[np.ndarray, np.ndarray, MinMaxScaler]:
    scaler = MinMaxScaler(feature_range=(0.0, 1.0), copy=False)
    scaler.fit(X_train)
    X_train_s = scaler.transform(X_train)
    X_test_s = scaler.transform(X_test)
    return X_train_s, X_test_s, scaler


def outlier_filter_with_pca_iforest(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    contamination: float = 0.1,
    random_state: int = 0,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    pca = PCA(n_components=2, random_state=random_state)
    X_train_2d = pca.fit_transform(X_train)

    iso = IsolationForest(
        contamination=contamination,
        n_estimators=100,
        random_state=random_state,
        verbose=0,
    )
    train_inlier_mask = iso.fit_predict(X_train_2d) != -1

    X_train_f = X_train[train_inlier_mask]
    y_train_f = y_train[train_inlier_mask]

    X_test_2d = pca.fit_transform(X_test)
    test_inlier_mask = iso.fit_predict(X_test_2d) != -1

    X_test_f = X_test[test_inlier_mask]
    y_test_f = y_test[test_inlier_mask]
    return X_train_f, y_train_f, X_test_f, y_test_f


def build_model(input_dim: int) -> Sequential:
    model = Sequential(
        [
            BatchNormalization(),
            Dense(
                200,
                input_dim=input_dim,
                activation="relu",
                kernel_initializer="random_uniform",
                kernel_regularizer=None,
                kernel_constraint="MaxNorm",
            ),
            BatchNormalization(),
            Dense(200, activation="relu"),
            Dense(200, activation="relu"),
            Dense(1, activation="sigmoid"),
        ]
    )
    model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
    return model


def main() -> None:
    set_reproducibility(1)

    train_df = robust_read_csv("herg_train_activity.csv")
    test_df = robust_read_csv("herg_test_activity.csv")

    X_train, y_train, feature_cols = align_and_split_features(
        train_df, target_col="Activity_value", expected_headers=DATASET_HEADERS
    )

    X_test, y_test, _ = align_and_split_features(
        test_df, target_col="Activity_value", expected_headers=DATASET_HEADERS
    )

    X_train, X_test, scaler = scale_with_train_fit(X_train, X_test)

    X_train, y_train, X_test, y_test = outlier_filter_with_pca_iforest(
        X_train, y_train, X_test, y_test, contamination=0.1, random_state=0
    )

    model = build_model(input_dim=X_train.shape[1])
    model.fit(
        X_train,
        y_train,
        epochs=200,
        batch_size=100,
        shuffle=True,
        verbose=0,
        validation_data=(X_test, y_test),
    )

    y_test_prob = model.predict(X_test, verbose=0)
    y_test_pred = (y_test_prob > 0.5).astype(np.int32).ravel()
    y_test_true = y_test.astype(np.int32).ravel()
    accuracy = float(accuracy_score(y_test_true, y_test_pred))
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused imports and all non-essential metrics computations to cut CPU work and dependencies.
# - Eliminated redundant scalers (fit only on training data) and avoided extra data movement by using copy=False where safe.
# - Avoided repeated model.predict calls by computing test predictions once and deriving labels from probabilities.
# - Removed all prints/logging/plots and file-saving side effects to reduce I/O overhead.
# - Added robust CSV parsing fallback (default, then sep=';' and decimal=',') to prevent expensive downstream errors/retries.
# - Enforced reproducibility via fixed seeds and best-effort deterministic TensorFlow ops to stabilize results across runs.
# - Kept preprocessing/model intent intact (scaling, PCA+IsolationForest filtering, same NN architecture/training settings).