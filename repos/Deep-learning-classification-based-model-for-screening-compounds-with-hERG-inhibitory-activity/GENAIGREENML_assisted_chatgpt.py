# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
from typing import Tuple

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras import Sequential
from tensorflow.keras.layers import BatchNormalization, Dense


DATASET_PATH_TRAIN = "herg_train_activity.csv"
DATASET_PATH_TEST = "herg_test_activity.csv"
DATASET_PATH_CAS = "cas.csv"

TARGET_COL = "Activity_value"

SEED_PYTHON = 0
SEED_NUMPY = 100
SEED_TF = 1
SEED_SKLEARN = 0

EPOCHS = 200
BATCH_SIZE = 100
CUTOFF = 0.5

IFOREST_CONTAMINATION = 0.1
IFOREST_ESTIMATORS = 100

PCA_COMPONENTS = 2


def set_reproducibility() -> None:
    os.environ.setdefault("PYTHONHASHSEED", str(SEED_PYTHON))
    os.environ.setdefault("TF_DETERMINISTIC_OPS", "1")
    random.seed(SEED_PYTHON)
    np.random.seed(SEED_NUMPY)
    tf.random.set_seed(SEED_TF)
    try:
        tf.config.experimental.enable_op_determinism()
    except Exception:
        pass


def _parsing_looks_wrong(df: pd.DataFrame, expected_headers: Tuple[str, ...]) -> bool:
    if df is None or df.empty:
        return True
    if df.shape[1] == 1:
        return True
    if not set(expected_headers).issubset(set(df.columns)):
        if any("," in str(c) for c in df.columns):
            return True
    return False


def read_csv_robust(path: str, expected_headers: Tuple[str, ...]) -> pd.DataFrame:
    df = pd.read_csv(path)
    if _parsing_looks_wrong(df, expected_headers):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def get_feature_columns(df: pd.DataFrame) -> list:
    cols = list(df.columns)
    if TARGET_COL in cols:
        cols.remove(TARGET_COL)
    return cols


def to_xy(df: pd.DataFrame, feature_cols: list) -> Tuple[np.ndarray, np.ndarray]:
    x = df.loc[:, feature_cols].to_numpy(dtype=np.float32, copy=False)
    y = df.loc[:, [TARGET_COL]].to_numpy(dtype=np.float32, copy=False)
    return x, y


def scale_train_test(x_train: np.ndarray, x_test: np.ndarray) -> Tuple[np.ndarray, np.ndarray, MinMaxScaler]:
    scaler = MinMaxScaler(feature_range=(0, 1), copy=False)
    x_train_s = scaler.fit_transform(x_train)
    x_test_s = scaler.transform(x_test)
    return x_train_s, x_test_s, scaler


def filter_outliers_with_pca_iforest(
    x: np.ndarray, y: np.ndarray, x_test: np.ndarray, y_test: np.ndarray
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    pca = PCA(n_components=PCA_COMPONENTS, random_state=SEED_SKLEARN)
    iso = IsolationForest(
        contamination=IFOREST_CONTAMINATION,
        n_estimators=IFOREST_ESTIMATORS,
        random_state=SEED_SKLEARN,
        verbose=0,
        n_jobs=1,
    )

    x_pca = pca.fit_transform(x)
    train_inlier_mask = iso.fit_predict(x_pca) != -1

    x_test_pca = pca.fit_transform(x_test)
    test_inlier_mask = iso.fit_predict(x_test_pca) != -1

    return x[train_inlier_mask], y[train_inlier_mask], x_test[test_inlier_mask], y_test[test_inlier_mask]


def build_model(input_dim: int) -> Sequential:
    model = Sequential(
        [
            BatchNormalization(),
            Dense(
                200,
                input_dim=input_dim,
                activation="relu",
                kernel_initializer="random_uniform",
                kernel_regularizer=None,
                kernel_constraint="MaxNorm",
            ),
            BatchNormalization(),
            Dense(200, activation="relu"),
            Dense(200, activation="relu"),
            Dense(1, activation="sigmoid"),
        ]
    )
    model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
    return model


def predict_classes(model: tf.keras.Model, x: np.ndarray, cutoff: float = CUTOFF) -> np.ndarray:
    proba = model.predict(x, verbose=0)
    return (proba > cutoff).astype(np.int8)


def main() -> None:
    set_reproducibility()

    expected_headers = (
        "a_acc",
        "a_don",
        "b_rotN",
        "density",
        "logP(o/w)",
        "logS",
        "rings",
        "Weight",
        "Activity_value",
    )

    train_df = read_csv_robust(DATASET_PATH_TRAIN, expected_headers)
    test_df = read_csv_robust(DATASET_PATH_TEST, expected_headers)

    feature_cols = get_feature_columns(train_df)
    x_train, y_train = to_xy(train_df, feature_cols)
    x_test, y_test = to_xy(test_df, feature_cols)

    x_train, x_test, scaler = scale_train_test(x_train, x_test)

    x_train, y_train, x_test, y_test = filter_outliers_with_pca_iforest(x_train, y_train, x_test, y_test)

    model = build_model(input_dim=x_train.shape[1])
    model.fit(
        x_train,
        y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        shuffle=True,
        verbose=0,
        validation_data=(x_test, y_test),
    )

    _, test_acc = model.evaluate(x_test, y_test, verbose=0)

    cas_df = read_csv_robust(
        DATASET_PATH_CAS,
        tuple(feature_cols),
    )
    if set(feature_cols).issubset(set(cas_df.columns)):
        x_cas = cas_df.loc[:, feature_cols].to_numpy(dtype=np.float32, copy=False)
    else:
        x_cas = cas_df.to_numpy(dtype=np.float32, copy=False)
    _ = predict_classes(model, scaler.transform(x_cas), cutoff=CUTOFF)

    accuracy = float(test_acc)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed redundant scaling: avoided fitting a second MinMaxScaler on the test set and reused the train-fitted scaler (same effective behavior as original since scaler1 was unused).
# - Eliminated duplicate predictions: replaced repeated model.predict calls on the same arrays with a single helper (predict_classes) to reduce compute.
# - Reduced data movement: used pandas .to_numpy(copy=False) and float32 arrays to lower memory footprint and speed up TensorFlow/NumPy operations.
# - Limited parallel overhead: set IsolationForest n_jobs=1 to avoid extra process/thread spawning energy costs for small PCA-transformed inputs.
# - Ensured reproducibility: set Python/NumPy/TensorFlow seeds and enabled deterministic TensorFlow ops when available.
# - Removed non-essential I/O and side effects: removed all prints/logging, loops, metrics reports, and CSV artifact saving while keeping core training/evaluation and CAS inference intent.