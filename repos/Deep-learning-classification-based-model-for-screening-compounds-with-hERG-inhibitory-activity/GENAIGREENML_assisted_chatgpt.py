# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os

os.environ.setdefault("PYTHONHASHSEED", "0")
os.environ.setdefault("TF_DETERMINISTIC_OPS", "1")
os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")

import numpy as np
import pandas as pd
import tensorflow as tf
from numpy.random import seed as np_seed
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.layers import BatchNormalization, Dense
from tensorflow.keras.models import Sequential

tf.random.set_seed(1)
np.random.seed(100)
np_seed(0)


def load_xy(csv_path: str):
    df = pd.read_csv(csv_path)
    x = df.drop(columns=["Activity_value"]).to_numpy(dtype=np.float32, copy=False)
    y = df["Activity_value"].to_numpy(dtype=np.float32, copy=False).reshape(-1, 1)
    return x, y


def fit_scale(train_x: np.ndarray, test_x: np.ndarray):
    scaler = MinMaxScaler(feature_range=(0, 1))
    train_x = scaler.fit_transform(train_x).astype(np.float32, copy=False)
    test_x = scaler.transform(test_x).astype(np.float32, copy=False)
    return train_x, test_x, scaler


def filter_outliers(train_x: np.ndarray, train_y: np.ndarray, test_x: np.ndarray, test_y: np.ndarray):
    pca = PCA(n_components=2, svd_solver="randomized", random_state=0)
    iso = IsolationForest(
        contamination=0.1,
        n_estimators=100,
        random_state=0,
        verbose=0,
        n_jobs=-1,
    )

    train_emb = pca.fit_transform(train_x)
    train_mask = iso.fit_predict(train_emb) != -1
    train_x = train_x[train_mask]
    train_y = train_y[train_mask]

    test_emb = pca.transform(test_x)
    test_mask = iso.predict(test_emb) != -1
    test_x = test_x[test_mask]
    test_y = test_y[test_mask]
    return train_x, train_y, test_x, test_y


def build_model(input_dim: int):
    model = Sequential(
        [
            BatchNormalization(input_shape=(input_dim,)),
            Dense(200, activation="relu", kernel_initializer="random_uniform"),
            BatchNormalization(),
            Dense(200, activation="relu"),
            Dense(200, activation="relu"),
            Dense(1, activation="sigmoid"),
        ]
    )
    model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
    return model


def main():
    x_train, y_train = load_xy("herg_train_activity.csv")
    x_test, y_test = load_xy("herg_test_activity.csv")

    x_train, x_test, scaler = fit_scale(x_train, x_test)
    x_train, y_train, x_test, y_test = filter_outliers(x_train, y_train, x_test, y_test)

    model = build_model(input_dim=x_train.shape[1])
    model.fit(
        x_train,
        y_train,
        epochs=200,
        batch_size=100,
        shuffle=True,
        verbose=0,
        validation_data=(x_test, y_test),
    )

    y_prob = model.predict(x_test, verbose=0)
    y_pred = (y_prob > 0.5).astype(np.int32, copy=False).ravel()
    accuracy = accuracy_score(y_test.ravel().astype(np.int32, copy=False), y_pred)
    print(f"ACCURACY={accuracy:.6f}")

    x_new = pd.read_csv("cas.csv").to_numpy(dtype=np.float32, copy=False)
    x_new = scaler.transform(x_new).astype(np.float32, copy=False)
    _ = model.predict(x_new, verbose=0)


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Removed unused imports, metrics, prints, plots, and file outputs to cut I/O and overhead.
# - Used a single MinMaxScaler fitted on training data and reused for test/new data; removed redundant scaler fit.
# - Ensured outlier detector is trained only on training data (fit on train PCA, predict on test) to avoid extra fitting work.
# - Reused PCA transform for test set to reduce computation (fit once, transform many).
# - Switched PCA to randomized SVD for lower compute on small component extraction.
# - Enabled IsolationForest parallelism via n_jobs=-1 to reduce wall-clock time where available.
# - Eliminated duplicate model.predict calls and redundant class arrays; computed predictions once per split.
# - Converted arrays to float32 early to reduce memory footprint and data movement in TensorFlow.
# - Added deterministic seeds/environment flags for reproducibility and stable results without extra runs.