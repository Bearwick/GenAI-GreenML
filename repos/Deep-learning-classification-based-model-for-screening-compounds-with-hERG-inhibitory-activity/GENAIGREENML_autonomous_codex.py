# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import glob
import re
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, mean_absolute_error

warnings.filterwarnings("ignore")

RANDOM_STATE = 42

dataset_headers = [h.strip() for h in "a_acc,a_don,b_rotN,density,logP(o/w),logS,rings,Weight,Activity_value".split(",") if h.strip()]

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
        return df
    if df.shape[1] <= 1:
        try:
            df2 = pd.read_csv(path, sep=';', decimal=',')
            if df2.shape[1] > df.shape[1]:
                df = df2
        except Exception:
            pass
    return df

def clean_columns(df):
    new_cols = []
    keep_idx = []
    for i, col in enumerate(df.columns):
        col_str = str(col).strip()
        col_str = re.sub(r'\s+', ' ', col_str)
        if col_str == '' or col_str.lower().startswith('unnamed'):
            continue
        new_cols.append(col_str)
        keep_idx.append(i)
    df = df.iloc[:, keep_idx]
    seen = {}
    unique_cols = []
    for col in new_cols:
        if col not in seen:
            seen[col] = 1
            unique_cols.append(col)
        else:
            seen[col] += 1
            unique_cols.append(f"{col}_{seen[col]}")
    df.columns = unique_cols
    return df

def normalize_name(name):
    return re.sub(r'[^a-z0-9]+', '', str(name).lower())

def choose_target(df, dataset_headers):
    candidates = []
    if dataset_headers:
        candidates.append(dataset_headers[-1])
    candidates += ['target', 'label', 'class', 'y']
    norm_cols = {normalize_name(c): c for c in df.columns}
    for cand in candidates:
        norm = normalize_name(cand)
        if norm in norm_cols:
            return norm_cols[norm]
    best_col = None
    best_unique = -1
    for c in df.columns:
        series = pd.to_numeric(df[c], errors='coerce')
        if series.notna().sum() == 0:
            continue
        unique_count = series.dropna().nunique()
        if unique_count > best_unique:
            best_unique = unique_count
            best_col = c
    if best_col is not None:
        return best_col
    return df.columns[0]

def infer_task_and_process(y):
    y_series = y.copy()
    y_series = y_series.replace('', np.nan)
    y_num = pd.to_numeric(y_series, errors='coerce')
    numeric_ratio = y_num.notna().mean() if len(y_series) > 0 else 0
    if numeric_ratio >= 0.8:
        y_proc = y_num
        uniq = y_proc.dropna().unique()
        if len(uniq) <= 20:
            return 'classification', y_proc, True
        else:
            return 'regression', y_proc, True
    else:
        return 'classification', y_series.astype(str), False

def process_target(y, numeric_flag, task_type):
    y_series = y.copy()
    y_series = y_series.replace('', np.nan)
    if numeric_flag or task_type == 'regression':
        y_proc = pd.to_numeric(y_series, errors='coerce')
    else:
        y_proc = y_series.astype(str)
    return y_proc

def identify_feature_types(X):
    X = X.copy()
    numeric_features = []
    categorical_features = []
    for col in X.columns:
        series = X[col]
        if pd.api.types.is_numeric_dtype(series):
            numeric_features.append(col)
        else:
            series_num = pd.to_numeric(series, errors='coerce')
            if series_num.notna().mean() >= 0.8:
                X[col] = series_num
                numeric_features.append(col)
            else:
                categorical_features.append(col)
    if numeric_features:
        X[numeric_features] = X[numeric_features].replace([np.inf, -np.inf], np.nan)
    for col in categorical_features:
        X[col] = X[col].astype(str)
    return X, numeric_features, categorical_features

def apply_feature_types(X, numeric_features, categorical_features):
    X = X.copy()
    for col in numeric_features:
        if col in X.columns:
            X[col] = pd.to_numeric(X[col], errors='coerce')
    if numeric_features:
        X[numeric_features] = X[numeric_features].replace([np.inf, -np.inf], np.nan)
    for col in categorical_features:
        if col in X.columns:
            X[col] = X[col].astype(str)
    return X

csv_files = [f for f in glob.glob("*.csv") if os.path.isfile(f)]
if not csv_files:
    raise FileNotFoundError("No CSV files found in the working directory.")

exclude_keywords = ['cas', 'pred', 'submission', 'sample']
filtered_files = [f for f in csv_files if not any(k in f.lower() for k in exclude_keywords)]
if not filtered_files:
    filtered_files = csv_files

train_files = [f for f in filtered_files if 'train' in f.lower()]
test_files = [f for f in filtered_files if 'test' in f.lower()]

external_test = False
df_train = None
df_test = None

if train_files and test_files:
    train_file = sorted(train_files)[0]
    test_file = sorted(test_files)[0]
    df_train = clean_columns(read_csv_robust(train_file))
    df_test = clean_columns(read_csv_robust(test_file))
    external_test = True
else:
    file_sizes = [(os.path.getsize(f), f) for f in filtered_files]
    dataset_file = max(file_sizes, key=lambda x: x[0])[1]
    df = clean_columns(read_csv_robust(dataset_file))

if external_test:
    if df_train is None or df_train.empty:
        external_test = False
    else:
        target_col = choose_target(df_train, dataset_headers)
        if target_col not in df_train.columns:
            external_test = False
        elif df_test is None or df_test.empty or target_col not in df_test.columns:
            external_test = False

if external_test:
    X_train = df_train.drop(columns=[target_col])
    y_train_raw = df_train[target_col].replace('', np.nan)
    train_mask = y_train_raw.notna()
    X_train = X_train.loc[train_mask]
    y_train_raw = y_train_raw.loc[train_mask]

    X_test = df_test.drop(columns=[target_col])
    y_test_raw = df_test[target_col].replace('', np.nan)
    test_mask = y_test_raw.notna()
    X_test = X_test.loc[test_mask]
    y_test_raw = y_test_raw.loc[test_mask]

    task_type, y_train_proc, numeric_flag = infer_task_and_process(y_train_raw)
    y_test_proc = process_target(y_test_raw, numeric_flag, task_type)

    train_mask = y_train_proc.notna()
    X_train = X_train.loc[train_mask]
    y_train = y_train_proc.loc[train_mask]

    test_mask = y_test_proc.notna()
    X_test = X_test.loc[test_mask]
    y_test = y_test_proc.loc[test_mask]

    if len(X_train) == 0 or len(X_test) == 0:
        external_test = False

if not external_test:
    if 'df' not in locals():
        if df_train is not None and df_test is not None:
            all_cols = list(dict.fromkeys(list(df_train.columns) + list(df_test.columns)))
            df = pd.concat([df_train.reindex(columns=all_cols), df_test.reindex(columns=all_cols)], ignore_index=True)
        elif df_train is not None:
            df = df_train.copy()
        else:
            raise ValueError("No usable dataset found.")
    target_col = choose_target(df, dataset_headers)
    if target_col not in df.columns:
        target_col = df.columns[0]
    X = df.drop(columns=[target_col])
    y_raw = df[target_col].replace('', np.nan)
    mask = y_raw.notna()
    X = X.loc[mask]
    y_raw = y_raw.loc[mask]

    task_type, y_proc, numeric_flag = infer_task_and_process(y_raw)
    y_proc = process_target(y_raw, numeric_flag, task_type)
    mask = y_proc.notna()
    X = X.loc[mask]
    y_proc = y_proc.loc[mask]

    n_samples = len(y_proc)
    assert n_samples > 0
    if n_samples < 2:
        X_train = X.copy()
        X_test = X.copy()
        y_train = y_proc.copy()
        y_test = y_proc.copy()
    else:
        if n_samples < 5:
            test_size = 1
            stratify = None
        else:
            test_size = 0.2
            stratify = None
            if task_type == 'classification' and y_proc.nunique() >= 2:
                class_counts = y_proc.value_counts()
                if class_counts.min() >= 2 and n_samples >= len(class_counts) * 2:
                    stratify = y_proc
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_proc, test_size=test_size, random_state=RANDOM_STATE, stratify=stratify
        )

assert len(X_train) > 0 and len(X_test) > 0

X_train = X_train.replace('', np.nan)
X_train = X_train.dropna(axis=1, how='all')
X_test = X_test.reindex(columns=X_train.columns)

X_train, numeric_features, categorical_features = identify_feature_types(X_train)
X_test = apply_feature_types(X_test, numeric_features, categorical_features)

if X_train.shape[1] == 0:
    X_train = pd.DataFrame({'dummy': np.zeros(len(X_train))})
    X_test = pd.DataFrame({'dummy': np.zeros(len(X_test))})
    numeric_features = ['dummy']
    categorical_features = []

transformers = []
if numeric_features:
    num_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    transformers.append(('num', num_transformer, numeric_features))
if categorical_features:
    try:
        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)
    except TypeError:
        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
    cat_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', ohe)
    ])
    transformers.append(('cat', cat_transformer, categorical_features))

if transformers:
    preprocess = ColumnTransformer(transformers=transformers)
else:
    preprocess = 'passthrough'

if task_type == 'classification':
    classes = pd.Series(y_train).dropna().unique()
    if len(classes) < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        if len(classes) == 2:
            model = LogisticRegression(max_iter=200, solver='liblinear', random_state=RANDOM_STATE)
        else:
            model = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='auto', random_state=RANDOM_STATE)
else:
    y_train = pd.to_numeric(y_train, errors='coerce')
    y_test = pd.to_numeric(y_test, errors='coerce')
    train_mask = y_train.notna()
    test_mask = y_test.notna()
    X_train = X_train.loc[train_mask]
    y_train = y_train.loc[train_mask]
    X_test = X_test.loc[test_mask]
    y_test = y_test.loc[test_mask]
    assert len(X_train) > 0 and len(X_test) > 0
    if y_train.nunique() <= 1:
        model = DummyRegressor(strategy='mean')
    else:
        model = Ridge(alpha=1.0)

pipeline = Pipeline(steps=[('preprocess', preprocess), ('model', model)])
pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

if task_type == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    mae = mean_absolute_error(y_test, y_pred)
    accuracy = 1.0 / (1.0 + mae)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Selected lightweight linear or dummy models to minimize CPU usage and energy consumption.
# - Used a reproducible ColumnTransformer pipeline with simple imputation, scaling, and optional one-hot encoding.
# - Implemented robust CSV loading, column normalization, and defensive fallbacks for unknown schemas.
# - For regression, reported a bounded accuracy proxy (1/(1+MAE)) to keep scores in [0,1].