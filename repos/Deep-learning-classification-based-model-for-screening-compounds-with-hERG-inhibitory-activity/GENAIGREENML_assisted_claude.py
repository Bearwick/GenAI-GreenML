# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
import numpy as np
from numpy.random import seed
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn.metrics import accuracy_score
import tensorflow as tf

tf.random.set_seed(1)
np.random.seed(100)
seed(0)

training_data_df = pd.read_csv("herg_train_activity.csv")
X = training_data_df.drop('Activity_value', axis=1).values
Y = training_data_df['Activity_value'].values

test_data_df = pd.read_csv("herg_test_activity.csv")
X_test = test_data_df.drop('Activity_value', axis=1).values
Y_test = test_data_df['Activity_value'].values

scaler = MinMaxScaler(feature_range=(0, 1)).fit(X)
X = scaler.transform(X)
X_test = scaler.transform(X_test)

pca = PCA(n_components=2)
pca1 = pca.fit_transform(X)
iso = IsolationForest(contamination=0.1, n_estimators=100, random_state=0, verbose=0)
yhat = iso.fit_predict(pca1)
pca2 = pca.transform(X_test)
yhat_1 = iso.predict(pca2)

mask = yhat != -1
X, Y = X[mask], Y[mask]

mask_test = yhat_1 != -1
X_test, Y_test = X_test[mask_test], Y_test[mask_test]

model = tf.keras.Sequential([
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(200, input_dim=8, activation='relu', kernel_initializer='random_uniform', kernel_constraint='MaxNorm'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(200, activation='relu'),
    tf.keras.layers.Dense(200, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(
    X, Y,
    epochs=200,
    batch_size=100,
    shuffle=True,
    verbose=0,
    validation_data=(X_test, Y_test)
)

y_pred_test = (model.predict(X_test, verbose=0) > 0.5).astype(int).ravel()
accuracy = accuracy_score(Y_test, y_pred_test)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Removed unused scaler1 for test data (test data already scaled with training scaler)
# Used pca.transform instead of fit_transform for test set and iso.predict instead of fit_predict for consistency and less computation
# Flattened Y arrays to 1D to avoid unnecessary reshaping later
# Removed redundant duplicate predictions (y_pred, y_pred_class, y_pred_1, y_pred_class_1 were identical)
# Removed prediction on external cas.csv and CSV saving as not part of model evaluation
# Removed all print statements, plots, classification reports, and metrics not needed for final accuracy
# Reduced imports to only what is needed
# Single predict call for test set instead of multiple redundant calls
# Used verbose=0 on predict to suppress output