# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn import metrics

SEED = 0
np.random.seed(SEED)

import tensorflow as tf
tf.random.set_seed(1)

def robust_read_csv(path):
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=';', decimal=',')
    return df

training_data_df = robust_read_csv("herg_train_activity.csv")

target_col = "Activity_value"
feature_cols = [c for c in training_data_df.columns if c != target_col]

X = training_data_df[feature_cols].values
Y = training_data_df[[target_col]].values

test_file = "herg_test_activity.csv"
has_test = os.path.exists(test_file)

if has_test:
    test_data_df = robust_read_csv(test_file)
    X_test = test_data_df[feature_cols].values
    Y_test = test_data_df[[target_col]].values
else:
    from sklearn.model_selection import train_test_split
    X, X_test, Y, Y_test = train_test_split(X, Y, test_size=0.3, random_state=SEED, stratify=Y)

scaler = MinMaxScaler(feature_range=(0, 1)).fit(X)
X = scaler.transform(X)
X_test = scaler.transform(X_test)

pca = PCA(n_components=2)
pca1 = pca.fit_transform(X)
iso = IsolationForest(contamination=0.1, n_estimators=100, random_state=SEED, verbose=0)
yhat = iso.fit_predict(pca1)

pca2 = pca.transform(X_test)
yhat_1 = iso.predict(pca2)

mask_train = yhat != -1
X, Y = X[mask_train], Y[mask_train]

mask_test = yhat_1 != -1
X_test, Y_test = X_test[mask_test], Y_test[mask_test]

n_features = X.shape[1]

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization

model = Sequential([
    BatchNormalization(),
    Dense(200, input_dim=n_features, activation='relu', kernel_initializer='random_uniform', kernel_constraint='MaxNorm'),
    BatchNormalization(),
    Dense(200, activation='relu'),
    Dense(200, activation='relu'),
    Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X, Y, epochs=200, batch_size=100, shuffle=True, verbose=0, validation_data=(X_test, Y_test))

y_pred_test = model.predict(X_test, verbose=0)
y_pred_class_test = (y_pred_test > 0.5).astype(int).ravel()
Y_test_flat = Y_test.ravel()

accuracy = metrics.accuracy_score(Y_test_flat, y_pred_class_test)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Removed duplicate model.predict calls: original called predict 4 times on train and 4 times on test; reduced to single call per set needed.
# 2. Removed CAS dataset loading and prediction since it was only used to produce a side-effect CSV file (CAS_full_herg.csv), which is not the core ML evaluation output.
# 3. Removed all print statements, plots, and logging per requirements.
#