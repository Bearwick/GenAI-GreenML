# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
SEED = 0
os.environ["PYTHONHASHSEED"] = str(SEED)
os.environ.setdefault("TF_DETERMINISTIC_OPS", "1")

import random
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization
from tensorflow.keras.constraints import MaxNorm
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split

random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(1)
try:
    tf.config.experimental.enable_op_determinism()
except Exception:
    pass

DATASET_HEADERS = ["a_acc", "a_don", "b_rotN", "density", "logP(o/w)", "logS", "rings", "Weight", "Activity_value"]
FEATURE_HEADERS = DATASET_HEADERS[:-1]

def read_csv_robust(path, headers=None):
    df = pd.read_csv(path)
    expected_cols = len(headers) if headers is not None else None
    if df.shape[1] == 1 or (expected_cols is not None and df.shape[1] < expected_cols):
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > df.shape[1]:
                df = df_alt
        except Exception:
            pass
    df.columns = [str(c).strip() for c in df.columns]
    unnamed = [c for c in df.columns if str(c).lower().startswith("unnamed")]
    if unnamed:
        df = df.drop(columns=unnamed)
    if headers is not None and len(df.columns) == len(headers) and list(df.columns) != headers:
        df.columns = headers
    return df

def infer_label_column(df, headers):
    for col in df.columns:
        if str(col).strip().lower() == "activity_value":
            return col
    if headers is not None and len(df.columns) == len(headers):
        if list(df.columns) != headers:
            df.columns = headers
        if "Activity_value" in df.columns:
            return "Activity_value"
    return df.columns[-1]

def split_features_labels(df, label_col, feature_cols=None):
    if label_col in df.columns:
        y = df[label_col].to_numpy()
        X_df = df.drop(columns=[label_col])
    else:
        y = df.iloc[:, -1].to_numpy()
        X_df = df.iloc[:, :-1]
    if y.ndim == 1:
        y = y.reshape(-1, 1)
    if feature_cols is not None:
        if all(col in X_df.columns for col in feature_cols):
            X_df = X_df[feature_cols]
        elif X_df.shape[1] == len(feature_cols):
            X_df = X_df.iloc[:, :len(feature_cols)]
    return X_df.to_numpy(), y, list(X_df.columns)

def extract_features(df, feature_cols, label_col=None):
    if label_col and label_col in df.columns and df.shape[1] == len(feature_cols) + 1:
        df = df.drop(columns=[label_col])
    if feature_cols and all(col in df.columns for col in feature_cols):
        df = df[feature_cols]
    elif feature_cols and df.shape[1] == len(feature_cols):
        df = df.iloc[:, :len(feature_cols)]
    return df.to_numpy()

def remove_outliers(X, y, seed):
    n_comp = 2 if X.shape[1] >= 2 else 1
    pca = PCA(n_components=n_comp, random_state=seed)
    X_pca = pca.fit_transform(X)
    iso = IsolationForest(contamination=0.1, n_estimators=100, random_state=0, verbose=0)
    mask = iso.fit_predict(X_pca) != -1
    return X[mask], y[mask]

train_path = "herg_train_activity.csv"
test_path = "herg_test_activity.csv"
cas_path = "cas.csv"

train_df = read_csv_robust(train_path, DATASET_HEADERS)
label_col = infer_label_column(train_df, DATASET_HEADERS)
X_all, y_all, feature_cols = split_features_labels(train_df, label_col)

if os.path.exists(test_path):
    test_df = read_csv_robust(test_path, DATASET_HEADERS)
    test_label_col = infer_label_column(test_df, DATASET_HEADERS)
    X_test, y_test, _ = split_features_labels(test_df, test_label_col, feature_cols)
    X_train, y_train = X_all, y_all
else:
    y_flat = y_all.reshape(-1)
    stratify = y_flat if len(np.unique(y_flat)) > 1 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X_all, y_all, test_size=0.3, random_state=SEED, stratify=stratify
    )

del train_df, X_all, y_all
try:
    del test_df
except NameError:
    pass

scaler = MinMaxScaler(feature_range=(0, 1), copy=False)
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train, y_train = remove_outliers(X_train, y_train, SEED)
X_test, y_test = remove_outliers(X_test, y_test, SEED)

X_train = X_train.astype(np.float32, copy=False)
X_test = X_test.astype(np.float32, copy=False)
y_train = y_train.astype(np.float32, copy=False)
y_test = y_test.astype(np.float32, copy=False)

tf.keras.backend.clear_session()

model = Sequential([
    BatchNormalization(input_shape=(X_train.shape[1],)),
    Dense(200, activation='relu', kernel_initializer='random_uniform', kernel_constraint=MaxNorm()),
    BatchNormalization(),
    Dense(200, activation='relu'),
    Dense(200, activation='relu'),
    Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=200, batch_size=100, shuffle=True, verbose=0)

test_pred = model.predict(X_test, verbose=0)
test_pred_class = (test_pred >= 0.5).astype(np.int32).reshape(-1)
y_test_int = y_test.astype(np.int32).reshape(-1)
accuracy = float(np.mean(test_pred_class == y_test_int))

if os.path.exists(cas_path):
    cas_df = read_csv_robust(cas_path, FEATURE_HEADERS)
    X_cas = extract_features(cas_df, feature_cols, label_col=label_col)
    if X_cas.size > 0:
        X_cas = scaler.transform(X_cas)
        X_cas = X_cas.astype(np.float32, copy=False)
        cas_pred = model.predict(X_cas, verbose=0)
        cas_class = (cas_pred >= 0.5).astype(np.int32)
        pd.DataFrame(cas_class, columns=['prediction']).to_csv('CAS_full_herg.csv')
    del cas_df

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Consolidated robust CSV loading with delimiter fallback and header normalization to avoid repeated parsing and schema errors.
# - Removed redundant scaler objects, duplicate predictions, and per-epoch validation evaluation to cut unnecessary computation.
# - Vectorized outlier filtering and accuracy calculation while discarding unused intermediates to reduce memory footprint.
# - Used in-place scaling and float32 conversion after preprocessing to minimize data movement and storage.
# - Enforced deterministic seeding and optional deterministic TF ops plus conditional CAS processing for reproducible, efficient runs.