# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import BatchNormalization, Dense
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest

np.random.seed(0)
tf.random.set_seed(1)

def load_dataset(path):
    data = pd.read_csv(path)
    y = data.pop("Activity_value").to_numpy()
    X = data.to_numpy()
    return X, y

def scale_features(train_X, test_X):
    scaler = MinMaxScaler(feature_range=(0, 1), copy=False)
    train_X = scaler.fit_transform(train_X)
    test_X = scaler.transform(test_X)
    return train_X, test_X

def remove_outliers(X, y, pca, iso):
    mask = iso.fit_predict(pca.fit_transform(X)) != -1
    return X[mask], y[mask]

def build_model(input_dim):
    model = Sequential()
    model.add(BatchNormalization(input_shape=(input_dim,)))
    model.add(Dense(200, activation='relu', kernel_initializer='random_uniform', kernel_constraint='MaxNorm'))
    model.add(BatchNormalization())
    model.add(Dense(200, activation='relu'))
    model.add(Dense(200, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def main():
    X_train, y_train = load_dataset("herg_train_activity.csv")
    X_test, y_test = load_dataset("herg_test_activity.csv")
    X_train, X_test = scale_features(X_train, X_test)
    pca = PCA(n_components=2)
    iso = IsolationForest(contamination=0.1, n_estimators=100, random_state=0, verbose=0)
    X_train, y_train = remove_outliers(X_train, y_train, pca, iso)
    X_test, y_test = remove_outliers(X_test, y_test, pca, iso)
    X_train = X_train.astype(np.float32, copy=False)
    X_test = X_test.astype(np.float32, copy=False)
    y_train = y_train.astype(np.float32, copy=False)
    y_test = y_test.astype(np.float32, copy=False)
    model = build_model(X_train.shape[1])
    model.fit(X_train, y_train, epochs=200, batch_size=100, shuffle=True, verbose=0)
    _, accuracy = model.evaluate(X_test, y_test, verbose=0)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# Reduced imports to essentials to lower load time and memory usage.
# Used DataFrame.pop and in-place scaling to minimize data copies and movement.
# Removed redundant predictions, metrics, validation passes, and file outputs to cut computation and I/O.
# Reused PCA/IsolationForest instances with direct masking and converted to float32 after filtering for smaller training footprint.
# Modularized preprocessing and model creation for clearer, maintainable code without extra overhead.