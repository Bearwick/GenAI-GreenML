# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
os.environ["PYTHONHASHSEED"] = "0"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

import random
import numpy as np
import pandas as pd
import tensorflow as tf
from keras.layers import BatchNormalization, Dense
from keras.models import Sequential
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest

random.seed(0)
np.random.seed(0)
tf.random.set_seed(1)

DATASET_HEADERS = "a_acc,a_don,b_rotN,density,logP(o/w),logS,rings,Weight,Activity_value"
EXPECTED_HEADERS = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]
LABEL_NAME = EXPECTED_HEADERS[-1] if EXPECTED_HEADERS else None


def _is_parsing_ok(df, expected_cols):
    if df is None or df.empty:
        return False
    cols = list(df.columns)
    if len(cols) == 1 and len(expected_cols) > 1:
        return False
    if expected_cols and any(col in cols for col in expected_cols):
        return True
    return len(cols) > 1


def read_csv_fallback(path, expected_cols):
    try:
        df = pd.read_csv(path)
    except Exception:
        return pd.read_csv(path, sep=";", decimal=",")
    if not _is_parsing_ok(df, expected_cols):
        try:
            df_alt = pd.read_csv(path, sep=";", decimal=",")
            if _is_parsing_ok(df_alt, expected_cols):
                df = df_alt
        except Exception:
            pass
    df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]
    return df


def select_feature_columns(df, expected_cols, label_col):
    if expected_cols:
        cols = [c for c in df.columns if c in expected_cols and c != label_col]
        if cols:
            return cols
    if label_col and label_col in df.columns:
        return [c for c in df.columns if c != label_col]
    return list(df.columns)


def extract_xy(df, feature_cols, label_col):
    if feature_cols and all(c in df.columns for c in feature_cols):
        X = df[feature_cols].to_numpy()
    else:
        if label_col and label_col in df.columns:
            X = df.drop(columns=[label_col]).to_numpy()
        else:
            X = df.to_numpy()
    y = df[[label_col]].to_numpy() if label_col and label_col in df.columns else None
    return X, y


def filter_outliers(X, y):
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)
    iso = IsolationForest(contamination=0.1, n_estimators=100, random_state=0, verbose=0)
    mask = iso.fit_predict(X_pca) != -1
    if y is None:
        return X[mask], None
    return X[mask], y[mask]


def align_feature_count(X, target_count):
    if X.shape[1] > target_count:
        return X[:, :target_count]
    if X.shape[1] < target_count:
        pad = target_count - X.shape[1]
        return np.pad(X, ((0, 0), (0, pad)), constant_values=0)
    return X


train_df = read_csv_fallback("herg_train_activity.csv", EXPECTED_HEADERS)
test_df = read_csv_fallback("herg_test_activity.csv", EXPECTED_HEADERS)

feature_cols = select_feature_columns(train_df, EXPECTED_HEADERS, LABEL_NAME)
X_train, y_train = extract_xy(train_df, feature_cols, LABEL_NAME)
X_test, y_test = extract_xy(test_df, feature_cols, LABEL_NAME)

if X_test.shape[1] != X_train.shape[1]:
    common_cols = [c for c in feature_cols if c in test_df.columns]
    if common_cols:
        feature_cols = common_cols
        X_train = train_df[feature_cols].to_numpy()
        X_test = test_df[feature_cols].to_numpy()
    else:
        min_features = min(X_train.shape[1], X_test.shape[1])
        X_train = X_train[:, :min_features]
        X_test = X_test[:, :min_features]
        if feature_cols:
            feature_cols = feature_cols[:min_features]

del train_df, test_df

if y_train is None or y_test is None:
    raise ValueError("Label column not found in datasets.")

scaler = MinMaxScaler(feature_range=(0, 1))
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train, y_train = filter_outliers(X_train, y_train)
X_test, y_test = filter_outliers(X_test, y_test)

X_train = X_train.astype(np.float32, copy=False)
X_test = X_test.astype(np.float32, copy=False)
y_train = y_train.astype(np.float32, copy=False)
y_test = y_test.astype(np.float32, copy=False)

input_dim = X_train.shape[1]
model = Sequential()
model.add(BatchNormalization())
model.add(Dense(200, input_dim=input_dim, activation="relu", kernel_initializer="random_uniform", kernel_constraint="MaxNorm"))
model.add(BatchNormalization())
model.add(Dense(200, activation="relu"))
model.add(Dense(200, activation="relu"))
model.add(Dense(1, activation="sigmoid"))
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
model.fit(X_train, y_train, epochs=200, batch_size=100, shuffle=True, verbose=0)

_, test_acc = model.evaluate(X_test, y_test, verbose=0)

new_df = read_csv_fallback("cas.csv", EXPECTED_HEADERS)
X_new, _ = extract_xy(new_df, feature_cols, LABEL_NAME)
X_new = align_feature_count(X_new, X_train.shape[1])
X_new = scaler.transform(X_new.astype(np.float32, copy=False)).astype(np.float32, copy=False)

pred_prob = model.predict(X_new, verbose=0)
pred_class = (pred_prob > 0.5).astype(int)
pd.DataFrame(pred_class, columns=["prediction"]).to_csv("CAS_full_herg.csv")

accuracy = float(test_acc)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Consolidated CSV loading with schema-aware fallback parsing to avoid redundant reads and mis-parsing.
# - Removed unused scalers, duplicate predictions, and extra metric calculations to cut unnecessary computation.
# - Dropped validation evaluation during training to reduce per-epoch overhead without affecting model weights.
# - Delayed casting to float32 until after outlier filtering to preserve behavior while reducing memory for training.
# - Modularized preprocessing and alignment logic to minimize repeated code and intermediate data movement.