# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.constraints import MaxNorm
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score

def load_robust_csv(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError
    except:
        df = pd.read_csv(path, sep=';', decimal=',')
    return df

np.random.seed(100)
tf.random.set_seed(1)
from numpy.random import seed
seed(0)

train_df = load_robust_csv("herg_train_activity.csv")
test_df = load_robust_csv("herg_test_activity.csv")
cas_df = load_robust_csv("cas.csv")

target_col = 'Activity_value'
X = train_df.drop(target_col, axis=1).values.astype('float32')
Y = train_df[target_col].values.astype('int32')
X_test = test_df.drop(target_col, axis=1).values.astype('float32')
Y_test = test_df[target_col].values.astype('int32')
X_cas = cas_df.values.astype('float32')

scaler = MinMaxScaler(feature_range=(0, 1))
X = scaler.fit_transform(X)
X_test = scaler.transform(X_test)
X_cas = scaler.transform(X_cas)

pca = PCA(n_components=2)
iso = IsolationForest(contamination=0.1, n_estimators=100, random_state=0)

yhat_train = iso.fit_predict(pca.fit_transform(X))
mask_train = yhat_train != -1
X, Y = X[mask_train], Y[mask_train]

yhat_test = iso.fit_predict(pca.fit_transform(X_test))
mask_test = yhat_test != -1
X_test, Y_test = X_test[mask_test], Y_test[mask_test]

model = Sequential([
    BatchNormalization(input_shape=(X.shape[1],)),
    Dense(200, activation='relu', kernel_initializer='random_uniform', kernel_constraint=MaxNorm(3)),
    BatchNormalization(),
    Dense(200, activation='relu'),
    Dense(200, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(
    X, Y,
    epochs=200,
    batch_size=100,
    shuffle=True,
    verbose=0
)

y_pred_test_prob = model.predict(X_test, verbose=0)
y_pred_test = (y_pred_test_prob > 0.5).astype(int)

y_pred_cas_prob = model.predict(X_cas, verbose=0)
y_pred_cas = (y_pred_cas_prob > 0.5).astype(int)

pd.DataFrame(y_pred_cas, columns=['prediction']).to_csv('CAS_full_herg.csv', index=False)

accuracy = accuracy_score(Y_test, y_pred_test)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Reduced memory footprint by casting data to float32 and int32.
# 2. Eliminated redundant model prediction calls for the same data subsets.
# 3. Removed unused MinMaxScaler fitting (scaler1) to save computation.
# 4. Streamlined the outlier detection workflow by combining fit and transform steps.
# 5. Implemented robust CSV parsing to handle different delimiters and decimals efficiently.
# 6. Removed all visualization, logging, and iterative print statements to reduce I/O overhead.
# 7. Optimized Keras model execution by disabling verbose output in fit and predict methods.
# 8. Used vectorized numpy operations for thresholding predictions instead of loops.
# 9. Cleaned up redundant imports and simplified the neural network definition to improve readability and reduce initialization time.
# 10. Avoided unnecessary intermediate data structures by performing masking and filtering in-place where possible.