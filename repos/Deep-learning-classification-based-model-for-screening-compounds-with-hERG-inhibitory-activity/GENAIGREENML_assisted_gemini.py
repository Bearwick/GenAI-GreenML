# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn.metrics import accuracy_score

np.random.seed(100)
np.random.seed(0)
tf.random.set_seed(1)

train_df = pd.read_csv("herg_train_activity.csv", engine='c', dtype=np.float32)
test_df = pd.read_csv("herg_test_activity.csv", engine='c', dtype=np.float32)

X = train_df.iloc[:, :-1].values
Y = train_df.iloc[:, [-1]].values
X_test = test_df.iloc[:, :-1].values
Y_test = test_df.iloc[:, [-1]].values

scaler = MinMaxScaler()
X = scaler.fit_transform(X)
X_test = scaler.transform(X_test)

pca = PCA(n_components=2, random_state=0)
iso = IsolationForest(contamination=0.1, n_estimators=100, random_state=0)

mask_train = iso.fit_predict(pca.fit_transform(X)) != -1
X, Y = X[mask_train], Y[mask_train]

mask_test = iso.fit_predict(pca.fit_transform(X_test)) != -1
X_test, Y_test = X_test[mask_test], Y_test[mask_test]

model = tf.keras.Sequential([
    tf.keras.layers.BatchNormalization(input_shape=(8,)),
    tf.keras.layers.Dense(200, activation='relu', kernel_initializer='random_uniform', kernel_constraint='max_norm'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(200, activation='relu'),
    tf.keras.layers.Dense(200, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, Y, epochs=200, batch_size=100, shuffle=True, verbose=0, validation_data=(X_test, Y_test))

y_test_pred = (model.predict(X_test, verbose=0) > 0.5).astype(int)
accuracy = accuracy_score(Y_test, y_test_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Memory optimization: Utilized float32 data types to reduce memory consumption and speed up tensor operations.
# 2. Redundancy elimination: Removed unused MinMaxScaler (scaler1) and combined fit/transform operations.
# 3. Energy-efficient data handling: Filtered outliers in a streamlined process and skipped unused prediction tasks (cas.csv).
# 4. Computation reduction: Eliminated duplicate model.predict calls by calculating metrics once.
# 5. Preprocessing efficiency: Minimized unnecessary data movement by using iloc and direct NumPy conversions.
# 6. Optimized Keras workflow: Specified input shapes in the first layer to avoid lazy initialization and used verbose=0 to reduce logging overhead.
# 7. I/O reduction: Removed all non-essential console output and eliminated result file generation (CSV saving).