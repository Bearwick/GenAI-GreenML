# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def run_pipeline():
    file_path = 'xgboost_on _binary.csv'
    try:
        df = pd.read_csv(file_path)
        if len(df.columns) < 2:
            raise ValueError
    except Exception:
        df = pd.read_csv(file_path, sep=';', decimal=',')

    df['admit'] = df['admit'].astype('category').cat.codes
    df['rank'] = df['rank'].astype('category').cat.codes

    df_encoded = pd.get_dummies(df, columns=['admit', 'rank'], drop_first=True)

    y = df_encoded['admit_1']
    X = df_encoded.drop(columns=['admit_1'])

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    model = xgb.XGBClassifier(
        random_state=42, 
        use_label_encoder=False, 
        eval_metric='logloss',
        n_jobs=1
    )
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary
# 1. Removed heavy visualization libraries (seaborn, matplotlib) to reduce memory overhead and import time.
# 2. Eliminated redundant Exploratory Data Analysis (EDA) steps (head, info, describe, pairplot) to save CPU cycles.
# 3. Replaced Scikit-Learn's LabelEncoder with Pandas 'category' codes for faster, more memory-efficient categorical mapping.
# 4. Streamlined preprocessing by combining encoding and dummy variable creation into a single logical flow.
# 5. Set n_jobs=1 in XGBoost to avoid the energy overhead of multi-threading/process spawning on a very small dataset.
# 6. Used a robust CSV loading mechanism to prevent execution failure and unnecessary manual debugging.
# 7. Removed all intermediate logging, prints, and unused metric calculations to minimize I/O and computation.
# 8. Fixed random seeds (random_state=42) to ensure reproducibility without re-running experiments.
# 9. Reduced memory footprint by avoiding multiple intermediate copies of the dataframe.
# 10. Simplified the evaluation logic to focus solely on the required accuracy output.