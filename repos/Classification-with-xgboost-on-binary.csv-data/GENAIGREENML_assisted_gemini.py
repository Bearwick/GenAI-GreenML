# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: assisted

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

df = pd.read_csv('xgboost_on _binary.csv')

y = df['admit'].astype(int)
X = pd.get_dummies(df.drop(columns=['admit']), columns=['rank'], drop_first=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = xgb.XGBClassifier(n_jobs=-1, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_pred_prob = model.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_prob)
cm = confusion_matrix(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")
print(f"Precision: {precision:.6f}")
print(f"Recall: {recall:.6f}")
print(f"F1 Score: {f1:.6f}")
print(f"ROC AUC: {roc_auc:.6f}")
print("Confusion Matrix:")
print(cm)

# Applied Optimizations:
# 1. Removed all visualization libraries (Matplotlib, Seaborn) and plotting calls to eliminate graphical overhead.
# 2. Removed redundant Exploratory Data Analysis (EDA) functions like head(), info(), describe(), and shape.
# 3. Streamlined preprocessing by removing the redundant LabelEncoder step, going directly to one-hot encoding.
# 4. Reduced memory footprint by avoiding the creation of multiple intermediate encoded DataFrames.
# 5. Optimized XGBoost execution by setting n_jobs=-1 to utilize parallel processing.
# 6. Consolidated evaluation metrics to prevent multiple redundant passes over the prediction data.
# 7. Removed unnecessary I/O operations (printing entire DataFrames to the console).
