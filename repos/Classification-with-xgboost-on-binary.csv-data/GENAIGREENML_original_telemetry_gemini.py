# Generated by generate_llm_code.py
# LLM: gemini
# Mode: original_telemetry

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import accuracy_score

df = pd.read_csv('xgboost_on _binary.csv')

le_admit = LabelEncoder()
le_rank = LabelEncoder()

df['admit'] = le_admit.fit_transform(df['admit'])
df['rank'] = le_rank.fit_transform(df['rank'])

df_encoded = pd.get_dummies(df, columns=['admit', 'rank'], drop_first=True)

X = df_encoded.drop(['admit_1'], axis=1)
y = df_encoded['admit_1']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = xgb.XGBClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# - Removed all Exploratory Data Analysis (EDA) code including .head(), .info(), .describe(), .shape, and .isnull().
# - Removed all visualization code, including seaborn and matplotlib imports and plots.
# - Removed original print statements and variable inspections that produced console output.
# - Consolidated redundant imports from sklearn.metrics and other modules.
# - Retained the original algorithmic workflow: LabelEncoding followed by One-Hot Encoding via get_dummies.
# - Preserved the logic for splitting, training with XGBClassifier, and predicting on the test set.
# - Added the required formatted accuracy print statement.