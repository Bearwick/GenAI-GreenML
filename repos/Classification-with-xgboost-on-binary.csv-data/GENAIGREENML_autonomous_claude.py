# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("input_data.csv")

# Target and features
target_col = "admit"
feature_cols = [c for c in df.columns if c != target_col]

X = df[feature_cols].values
y = df[target_col].values

# Handle missing values if any
if np.any(np.isnan(X)):
    from sklearn.impute import SimpleImputer
    imputer = SimpleImputer(strategy="median")
    X = imputer.fit_transform(X)

# Preprocessing: scale features for logistic regression convergence
scaler = StandardScaler()

# Use stratified k-fold cross-validation for robust evaluation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

accuracies = []
for train_idx, test_idx in skf.split(X, y):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Logistic Regression: lightweight, interpretable, CPU-friendly
    # C tuned moderately; solver='lbfgs' is efficient for small datasets
    model = LogisticRegression(
        C=1.0,
        solver="lbfgs",
        max_iter=500,
        random_state=42
    )
    model.fit(X_train_scaled, y_train)
    preds = model.predict(X_test_scaled)
    accuracies.append(accuracy_score(y_test, preds))

accuracy = np.mean(accuracies)

# Final model trained on all data for reporting
X_scaled = scaler.fit_transform(X)
final_model = LogisticRegression(C=1.0, solver="lbfgs", max_iter=500, random_state=42)
final_model.fit(X_scaled, y)
final_preds = final_model.predict(X_scaled)
train_accuracy = accuracy_score(y, final_preds)

# Report cross-validated accuracy as the primary metric
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Logistic Regression chosen: lightweight, fast on CPU, suitable for binary classification on small tabular data (admit prediction).
# 2. StandardScaler used for feature normalization — improves convergence without computational overhead.
# 3. StratifiedKFold (5 folds) ensures robust evaluation while respecting class imbalance in admission data.
# 4. No deep learning or heavy ensembles — unnecessary for ~400-row dataset with 3 features.
# 5. Minimal memory footprint; entire pipeline runs in milliseconds on CPU.
# 6. 'rank' feature treated as numeric ordinal which is appropriate given its nature (1-4 prestige ranking).