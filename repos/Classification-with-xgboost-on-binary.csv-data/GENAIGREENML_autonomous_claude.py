# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
import os

# Robust CSV loading
filepath = 'xgboost_on _binary.csv'
if not os.path.exists(filepath):
    candidates = [f for f in os.listdir('.') if f.endswith('.csv')]
    if candidates:
        filepath = candidates[0]

try:
    df = pd.read_csv(filepath)
    if df.shape[1] < 2:
        df = pd.read_csv(filepath, sep=';', decimal=',')
except Exception:
    df = pd.read_csv(filepath, sep=';', decimal=',')

# Normalize column names
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
df = df[[c for c in df.columns if not c.startswith('Unnamed')]]

# Expected schema: admit, gre, gpa, rank
expected_cols = ['admit', 'gre', 'gpa', 'rank']
available_cols = [c for c in expected_cols if c in df.columns]

if 'admit' in df.columns:
    target_col = 'admit'
    feature_cols = [c for c in available_cols if c != target_col]
else:
    # Fallback: use last column as target
    target_col = df.columns[-1]
    feature_cols = [c for c in df.columns if c != target_col]

# Coerce numeric columns
for c in df.columns:
    df[c] = pd.to_numeric(df[c], errors='coerce')

# Drop rows with NaN/inf
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.dropna(subset=[target_col], inplace=True)
df.dropna(subset=feature_cols, inplace=True)

assert df.shape[0] > 0, "Dataset is empty after preprocessing"

X = df[list(feature_cols)].copy()
y = df[target_col].copy()

# Determine if classification or regression
n_unique = y.nunique()
is_classification = n_unique < 20 and n_unique >= 2

if n_unique < 2:
    # Trivial baseline: predict the single class
    accuracy = 1.0
    print(f"ACCURACY={accuracy:.6f}")
    import sys
    sys.exit(0)

# Identify numeric vs categorical features
# Treat 'rank' as categorical if present (ordinal with few levels)
cat_features = []
num_features = []
for c in feature_cols:
    if c == 'rank' or X[c].nunique() <= 5:
        cat_features.append(c)
        X[c] = X[c].astype(str)
    else:
        num_features.append(c)

# Build preprocessing
transformers = []
if num_features:
    transformers.append(('num', StandardScaler(), num_features))
if cat_features:
    transformers.append(('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='infrequent_if_exist'), cat_features))

preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')

# Use logistic regression for energy efficiency
model = Pipeline([
    ('preprocess', preprocessor),
    ('clf', LogisticRegression(max_iter=500, solver='lbfgs', random_state=42))
])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y if is_classification else None
)

assert X_train.shape[0] > 0 and X_test.