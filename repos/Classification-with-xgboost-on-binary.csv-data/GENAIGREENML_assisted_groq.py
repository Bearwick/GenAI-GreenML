# Generated by generate_llm_code_2.py
# LLM: groq
# Mode: assisted

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import xgboost as xgb

# Load dataset
df = pd.read_csv('xgboost_on_binary.csv')

# Encode target and ordinal feature
df['admit'] = LabelEncoder().fit_transform(df['admit'])
df['rank'] = LabelEncoder().fit_transform(df['rank'])

# Features and target
X = df[['gre', 'gpa', 'rank']]
y = df['admit']

# Train‑test split with stratification for balanced classes
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# XGBoost classifier with lightweight settings
model = xgb.XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    verbosity=0,
    random_state=42
)
model.fit(X_train, y_train)

# Prediction and evaluation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# ----------------------------------------------------------------------------------
# Optimizations applied:
# 1. Removed all exploratory visualizations and unnecessary imports to lower CPU usage.
# 2. Simplified preprocessing: only encoded the target and rank; no redundant one‑hot on target.
# 3. Selected essential numeric features (gre, gpa, rank) to reduce dimensionality.
# 4. Set XGBoost to minimal, fast configuration (low n_estimators, max_depth) to cut training time.
# 5. Disabled verbose output and label encoder deprecation warnings for cleaner execution.
# 6. Used stratified split to preserve class balance without extra computation.
# 7. Eliminated any intermediate data structures (e.g., temporary DataFrames) to save memory.