# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import warnings
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, mean_absolute_error
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")

DATASET_PATH = "xgboost_on _binary.csv"
DATASET_HEADERS = "admit,gre,gpa,rank"

def robust_read_csv(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        return pd.read_csv(path, sep=';', decimal=',')
    if df.shape[1] == 1 or any(';' in str(c) for c in df.columns):
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > 1:
                df = df_alt
        except Exception:
            pass
    return df

def clean_columns(df):
    cols = []
    for c in df.columns:
        c_str = str(c)
        c_str = " ".join(c_str.strip().split())
        cols.append(c_str)
    df.columns = cols
    drop_cols = [c for c in df.columns if c.lower().startswith('unnamed')]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df

def coerce_numeric(df):
    for col in df.columns:
        series = df[col]
        if series.dtype == object or pd.api.types.is_string_dtype(series):
            coerced = pd.to_numeric(series, errors='coerce')
            if coerced.notna().mean() > 0.5:
                df[col] = coerced
        elif pd.api.types.is_numeric_dtype(series):
            df[col] = pd.to_numeric(series, errors='coerce')
    return df

def choose_target(df):
    priority = ['target', 'label', 'class', 'y', 'admit']
    for p in priority:
        if p in df.columns and df[p].notna().sum() > 0:
            return p
    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    for c in numeric_cols:
        if df[c].notna().sum() > 0 and df[c].nunique(dropna=True) > 1:
            return c
    for c in df.columns:
        if df[c].notna().sum() > 0 and df[c].nunique(dropna=True) > 1:
            return c
    for c in df.columns:
        if df[c].notna().sum() > 0:
            return c
    return df.columns[0]

def safe_dropna_target(df, target):
    df_drop = df.dropna(subset=[target])
    if not df_drop.empty:
        return df_drop, target
    for c in df.columns:
        if c == target:
            continue
        if df[c].notna().sum() > 0 and df[c].nunique(dropna=True) > 1:
            df_alt = df.dropna(subset=[c])
            if not df_alt.empty:
                return df_alt, c
    df[target] = df[target].fillna(0)
    return df, target

path = DATASET_PATH.strip()
df = robust_read_csv(path)
df = clean_columns(df)

expected_headers = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]
if expected_headers and df.shape[1] == len(expected_headers):
    if not set(expected_headers).intersection(set(df.columns)):
        numeric_like = True
        for c in df.columns:
            s = str(c)
            if not s.replace(".", "", 1).isdigit():
                numeric_like = False
                break
        if numeric_like:
            try:
                df = pd.read_csv(path, header=None)
            except Exception:
                df = pd.read_csv(path, sep=';', decimal=',', header=None)
            if df.shape[1] == len(expected_headers):
                df.columns = expected_headers
            df = clean_columns(df)

df.replace([np.inf, -np.inf], np.nan, inplace=True)
df = coerce_numeric(df)

target = choose_target(df)
df, target = safe_dropna_target(df, target)

assert not df.empty

X = df.drop(columns=[target])
y = df[target]

if X.shape[1] == 0:
    X = pd.DataFrame({'const': np.ones(len(df))})

classification = False
if y.dtype == object or y.dtype == bool or pd.api.types.is_categorical_dtype(y):
    classification = True
elif pd.api.types.is_numeric_dtype(y):
    if y.nunique(dropna=True) <= 20:
        classification = True

if classification and y.nunique(dropna=True) < 2:
    classification = False

if not classification and not pd.api.types.is_numeric_dtype(y):
    y = pd.Series(pd.factorize(y)[0], index=y.index)

n_samples = len(df)
if n_samples < 2:
    X_train = X_test = X
    y_train = y_test = y
else:
    test_size = 0.2 if n_samples > 5 else 0.5
    stratify = None
    if classification:
        counts = y.value_counts(dropna=True)
        if not counts.empty and counts.min() >= 2:
            stratify = y
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=stratify
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=None
        )

if len(X_train) == 0 or len(X_test) == 0:
    X_train = X_test = X
    y_train = y_test = y

assert len(X_train) > 0 and len(X_test) > 0

numeric_features = [c for c in X_train.columns if pd.api.types.is_numeric_dtype(X_train[c])]
categorical_features = [c for c in X_train.columns if c not in numeric_features]

if len(numeric_features) + len(categorical_features) == 0:
    X_train = pd.DataFrame({'const': np.ones(len(X_train))})
    X_test = pd.DataFrame({'const': np.ones(len(X_test))})
    numeric_features = ['const']
    categorical_features = []

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler(with_mean=False))
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

transformers = []
if numeric_features:
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    transformers.append(('cat', categorical_transformer, categorical_features))

preprocess = ColumnTransformer(transformers=transformers, remainder='drop')

if classification:
    if y_train.nunique(dropna=True) < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[
    ('preprocess', preprocess),
    ('model', model)
])

try:
    clf.fit(X_train, y_train)
except Exception:
    if classification:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = DummyRegressor(strategy='mean')
    clf = Pipeline(steps=[
        ('preprocess', preprocess),
        ('model', model)
    ])
    clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

if classification:
    accuracy = accuracy_score(y_test, y_pred) if len(y_test) > 0 else 0.0
else:
    if len(y_test) == 0:
        accuracy = 0.0
    else:
        y_true = pd.to_numeric(pd.Series(y_test), errors='coerce').to_numpy()
        y_hat = pd.to_numeric(pd.Series(y_pred), errors='coerce').to_numpy()
        mask = ~np.isnan(y_true) & ~np.isnan(y_hat)
        if mask.sum() == 0:
            accuracy = 0.0
        else:
            y_true = y_true[mask]
            y_hat = y_hat[mask]
            mae = mean_absolute_error(y_true, y_hat)
            range_ = np.max(y_true) - np.min(y_true)
            if range_ == 0 or np.isnan(range_):
                accuracy = 1.0 if mae == 0 else 0.0
            else:
                accuracy = 1.0 - mae / (range_ + 1e-9)
            accuracy = float(max(0.0, min(1.0, accuracy)))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Selected lightweight linear models (LogisticRegression/Ridge) to minimize CPU and energy usage.
# - Used a compact preprocessing pipeline with imputation and one-hot encoding for reproducibility.
# - Scaling is applied only to numeric features with sparse-friendly settings to avoid densification.
# - Regression fallback converts normalized MAE into a bounded accuracy proxy within [0,1].