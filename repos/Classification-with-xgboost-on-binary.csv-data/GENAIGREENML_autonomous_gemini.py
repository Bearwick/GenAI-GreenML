# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

df = pd.read_csv('xgboost_on _binary.csv')
df = df.dropna()

X = df[['gre', 'gpa', 'rank']]
y = df['admit']

X = pd.get_dummies(X, columns=['rank'], drop_first=True)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = LogisticRegression(solver='liblinear', max_iter=1000)
model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

"""
JUSTIFICATION FOR ENERGY-EFFICIENT DESIGN:
1. Model Choice: Logistic Regression was selected instead of Gradient Boosted Trees (XGBoost). For small-scale tabular datasets, linear models provide comparable performance with significantly lower CPU overhead and memory usage.
2. Preprocessing Efficiency: Standard scaling was applied to ensure the optimization solver converges in fewer iterations, directly reducing the total number of floating-point operations.
3. Feature Engineering: One-hot encoding was used only for the 'rank' variable, keeping the feature matrix sparse and memory-efficient.
4. Library Selection: The solution relies on scikit-learn's optimized C-extensions rather than deep learning frameworks (PyTorch/TensorFlow), which would require significantly more energy for initialization and training.
5. Resource Management: The script avoids all visualization libraries (Matplotlib/Seaborn) and intensive EDA processes in the production path to minimize the execution footprint.
"""
