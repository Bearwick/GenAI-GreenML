# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

df = pd.read_csv('data.csv')
df = pd.get_dummies(df, columns=['rank'], drop_first=True)

X = df.drop('admit', axis=1)
y = df['admit']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
num_cols = ['gre', 'gpa']
X_train.loc[:, num_cols] = scaler.fit_transform(X_train[num_cols])
X_test.loc[:, num_cols] = scaler.transform(X_test[num_cols])

model = LogisticRegression(solver='lbfgs', max_iter=200, random_state=42)
model.fit(X_train, y_train)

accuracy = accuracy_score(y_test, model.predict(X_test))
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Algorithm Selection: Logistic Regression was chosen as the most energy-efficient model for binary classification on small tabular data, requiring significantly fewer FLOPS than neural networks.
# 2. Solver Efficiency: The 'lbfgs' solver is used for its low memory footprint and fast convergence properties on small-scale feature sets.
# 3. Convergence Optimization: Standardizing 'gre' and 'gpa' features ensures the optimization gradient is well-conditioned, reducing the number of iterations and CPU cycles needed for training.
# 4. Minimal Dependency: The solution relies on lightweight libraries (pandas, scikit-learn) rather than heavy deep-learning frameworks, reducing the energy cost of loading and executing the runtime environment.
# 5. Computational Simplicity: Categorical encoding for 'rank' is handled via binary dummy variables, which maintains a small model size and avoids the overhead of high-dimensional embeddings.
# 6. Hardware Compatibility: The design is optimized for efficient execution on standard CPU hardware, eliminating the high idle power consumption and cooling requirements associated with dedicated GPUs.