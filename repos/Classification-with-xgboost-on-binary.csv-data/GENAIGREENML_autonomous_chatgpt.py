# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _load_dataset() -> pd.DataFrame:
    candidates = [
        "data.csv",
        "dataset.csv",
        "admission.csv",
        "admissions.csv",
        "train.csv",
        "input.csv",
    ]
    env_path = os.environ.get("DATASET_PATH", "").strip()
    if env_path and os.path.exists(env_path):
        return pd.read_csv(env_path)

    for p in candidates:
        if os.path.exists(p):
            return pd.read_csv(p)

    csv_files = [f for f in os.listdir(".") if f.lower().endswith(".csv")]
    if len(csv_files) == 1:
        return pd.read_csv(csv_files[0])

    for f in csv_files:
        try:
            df = pd.read_csv(f)
            cols = {c.strip().lower() for c in df.columns}
            if {"admit", "gre", "gpa", "rank"}.issubset(cols):
                return df
        except Exception:
            continue

    raise FileNotFoundError(
        "Could not find dataset CSV. Set DATASET_PATH or place a CSV in the working directory."
    )


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip().lower() for c in df.columns]
    return df


def _coerce_types(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    for col in ["admit", "gre", "gpa", "rank"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
    return df


def main() -> None:
    df = _load_dataset()
    df = _normalize_columns(df)
    df = _coerce_types(df)

    required = ["admit", "gre", "gpa", "rank"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns: {missing}")

    X = df[["gre", "gpa", "rank"]]
    y = df["admit"].astype("Int64")

    numeric_features = ["gre", "gpa"]
    categorical_features = ["rank"]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    model = LogisticRegression(
        solver="liblinear",
        max_iter=200,
        C=1.0,
        class_weight=None,
        random_state=42,
    )

    clf = Pipeline(
        steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ]
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y if y.notna().all() and y.nunique(dropna=True) > 1 else None,
    )

    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Chose logistic regression (linear model) for strong baseline accuracy with minimal compute/memory on CPU.
# - Used ColumnTransformer + simple imputers to keep preprocessing reproducible and efficient (vectorized ops).
# - One-hot encoded only the small categorical feature (rank); kept sparse output to reduce RAM.
# - Standardized only numeric features for stable optimization; avoided heavy feature engineering and any deep models.
# - Used liblinear solver for small tabular datasets; low iteration cap to limit CPU cycles while converging reliably.
# - Avoided plots, interactive prompts, and model serialization to reduce unnecessary I/O and runtime overhead.