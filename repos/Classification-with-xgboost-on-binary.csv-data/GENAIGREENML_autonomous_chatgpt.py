# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "xgboost_on _binary.csv"
DATASET_HEADERS = ["admit", "gre", "gpa", "rank"]


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _drop_unnamed_columns(df):
    drop_cols = []
    for c in df.columns:
        if re.match(r"^Unnamed:?\s*\d+$", str(c).strip()):
            drop_cols.append(c)
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _robust_read_csv(path):
    # Attempt default CSV parsing, then fallback to ; separator and , decimal if parsing seems wrong.
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(dfx):
        if dfx is None or dfx.empty:
            return True
        # If only 1 column but expected multiple, likely wrong delimiter.
        if dfx.shape[1] == 1:
            return True
        # If columns contain the entire header line as one column, likely wrong delimiter.
        col0 = str(dfx.columns[0])
        if "," in col0 and dfx.shape[1] == 1:
            return True
        return False

    if _looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df = None

    if df is None:
        # Last resort: try python engine with automatic separator detection
        try:
            df = pd.read_csv(path, sep=None, engine="python")
        except Exception as e:
            raise RuntimeError(f"Failed to read dataset: {e}") from e

    return df


def _choose_target_and_features(df, declared_headers):
    cols = list(df.columns)

    # Prefer declared target if present
    target = None
    for cand in declared_headers:
        if cand in cols and cand.lower().strip() in ("target", "label", "y", "admit", "class"):
            target = cand
            break
    if target is None and "admit" in cols:
        target = "admit"

    # If still None, pick a non-constant numeric column as target, else any column
    if target is None:
        numeric_candidates = []
        for c in cols:
            s = pd.to_numeric(df[c], errors="coerce")
            nunique = int(s.nunique(dropna=True))
            if nunique >= 2:
                numeric_candidates.append((nunique, c))
        if numeric_candidates:
            numeric_candidates.sort(reverse=True)
            target = numeric_candidates[0][1]
        else:
            # fallback to first column
            target = cols[0]

    # Features are all other columns
    features = [c for c in cols if c != target]
    if not features:
        # If only one column, create an empty feature set; we'll handle with a trivial baseline later.
        features = []

    return target, features


def _is_classification_target(y):
    # Determine if target is classification with >=2 classes and "small" unique set.
    y_series = pd.Series(y)
    y_nonnull = y_series.dropna()
    if y_nonnull.empty:
        return False
    # Try numeric coercion; if most values become numeric and few unique integers -> classification likely
    y_num = pd.to_numeric(y_nonnull, errors="coerce")
    frac_num = float(y_num.notna().mean())
    if frac_num > 0.9:
        uniq = y_num.dropna().unique()
        if len(uniq) < 2:
            return False
        # If values are 0/1 or small integer set -> treat as classification
        uniq_rounded = np.unique(np.round(uniq, 6))
        if len(uniq_rounded) <= 20:
            # Further prefer classification if all are near integers
            near_int = np.mean(np.isclose(uniq_rounded, np.round(uniq_rounded)))
            if near_int > 0.95:
                return True
        # Otherwise treat as regression
        return False
    # Non-numeric: classification if at least 2 unique categories and not too many
    nunique = int(y_nonnull.nunique(dropna=True))
    return 2 <= nunique <= 50


def _safe_accuracy_proxy_r2(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    if y_true.size == 0:
        return 0.0
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    y_mean = float(np.mean(y_true))
    ss_tot = float(np.sum((y_true - y_mean) ** 2))
    if ss_tot <= 1e-12:
        # No variance; perfect if predictions match, else 0
        return 1.0 if ss_res <= 1e-12 else 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    # Bound to [0,1] as "accuracy proxy"
    return float(np.clip(r2, 0.0, 1.0))


def main():
    df = _robust_read_csv(DATASET_PATH)

    # Normalize columns: strip, collapse whitespace
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed_columns(df)

    # If the CSV came without headers and first row equals expected headers, re-read with header=None + names
    # (kept lightweight; only attempt if current columns are integer-like or generic)
    if all(re.match(r"^\d+$", str(c)) for c in df.columns) and len(df.columns) == len(DATASET_HEADERS):
        try:
            df2 = pd.read_csv(DATASET_PATH, header=None, names=DATASET_HEADERS)
            df2.columns = _normalize_columns(df2.columns)
            df2 = _drop_unnamed_columns(df2)
            if not df2.empty:
                df = df2
        except Exception:
            pass

    # Ensure non-empty
    df = df.copy()
    assert df.shape[0] > 0 and df.shape[1] > 0, "Dataset empty after loading."

    # Choose target/features defensively
    target_col, feature_cols = _choose_target_and_features(df, DATASET_HEADERS)

    # Basic row filtering: drop rows where target is missing
    y_raw = df[target_col]
    keep = ~pd.isna(y_raw)
    df = df.loc[keep].reset_index(drop=True)
    assert df.shape[0] > 1, "Not enough rows after dropping missing target."

    y_raw = df[target_col]
    X = df[feature_cols] if feature_cols else pd.DataFrame(index=df.index)

    # Identify column types
    numeric_features = []
    categorical_features = []
    for c in X.columns:
        s = X[c]
        # Attempt numeric coercion for detection
        s_num = pd.to_numeric(s, errors="coerce")
        if float(s_num.notna().mean()) > 0.8:
            numeric_features.append(c)
        else:
            categorical_features.append(c)

    # Build preprocessing
    numeric_transformer = Pipeline(
        steps=[
            ("to_num", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Decide task type
    is_classif = _is_classification_target(y_raw)

    # Prepare y
    if is_classif:
        # Coerce to numeric if possible (e.g., 0/1), else keep as string labels
        y_num = pd.to_numeric(y_raw, errors="coerce")
        if float(y_num.notna().mean()) > 0.9:
            y = y_num.astype(int)
        else:
            y = y_raw.astype(str)

        # Ensure at least 2 classes
        if pd.Series(y).nunique(dropna=True) < 2:
            is_classif = False
    if not is_classif:
        y = pd.to_numeric(y_raw, errors="coerce")
        # Drop rows with non-numeric y for regression
        mask = np.isfinite(y.to_numpy(dtype=float))
        X = X.loc[mask].reset_index(drop=True)
        y = y.loc[mask].reset_index(drop=True)
        assert len(y) > 1, "Not enough valid numeric targets for regression."

    # Train/test split
    if is_classif:
        # Stratify if possible
        y_series = pd.Series(y)
        strat = y_series if y_series.nunique() > 1 and y_series.value_counts().min() >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=strat
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Empty split."

    # Model selection (lightweight CPU-friendly)
    if is_classif:
        clf = LogisticRegression(
            solver="liblinear",  # efficient on small datasets / CPU
            max_iter=200,
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _safe_accuracy_proxy_r2(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used a simple sklearn Pipeline + ColumnTransformer for single-pass preprocessing and reproducibility.
# - Chose LogisticRegression (liblinear) for classification and Ridge for regression: fast, CPU-friendly linear baselines.
# - Minimal feature engineering: median imputation + standardization for numeric, most_frequent + one-hot for categoricals.
# - Implemented robust CSV loading with delimiter/decimal fallbacks and defensive schema inference to avoid hard failures.
# - For regression fallback, reports a bounded [0,1] "accuracy" proxy by clipping R^2, keeping output stable and comparable.