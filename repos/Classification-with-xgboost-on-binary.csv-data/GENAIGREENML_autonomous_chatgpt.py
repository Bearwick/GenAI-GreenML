# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        normed.append(c2)
    return normed


def _read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(d):
        if d is None or d.empty:
            return True
        if d.shape[1] <= 1:
            return True
        if all(str(c).startswith("Unnamed:") for c in d.columns):
            return True
        return False

    if looks_wrong(df):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if not looks_wrong(df2):
                df = df2
        except Exception:
            pass

    if df is None:
        raise FileNotFoundError(f"Could not read dataset: {path}")
    return df


def _pick_target(df, preferred_headers=None):
    cols = list(df.columns)
    if preferred_headers:
        for cand in preferred_headers:
            if cand in cols:
                return cand

    # Prefer binary-like / low-cardinality columns for classification if possible
    best_col = None
    best_score = -1.0
    for c in cols:
        s = df[c]
        non_na = s.dropna()
        if non_na.empty:
            continue
        nunique = non_na.nunique(dropna=True)
        if nunique <= 1:
            continue

        # Score: prefer few unique values and not-too-high missingness
        missing_rate = 1.0 - (len(non_na) / max(len(s), 1))
        score = 0.0
        if nunique == 2:
            score += 10.0
        elif nunique <= 10:
            score += 5.0
        else:
            score += 1.0
        score += max(0.0, 2.0 - missing_rate * 2.0)

        if score > best_score:
            best_score = score
            best_col = c

    if best_col is not None:
        return best_col

    # Fallback: first column
    return cols[0] if cols else None


def _coerce_numeric_safely(df, cols):
    for c in cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _bounded_regression_accuracy(y_true, y_pred):
    # Bounded proxy in [0,1]: 1 - min(1, MSE / Var(y))
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    mse = np.mean((yt - yp) ** 2)
    var = np.var(yt)
    if not np.isfinite(var) or var <= 1e-12:
        return float(mse <= 1e-12)
    score = 1.0 - min(1.0, mse / (var + 1e-12))
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    dataset_path = "xgboost_on _binary.csv"
    if not os.path.exists(dataset_path):
        # Fallback to any csv in cwd
        for f in os.listdir("."):
            if f.lower().endswith(".csv"):
                dataset_path = f
                break

    df = _read_csv_robust(dataset_path)

    df.columns = _normalize_columns(df.columns)
    df = df.loc[:, ~df.columns.astype(str).str.match(r"^Unnamed:\s*\d+$")]

    # Defensive: drop all-empty rows/cols
    df = df.dropna(axis=0, how="all").dropna(axis=1, how="all")
    assert df.shape[0] > 0 and df.shape[1] > 0

    # Choose target
    preferred_headers = ["admit"]  # from DATASET_HEADERS; used only if present
    target_col = _pick_target(df, preferred_headers=preferred_headers)
    assert target_col is not None

    # Separate X/y
    y_raw = df[target_col]
    X = df.drop(columns=[target_col])

    # If no features remain, create a constant feature to keep pipeline valid
    if X.shape[1] == 0:
        X = pd.DataFrame({"__constant__": np.ones(len(df), dtype=float)})

    # Try to coerce numeric-looking columns (helps when numbers are stored as strings)
    obj_cols = [c for c in X.columns if X[c].dtype == "object"]
    for c in obj_cols:
        # If coercion produces many numbers, keep numeric
        coerced = pd.to_numeric(X[c], errors="coerce")
        if coerced.notna().mean() >= 0.7:
            X[c] = coerced

    # Determine column types after coercion
    numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    cat_cols = [c for c in X.columns if c not in numeric_cols]

    # Coerce numeric columns again (defensive for mixed types), then replace inf with NaN
    X = _coerce_numeric_safely(X, numeric_cols)
    X = X.replace([np.inf, -np.inf], np.nan)

    # Prepare y for classification or regression
    y = y_raw.copy()
    if pd.api.types.is_numeric_dtype(y):
        y = pd.to_numeric(y, errors="coerce")
    y = y.replace([np.inf, -np.inf], np.nan)

    # Drop rows with missing y (avoid leaking target into imputer)
    keep = y.notna()
    X = X.loc[keep].reset_index(drop=True)
    y = y.loc[keep].reset_index(drop=True)
    assert len(y) > 0

    # Decide task
    task = "classification"
    y_for_class = None
    unique_vals = pd.Series(y).dropna().unique()

    if pd.api.types.is_numeric_dtype(y):
        # If numeric with exactly two unique finite values -> classification
        finite_unique = np.array([v for v in unique_vals if np.isfinite(v)], dtype=float) if len(unique_vals) else np.array([])
        if len(finite_unique) == 2:
            task = "classification"
            y_for_class = y.astype(float)
        elif len(finite_unique) < 2:
            task = "regression"  # trivial or constant target; handle below
        else:
            # If looks like 0/1-ish values with small cardinality
            if len(finite_unique) <= 10:
                task = "classification"
                y_for_class = y.astype(float)
            else:
                task = "regression"
    else:
        # Object/categorical target
        non_na = pd.Series(y).dropna()
        if non_na.nunique(dropna=True) >= 2:
            task = "classification"
            y_for_class = non_na
        else:
            task = "regression"

    # Build preprocessing
    numeric_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])

    categorical_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_pipe, numeric_cols),
            ("cat", categorical_pipe, cat_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y if task == "regression" else y_for_class,
        test_size=0.2,
        random_state=42,
        shuffle=True,
        stratify=(y_for_class if (task == "classification" and pd.Series(y_for_class).nunique() >= 2) else None),
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    if task == "classification":
        # Ensure at least 2 classes in train; else fallback to regression proxy
        y_train_ser = pd.Series(y_train).dropna()
        if y_train_ser.nunique(dropna=True) < 2:
            task = "regression"

    if task == "classification":
        # Lightweight, strong baseline for tabular: logistic regression with saga handles sparse OHE efficiently
        clf = LogisticRegression(
            solver="saga",
            max_iter=200,
            n_jobs=1,
            random_state=42,
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Lightweight regression baseline; accuracy proxy bounded in [0,1]
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        # If y is constant/degenerate, Ridge still fits; proxy score handles it
        model.fit(X_train, pd.to_numeric(pd.Series(y_train), errors="coerce"))
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_accuracy(pd.to_numeric(pd.Series(y_test), errors="coerce"), y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Chosen models: LogisticRegression (sparse-friendly SAGA) for classification and Ridge for regression fallback: both CPU-efficient and low-memory.
# - Preprocessing uses ColumnTransformer + Pipelines to avoid repeated work and keep reproducibility with a fixed random_state.
# - Robust schema handling: auto target selection (prefers 'admit' if present), numeric coercion for object columns, safe NaN/inf handling, and fallback constant feature if needed.
# - Robust CSV parsing: retries with sep=';' and decimal=',' if default parsing yields likely-bad shape/headers.
# - Regression fallback prints a bounded [0,1] "accuracy" proxy: 1 - min(1, MSE/Var(y)), stable for different scales and safe for near-constant targets.