# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import xgboost as xgb


def load_data(path: str) -> pd.DataFrame:
    return pd.read_csv(path, usecols=["admit", "gre", "gpa", "rank"])


def preprocess(df: pd.DataFrame):
    df = df.copy()

    for col in ("admit", "rank"):
        if not pd.api.types.is_integer_dtype(df[col]):
            df[col] = pd.to_numeric(df[col], errors="ignore")
        if not pd.api.types.is_integer_dtype(df[col]):
            df[col] = df[col].astype("category")

    df_encoded = pd.get_dummies(df, columns=["admit", "rank"], drop_first=True, dtype="uint8")

    y_col = "admit_1"
    X = df_encoded.drop(columns=[y_col])
    y = df_encoded[y_col].astype("uint8")

    return X, y


def train_model(X_train, y_train, random_state: int = 42):
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=3,
        learning_rate=0.1,
        subsample=1.0,
        colsample_bytree=1.0,
        reg_lambda=1.0,
        tree_method="hist",
        n_jobs=1,
        random_state=random_state,
        eval_metric="logloss",
        use_label_encoder=False,
    )
    model.fit(X_train, y_train)
    return model


def main():
    df = load_data("xgboost_on _binary.csv")
    X, y = preprocess(df)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    model = train_model(X_train, y_train, random_state=42)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# Removed EDA calls, plotting, and unused imports to cut CPU/GPU work and avoid unnecessary data scans.
# Dropped LabelEncoder usage (redundant for numeric/binary target) and performed a single get_dummies pass.
# Limited CSV read to required columns (usecols) to reduce I/O, memory footprint, and parsing overhead.
# Used dtype="uint8" for one-hot features/labels to reduce memory and improve cache efficiency.
# Avoided repeated metric computations and probability predictions that were not required for final output.
# Enabled XGBoost 'hist' tree_method and set n_jobs=1 to reduce energy use and ensure stable, reproducible runs.
# Added stratified split and fixed random_state for reproducibility without extra computation.