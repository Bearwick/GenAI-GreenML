# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import xgboost as xgb


SEED = 42
DATASET_HEADERS = ["admit", "gre", "gpa", "rank"]
CSV_PATH = "xgboost_on _binary.csv"


def _read_csv_with_fallback(path: str) -> pd.DataFrame:
    df_try = pd.read_csv(path)
    if _looks_misparsed(df_try):
        df_try = pd.read_csv(path, sep=";", decimal=",")
    return df_try


def _looks_misparsed(df: pd.DataFrame) -> bool:
    if df.shape[1] == 1:
        return True
    cols = [str(c).strip().lower() for c in df.columns]
    expected = set(DATASET_HEADERS)
    present = set(cols)
    missing = len(expected - present)
    return missing >= 2


def _normalize_schema(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip().lower() for c in df.columns]

    if "admit" not in df.columns:
        for cand in ("admit ", " admit", "admission", "y"):
            if cand in df.columns:
                df = df.rename(columns={cand: "admit"})
                break
    if "rank" not in df.columns:
        for cand in ("Rank", "ranking", "univ_rank"):
            if str(cand).strip().lower() in df.columns:
                df = df.rename(columns={str(cand).strip().lower(): "rank"})
                break

    usable = [c for c in DATASET_HEADERS if c in df.columns]
    df = df[usable].copy()

    for col in df.columns:
        if df[col].dtype == "object":
            df[col] = df[col].astype(str).str.strip()

    for col in ("admit", "rank"):
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    for col in ("gre", "gpa"):
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    df = df.dropna()
    for col in df.columns:
        if pd.api.types.is_float_dtype(df[col]) and np.all(np.isfinite(df[col].to_numpy())):
            if np.all(df[col].to_numpy() == np.floor(df[col].to_numpy())):
                df[col] = df[col].astype(np.int64, copy=False)

    return df


def _prepare_features(df: pd.DataFrame):
    df = df.copy()

    if "admit" in df.columns:
        df["admit"] = (df["admit"].astype(np.int64, copy=False) != 0).astype(np.int64, copy=False)

    if "rank" in df.columns:
        rmin = int(df["rank"].min())
        if rmin == 1:
            df["rank"] = df["rank"].astype(np.int64, copy=False) - 1
        else:
            df["rank"] = df["rank"].astype(np.int64, copy=False)

    df_encoded = pd.get_dummies(df, columns=[c for c in ["admit", "rank"] if c in df.columns], drop_first=True)

    if "admit_1" not in df_encoded.columns:
        if "admit" in df.columns:
            y = df["admit"].to_numpy(dtype=np.int64, copy=False)
            X = df.drop(columns=["admit"])
            X = pd.get_dummies(X, columns=[c for c in ["rank"] if c in X.columns], drop_first=True)
            return X, y
        raise ValueError("Target column could not be derived (missing 'admit'/'admit_1').")

    y = df_encoded["admit_1"].to_numpy(dtype=np.int64, copy=False)
    X = df_encoded.drop(columns=["admit_1"])
    return X, y


def main():
    np.random.seed(SEED)

    df = _read_csv_with_fallback(CSV_PATH)
    df = _normalize_schema(df)

    X, y = _prepare_features(df)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=SEED, stratify=y if len(np.unique(y)) > 1 else None
    )

    model = xgb.XGBClassifier(
        random_state=SEED,
        n_estimators=100,
        max_depth=3,
        learning_rate=0.1,
        subsample=1.0,
        colsample_bytree=1.0,
        tree_method="hist",
        n_jobs=1,
        eval_metric="logloss",
    )

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed EDA/plotting and all non-essential computations (head/info/describe/pairplots/ROC curve) to cut runtime and energy use.
# - Implemented robust CSV parsing fallback (default read_csv, then retry with sep=';' and decimal=',') to avoid repeated manual fixes.
# - Normalized schema programmatically from df.columns and DATASET_HEADERS and dropped unused columns early to reduce memory footprint.
# - Eliminated LabelEncoder + double encoding; used direct numeric normalization and a single get_dummies pass to minimize redundant preprocessing.
# - Dropped rows with missing values once after coercion instead of repeatedly querying nulls, reducing unnecessary scans.
# - Converted arrays to NumPy early for y and used copy=False where safe to reduce data movement and allocations.
# - Set fixed seeds (NumPy + model random_state) and deterministic single-thread execution (n_jobs=1) for reproducibility and stable results.
# - Used XGBoost 'hist' tree_method for faster, lower-energy training on tabular data while preserving the same model family and task.