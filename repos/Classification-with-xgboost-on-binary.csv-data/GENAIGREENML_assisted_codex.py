# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
import xgboost as xgb


def main():
    df = pd.read_csv("xgboost_on _binary.csv")
    df = pd.get_dummies(df, columns=["rank"], drop_first=True)
    X = df.drop(columns="admit")
    y = df["admit"]
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    model = xgb.XGBClassifier()
    if "use_label_encoder" in model.get_params():
        model.set_params(use_label_encoder=False, eval_metric="logloss")
    model.fit(X_train, y_train)
    accuracy = model.score(X_test, y_test)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# Simplified preprocessing by one-hot encoding only the rank feature and using admit directly as the target.
# Removed exploratory analysis, plotting, and unused metric computations to avoid redundant work and I/O.
# Disabled internal label encoding when supported and used model.score to avoid extra prediction storage.