# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
import random
import warnings
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import xgboost as xgb

DATASET_PATH = "xgboost_on _binary.csv"
DATASET_HEADERS = "admit,gre,gpa,rank"
SEED = 42

def read_csv_robust(path, expected_headers):
    df = pd.read_csv(path)
    if df.shape[1] != len(expected_headers):
        df = pd.read_csv(path, sep=";", decimal=",")
    df.columns = [str(c).strip() for c in df.columns]
    if df.shape[1] == len(expected_headers) and set(df.columns) != set(expected_headers):
        df.columns = expected_headers
    return df

def match_column(expected, columns):
    if expected is None:
        return None
    expected_lower = str(expected).lower()
    for col in columns:
        if str(col).lower() == expected_lower:
            return col
    return None

def build_model(seed):
    try:
        return xgb.XGBClassifier(random_state=seed, use_label_encoder=False, eval_metric="logloss")
    except TypeError:
        return xgb.XGBClassifier(random_state=seed)

def main():
    warnings.filterwarnings("ignore")
    random.seed(SEED)
    np.random.seed(SEED)

    expected_headers = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]
    df = read_csv_robust(DATASET_PATH, expected_headers)

    target_expected = expected_headers[0] if expected_headers else None
    rank_expected = expected_headers[-1] if expected_headers else None

    target_base = match_column(target_expected, df.columns) or (df.columns[0] if len(df.columns) > 0 else None)
    rank_col = match_column(rank_expected, df.columns)

    categorical_cols = []
    if target_base and target_base in df.columns:
        categorical_cols.append(target_base)
    if rank_col and rank_col in df.columns and rank_col != target_base:
        categorical_cols.append(rank_col)

    if categorical_cols:
        df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
    else:
        df_encoded = df

    admit_cols = [c for c in df_encoded.columns if target_base and str(c).startswith(f"{target_base}_")]
    if admit_cols:
        target_col = admit_cols[0]
    elif target_base in df_encoded.columns:
        target_col = target_base
    else:
        target_col = df_encoded.columns[0]

    X = df_encoded.drop(columns=[target_col])
    y = df_encoded[target_col]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=SEED
    )

    model = build_model(SEED)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed exploratory analysis, plotting, and unused metrics to eliminate redundant computation and I/O.
# - Simplified preprocessing by directly one-hot encoding categorical columns without redundant label encoding.
# - Implemented robust CSV loading with delimiter/decimal fallback and standardized headers.
# - Set fixed random seeds and model random_state to ensure reproducible results.
# - Minimized intermediate structures by computing only the required target and accuracy metric.