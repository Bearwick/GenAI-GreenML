# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import xgboost as xgb

np.random.seed(42)

try:
    df = pd.read_csv('xgboost_on _binary.csv')
    if df.shape[1] <= 1:
        df = pd.read_csv('xgboost_on _binary.csv', sep=';', decimal=',')
except Exception:
    df = pd.read_csv('xgboost_on _binary.csv', sep=';', decimal=',')

le_admit = LabelEncoder()
le_rank = LabelEncoder()
df['admit'] = le_admit.fit_transform(df['admit'])
df['rank'] = le_rank.fit_transform(df['rank'])

df_encoded = pd.get_dummies(df, columns=['admit', 'rank'], drop_first=True)

target_col = 'admit_1'
X = df_encoded.drop(target_col, axis=1)
y = df_encoded[target_col]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss', n_jobs=1)
model.fit(X_train, y_train, verbose=False)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Removed all visualization imports (seaborn, matplotlib) and plotting calls to save energy.
# Removed redundant EDA calls (head, info, describe, shape, crosstab, value_counts) that don't affect output.
# Removed unused metric computations (precision, recall, f1, roc_auc, confusion_matrix, roc_curve) not needed for final output.
# Removed all print statements except the required accuracy print.
# Removed pd.option_context call that served no computational purpose.
# Consolidated imports at the top to reduce overhead.
# Set random seed for reproducibility.
# Added robust CSV fallback parsing for reliability.
# Set n_jobs=1 for deterministic XGBoost behavior and reduced thread overhead on small data.
# Suppressed XGBoost warnings with explicit eval_metric and use_label_encoder settings.