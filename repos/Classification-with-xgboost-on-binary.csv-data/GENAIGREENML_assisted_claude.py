# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import xgboost as xgb

df = pd.read_csv('xgboost_on _binary.csv')

le_admit = LabelEncoder()
le_rank = LabelEncoder()
df['admit'] = le_admit.fit_transform(df['admit'])
df['rank'] = le_rank.fit_transform(df['rank'])

df_encoded = pd.get_dummies(df, columns=['admit', 'rank'], drop_first=True)

X = df_encoded.drop(['admit_1'], axis=1)
y = df_encoded['admit_1']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = xgb.XGBClassifier(n_estimators=100, tree_method='hist', random_state=42, n_jobs=1)
model.fit(X_train, y_train, eval_metric='logloss', verbose=False)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Removed all visualization imports (seaborn, matplotlib) and plotting calls to save energy
# Removed redundant EDA calls (head, info, describe, shape, crosstab, value_counts) that don't affect results
# Removed unused metric computations (precision, recall, f1, roc_auc, confusion_matrix, roc_curve)
# Removed unnecessary prints and model saving
# Combined imports at the top to reduce overhead
# Set tree_method='hist' for faster, more memory-efficient XGBoost training
# Set n_jobs=1 to avoid overhead from parallelism on small datasets
# Set verbose=False to suppress training output
# Preserved original data pipeline, encoding, split, and model for identical accuracy