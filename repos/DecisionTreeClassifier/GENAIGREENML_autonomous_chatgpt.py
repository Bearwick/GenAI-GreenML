# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    out = []
    for c in cols:
        c = str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        out.append(c)
    return out


def _robust_read_csv(path):
    # Try default parsing first
    try:
        df0 = pd.read_csv(path)
    except Exception:
        df0 = None

    # Heuristic check for "wrong" parsing (e.g., single column due to delimiter mismatch)
    def looks_wrong(df):
        if df is None or df.empty:
            return True
        if df.shape[1] == 1:
            col0 = str(df.columns[0])
            # If header contains separators, likely delimiter issue
            if ("," in col0) or (";" in col0) or ("\t" in col0):
                return True
            # If most rows are strings with separators, likely delimiter issue
            s = df.iloc[:, 0].astype(str).head(20)
            if (s.str.contains(";").mean() > 0.5) or (s.str.contains(",").mean() > 0.5):
                return True
        return False

    if looks_wrong(df0):
        try:
            df1 = pd.read_csv(path, sep=";", decimal=",")
            if df1 is not None and not df1.empty and df1.shape[1] >= 2:
                return df1
        except Exception:
            pass
    return df0


def _drop_unnamed(df):
    keep_cols = [c for c in df.columns if not re.match(r"^Unnamed:\s*\d+$", str(c))]
    return df[keep_cols]


def _choose_target(df, provided_headers):
    cols = list(df.columns)
    lower_map = {str(c).strip().lower(): c for c in cols}

    # Prefer last provided header if present (e.g., "genre")
    if provided_headers:
        cand = str(provided_headers[-1]).strip().lower()
        if cand in lower_map:
            return lower_map[cand]

    # If any non-numeric-like object column exists, prefer it as classification target
    obj_cols = [c for c in cols if df[c].dtype == "object"]
    for c in obj_cols:
        nunique = df[c].nunique(dropna=True)
        if nunique >= 2 and nunique <= max(2, min(50, int(0.5 * len(df)))):  # avoid high-cardinality IDs
            return c

    # Else pick a numeric column with variability
    numeric_candidates = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() >= max(5, int(0.2 * len(df))):
            if s.nunique(dropna=True) >= 2:
                numeric_candidates.append((s.nunique(dropna=True), c))
    if numeric_candidates:
        numeric_candidates.sort(reverse=True)
        return numeric_candidates[0][1]

    # Fallback: last column
    return cols[-1] if cols else None


def _infer_task(y):
    # Decide classification vs regression robustly
    if y is None:
        return "unknown"
    if y.dtype == "object":
        return "classification"
    # For numeric targets: classify if integer-like with few unique values
    y_num = pd.to_numeric(y, errors="coerce")
    uniq = y_num.nunique(dropna=True)
    if uniq < 2:
        return "degenerate"
    if np.all(np.isfinite(y_num.dropna().values)):
        # integer-like check
        y_drop = y_num.dropna()
        if len(y_drop) > 0:
            frac = np.mean(np.abs(y_drop - np.round(y_drop)) < 1e-9)
            if frac > 0.98 and uniq <= 20:
                return "classification"
    return "regression"


def _bounded_regression_score(y_true, y_pred):
    # Bounded "accuracy proxy" in [0,1] using normalized MAE:
    # score = 1 - MAE / (range + eps), clipped to [0,1]
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    mae = np.mean(np.abs(yt - yp))
    rng = np.nanmax(yt) - np.nanmin(yt)
    denom = rng if np.isfinite(rng) and rng > 0 else (np.nanstd(yt) + 1e-9)
    score = 1.0 - (mae / (denom + 1e-9))
    return float(np.clip(score, 0.0, 1.0))


# ---- Load data ----
DATASET_PATH = "music.csv"
DATASET_HEADERS = ["age", "gender", "genre"]

df = _robust_read_csv(DATASET_PATH)
if df is None:
    df = pd.DataFrame()

# Normalize columns and drop unnamed
df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)

# If headers are wrong (e.g., single column with commas), attempt manual split fallback
if df.shape[1] == 1:
    col = df.columns[0]
    # Try splitting column by common delimiters into multiple columns
    sample = df[col].astype(str).head(20)
    delim = None
    if sample.str.contains(";").mean() > 0.5:
        delim = ";"
    elif sample.str.contains(",").mean() > 0.5:
        delim = ","
    if delim is not None:
        expanded = df[col].astype(str).str.split(delim, expand=True)
        # Try to set headers if matches expected count
        if expanded.shape[1] >= 2:
            if expanded.shape[1] == len(DATASET_HEADERS):
                expanded.columns = DATASET_HEADERS
            else:
                expanded.columns = [f"col_{i}" for i in range(expanded.shape[1])]
            df = expanded.copy()
            df.columns = _normalize_columns(df.columns)

# Basic sanity
df = df.copy()
assert df.shape[0] > 0, "Empty dataset after loading."

# Choose target robustly
target_col = _choose_target(df, DATASET_HEADERS)
if target_col is None or target_col not in df.columns:
    # Fallback: create dummy target to keep pipeline running
    df["__target__"] = 0
    target_col = "__target__"

# Separate X/y
y_raw = df[target_col]
X = df.drop(columns=[target_col])

# Coerce numeric columns conservatively (only attempt for non-object columns)
for c in X.columns:
    if X[c].dtype != "object":
        X[c] = pd.to_numeric(X[c], errors="coerce")

# If many numeric-looking strings exist, attempt to coerce those too (limited to avoid heavy processing)
obj_cols = [c for c in X.columns if X[c].dtype == "object"]
for c in obj_cols:
    s = X[c].astype(str)
    # If mostly digits-like, coerce to numeric
    numeric_like = s.str.fullmatch(r"\s*[-+]?\d+(\.\d+)?\s*", na=False).mean()
    if numeric_like > 0.9:
        X[c] = pd.to_numeric(X[c], errors="coerce")

# Determine task
task = _infer_task(y_raw)

# Prepare y
if task == "classification":
    y = y_raw.astype(str).fillna("NA")
else:
    y = pd.to_numeric(y_raw, errors="coerce")

# Drop rows with missing y for regression/classification (classification can keep NA label)
if task == "regression":
    valid = y.notna() & np.isfinite(y.values)
    X = X.loc[valid].reset_index(drop=True)
    y = y.loc[valid].reset_index(drop=True)

assert X.shape[0] > 0, "No samples after target cleaning."

# Re-infer after cleaning
task = _infer_task(y if task != "classification" else y)

# Feature types
numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
categorical_features = [c for c in X.columns if c not in numeric_features]

numeric_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ]
)

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Model selection (lightweight, CPU-friendly)
if task == "classification":
    # Use multinomial-capable, efficient solver; small max_iter for energy efficiency
    model = LogisticRegression(max_iter=200, solver="lbfgs", n_jobs=1)
else:
    # Ridge is stable and cheap; works with sparse one-hot via solver='auto'
    model = Ridge(alpha=1.0, random_state=42)

clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])

# Split data safely
if task == "classification":
    # Stratify only when feasible
    y_series = pd.Series(y)
    class_counts = y_series.value_counts(dropna=False)
    stratify = y_series if (class_counts.size >= 2 and class_counts.min() >= 2) else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_series, test_size=0.2, random_state=42, stratify=stratify
    )
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X, pd.Series(y), test_size=0.2, random_state=42
    )

assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Train/test split failed."

accuracy = 0.0
if task == "classification":
    # Handle degenerate case (single class) without failing
    if pd.Series(y_train).nunique(dropna=False) < 2:
        # Predict the only class
        only_class = pd.Series(y_train).iloc[0]
        y_pred = np.full(shape=(len(y_test),), fill_value=only_class, dtype=object)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Regression path (and any degenerate numeric target)
    if pd.Series(y_train).nunique(dropna=True) < 2:
        # Predict constant mean
        const = float(pd.Series(y_train).dropna().mean()) if pd.Series(y_train).dropna().shape[0] else 0.0
        y_pred = np.full(shape=(len(y_test),), fill_value=const, dtype=float)
        accuracy = _bounded_regression_score(y_test.values.astype(float, copy=False), y_pred)
    else:
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = _bounded_regression_score(y_test.values.astype(float, copy=False), np.asarray(y_pred, dtype=float))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Uses lightweight classical ML (LogisticRegression / Ridge) to keep CPU cost low while providing strong baselines.
# - Employs ColumnTransformer + Pipeline for reproducibility and to avoid repeated preprocessing work.
# - Robust CSV loading retries with ';' separator and ',' decimal, plus single-column split fallback to handle delimiter issues.
# - Defensive schema handling: normalizes column names, drops 'Unnamed' columns, auto-selects a target if headers mismatch.
# - Minimal feature engineering: median imputation + standard scaling for numeric, most-frequent + one-hot for categorical.
# - Bounded regression "accuracy" proxy: 1 - MAE/(range+eps), clipped to [0,1], ensuring stable reporting as ACCURACY.