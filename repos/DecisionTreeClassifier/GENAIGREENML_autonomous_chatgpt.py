# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression, Ridge


warnings.filterwarnings("ignore")


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _read_csv_robust(path):
    # Try default parsing
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d):
        if d is None or d.empty:
            return True
        if d.shape[1] <= 1:
            return True
        # if a single column contains many separators, it's likely mis-parsed
        if d.shape[1] == 1:
            sample = d.iloc[:10, 0].astype(str)
            if sample.str.contains(";").mean() > 0.5 or sample.str.contains(",").mean() > 0.5:
                return True
        return False

    if _looks_wrong(df):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df2 is not None and not df2.empty and df2.shape[1] > 1:
                df = df2
        except Exception:
            pass

    if df is None:
        raise RuntimeError("Failed to read dataset.")
    return df


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _pick_target_and_features(df, preferred_headers):
    cols = list(df.columns)
    cols_l = [c.lower() for c in cols]
    preferred_l = [h.lower() for h in preferred_headers]

    # Try to match provided headers; target is last header if present
    target = None
    if preferred_l:
        for h in reversed(preferred_l):
            if h in cols_l:
                target = cols[cols_l.index(h)]
                break

    # If no match, pick a non-constant column (prefer object/category for classification)
    if target is None:
        non_const = []
        for c in cols:
            nunique = df[c].nunique(dropna=True)
            if nunique is not None and nunique > 1:
                non_const.append(c)
        if non_const:
            obj_cols = [c for c in non_const if df[c].dtype == "object"]
            target = obj_cols[0] if obj_cols else non_const[-1]
        else:
            target = cols[-1] if cols else None

    features = [c for c in cols if c != target]

    # If features empty, try to use any other column
    if not features and target is not None:
        features = [c for c in cols if c != target]

    return target, features


def _coerce_numeric_safe(s):
    return pd.to_numeric(s, errors="coerce")


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    denom = np.sum((y_true - np.mean(y_true)) ** 2)
    if denom <= 0:
        return 0.0
    r2 = 1.0 - (np.sum((y_true - y_pred) ** 2) / denom)
    # map to [0,1] to satisfy ACCURACY-style output
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    dataset_path = "music.csv"
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(dataset_path)

    df = _read_csv_robust(dataset_path)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Defensive: remove fully empty rows/cols
    df = df.dropna(axis=0, how="all").dropna(axis=1, how="all")
    assert df.shape[0] > 0 and df.shape[1] > 0

    DATASET_HEADERS = ["age", "gender", "genre"]
    target_col, feature_cols = _pick_target_and_features(df, DATASET_HEADERS)

    # If target is still missing, fallback to last column
    if target_col is None or target_col not in df.columns:
        target_col = df.columns[-1]
    feature_cols = [c for c in df.columns if c != target_col]
    if not feature_cols:
        # Minimal viable feature: create a constant feature
        df["_const_feat"] = 1
        feature_cols = ["_const_feat"]

    y_raw = df[target_col].copy()
    X_raw = df[feature_cols].copy()

    # Determine task: classification if target is non-numeric or small discrete numeric
    y_num = _coerce_numeric_safe(y_raw)
    y_is_numeric = y_num.notna().mean() > 0.95  # mostly numeric
    if y_is_numeric:
        # consider classification if few unique values
        uniq = pd.Series(y_num).dropna().nunique()
        is_classification = uniq >= 2 and uniq <= max(20, int(0.05 * len(y_num.dropna())) + 2)
    else:
        is_classification = True

    # Build preprocessors based on feature dtypes; coerce numerics safely
    X = X_raw.copy()
    for c in X.columns:
        if X[c].dtype != "object":
            X[c] = _coerce_numeric_safe(X[c])

    # Identify numeric/categorical columns
    numeric_features = []
    categorical_features = []
    for c in X.columns:
        if X[c].dtype == "object":
            categorical_features.append(c)
        else:
            numeric_features.append(c)

    # If numeric columns ended up all-NaN, treat them as categorical via string
    for c in list(numeric_features):
        if pd.Series(X[c]).notna().sum() == 0:
            numeric_features.remove(c)
            X[c] = X[c].astype("object")
            categorical_features.append(c)

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Prepare y and handle degenerate targets
    if is_classification:
        y = y_raw.astype(str).fillna("NA")
        n_classes = pd.Series(y).nunique(dropna=False)
        if n_classes < 2:
            # fallback to regression-like scoring on numeric coercion if possible
            is_classification = False
        else:
            model = DecisionTreeClassifier(
                random_state=42,
                max_depth=4,
                min_samples_leaf=2,
            )
            clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y if n_classes > 1 else None
            )
            assert len(X_train) > 0 and len(X_test) > 0

            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

    # Regression fallback
    y = _coerce_numeric_safe(y_raw)
    # Drop rows where y is NaN
    mask = y.notna()
    Xr = X.loc[mask].copy()
    yr = y.loc[mask].copy()
    assert len(yr) > 1

    # If y constant, trivial predictor
    if pd.Series(yr).nunique() < 2:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    model = Ridge(alpha=1.0, random_state=42)
    reg = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

    X_train, X_test, y_train, y_test = train_test_split(
        Xr, yr, test_size=0.2, random_state=42
    )
    assert len(X_train) > 0 and len(X_test) > 0

    reg.fit(X_train, y_train)
    y_pred = reg.predict(X_test)
    accuracy = _bounded_regression_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight models (small DecisionTreeClassifier for classification; Ridge for regression fallback) to stay CPU/energy efficient.
# - Keeps preprocessing minimal and reproducible via Pipeline/ColumnTransformer; avoids repeated work and ensures deterministic behavior.
# - Robust CSV parsing fallback (default then sep=';' with decimal=',') and defensive schema handling to avoid hard failures.
# - OneHotEncoder(handle_unknown='ignore') prevents costly/fragile manual category handling; StandardScaler used only for numeric columns.
# - Caps tree complexity (max_depth, min_samples_leaf) to reduce compute and overfitting on small datasets.
# - Regression fallback maps R^2 to a bounded [0,1] proxy accuracy: accuracy = clip((R^2+1)/2, 0, 1).