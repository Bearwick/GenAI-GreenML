# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _find_dataset_path() -> str:
    candidates = [
        "data.csv",
        "dataset.csv",
        "train.csv",
        "Train.csv",
        "DATA.csv",
        "Dataset.csv",
        "input.csv",
    ]
    for c in candidates:
        if os.path.isfile(c):
            return c
    csvs = [f for f in os.listdir(".") if f.lower().endswith(".csv")]
    for f in csvs:
        try:
            df_head = pd.read_csv(f, nrows=5)
            cols = [c.strip().lower() for c in df_head.columns]
            if all(x in cols for x in ["age", "gender", "genre"]):
                return f
        except Exception:
            continue
    if csvs:
        return csvs[0]
    raise FileNotFoundError("No CSV dataset found in the current directory.")


def main() -> None:
    path = _find_dataset_path()
    df = pd.read_csv(path)

    df.columns = [c.strip().lower() for c in df.columns]
    required = ["age", "gender", "genre"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns: {missing}")

    df = df[required].copy()
    df = df.dropna(subset=["genre"])

    X = df[["age", "gender"]]
    y = df["genre"].astype(str)

    numeric_features = ["age"]
    categorical_features = ["gender"]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    model = LogisticRegression(
        max_iter=200,
        solver="lbfgs",
        n_jobs=1,
    )

    clf = Pipeline(
        steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ]
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y if y.nunique() > 1 and y.value_counts().min() >= 2 else None,
    )

    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Used a linear LogisticRegression classifier: lightweight, fast on CPU, strong baseline for tabular data.
# - Kept preprocessing minimal and reproducible via a single sklearn Pipeline + ColumnTransformer.
# - OneHotEncoder is used only for a single low-cardinality categorical feature (gender); no embeddings/NNs.
# - SimpleImputer avoids dropping rows unnecessarily while staying computationally cheap.
# - Sparse one-hot output reduces memory/compute; no plotting, no interactive input, no model saving.