# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

music_data = pd.read_csv('music.csv', dtype={'age': 'int8', 'gender': 'int8'})

X = music_data[['age', 'gender']]
y = music_data['genre']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

accuracy = model.score(X_test, y_test)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Optimized memory footprint by downcasting numerical columns to int8 during the CSV loading phase.
# 2. Reduced unnecessary data movement by using direct column selection for features instead of the computationally heavier drop() method.
# 3. Improved execution stability and eliminated energy waste from redundant testing by setting a fixed random_state for both the data split and the model.
# 4. Streamlined the evaluation process by utilizing model.score(), which computes accuracy efficiently without creating additional intermediate prediction objects.
# 5. Minimized library overhead by avoiding redundant preprocessing steps on a simple structured dataset.