# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, r2_score

def load_data(filepath):
    """Robustly load CSV data with fallback delimiters."""
    try:
        df = pd.read_csv(filepath)
        if df.shape[1] <= 1:
            raise ValueError
    except:
        try:
            df = pd.read_csv(filepath, sep=';', decimal=',')
        except:
            # Create a dummy dataframe if file is missing to ensure script doesn't hard-fail
            # in a way that prevents the execution flow, though normally the file should exist.
            df = pd.DataFrame()
    
    if not df.empty:
        # Standardize column names
        df.columns = [str(c).strip().replace('  ', ' ') for c in df.columns]
        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def build_and_run():
    # Load data
    df = load_data('music.csv')
    
    if df.empty:
        print("ACCURACY=0.000000")
        return

    # Identify target and features
    # Based on provided headers: age, gender, genre
    target_candidate = 'genre'
    if target_candidate not in df.columns:
        # Fallback: choose the last column if 'genre' is missing
        target_col = df.columns[-1]
    else:
        target_col = target_candidate

    y = df[target_col]
    X = df.drop(columns=[target_col])

    # Ensure dataset is not empty
    if X.empty or len(y) < 2:
        print("ACCURACY=0.000000")
        return

    # Identify feature types
    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

    # Preprocessing pipelines
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ],
        remainder='drop'
    )

    # Determine if Classification or Regression
    is_classification = True
    if y.dtype in ['float64', 'int64'] and y.nunique() > 10:
        is_classification = False

    # Choose energy-efficient model (Small Decision Tree)
    if is_classification:
        model_type = DecisionTreeClassifier(max_depth=5, random_state=42)
    else:
        model_type = DecisionTreeRegressor(max_depth=5, random_state=42)

    # Create modeling pipeline
    clf = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', model_type)
    ])

    # Train/Test Split
    try:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Check if we have enough data to train
        if len(X_train) == 0:
            print("ACCURACY=0.000000")
            return

        clf.fit(X_train, y_train)
        predictions = clf.predict(X_test)

        if is_classification:
            accuracy = accuracy_score(y_test, predictions)
        else:
            # For regression, we use R^2 bounded to [0,1] as an accuracy proxy
            r2 = r2_score(y_test, predictions)
            accuracy = max(0, min(1, r2))
            
    except Exception:
        accuracy = 0.000000

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    build_and_run()

# OPTIMIZATION SUMMARY
# 1. Model Choice: Used DecisionTreeClassifier/Regressor with max_depth=5. These are computationally 
#    inexpensive (O(n_features * n_samples * log n_samples) training, O(depth) inference) and CPU-friendly.
# 2. Preprocessing: Utilized Scikit-learn Pipelines and ColumnTransformer to avoid redundant 
#    data copies and ensure a streamlined, reproducible execution flow.
# 3. Energy Efficiency: Avoided heavy ensembles (RandomForest/XGBoost) and Deep Learning which 
#    consume significantly more CO2 during hyperparameter tuning and training.
# 4. Robustness: Implemented a multi-stage CSV loader and schema cleaner to prevent execution 
#    interruptions from formatting issues (delimiters, whitespace, trailing columns).
# 5. Data Handling: Used SimpleImputer and OneHotEncoder to handle missing values and 
#    categorical strings efficiently without manual mapping.
# 6. Fallback Logic: Included a regression fallback for numeric targets, calculating a 
#    bounded R^2 score to meet the strict [0,1] accuracy output requirement.