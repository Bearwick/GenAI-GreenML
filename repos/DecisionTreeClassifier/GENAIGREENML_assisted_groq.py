# Generated by generate_llm_code.py
# LLM: groq
# Mode: assisted

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib

# Load dataset with specified dtypes to reduce memory usage
data = pd.read_csv('music.csv', dtype={'age': 'int32', 'gender': 'category'})
# Encode categorical gender as integer codes
data['gender'] = data['gender'].cat.codes

X = data.drop(columns=['genre'])
y = data['genre']

# Reproducible train-test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    shuffle=True,
    stratify=y
)

# Train lightweight Decision Tree model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Evaluate
pred = model.predict(X_test)
acc = accuracy_score(y_test, pred)
print(f"ACCURACY={acc:.6f}")

# Save the trained model for future use
joblib.dump(model, 'decision_tree_music_model.pkl')

# Optimizations applied:
# 1. Explicit dtype specification reduces memory footprint during load.
# 2. Converting categorical gender to numeric codes eliminates unnecessary string handling.
# 3. Random_state ensures reproducibility and consistent evaluation.
# 4. Stratified split maintains class distribution in train/test sets.
# 5. Removed unnecessary prints, plots, and interactive prompts to lower runtime overhead.
# 6. Model persistence allows reuse without retraining, saving future compute cycles.