# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline

DATASET_PATH = "music.csv"

# Robust CSV loading
try:
    df = pd.read_csv(DATASET_PATH)
    if df.shape[1] < 2:
        df = pd.read_csv(DATASET_PATH, sep=';', decimal=',')
except Exception:
    df = pd.read_csv(DATASET_PATH, sep=';', decimal=',')

# Normalize column names
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
df = df.loc[:, ~df.columns.str.startswith('Unnamed')]

# Identify columns based on expected schema: age, gender, genre
expected_features = ['age', 'gender']
expected_target = 'genre'

# Case-insensitive column matching
col_map = {c.lower(): c for c in df.columns}

feature_cols = []
for ef in expected_features:
    if ef.lower() in col_map:
        feature_cols.append(col_map[ef.lower()])

target_col = None
if expected_target.lower() in col_map:
    target_col = col_map[expected_target.lower()]

# Fallback if expected columns not found
if not feature_cols or target_col is None:
    # Pick last column as target, rest as features
    all_cols = list(df.columns)
    if len(all_cols) >= 2:
        target_col = all_cols[-1]
        feature_cols = all_cols[:-1]
    else:
        target_col = all_cols[0]
        feature_cols = all_cols[:1]

# Coerce numeric features
for fc in feature_cols:
    df[fc] = pd.to_numeric(df[fc], errors='coerce')

# Drop rows with NaN in features
df = df.dropna(subset=feature_cols)

# Encode target if it's categorical
le = None
if df[target_col].dtype == object or df[target_col].dtype.name == 'category':
    le = LabelEncoder()
    df[target_col] = le.fit_transform(df[target_col].astype(str))
else:
    df[target_col] = pd.to_numeric(df[target_col], errors='coerce')
    df = df.dropna(subset=[target_col])

# Handle inf values
df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=feature_cols + [target_col])

assert df.shape[0] > 0, "Dataset empty after preprocessing"

X = df[list(feature_cols)].values
y = df[target_col].values

n_classes = len(np.unique(y))

# Check if classification is viable
if n_classes < 2:
    # Trivial baseline: predict the single class, accuracy = 1.0
    accuracy = 1.0
    print(f"ACCURACY={accuracy:.6f}")
else:
    # Train/test split
    test_size = 0.2
    if X.shape[0] < 10:
        test_size = 0.5
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Train/test split has no samples"

    # Use a small DecisionTreeClassifier as per project context
    # max_depth limited for energy efficiency and to avoid overfitting on small data
    model = DecisionTreeClassifier(
        max_depth=5,
        random_state=42
    )
    model.fit(X_train, y_train)
    accuracy = model.score(X_test, y_test