# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load dataset - assuming standard music genre prediction dataset
df = pd.read_csv("data.csv")

# Encode categorical features
le_gender = LabelEncoder()
df["gender"] = le_gender.fit_transform(df["gender"])

# Separate features and target
X = df[["age", "gender"]]
y = df["genre"]

le_target = LabelEncoder()
y_encoded = le_target.fit_transform(y)

# Split data with fixed random state for reproducibility
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# Decision tree is ideal here: 2 features, small dataset, interpretable, fast on CPU
# Limiting depth to avoid overfitting on small data and reduce computation
model = DecisionTreeClassifier(
    max_depth=5,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# - DecisionTreeClassifier chosen for its O(n log n) training complexity and minimal memory footprint
# - Only 2 numeric features (age, encoded gender) so no scaling needed for tree-based model
# - LabelEncoder used instead of OneHotEncoder since gender is binary and tree handles ordinal encoding well
# - Max depth limited to 5 to prevent overfitting on small dataset and reduce tree size
# - No deep learning or ensemble methods needed given the simplicity of the feature space
# - Entire pipeline runs efficiently on CPU with negligible energy consumption
# - Stratified split ensures balanced class representation in train/test sets