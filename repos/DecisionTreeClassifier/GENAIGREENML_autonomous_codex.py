# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor

path = "music.csv"

def read_dataset(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    if df.shape[1] == 1:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > 1:
                df = df_alt
        except Exception:
            pass
    return df

df = read_dataset(path)

def normalize_columns(cols):
    norm = []
    for c in cols:
        c = str(c).strip()
        c = " ".join(c.split())
        norm.append(c)
    return norm

df.columns = normalize_columns(df.columns)
df = df.loc[:, ~df.columns.str.match(r"^Unnamed")]

assert df.shape[0] > 0

def choose_target(df):
    lower_cols = {c.lower(): c for c in df.columns}
    for cand in ['genre', 'target', 'label', 'y']:
        if cand in lower_cols:
            return lower_cols[cand]
    n_samples = len(df)
    numeric_candidates = []
    for col in df.columns:
        coerced = pd.to_numeric(df[col], errors='coerce')
        if coerced.notna().sum() >= max(1, int(np.ceil(0.5 * n_samples))):
            numeric_candidates.append(col)
    for col in numeric_candidates:
        if df[col].nunique(dropna=True) > 1:
            return col
    return df.columns[-1]

target_col = choose_target(df)

if target_col in df.columns:
    X_df = df.drop(columns=[target_col])
    y_raw = df[target_col]
else:
    X_df = df.copy()
    y_raw = pd.Series(np.zeros(len(df)))

if X_df.shape[1] == 0:
    X_df = pd.DataFrame({'constant': np.zeros(len(df))})

def detect_task(y):
    if y is None:
        return 'regression'
    y_coerced = pd.to_numeric(y, errors='coerce')
    non_numeric = y_coerced.isna().sum()
    if y.dtype == object and non_numeric >= len(y) / 2:
        return 'classification'
    unique = y_coerced.dropna().unique()
    n_unique = len(unique)
    if n_unique <= 20 and n_unique / max(1, len(y)) <= 0.2 and n_unique > 1 and np.all(np.isclose(unique, np.round(unique))):
        return 'classification'
    return 'regression'

def prepare_regression(y_raw, X_df):
    y_numeric = pd.to_numeric(y_raw, errors='coerce')
    y_numeric = y_numeric.replace([np.inf, -np.inf], np.nan)
    if y_numeric.notna().sum() == 0:
        y_numeric = pd.Series(np.zeros(len(y_raw)))
    mask = y_numeric.notna()
    X_new = X_df.loc[mask].reset_index(drop=True)
    y_new = y_numeric.loc[mask].reset_index(drop=True)
    if len(y_new) == 0:
        X_new = X_df.copy().reset_index(drop=True)
        y_new = pd.Series(np.zeros(len(X_new)))
    return X_new, y_new

def prepare_classification(y_raw, X_df):
    mask = y_raw.notna()
    if mask.sum() == 0:
        return None, None
    X_new = X_df.loc[mask].reset_index(drop=True)
    y_new = y_raw.loc[mask].astype(str).reset_index(drop=True)
    return X_new, y_new

task = detect_task(y_raw)

if task == 'classification':
    X_processed, y = prepare_classification(y_raw, X_df)
    if X_processed is None or y.nunique() < 2:
        task = 'regression'
        X_processed, y = prepare_regression(y_raw, X_df)
else:
    X_processed, y = prepare_regression(y_raw, X_df)

assert len(X_processed) > 0

if X_processed.shape[1] == 0:
    X_processed = pd.DataFrame({'constant': np.zeros(len(X_processed))})

n_samples = len(X_processed)
num_cols = []
cat_cols = []
for col in X_processed.columns:
    series = X_processed[col]
    coerced = pd.to_numeric(series, errors='coerce')
    coerced = coerced.replace([np.inf, -np.inf], np.nan)
    non_na = coerced.notna().sum()
    if non_na >= max(1, int(np.ceil(0.5 * n_samples))):
        X_processed[col] = coerced
        num_cols.append(col)
    else:
        X_processed[col] = series.astype(str)
        cat_cols.append(col)

if len(num_cols) == 0 and len(cat_cols) == 0:
    X_processed = pd.DataFrame({'constant': np.zeros(n_samples)})
    num_cols = ['constant']

transformers = []
if len(num_cols) > 0:
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler(with_mean=False))
    ])
    transformers.append(('num', numeric_transformer, num_cols))
if len(cat_cols) > 0:
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
    ])
    transformers.append(('cat', categorical_transformer, cat_cols))

preprocess = ColumnTransformer(transformers=transformers)

n_samples = len(X_processed)
if n_samples < 2:
    X_processed = pd.concat([X_processed, X_processed], ignore_index=True)
    y = pd.concat([y, y], ignore_index=True)
    n_samples = len(X_processed)

test_size = 0.2 if n_samples >= 5 else 0.5

stratify = None
if task == 'classification' and y.nunique() > 1:
    counts = y.value_counts()
    if (counts >= 2).all() and int(n_samples * test_size) >= y.nunique():
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X_processed, y, test_size=test_size, random_state=42, shuffle=True, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0

if task == 'classification':
    if y_train.nunique() < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    if y_train.nunique() < 2:
        model = DummyRegressor(strategy='mean')
    else:
        model = Ridge(alpha=1.0)

pipeline = Pipeline(steps=[('preprocess', preprocess), ('model', model)])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    y_true = np.array(y_test, dtype=float)
    y_pred_num = np.array(y_pred, dtype=float)
    mse = np.mean((y_true - y_pred_num) ** 2) if len(y_true) > 0 else 0.0
    accuracy = 1.0 / (1.0 + mse)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear or dummy models to minimize CPU and energy use.
# - Applied simple imputation, scaling, and one-hot encoding via ColumnTransformer for reproducible preprocessing.
# - Implemented robust CSV parsing and schema inference to handle unknown headers safely.
# - For regression fallback, reported accuracy as 1/(1+MSE) to keep a bounded [0,1] score.