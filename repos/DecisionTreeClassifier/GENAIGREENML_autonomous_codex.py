# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import glob
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.dummy import DummyClassifier

def find_dataset():
    candidates = glob.glob("*.csv")
    if not candidates:
        candidates = glob.glob("**/*.csv", recursive=True)
    required = {"age", "gender", "genre"}
    for path in candidates:
        try:
            cols = pd.read_csv(path, nrows=0).columns
        except Exception:
            continue
        cols_lower = [c.strip().lower() for c in cols]
        if required.issubset(set(cols_lower)):
            return path
    if candidates:
        return candidates[0]
    raise FileNotFoundError("No CSV dataset found")

path = find_dataset()
df = pd.read_csv(path)
df.columns = [c.strip().lower() for c in df.columns]

if not {"age", "gender", "genre"}.issubset(df.columns):
    raise ValueError("Required columns not found")

df = df[["age", "gender", "genre"]].copy()
df["age"] = pd.to_numeric(df["age"], errors="coerce")
df = df.dropna(subset=["genre"])

if len(df) == 0:
    accuracy = 0.0
    print(f"ACCURACY={accuracy:.6f}")
else:
    X = df[["age", "gender"]]
    y = df["genre"].astype(str)
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)

    def split_data(X_data, y_data):
        n = len(y_data)
        if n < 2:
            return X_data, X_data, y_data, y_data
        try:
            return train_test_split(X_data, y_data, test_size=0.2, random_state=42, stratify=y_data)
        except ValueError:
            try:
                return train_test_split(X_data, y_data, test_size=0.2, random_state=42)
            except ValueError:
                return train_test_split(X_data, y_data, test_size=1, random_state=42)

    X_train, X_test, y_train, y_test = split_data(X, y_encoded)

    numeric_features = ["age"]
    categorical_features = ["gender"]

    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=False))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features)
        ],
        sparse_threshold=0.0
    )

    if len(np.unique(y_train)) < 2:
        model = DummyClassifier(strategy="most_frequent")
    else:
        model = LogisticRegression(max_iter=200, solver="liblinear", n_jobs=1, random_state=42)

    clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Used a simple preprocessing pipeline with imputation and one-hot encoding to keep transformations lightweight.
# Selected logistic regression with a small iteration cap for efficient CPU training on small tabular data.
# Included a dummy classifier fallback to avoid training overhead when only one class is present.
# Limited features to age and gender only, reducing memory and compute requirements.
# Fixed random seeds to ensure reproducible, deterministic results with minimal overhead.