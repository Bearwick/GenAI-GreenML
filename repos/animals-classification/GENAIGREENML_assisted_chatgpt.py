# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score


SEED = 0


def set_reproducibility(seed: int = 0) -> None:
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)


def read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def build_xy(df: pd.DataFrame, dataset_headers: str) -> tuple[np.ndarray, np.ndarray]:
    expected_cols = [c.strip() for c in dataset_headers.split(",") if c.strip()]
    col_set = set(df.columns)

    if set(expected_cols).issubset(col_set):
        feature_cols = expected_cols[1:17]
        target_col = expected_cols[17]
        X = df.loc[:, feature_cols].to_numpy(dtype=np.int64, copy=False)
        y = df.loc[:, target_col].to_numpy(copy=False)
        return X, y

    X = df.iloc[:, 1:17].to_numpy(dtype=np.int64, copy=False)
    y = df.iloc[:, 17].to_numpy(copy=False)
    return X, y


def main() -> None:
    set_reproducibility(SEED)

    dataset_headers = (
        "animal_name,hair,feathers,eggs,milk,airborne,aquatic,predator,toothed,"
        "backbone,breathes,venomous,fins,legs,tail,domestic,catsize,class_type"
    )

    df = read_csv_robust("zoo.csv")
    X, y = build_xy(df, dataset_headers)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=SEED
    )

    classifier = SVC()
    classifier.fit(X_train, y_train)

    y_pred = classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused imports (seaborn, matplotlib, numpy-only utilities) to reduce startup time and memory footprint.
# - Removed plotting and intermediate computations (head(), extra predict slice, score call) to avoid redundant work.
# - Replaced confusion-matrix manual diagonal summation with accuracy_score for equivalent accuracy computation and less overhead.
# - Implemented robust CSV parsing fallback (default read_csv then retry with sep=';' and decimal=',') to avoid mis-parsing retries elsewhere.
# - Derived feature/target columns from provided DATASET_HEADERS when available; otherwise falls back to positional slicing to preserve behavior.
# - Used to_numpy(..., copy=False) and compact integer dtype for features to reduce memory usage and unnecessary data movement.
# - Centralized reproducibility (PYTHONHASHSEED, random, numpy seed; fixed train_test_split random_state) for stable, deterministic runs.