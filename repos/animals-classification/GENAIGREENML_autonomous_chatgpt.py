# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "zoo.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c = re.sub(r"\s+", " ", str(c).strip())
        out.append(c)
    return out


def _read_csv_robust(path):
    # Try default CSV parsing first; if it yields a single column or suspiciously few columns, retry with ';' and ',' decimal.
    df1 = pd.read_csv(path)
    if df1.shape[1] <= 1:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        if df2.shape[1] > df1.shape[1]:
            return df2
    return df1


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _coerce_numeric_columns(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _pick_target(df, preferred_targets):
    # Prefer targets by name if present; else pick a non-constant numeric column with few unique values.
    for t in preferred_targets:
        if t in df.columns:
            return t

    numeric_cols = list(df.select_dtypes(include=["number"]).columns)
    best = None
    best_score = -1.0
    for c in numeric_cols:
        s = df[c]
        s_nonan = s.dropna()
        if s_nonan.empty:
            continue
        nunique = int(s_nonan.nunique())
        if nunique <= 1:
            continue
        # Heuristic: classification targets often have small integer cardinality
        score = 1.0 / max(nunique, 2)
        # Prefer integer-ish columns
        frac = (s_nonan % 1).abs().mean()
        score += float(frac < 1e-6)
        if score > best_score:
            best_score = score
            best = c

    if best is not None:
        return best

    # Fallback: last column
    return df.columns[-1]


def _safe_accuracy_from_r2(y_true, y_pred):
    # Convert potentially negative R^2-like score into bounded [0,1] proxy for stable "ACCURACY" printing
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    denom = np.sum((y_true - np.mean(y_true)) ** 2)
    if denom <= 0:
        return 0.0
    r2 = 1.0 - (np.sum((y_true - y_pred) ** 2) / denom)
    acc = float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))
    return acc


df = _read_csv_robust(DATASET_PATH)
df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)

# Ensure non-empty
assert df.shape[0] > 0 and df.shape[1] > 0

# Best-effort numeric coercion for columns that look numeric (excluding obvious text id/name columns)
# Coerce object columns that are mostly numeric
for col in list(df.columns):
    if df[col].dtype == "object":
        s = df[col].astype(str)
        # Skip columns that look like identifiers/names (high uniqueness relative to rows)
        uniq_ratio = df[col].nunique(dropna=True) / max(len(df), 1)
        if uniq_ratio > 0.7 and len(df) > 20:
            continue
        # Try convert if majority of entries look numeric-like
        sample = s.head(min(50, len(s)))
        numeric_like = sample.str.match(r"^\s*[-+]?\d+([.,]\d+)?\s*$", na=False).mean()
        if numeric_like >= 0.8:
            df[col] = pd.to_numeric(s.str.replace(",", ".", regex=False), errors="coerce")

preferred_targets = ["class_type", "class", "target", "label", "y"]
target_col = _pick_target(df, preferred_targets)

# If target is all-NaN or constant, try picking another
def _is_bad_target(series):
    s = series.dropna()
    return s.empty or (s.nunique() < 2)

if _is_bad_target(df[target_col]):
    alt_targets = [c for c in df.columns if c != target_col]
    found = None
    for c in alt_targets:
        if not _is_bad_target(df[c]) and (df[c].dtype != "object" or df[c].nunique(dropna=True) < max(2, min(30, len(df) // 2))):
            found = c
            break
    if found is not None:
        target_col = found

# Basic row filtering: drop rows with missing target
df = df.copy()
df = df.loc[~df[target_col].isna()].reset_index(drop=True)
assert len(df) > 0

X = df.drop(columns=[target_col])
y_raw = df[target_col]

# Decide task type
is_classification = False
y = y_raw

if y_raw.dtype == "object":
    is_classification = True
else:
    # Numeric target: classify if integer-like with small cardinality
    y_nonan = y_raw.dropna()
    nunique = int(y_nonan.nunique()) if not y_nonan.empty else 0
    frac = float(((y_nonan % 1).abs().mean()) if (not y_nonan.empty) else 1.0)
    if nunique >= 2 and nunique <= 30 and frac < 1e-6:
        is_classification = True

# Build preprocessing
numeric_features = list(X.select_dtypes(include=["number"]).columns)
categorical_features = [c for c in X.columns if c not in numeric_features]

numeric_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ]
)

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Train/test split with defensive sizing
test_size = 0.2
if len(df) < 20:
    test_size = 0.3

if is_classification:
    # Encode y robustly
    le = LabelEncoder()
    y_enc = le.fit_transform(y_raw.astype(str))

    # If only one class, fallback to regression-style proxy (still runs end-to-end)
    if len(np.unique(y_enc)) < 2:
        is_classification = False
        y = pd.to_numeric(y_raw, errors="coerce").fillna(0.0).to_numpy()
    else:
        stratify = y_enc if len(np.unique(y_enc)) > 1 and len(df) >= 10 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_enc, test_size=test_size, random_state=RANDOM_STATE, stratify=stratify
        )
        assert len(X_train) > 0 and len(X_test) > 0

        model = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            C=1.0,
        )

        clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
        print(f"ACCURACY={accuracy:.6f}")
        raise SystemExit(0)

# Regression fallback path
y_num = pd.to_numeric(y_raw, errors="coerce")
if y_num.isna().any():
    # Drop rows with NaN y for regression
    mask = ~y_num.isna()
    X = X.loc[mask].reset_index(drop=True)
    y_num = y_num.loc[mask].reset_index(drop=True)
assert len(X) > 1

X_train, X_test, y_train, y_test = train_test_split(
    X, y_num.to_numpy(), test_size=test_size, random_state=RANDOM_STATE
)
assert len(X_train) > 0 and len(X_test) > 0

reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
regr = Pipeline(steps=[("preprocess", preprocess), ("model", reg)])
regr.fit(X_train, y_train)
y_pred = regr.predict(X_test)
accuracy = _safe_accuracy_from_r2(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used robust CSV parsing with a lightweight fallback (sep=';' and decimal=',') to avoid manual edits and re-runs.
# - Normalized column names and dropped 'Unnamed:' columns to prevent schema-related bugs without extra compute.
# - Minimal preprocessing via ColumnTransformer (median/mode imputation + StandardScaler + OneHotEncoder) for reproducibility and single-pass transforms.
# - Chose LogisticRegression (liblinear) as a CPU-efficient baseline for multiclass classification; sparse one-hot keeps memory use low.
# - Implemented defensive target selection to keep pipeline running even with missing/changed target columns.
# - Regression fallback uses Ridge (closed-form-like, CPU-friendly); "accuracy" is a bounded proxy derived from R^2 mapped to [0,1] for stable reporting.