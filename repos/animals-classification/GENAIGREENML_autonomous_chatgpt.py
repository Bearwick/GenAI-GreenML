# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge

warnings.filterwarnings("ignore")


DATASET_PATH = "zoo.csv"
DATASET_HEADERS = [
    "animal_name", "hair", "feathers", "eggs", "milk", "airborne", "aquatic",
    "predator", "toothed", "backbone", "breathes", "venomous", "fins", "legs",
    "tail", "domestic", "catsize", "class_type"
]


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _read_csv_robust(path):
    df = None
    # Attempt 1: default parsing
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(d):
        if d is None or d.empty:
            return True
        # If only 1 column and it contains separators, likely wrong delimiter
        if d.shape[1] == 1:
            col0 = d.columns[0]
            sample = d[col0].astype(str).head(5).tolist()
            joined = " ".join(sample)
            if (";" in joined) or ("," in joined and col0.count(",") > 5):
                return True
        return False

    if looks_wrong(df):
        # Attempt 2: semicolon separator and comma decimal
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df = None

    if df is None:
        # Last resort: create empty frame to trigger defensive behavior
        df = pd.DataFrame()

    return df


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c).strip())]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _choose_target(df):
    cols = list(df.columns)

    # Prefer provided header if present
    preferred = "class_type"
    if preferred in cols:
        return preferred

    # Otherwise, choose a non-constant numeric-like column with few unique values (classification-friendly)
    numeric_candidates = []
    for c in cols:
        if c.lower() == "animal_name":
            continue
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() >= max(5, int(0.2 * len(df))):
            nunique = s.dropna().nunique()
            if nunique >= 2:
                numeric_candidates.append((c, nunique, s.notna().mean()))
    if numeric_candidates:
        # prioritize low-cardinality (likely class labels), then high completeness
        numeric_candidates.sort(key=lambda t: (t[1], -t[2]))
        return numeric_candidates[0][0]

    # Fallback: any non-constant column
    for c in cols:
        if df[c].nunique(dropna=True) >= 2:
            return c

    return None


def _split_features_target(df, target_col):
    if target_col is None or target_col not in df.columns:
        return df.copy(), None

    X = df.drop(columns=[target_col], errors="ignore").copy()
    y = df[target_col].copy()
    return X, y


def _make_preprocessor(X):
    # Identify columns
    cols = list(X.columns)

    # Treat as numeric if coercion yields substantial non-NaN
    numeric_cols = []
    categorical_cols = []
    for c in cols:
        s = X[c]
        if pd.api.types.is_numeric_dtype(s):
            numeric_cols.append(c)
        else:
            coerced = pd.to_numeric(s, errors="coerce")
            if coerced.notna().mean() >= 0.8:
                numeric_cols.append(c)
            else:
                categorical_cols.append(c)

    # Ensure stable behavior if no features
    if not numeric_cols and not categorical_cols and cols:
        # Put everything as categorical as a safe default
        categorical_cols = cols[:]

    numeric_transformer = Pipeline(steps=[
        ("to_numeric", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False))
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3
    )
    return preprocessor, numeric_cols, categorical_cols


def _is_classification_target(y):
    if y is None:
        return False, None

    y_series = y.copy()

    # If numeric, consider classification when unique count is small relative to n
    if pd.api.types.is_numeric_dtype(y_series):
        y_num = pd.to_numeric(y_series, errors="coerce")
        y_num = y_num.replace([np.inf, -np.inf], np.nan).dropna()
        if len(y_num) == 0:
            return False, None
        nunique = y_num.nunique()
        if nunique < 2:
            return False, None
        # heuristic: classification if integer-ish and low cardinality
        is_int_like = np.all(np.isclose(y_num.values, np.round(y_num.values)))
        if is_int_like and nunique <= max(20, int(0.2 * len(y_num))):
            return True, y_num.astype(int)
        # If known label column name appears, still allow classification
        return False, y_num
    else:
        y_str = y_series.astype(str).replace("nan", np.nan).dropna()
        if y_str.nunique() >= 2:
            return True, y_str
        return False, y_str


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    y_true = np.where(np.isfinite(y_true), y_true, np.nan)
    y_pred = np.where(np.isfinite(y_pred), y_pred, np.nan)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() < 2:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    sse = np.sum((yt - yp) ** 2)
    var = np.sum((yt - np.mean(yt)) ** 2)
    if var <= 1e-12:
        return 0.0
    r2 = 1.0 - (sse / var)
    # Bound to [0, 1] for stable "accuracy proxy"
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    df = _read_csv_robust(DATASET_PATH)

    if df is None or df.empty:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # If file has no header or wrong header, attempt to assign known headers when lengths match
    if (len(df.columns) == len(DATASET_HEADERS)) and (set(df.columns) == set([str(i) for i in range(len(df.columns))]) or any("Unnamed" in c for c in df.columns)):
        df.columns = DATASET_HEADERS[:]

    # Defensive: drop fully empty rows
    df = df.dropna(how="all").reset_index(drop=True)

    assert not df.empty, "Dataset is empty after cleaning."

    target_col = _choose_target(df)
    X, y = _split_features_target(df, target_col)

    # If no target found, create a trivial one to keep pipeline end-to-end
    if y is None:
        y = pd.Series(np.zeros(len(df), dtype=int), name="target")
        X = df.copy()

    # Remove high-cardinality identifier-like column if present (common for names)
    for name_col in ["animal_name", "name", "id"]:
        if name_col in X.columns and X[name_col].nunique(dropna=True) > max(20, int(0.5 * len(X))):
            X = X.drop(columns=[name_col], errors="ignore")

    # Coerce numeric columns broadly where safe; keep objects for categorical handling
    for c in X.columns:
        if pd.api.types.is_numeric_dtype(X[c]):
            X[c] = pd.to_numeric(X[c], errors="coerce")
        else:
            # If mostly numeric, coerce to numeric to reduce one-hot overhead
            coerced = pd.to_numeric(X[c], errors="coerce")
            if coerced.notna().mean() >= 0.8:
                X[c] = coerced

    # Replace inf with NaN
    X = X.replace([np.inf, -np.inf], np.nan)

    preprocessor, numeric_cols, categorical_cols = _make_preprocessor(X)

    is_clf, y_processed = _is_classification_target(y)

    # Align y with X index and drop rows with missing y for classification/regression
    y_aligned = pd.Series(y_processed, index=y.index if hasattr(y, "index") else None)
    if hasattr(X, "index") and hasattr(y_aligned, "reindex"):
        y_aligned = y_aligned.reindex(X.index)

    valid_mask = y_aligned.notna()
    X2 = X.loc[valid_mask].copy()
    y2 = y_aligned.loc[valid_mask].copy()

    assert len(X2) > 1, "Not enough valid samples after target cleaning."

    # Handle degenerate targets
    if is_clf:
        # Ensure at least 2 classes
        if pd.Series(y2).nunique() < 2:
            is_clf = False

    # Train/test split
    # For classification, stratify when possible
    strat = y2 if (is_clf and pd.Series(y2).nunique() > 1) else None
    X_train, X_test, y_train, y_test = train_test_split(
        X2, y2, test_size=0.2, random_state=42, stratify=strat
    )

    assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

    if is_clf:
        # Lightweight linear classifier; lbfgs handles small datasets well on CPU
        model = LogisticRegression(
            max_iter=200,
            solver="lbfgs",
            n_jobs=1,
            multi_class="auto"
        )
        pipe = Pipeline(steps=[("prep", preprocessor), ("model", model)])
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Lightweight linear regressor; stable on small datasets
        model = Ridge(alpha=1.0, random_state=42)
        pipe = Pipeline(steps=[("prep", preprocessor), ("model", model)])
        y_train_num = pd.to_numeric(y_train, errors="coerce")
        y_test_num = pd.to_numeric(y_test, errors="coerce")
        # If still invalid, use constant baseline
        if y_train_num.notna().sum() < 2 or y_test_num.notna().sum() < 1:
            accuracy = 0.0
        else:
            # Drop NaNs in y by aligning (minimal handling)
            tr_mask = y_train_num.notna()
            te_mask = y_test_num.notna()
            X_train_r = X_train.loc[tr_mask]
            y_train_r = y_train_num.loc[tr_mask]
            X_test_r = X_test.loc[te_mask]
            y_test_r = y_test_num.loc[te_mask]
            if len(X_train_r) < 2 or len(X_test_r) < 1:
                accuracy = 0.0
            else:
                pipe.fit(X_train_r, y_train_r)
                y_pred = pipe.predict(X_test_r)
                accuracy = _bounded_regression_score(y_test_r.values, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight CPU-friendly models: LogisticRegression for classification; Ridge as regression fallback.
# - Preprocessing via ColumnTransformer + Pipeline ensures single-pass transformations and reproducibility.
# - Avoids heavy ensembles/deep learning; one-hot encodes categoricals and standardizes numerics (with_mean=False to keep sparse efficiency).
# - Robust CSV parsing with delimiter/decimal fallback; normalizes headers; drops "Unnamed" columns.
# - Defensive schema handling: target auto-selection, numeric coercion, NaN/inf sanitization, and safe regression fallback.
# - Regression fallback reports bounded proxy accuracy: clip((R2 + 1)/2, 0, 1) to keep ACCURACY in [0,1].