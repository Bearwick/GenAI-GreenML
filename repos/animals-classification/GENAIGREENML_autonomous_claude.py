# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("dataset.csv")

# Separate features and target
# animal_name is an identifier, not a feature — drop it
X = df.drop(columns=["animal_name", "class_type"])
y = df["class_type"]

# All features are already numeric (binary or integer for legs)
# StandardScaler is lightweight and helps some classifiers, but RF doesn't need it.
# We skip heavy preprocessing since features are already clean binary/integer values.

# Use RandomForestClassifier: lightweight, CPU-friendly, excellent for small tabular datasets
# with mixed binary/integer features. No deep learning needed for ~100 samples.
# n_estimators=100 is a good balance of accuracy and efficiency for small data.

model = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    random_state=42,
    n_jobs=1  # Single core to minimize energy usage
)

# Stratified K-Fold cross-validation for robust accuracy estimate on small dataset
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

accuracies = []
for train_idx, test_idx in skf.split(X, y):
    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
    
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(y_test, y_pred))

accuracy = np.mean(accuracies)

# Also train final model on full data for completeness
model.fit(X, y)
full_accuracy = accuracy_score(y, model.predict(X))

# Report cross-validated accuracy as the honest estimate
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Dropped animal_name (identifier column, no predictive value).
# 2. No heavy preprocessing needed: features are already binary/integer, no missing values.
# 3. RandomForestClassifier chosen for its strong performance on small tabular datasets,
#    interpretability, and minimal computational cost on CPU.
# 4. n_jobs=1 to minimize energy consumption (single-threaded execution).
# 5. Stratified 5-fold CV used for reliable accuracy on small dataset (~101 samples, 7 classes).
# 6. No deep learning, embeddings, or GPU-dependent components — fully CPU-efficient.
# 7. No unnecessary scaling since tree-based models are invariant to feature magnitude.