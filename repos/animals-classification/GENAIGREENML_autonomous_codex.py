# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")

path = "zoo.csv"

def load_csv(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            try:
                df = pd.read_csv(path, sep=";", decimal=",", engine="python")
            except Exception:
                df = None
    if df is not None and df.shape[1] == 1:
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df2.shape[1] > 1:
                df = df2
        except Exception:
            pass
    return df

df = load_csv(path)
if df is None or df.empty:
    df = pd.DataFrame({"_dummy_feature": [0], "_dummy_target": [0]})

df.columns = [re.sub(r"\s+", " ", str(c).strip()) for c in df.columns]
df = df.loc[:, ~df.columns.str.startswith("Unnamed")]
df = df.dropna(axis=1, how="all")
if df.empty or df.shape[1] == 0:
    df = pd.DataFrame({"_dummy_feature": [0], "_dummy_target": [0]})
    df.columns = [re.sub(r"\s+", " ", str(c).strip()) for c in df.columns]

assert df.shape[0] > 0 and df.shape[1] > 0

cols = list(df.columns)
target = None
if "class_type" in cols:
    target = "class_type"
else:
    for c in cols:
        if "class" in c.lower():
            target = c
            break
if target is None:
    numeric_candidates = []
    for c in cols:
        col_num = pd.to_numeric(df[c], errors="coerce")
        if col_num.notna().sum() > 0 and col_num.nunique(dropna=True) > 1:
            numeric_candidates.append(c)
    if not numeric_candidates:
        for c in cols:
            col_num = pd.to_numeric(df[c], errors="coerce")
            if col_num.notna().sum() > 0:
                numeric_candidates.append(c)
    if numeric_candidates:
        target = numeric_candidates[-1]
    else:
        target = cols[-1]
if target is None:
    df["_target"] = 0
    target = "_target"

feature_cols = [c for c in df.columns if c != target]
if not feature_cols:
    df["_index_feat"] = np.arange(len(df))
    feature_cols = ["_index_feat"]

target_series = df[target]
target_num = pd.to_numeric(target_series, errors="coerce")
num_ratio = target_num.notna().mean() if len(target_num) > 0 else 0.0
task = "classification"
if num_ratio > 0.8:
    df[target] = target_num
    uniq = df[target].dropna().unique()
    if len(uniq) >= 2 and len(uniq) <= max(20, int(0.2 * len(df))):
        if np.all(np.isclose(uniq, np.round(uniq))):
            task = "classification"
        else:
            task = "regression"
    else:
        task = "regression"
else:
    task = "classification"

df = df.loc[~pd.isna(df[target])].copy()
if df.empty:
    df = pd.DataFrame({"_dummy_feature": [0], "_dummy_target": [0]})
    target = "_dummy_target"
    feature_cols = ["_dummy_feature"]
    task = "regression"

if task == "classification":
    y = df[target]
else:
    y = pd.to_numeric(df[target], errors="coerce")
    if y.isna().all():
        y = pd.Series(pd.factorize(df[target])[0], index=df.index).astype(float)

mask = ~pd.isna(y)
df = df.loc[mask].copy()
y = y.loc[mask]

X = df[feature_cols].copy()

numeric_features = []
categorical_features = []
for col in feature_cols:
    col_num = pd.to_numeric(X[col], errors="coerce")
    non_null_ratio = col_num.notna().mean() if len(col_num) > 0 else 0.0
    if non_null_ratio > 0.5:
        X[col] = col_num
        numeric_features.append(col)
    else:
        categorical_features.append(col)

X.replace([np.inf, -np.inf], np.nan, inplace=True)

if task == "classification" and y.nunique() < 2:
    task = "regression"
    y = pd.to_numeric(df[target], errors="coerce")
    if y.isna().all():
        y = pd.Series(pd.factorize(df[target])[0], index=df.index).astype(float)

mask = ~pd.isna(y)
X = X.loc[mask].copy()
y = y.loc[mask]

if X.empty or len(y) == 0:
    X = pd.DataFrame({"_dummy_feature": [0]})
    y = pd.Series([0.0])
    numeric_features = ["_dummy_feature"]
    categorical_features = []
    task = "regression"

assert len(X) > 0

n_samples = len(X)
if n_samples < 2:
    X_train = X.copy()
    X_test = X.copy()
    y_train = y.copy()
    y_test = y.copy()
else:
    if n_samples >= 5:
        test_size = 0.2
    elif n_samples == 4:
        test_size = 0.25
    elif n_samples == 3:
        test_size = 1 / 3
    else:
        test_size = 0.5
    stratify = None
    if task == "classification" and y.nunique() >= 2:
        vc = y.value_counts()
        if vc.min() >= 2:
            stratify = y
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=stratify
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=None
        )

assert len(X_train) > 0 and len(X_test) > 0

transformers = []
if numeric_features:
    num_pipeline = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ])
    transformers.append(("num", num_pipeline, numeric_features))
if categorical_features:
    cat_pipeline = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])
    transformers.append(("cat", cat_pipeline, categorical_features))

if transformers:
    preprocessor = ColumnTransformer(transformers=transformers, sparse_threshold=0)
else:
    preprocessor = "passthrough"

if task == "classification":
    model = LogisticRegression(max_iter=200, solver="lbfgs")
else:
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if task == "classification":
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    if not np.isfinite(r2):
        r2 = 0.0
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) to keep computation CPU-friendly.
# - Simple preprocessing with imputation, scaling, and one-hot encoding ensures reproducibility with minimal overhead.
# - Regression fallback maps R2 to a bounded [0,1] proxy via (r2+1)/2 for a stable accuracy-like metric.