# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

dataset = pd.read_csv('zoo.csv')
X = dataset.iloc[:, 1:17].values
y = dataset.iloc[:, 17].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

clf = DecisionTreeClassifier(criterion='entropy', random_state=0)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

"""
JUSTIFICATION FOR ENERGY-EFFICIENT DESIGN:
1. Model Selection: A Decision Tree classifier was chosen over resource-intensive models (like Deep Learning or SVMs) because it is computationally lightweight and highly effective for small, low-dimensional tabular datasets.
2. Computational Complexity: Decision Trees have a logarithmic inference time complexity, which minimizes CPU cycles and energy consumption per prediction.
3. Preprocessing Efficiency: The model handles categorical/boolean data natively, eliminating the need for energy-expensive feature scaling or complex normalization pipelines.
4. Resource Optimization: The solution avoids heavy libraries (like TensorFlow or PyTorch) and hardware acceleration (GPUs), ensuring a low power footprint by running efficiently on standard CPU architectures.
5. Data Handling: By stripping unnecessary columns (animal_name) and avoiding data augmentation, memory usage is kept to a minimum.
"""