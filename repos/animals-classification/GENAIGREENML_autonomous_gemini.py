# Generated by generate_llm_code_2.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

def run_efficient_pipeline():
    try:
        df = pd.read_csv('zoo.csv')
    except FileNotFoundError:
        return

    X = df.iloc[:, 1:17]
    y = df.iloc[:, 17]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.20, random_state=42, stratify=y
    )

    model = DecisionTreeClassifier(
        criterion='entropy', 
        max_depth=5, 
        random_state=42
    )
    
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_efficient_pipeline()

"""
JUSTIFICATION FOR DESIGN DECISIONS:

1. Model Choice: A Decision Tree Classifier was selected over SVM or Deep Learning. 
   Decision trees are computationally inexpensive, requiring O(n_features * n_samples * log(n_samples)) 
   for construction and O(tree_depth) for inference. This is significantly more 
   energy-efficient than iterative optimization methods for small tabular datasets.

2. Preprocessing Efficiency: The dataset consists mostly of binary features. 
   Decision trees handle these natively without requiring feature scaling (StandardScaler) 
   or normalization, reducing the CPU cycles spent on data transformation.

3. Hyperparameter Constraints: The 'max_depth' is capped at 5. This prevents overfitting 
   on a tiny dataset (~101 samples) and ensures the model remains a small, 
   low-memory structure.

4. Resource Usage: The implementation uses scikit-learn's optimized CPU-bound 
   algorithms, avoiding the overhead of heavy deep learning frameworks like 
   PyTorch or TensorFlow which are unnecessary for 16-feature classification.

5. Energy-Saving Pipeline: By using 'stratify' in the split, we ensure representative 
   class distributions in a single run, reducing the need for expensive cross-validation 
   loops in small-scale tasks.
"""