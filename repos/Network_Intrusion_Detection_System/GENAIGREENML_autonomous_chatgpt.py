# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        normed.append(c2)
    return normed


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c).strip(), flags=re.IGNORECASE)]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _read_csv_robust(path):
    # Try default read_csv; if single-column parsing likely, retry with European-style separators.
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
        return df

    if df.shape[1] <= 1:
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df2.shape[1] > df.shape[1]:
                return df2
        except Exception:
            pass
    return df


def _read_table_robust(path):
    # For .txt data that may be comma/space separated; try multiple lightweight parsers.
    attempts = []
    # 1) CSV default
    try:
        attempts.append(pd.read_csv(path, header=None))
    except Exception:
        attempts.append(None)

    # 2) CSV with ';' and decimal ','
    try:
        attempts.append(pd.read_csv(path, header=None, sep=";", decimal=","))
    except Exception:
        attempts.append(None)

    # 3) Whitespace-delimited (common for KDD-style)
    try:
        attempts.append(pd.read_csv(path, header=None, delim_whitespace=True))
    except Exception:
        attempts.append(None)

    # Choose parse with the most columns and non-empty rows
    best = None
    best_score = -1
    for df in attempts:
        if df is None:
            continue
        score = df.shape[1] * (1 if df.shape[0] > 0 else 0)
        if score > best_score:
            best_score = score
            best = df
    if best is None:
        raise RuntimeError("Failed to read dataset with robust parsers.")
    return best


def _load_dataset():
    # Prefer provided paths if they exist; otherwise try common fallbacks.
    candidates = [
        ("data/raw/Train.txt", "data/raw/Test.txt"),
        ("Train.txt", "Test.txt"),
        ("data/Train.txt", "data/Test.txt"),
        ("data/raw/train.txt", "data/raw/test.txt"),
        ("train.csv", "test.csv"),
        ("data/train.csv", "data/test.csv"),
    ]

    for train_path, test_path in candidates:
        if os.path.exists(train_path) and os.path.exists(test_path):
            train_df = _read_table_robust(train_path) if train_path.lower().endswith(".txt") else _read_csv_robust(train_path)
            test_df = _read_table_robust(test_path) if test_path.lower().endswith(".txt") else _read_csv_robust(test_path)
            return train_df, test_df, True

    # Single-file fallback
    single_candidates = [
        "data/raw/data.csv",
        "data/raw/dataset.csv",
        "data/dataset.csv",
        "dataset.csv",
        "data.csv",
        "data/raw/data.txt",
        "data/raw/dataset.txt",
        "dataset.txt",
        "data.txt",
    ]
    for path in single_candidates:
        if os.path.exists(path):
            df = _read_table_robust(path) if path.lower().endswith(".txt") else _read_csv_robust(path)
            return df, None, False

    # As a last resort, try any csv/txt in cwd or data folders (deterministic order).
    scan_dirs = [".", "data", "data/raw"]
    found = []
    for d in scan_dirs:
        if not os.path.isdir(d):
            continue
        for fn in sorted(os.listdir(d)):
            if fn.lower().endswith((".csv", ".txt")):
                found.append(os.path.join(d, fn))
    if found:
        path = found[0]
        df = _read_table_robust(path) if path.lower().endswith(".txt") else _read_csv_robust(path)
        return df, None, False

    raise FileNotFoundError("No dataset files found in expected locations.")


def _choose_target_and_features(df):
    df = df.copy()
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # If no columns (shouldn't happen), return minimal
    if df.shape[1] == 0:
        return df, None, [], []

    # Prefer last column as target (common in many tabular datasets)
    candidate_target = df.columns[-1]

    # If last column seems unusable (all missing), try find a better target
    def non_constant_series(s):
        s_non_na = s.dropna()
        if s_non_na.empty:
            return False
        return s_non_na.nunique(dropna=True) > 1

    y = df[candidate_target]
    if not non_constant_series(y):
        # Try other columns, prefer object then numeric (classification often string labels)
        object_cols = [c for c in df.columns if df[c].dtype == "object"]
        numeric_like_cols = [c for c in df.columns if c not in object_cols]
        ordered = object_cols + numeric_like_cols
        better = None
        for c in ordered:
            if c == candidate_target:
                continue
            if non_constant_series(df[c]):
                better = c
                break
        if better is not None:
            candidate_target = better

    target_col = candidate_target
    feature_cols = [c for c in df.columns if c != target_col]

    # If no feature columns, fallback: treat as trivial
    return df, target_col, feature_cols, df.columns.tolist()


def _coerce_numeric_inplace(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _build_preprocessor(X):
    # Detect types after basic coercion attempt on numeric-looking columns
    # Only coerce columns that are not clearly object with many non-numeric strings is expensive; keep it simple:
    numeric_cols = []
    categorical_cols = []
    for c in X.columns:
        if pd.api.types.is_numeric_dtype(X[c]):
            numeric_cols.append(c)
        else:
            # Try a light coercion check on a sample
            s = X[c]
            sample = s.dropna()
            if len(sample) > 0:
                sample = sample.iloc[:200]
                coerced = pd.to_numeric(sample, errors="coerce")
                numeric_ratio = np.mean(~np.isnan(coerced.values)) if len(coerced) > 0 else 0.0
                if numeric_ratio >= 0.95:
                    numeric_cols.append(c)
                else:
                    categorical_cols.append(c)
            else:
                categorical_cols.append(c)

    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )
    return preprocessor, numeric_cols, categorical_cols


def _is_classification_target(y):
    # Classification if object/string, boolean, or integer with small number of unique values.
    y_non_na = y.dropna()
    if y_non_na.empty:
        return False
    if pd.api.types.is_bool_dtype(y_non_na) or y_non_na.dtype == "object":
        return True

    # If numeric: decide by unique count and integer-likeness
    y_num = pd.to_numeric(y_non_na, errors="coerce")
    y_num = y_num[~np.isnan(y_num)]
    if y_num.empty:
        return True  # non-numeric mostly -> treat as classification
    uniq = pd.Series(y_num).nunique(dropna=True)
    if uniq <= 20:
        # If values are near integers, likely labels
        frac_part = np.abs(y_num - np.round(y_num))
        if np.nanmean(frac_part) < 1e-6:
            return True
    return False


def _accuracy_proxy_regression(y_true, y_pred):
    # Stable bounded proxy in [0,1]: 1 / (1 + normalized MAE)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    mae = np.mean(np.abs(y_true - y_pred))
    scale = np.std(y_true)
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = np.mean(np.abs(y_true)) + 1e-12
    nmae = mae / (scale + 1e-12)
    acc = 1.0 / (1.0 + nmae)
    return float(np.clip(acc, 0.0, 1.0))


def main():
    train_df, test_df, has_explicit_test = _load_dataset()

    # Normalize columns early
    train_df = train_df.copy()
    train_df.columns = _normalize_columns(train_df.columns)
    train_df = _drop_unnamed(train_df)

    if test_df is not None:
        test_df = test_df.copy()
        test_df.columns = _normalize_columns(test_df.columns)
        test_df = _drop_unnamed(test_df)

    assert train_df.shape[0] > 0 and train_df.shape[1] > 0

    train_df, target_col, feature_cols, _ = _choose_target_and_features(train_df)
    assert target_col is not None

    # Align test to train schema when possible; if mismatch, take intersection for features and keep target if present.
    if has_explicit_test and test_df is not None:
        if target_col not in test_df.columns:
            # If missing target in test, fallback to split from train only.
            has_explicit_test = False
        else:
            # Use only features present in test
            feature_cols = [c for c in feature_cols if c in test_df.columns]
            if len(feature_cols) == 0:
                has_explicit_test = False

    # Build X/y
    X_all = train_df[feature_cols].copy() if len(feature_cols) > 0 else pd.DataFrame(index=train_df.index)
    y_all = train_df[target_col].copy()

    # Basic cleanup on X: replace inf and coerce numerics where applicable later via preprocessor detection
    X_all = X_all.replace([np.inf, -np.inf], np.nan)

    # y cleanup
    y_all = y_all.replace([np.inf, -np.inf], np.nan)

    # Drop rows with missing target
    non_na_mask = ~pd.isna(y_all)
    X_all = X_all.loc[non_na_mask]
    y_all = y_all.loc[non_na_mask]
    assert X_all.shape[0] > 1

    is_clf = _is_classification_target(y_all)

    if has_explicit_test and test_df is not None:
        X_test = test_df[feature_cols].copy()
        X_test = X_test.replace([np.inf, -np.inf], np.nan)
        y_test = test_df[target_col].copy().replace([np.inf, -np.inf], np.nan)

        # Drop NA targets in explicit test
        m = ~pd.isna(y_test)
        X_test = X_test.loc[m]
        y_test = y_test.loc[m]

        # If test became empty, fallback to split
        if X_test.shape[0] == 0:
            has_explicit_test = False

    if not has_explicit_test:
        # Train/test split from training set
        stratify = None
        if is_clf:
            y_tmp = y_all.copy()
            # If too many classes or singletons, skip stratify to avoid split errors
            vc = y_tmp.value_counts(dropna=True)
            if (vc.size >= 2) and (vc.min() >= 2):
                stratify = y_tmp
        X_train, X_test, y_train, y_test = train_test_split(
            X_all, y_all, test_size=0.2, random_state=42, stratify=stratify
        )
    else:
        X_train, y_train = X_all, y_all

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    # If classification but fewer than 2 classes in train, fallback to regression-like trivial predictor
    if is_clf:
        y_train_non_na = y_train.dropna()
        if y_train_non_na.nunique(dropna=True) < 2:
            is_clf = False

    preprocessor, num_cols, cat_cols = _build_preprocessor(X_train)

    if is_clf:
        # Use small linear model for CPU efficiency
        clf = LogisticRegression(
            solver="lbfgs",
            max_iter=200,
            n_jobs=1,
        )
        model = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", clf),
        ])

        # Ensure y is label-encodable by sklearn; keep as-is (object ok).
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        # Lightweight regression with Ridge; compute bounded proxy accuracy in [0,1]
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", reg),
        ])

        # Coerce y to numeric for regression
        y_train_num = pd.to_numeric(y_train, errors="coerce")
        y_test_num = pd.to_numeric(y_test, errors="coerce")

        # Drop rows with non-numeric targets
        tr_mask = ~pd.isna(y_train_num)
        te_mask = ~pd.isna(y_test_num)
        X_train2 = X_train.loc[tr_mask]
        y_train2 = y_train_num.loc[tr_mask]
        X_test2 = X_test.loc[te_mask]
        y_test2 = y_test_num.loc[te_mask]

        if X_train2.shape[0] == 0 or X_test2.shape[0] == 0:
            # Trivial baseline: predict train mean
            mu = float(np.nanmean(y_train_num.values)) if np.isfinite(np.nanmean(y_train_num.values)) else 0.0
            y_pred2 = np.full(shape=(len(y_test_num),), fill_value=mu, dtype=float)
            accuracy = _accuracy_proxy_regression(y_test_num.values, y_pred2)
        else:
            model.fit(X_train2, y_train2)
            y_pred2 = model.predict(X_test2)
            accuracy = _accuracy_proxy_regression(y_test2.values, y_pred2)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced heavy XGBoost/PCA pipeline with a CPU-friendly linear baseline (LogisticRegression) and Ridge fallback.
# - Used ColumnTransformer + SimpleImputer + StandardScaler/OneHotEncoder for a reproducible, single-pass preprocessing graph.
# - Robust schema handling: normalize headers, drop Unnamed columns, infer target/feature columns, and align train/test safely.
# - Robust file parsing: try default CSV, then ';' separator with decimal ',', plus whitespace-delimited parsing for .txt.
# - Defensive checks avoid empty data/splits; classification falls back to regression when classes are insufficient.
# - Regression evaluation uses a bounded accuracy proxy in [0,1]: 1/(1+normalized MAE) for stability and comparability.