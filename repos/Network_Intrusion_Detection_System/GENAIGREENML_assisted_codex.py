# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import random
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from xgboost import XGBClassifier
from src.preprocessing import preprocess_data

SEED = 42
random.seed(SEED)
np.random.seed(SEED)

DATASET_HEADERS = "0,tcp,ftp_data,SF,491,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0.00,0.00,0.00,0.00,1.00,0.00,0.00,150,25,0.17,0.03,0.17,0.00,0.00,0.00,0.05,0.00,normal,20"

def read_csv_robust(path, headers_line):
    expected_cols = len(headers_line.split(","))
    try:
        df = pd.read_csv(path, header=None)
    except Exception:
        return pd.read_csv(path, header=None, sep=";", decimal=",")
    if df.shape[1] == 1 or (expected_cols > 1 and df.shape[1] != expected_cols):
        try:
            df_alt = pd.read_csv(path, header=None, sep=";", decimal=",")
            if expected_cols > 1:
                if abs(df_alt.shape[1] - expected_cols) < abs(df.shape[1] - expected_cols):
                    df = df_alt
            else:
                df = df_alt
        except Exception:
            pass
    return df

train_df = read_csv_robust("data/raw/Train.txt", DATASET_HEADERS)
test_df = read_csv_robust("data/raw/Test.txt", DATASET_HEADERS)

X_train_scaled, y_train, scaler, train_columns = preprocess_data(train_df, fit=True)
X_test_scaled, y_test = preprocess_data(test_df, fit=False, scaler=scaler, columns=train_columns)
del train_df, test_df

pca = PCA(n_components=0.95, random_state=SEED)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)
del X_train_scaled, X_test_scaled

model = XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric="logloss",
    random_state=SEED,
    n_jobs=1,
    use_label_encoder=False,
    verbosity=0
)
model.fit(X_train_pca, y_train)

accuracy = model.score(X_test_pca, y_test)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Removed nonessential imports, plotting, artifact persistence, and logging to cut I/O and runtime overhead.
# - Added robust CSV parsing with delimiter fallback and column-count validation from provided headers.
# - Fixed random seeds and limited parallelism for deterministic, reproducible training behavior.
# - Computed accuracy via model.score to avoid extra metric calculations and probability inference.
# - Released large intermediate datasets early to reduce memory footprint.