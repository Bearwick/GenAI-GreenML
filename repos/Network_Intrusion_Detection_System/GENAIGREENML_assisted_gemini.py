# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier

def load_data(path):
    try:
        df = pd.read_csv(path, header=None, engine='c')
    except (pd.errors.ParserError, ValueError):
        df = pd.read_csv(path, header=None, sep=';', decimal=',', engine='c')
    
    if df.shape[1] > 41:
        df.drop(columns=[df.columns[-1]], inplace=True)
    return df

def fast_preprocess(train_df, test_df):
    y_train = (train_df.iloc[:, -1] != 'normal').astype(np.int8)
    y_test = (test_df.iloc[:, -1] != 'normal').astype(np.int8)
    
    X_train = train_df.iloc[:, :-1]
    X_test = test_df.iloc[:, :-1]
    
    cat_cols = X_train.select_dtypes(include=['object']).columns
    for col in cat_cols:
        le = LabelEncoder()
        X_train[col] = le.fit_transform(X_train[col].astype(str))
        mapping = dict(zip(le.classes_, le.transform(le.classes_)))
        X_test[col] = X_test[col].astype(str).map(mapping).fillna(-1).astype(int)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)
    X_test_scaled = scaler.transform(X_test).astype(np.float32)
    
    return X_train_scaled, X_test_scaled, y_train, y_test

train_df = load_data("data/raw/Train.txt")
test_df = load_data("data/raw/Test.txt")

X_train_scaled, X_test_scaled, y_train, y_test = fast_preprocess(train_df, test_df)

pca = PCA(n_components=0.95, random_state=42, svd_solver='randomized')
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

model = XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric="logloss",
    random_state=42,
    tree_method='hist',
    n_jobs=-1
)
model.fit(X_train_pca, y_train)

y_pred = model.predict(X_test_pca)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Memory Efficiency: Used float32 and int8 dtypes to reduce memory footprint during processing and training.
# 2. Computational Speed: Implemented XGBoost with 'tree_method=hist', which significantly accelerates tree building via binning.
# 3. Algorithm Optimization: Switched PCA to 'svd_solver=randomized' for faster decomposition on high-dimensional data.
# 4. Redundancy Reduction: Consolidated preprocessing into a single pass and avoided redundant DataFrame copies.
# 5. IO Optimization: Used the 'c' engine for pd.read_csv and dropped the unnecessary 'difficulty level' column immediately.
# 6. Green Principles: Removed all visualization, logging, and artifact saving to minimize CPU cycles and disk IO.
# 7. Multi-threading: Enabled n_jobs=-1 in XGBoost to maximize hardware utilization and minimize wall-clock time.