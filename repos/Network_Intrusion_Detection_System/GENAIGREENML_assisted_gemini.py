# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
import random

def run_pipeline():
    random.seed(42)
    np.random.seed(42)

    def load_df(path):
        try:
            df = pd.read_csv(path, header=None, low_memory=False)
            if df.shape[1] < 5:
                df = pd.read_csv(path, header=None, sep=';', decimal=',', low_memory=False)
        except Exception:
            df = pd.read_csv(path, header=None, sep=';', decimal=',', low_memory=False)
        return df

    train_df = load_df("data/raw/Train.txt")
    test_df = load_df("data/raw/Test.txt")

    if not train_df.empty and not test_df.empty:
        y_train = (train_df.iloc[:, 41] != 'normal').astype(np.int32)
        y_test = (test_df.iloc[:, 41] != 'normal').astype(np.int32)

        X_train = train_df.iloc[:, :41]
        X_test = test_df.iloc[:, :41]

        cat_cols = [1, 2, 3]
        X_train = pd.get_dummies(X_train, columns=cat_cols)
        X_test = pd.get_dummies(X_test, columns=cat_cols)

        X_test = X_test.reindex(columns=X_train.columns, fill_value=0)

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)
        X_test_scaled = scaler.transform(X_test).astype(np.float32)

        pca = PCA(n_components=0.95, svd_solver='randomized', random_state=42)
        X_train_pca = pca.fit_transform(X_train_scaled)
        X_test_pca = pca.transform(X_test_scaled)

        model = XGBClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            eval_metric="logloss",
            tree_method='hist',
            n_jobs=-1,
            random_state=42
        )
        model.fit(X_train_pca, y_train)

        y_pred = model.predict(X_test_pca)
        accuracy = accuracy_score(y_test, y_pred)
        print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary
# 1. Reduced memory footprint by downcasting feature matrices to float32.
# 2. Accelerated XGBoost training using 'tree_method=hist', which is more energy-efficient for tabular data.
# 3. Optimized dimensionality reduction by using the 'randomized' SVD solver in PCA.
# 4. Eliminated energy-intensive disk I/O by removing model artifact saving (joblib.dump).
# 5. Reduced computational overhead by removing all visualization (matplotlib) and extensive logging.
# 6. Improved CPU utilization efficiency by enabling multi-threading with n_jobs=-1 in XGBoost.
# 7. Optimized preprocessing via vectorized pandas alignment and one-hot encoding instead of iterative loops.
# 8. Implemented robust and efficient CSV loading with fallback logic to handle potential format inconsistencies.