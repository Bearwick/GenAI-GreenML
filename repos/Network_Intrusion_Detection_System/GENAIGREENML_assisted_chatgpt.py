# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier

from src.preprocessing import preprocess_data

RANDOM_SEED = 42


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path, header=None)
    if df.shape[1] <= 2:
        df2 = pd.read_csv(path, header=None, sep=";", decimal=",")
        if df2.shape[1] > df.shape[1]:
            df = df2
    return df


def main() -> None:
    np.random.seed(RANDOM_SEED)

    train_path = os.path.join("data", "raw", "Train.txt")
    test_path = os.path.join("data", "raw", "Test.txt")

    train_df = _read_csv_robust(train_path)
    test_df = _read_csv_robust(test_path)

    X_train_scaled, y_train, scaler, train_columns = preprocess_data(train_df, fit=True)
    X_test_scaled, y_test = preprocess_data(test_df, fit=False, scaler=scaler, columns=train_columns)

    pca = PCA(n_components=0.95, random_state=RANDOM_SEED, svd_solver="full")
    X_train_pca = pca.fit_transform(X_train_scaled)
    X_test_pca = pca.transform(X_test_scaled)

    model = XGBClassifier(
        n_estimators=200,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        eval_metric="logloss",
        random_state=RANDOM_SEED,
        n_jobs=1,
        tree_method="hist",
        verbosity=0,
    )
    model.fit(X_train_pca, y_train)

    y_pred = model.predict(X_test_pca)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed artifact saving (joblib.dump) and all plotting/extra metrics to eliminate unnecessary I/O and rendering overhead.
# - Added robust CSV parsing with a fallback delimiter/decimal strategy to avoid repeated manual fixes and ensure correct schema ingestion.
# - Ensured reproducibility by fixing a global seed and using deterministic model parameters where feasible (random_state, n_jobs=1).
# - Set XGBoost to use the histogram tree method for faster, more energy-efficient training while keeping model intent/behavior intact.
# - Avoided redundant computations by computing only what is required for the final accuracy output (skipped predict_proba/ROC pipeline).
# - Reduced overhead from logging by disabling XGBoost verbosity and removing all intermediate prints.