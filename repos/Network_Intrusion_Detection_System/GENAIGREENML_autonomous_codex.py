# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import warnings
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, mean_squared_error

warnings.filterwarnings("ignore")

DATA_PATH = "data/raw/Train.txt"
PROVIDED_HEADERS_STR = "0,tcp,ftp_data,SF,491,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0.00,0.00,0.00,0.00,1.00,0.00,0.00,150,25,0.17,0.03,0.17,0.00,0.00,0.00,0.05,0.00,normal,20"

def is_numeric_like(x):
    try:
        float(str(x).strip())
        return True
    except Exception:
        return False

def looks_like_header_names(headers):
    if not headers:
        return False
    numeric_like = sum(is_numeric_like(h) for h in headers)
    alpha_like = sum(any(ch.isalpha() for ch in str(h)) for h in headers)
    return (numeric_like / len(headers) < 0.2) and (alpha_like / len(headers) > 0.5)

def header_looks_like_data(columns):
    if not columns:
        return True
    numeric_like = sum(is_numeric_like(c) for c in columns)
    return numeric_like / len(columns) > 0.3

def robust_read_csv(path):
    used_sep = ','
    used_decimal = '.'
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
        used_sep = ';'
        used_decimal = ','
    if df.shape[1] <= 1:
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
            used_sep = ';'
            used_decimal = ','
        except Exception:
            df = pd.read_csv(path, sep=',')
            used_sep = ','
            used_decimal = '.'
    if header_looks_like_data(df.columns):
        df = pd.read_csv(path, sep=used_sep, decimal=used_decimal, header=None)
    return df

def normalize_columns(df):
    new_cols = []
    keep_mask = []
    for col in df.columns:
        col_str = str(col).strip()
        col_str = " ".join(col_str.split())
        if col_str.lower().startswith('unnamed') or col_str == '':
            keep_mask.append(False)
        else:
            keep_mask.append(True)
            new_cols.append(col_str)
    df = df.loc[:, keep_mask]
    seen = {}
    unique_cols = []
    for c in new_cols:
        if c in seen:
            seen[c] += 1
            unique_cols.append(f"{c}_{seen[c]}")
        else:
            seen[c] = 0
            unique_cols.append(c)
    df.columns = unique_cols
    return df

def assign_generic_if_needed(df, provided_headers):
    cols = list(df.columns)
    if all(isinstance(c, (int, np.integer)) for c in cols) or header_looks_like_data(cols):
        if provided_headers and len(provided_headers) == len(cols) and looks_like_header_names(provided_headers):
            df.columns = provided_headers
        else:
            df.columns = [f"col_{i}" for i in range(len(cols))]
    return df

def select_target(df):
    cols = df.columns.tolist()
    label_keywords = ['label', 'target', 'class', 'attack', 'outcome']
    for col in cols:
        lower = col.lower()
        if any(k in lower for k in label_keywords):
            return col
    obj_cols = [c for c in cols if df[c].dtype == object or str(df[c].dtype).startswith('category')]
    for col in obj_cols:
        vals = df[col].dropna().astype(str).str.lower().unique()
        if any(('normal' in v) or ('attack' in v) or ('anomaly' in v) for v in vals[:100]):
            return col
    if obj_cols:
        return obj_cols[-1]
    nunique = df.nunique(dropna=True)
    num_cols = [c for c in cols if c not in obj_cols]
    candidates = [c for c in num_cols if nunique.get(c, 0) > 1]
    if candidates:
        return sorted(candidates, key=lambda c: nunique[c])[0]
    return cols[-1] if cols else None

df = robust_read_csv(DATA_PATH)
provided_headers = [h.strip() for h in PROVIDED_HEADERS_STR.split(',') if h.strip() != '']
df = assign_generic_if_needed(df, provided_headers)
df = normalize_columns(df)
df = df.dropna(axis=1, how='all')
assert not df.empty

target_col = select_target(df)
if target_col is None or target_col not in df.columns:
    target_col = df.columns[-1]

df = df.replace([np.inf, -np.inf], np.nan)
df = df.dropna(subset=[target_col])
assert not df.empty

y_raw = df[target_col]
X = df.drop(columns=[target_col])

if X.shape[1] == 0:
    X = pd.DataFrame({'dummy': np.zeros(len(df))}, index=df.index)

X = X.copy()
for col in X.columns:
    if X[col].dtype == object or str(X[col].dtype).startswith('category'):
        converted = pd.to_numeric(X[col].astype(str).str.replace(',', '.'), errors='coerce')
        if converted.notna().sum() >= 0.8 * len(converted):
            X[col] = converted

n_samples = len(y_raw)
threshold_unique = min(20, max(2, int(0.05 * max(n_samples, 1))))
y_candidate_numeric = None
if y_raw.dtype == object or str(y_raw.dtype).startswith('category'):
    converted_y = pd.to_numeric(y_raw.astype(str).str.replace(',', '.'), errors='coerce')
    if converted_y.notna().sum() >= 0.9 * len(converted_y):
        y_candidate_numeric = converted_y

if y_candidate_numeric is not None:
    nunique_y = y_candidate_numeric.nunique(dropna=True)
    if nunique_y <= threshold_unique:
        task = 'classification'
    else:
        task = 'regression'
else:
    if y_raw.dtype == object or str(y_raw.dtype).startswith('category'):
        task = 'classification'
    else:
        nunique_y = y_raw.nunique(dropna=True)
        task = 'classification' if nunique_y <= threshold_unique else 'regression'

if task == 'classification':
    y = y_raw.astype(str)
    if y.nunique() < 2:
        task = 'regression'

if task == 'regression':
    if y_candidate_numeric is not None:
        y = y_candidate_numeric
    else:
        y = pd.to_numeric(y_raw, errors='coerce')
    if y.isna().all():
        y = y_raw.astype('category').cat.codes.astype(float)
    mask = y.notna()
    X = X.loc[mask]
    y = y.loc[mask]

assert len(X) > 0 and len(y) > 0

numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = [c for c in X.columns if c not in numeric_features]

if numeric_features:
    X[numeric_features] = X[numeric_features].apply(pd.to_numeric, errors='coerce')

if not numeric_features and not categorical_features:
    X = pd.DataFrame({'dummy': np.zeros(len(X))}, index=X.index)
    numeric_features = ['dummy']
    categorical_features = []

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler(with_mean=False))
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

transformers = []
if numeric_features:
    transformers.append(('num', numeric_transformer, list(numeric_features)))
if categorical_features:
    transformers.append(('cat', categorical_transformer, list(categorical_features)))

preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')

n_samples = len(y)
if n_samples < 2:
    X_train, X_test, y_train, y_test = X, X, y, y
else:
    test_size = 0.2 if n_samples >= 5 else 0.5
    stratify = None
    if task == 'classification':
        class_counts = y.value_counts()
        if class_counts.min() >= 2:
            stratify = y
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )

assert len(X_train) > 0 and len(X_test) > 0

if task == 'classification':
    model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    mse = mean_squared_error(y_test, y_pred)
    accuracy = 1.0 / (1.0 + mse)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) to keep computation CPU-friendly and energy efficient.
# - Applied minimal preprocessing with simple imputers and one-hot encoding only for categorical features to avoid heavy feature engineering.
# - StandardScaler uses with_mean=False to preserve sparsity and reduce memory overhead.
# - Regression fallback reports a bounded accuracy proxy defined as 1/(1+MSE) within [0,1].