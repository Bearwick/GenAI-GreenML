# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd


SEED = 42
np.random.seed(SEED)


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _to_2d_numeric_array(df: pd.DataFrame) -> np.ndarray:
    arr = df.to_numpy()
    if arr.ndim == 1:
        arr = arr.reshape(-1, 1)
    return np.asarray(arr, dtype=np.float64, order="C")


def sigmoid(z: np.ndarray) -> np.ndarray:
    z = np.clip(z, -500.0, 500.0)
    return 1.0 / (1.0 + np.exp(-z))


def initialize(dim: int) -> tuple[np.ndarray, float]:
    return np.zeros((dim, 1), dtype=np.float64), 0.0


def propagate(w: np.ndarray, b: float, X: np.ndarray, Y: np.ndarray) -> tuple[dict, float]:
    m = X.shape[1]
    A = sigmoid(w.T @ X + b)
    eps = 1e-15
    A = np.clip(A, eps, 1.0 - eps)
    cost = (-1.0 / m) * np.sum(Y * np.log(A) + (1.0 - Y) * np.log(1.0 - A))
    dZ = A - Y
    dw = (1.0 / m) * (X @ dZ.T)
    db = (1.0 / m) * float(np.sum(dZ))
    return {"dw": dw, "db": db}, float(cost)


def optimize(
    w: np.ndarray,
    b: float,
    X: np.ndarray,
    Y: np.ndarray,
    num_iters: int,
    alpha: float,
) -> tuple[dict, dict, list]:
    costs = []
    for i in range(int(num_iters)):
        grads, cost = propagate(w, b, X, Y)
        w -= alpha * grads["dw"]
        b -= alpha * grads["db"]
        if i % 100 == 0:
            costs.append(cost)
    return {"w": w, "b": b}, grads, costs


def predict(w: np.ndarray, b: float, X: np.ndarray) -> np.ndarray:
    A = sigmoid(w.T @ X + b)
    return (A > 0.5).astype(np.float64, copy=False)


def _load_xy(x_path: str, y_path: str) -> tuple[np.ndarray, np.ndarray]:
    X_df = _read_csv_robust(x_path)
    Y_df = _read_csv_robust(y_path)

    X = _to_2d_numeric_array(X_df).T

    Y_arr = _to_2d_numeric_array(Y_df)
    if Y_arr.shape[1] != 1 and Y_arr.shape[0] == 1:
        Y_arr = Y_arr.T
    Y = Y_arr.reshape(1, -1).astype(np.float64, copy=False)

    if X.shape[1] != Y.shape[1]:
        if X.shape[0] == Y.shape[1] and X.shape[1] == Y.shape[0]:
            Y = Y.T.reshape(1, -1)
        else:
            raise ValueError(f"Mismatch between X examples ({X.shape[1]}) and Y labels ({Y.shape[1]}).")
    return X, Y


def model(X_train: np.ndarray, Y_train: np.ndarray, num_iters: int, alpha: float) -> dict:
    w, b = initialize(X_train.shape[0])
    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iters=num_iters, alpha=alpha)

    w = parameters["w"]
    b = parameters["b"]

    X_test_path = "test_cancer_data.csv"
    Y_test_path = "test_cancer_data_y.csv"
    if os.path.exists(X_test_path) and os.path.exists(Y_test_path):
        X_test, Y_test = _load_xy(X_test_path, Y_test_path)
        y_prediction_test = predict(w, b, X_test)
    else:
        X_test, Y_test, y_prediction_test = None, None, None

    y_prediction_train = predict(w, b, X_train)

    return {
        "costs": costs,
        "Y_prediction_test": y_prediction_test,
        "Y_prediction_train": y_prediction_train,
        "w": w,
        "b": b,
        "learning_rate": alpha,
        "num_iterations": num_iters,
        "Y_train": Y_train,
        "Y_test": Y_test,
    }


def main() -> None:
    X_train, Y_train = _load_xy("cancer_data.csv", "cancer_data_y.csv")
    d = model(X_train, Y_train, num_iters=190500, alpha=0.000000065)

    y_pred = d["Y_prediction_test"] if d["Y_prediction_test"] is not None else d["Y_prediction_train"]
    y_true = d["Y_test"] if d["Y_test"] is not None else d["Y_train"]

    accuracy = 1.0 - float(np.mean(np.abs(y_pred - y_true)))
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused imports and all plotting to eliminate heavy dependencies and rendering overhead.
# - Implemented robust CSV parsing with a fallback delimiter/decimal to avoid repeated manual fixes and ensure reliable I/O.
# - Ensured consistent numeric dtype (float64) and contiguous arrays to reduce implicit conversions and improve BLAS efficiency.
# - Vectorized prediction thresholding (removed Python loops) to reduce interpreter overhead and runtime.
# - Simplified gradient computations by reusing dZ = (A - Y) and using @ for efficient matrix multiplies.
# - Added numerical stability (clipping z and A) to prevent overflow/log(0) without changing the task or expected outputs.
# - Avoided unnecessary list conversions and redundant confusion-matrix loops that didnâ€™t affect final required output.
# - Centralized data loading and shape normalization to minimize data movement and repeated transformations.
# - Set a fixed random seed for reproducibility (even though initialization is deterministic zeros).