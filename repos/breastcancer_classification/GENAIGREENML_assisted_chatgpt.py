# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
from typing import Tuple

import numpy as np
import pandas as pd


SEED = 42


def set_reproducible(seed: int = SEED) -> None:
    random.seed(seed)
    np.random.seed(seed)


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] <= 1:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        if df2.shape[1] > df.shape[1]:
            df = df2
    return df


def _to_feature_matrix(df: pd.DataFrame) -> np.ndarray:
    x = df.to_numpy(copy=False)
    if x.ndim == 1:
        x = x.reshape(-1, 1)
    x = np.asarray(x, dtype=np.float64, order="C")
    return x


def _to_label_row(df: pd.DataFrame) -> np.ndarray:
    y = df.to_numpy(copy=False)
    if y.ndim == 1:
        y = y.reshape(-1, 1)
    y = np.asarray(y, dtype=np.float64, order="C")
    if y.shape[1] != 1 and y.shape[0] == 1:
        y = y.T
    return y.T


def sigmoid(z: np.ndarray) -> np.ndarray:
    z = np.clip(z, -500.0, 500.0)
    return 1.0 / (1.0 + np.exp(-z))


def initialize(dim: int) -> Tuple[np.ndarray, float]:
    return np.zeros((dim, 1), dtype=np.float64), 0.0


def propagate(w: np.ndarray, b: float, X: np.ndarray, Y: np.ndarray) -> Tuple[dict, float]:
    m = X.shape[1]
    Z = w.T @ X + b
    A = sigmoid(Z)

    eps = 1e-15
    A_clipped = np.clip(A, eps, 1.0 - eps)
    cost = (-1.0 / m) * np.sum(Y * np.log(A_clipped) + (1.0 - Y) * np.log(1.0 - A_clipped))

    dZ = A - Y
    dw = (1.0 / m) * (X @ dZ.T)
    db = float((1.0 / m) * np.sum(dZ))
    return {"dw": dw, "db": db}, float(np.squeeze(cost))


def optimize(
    w: np.ndarray,
    b: float,
    X: np.ndarray,
    Y: np.ndarray,
    num_iters: int,
    alpha: float,
) -> Tuple[dict, dict, list]:
    costs = []
    for i in range(num_iters):
        grads, cost = propagate(w, b, X, Y)
        w -= alpha * grads["dw"]
        b -= alpha * grads["db"]
        if i % 100 == 0:
            costs.append(cost)
    return {"w": w, "b": b}, grads, costs


def predict(w: np.ndarray, b: float, X: np.ndarray) -> np.ndarray:
    A = sigmoid(w.T @ X + b)
    return (A > 0.5).astype(np.float64, copy=False)


def model(X_train: np.ndarray, Y_train: np.ndarray, num_iters: int, alpha: float) -> dict:
    w, b = initialize(X_train.shape[0])
    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iters, alpha)
    w, b = parameters["w"], parameters["b"]

    X_test_df = _read_csv_robust("test_cancer_data.csv")
    Y_test_df = _read_csv_robust("test_cancer_data_y.csv")
    X_test = _to_feature_matrix(X_test_df).T
    Y_test = _to_label_row(Y_test_df)

    y_prediction_train = predict(w, b, X_train)
    y_prediction_test = predict(w, b, X_test)

    accuracy = 100.0 - float(np.mean(np.abs(y_prediction_test - Y_test)) * 100.0)

    d = {
        "costs": costs,
        "Y_prediction_test": y_prediction_test,
        "Y_prediction_train": y_prediction_train,
        "w": w,
        "b": b,
        "learning_rate": alpha,
        "num_iterations": num_iters,
    }
    return d, accuracy


def main() -> None:
    set_reproducible(SEED)

    X_train_df = _read_csv_robust("cancer_data.csv")
    Y_train_df = _read_csv_robust("cancer_data_y.csv")

    X_train = _to_feature_matrix(X_train_df).T
    Y_train = _to_label_row(Y_train_df)

    _, accuracy = model(X_train, Y_train, num_iters=190500, alpha=0.000000065)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused imports and all plotting/interactive output to reduce overhead and avoid unnecessary work.
# - Added robust CSV loader with delimiter/decimal fallback to prevent re-parsing failures and ensure end-to-end execution.
# - Avoided redundant pandas->numpy copies via to_numpy(copy=False) and enforced a single float64 conversion for stable numerics.
# - Vectorized prediction thresholding to eliminate Python loops and per-element branching.
# - Simplified training loop by removing cost-print logic while keeping the same optimization steps and recorded costs cadence.
# - Improved numerical stability (clipping in sigmoid and log inputs) to reduce risk of NaNs/retries without changing intent.
# - Centralized seeding for reproducibility and deterministic behavior across runs.