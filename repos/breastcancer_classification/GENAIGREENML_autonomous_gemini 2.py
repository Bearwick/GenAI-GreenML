# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_breast_cancer

def execute_energy_efficient_pipeline():
    data = load_breast_cancer()
    X = data.data
    y = data.target

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = LogisticRegression(
        penalty='l2',
        C=1.0,
        solver='lbfgs',
        max_iter=100,
        tol=1e-4,
        n_jobs=1
    )
    
    model.fit(X_train_scaled, y_train)

    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    execute_energy_efficient_pipeline()

# DESIGN JUSTIFICATION FOR ENERGY EFFICIENCY:
# 1. Algorithmic Efficiency: Replaced manual gradient descent (190,500 iterations) with the LBFGS solver. 
#    LBFGS is a quasi-Newton method that converges significantly faster (usually <50 iterations), 
#    drastically reducing CPU cycles and energy consumption.
# 2. Preprocessing for Convergence: Implemented StandardScaler. Linear models are sensitive to feature scales; 
#    standardization ensures the cost function is well-conditioned, allowing for larger steps and fewer 
#    computations during training.
# 3. Model Complexity: Selected Logistic Regression instead of deep learning or ensemble methods. 
#    For tabular datasets of this scale (~569 samples), linear models achieve near-optimal performance 
#    with minimal memory footprint and zero GPU requirement.
# 4. Resource Management: Set n_jobs=1 to avoid the overhead of process forking/multiprocessing, 
#    which is more efficient for small datasets where communication overhead exceeds computation time.
# 5. Library Optimization: Used Scikit-Learn's implementation, which relies on optimized C and 
#    Fortran backends (BLAS/LAPACK), ensuring the most efficient execution on the CPU.