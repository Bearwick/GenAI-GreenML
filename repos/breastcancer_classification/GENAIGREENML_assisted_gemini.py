# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import numpy as np
    import pandas as pd

def solve():
    try:
        tr_x_raw = pd.read_csv("cancer_data.csv", engine='c', memory_map=True)
        tr_y_raw = pd.read_csv("cancer_data_y.csv", engine='c', memory_map=True)
        te_x_raw = pd.read_csv("test_cancer_data.csv", engine='c', memory_map=True)
        te_y_raw = pd.read_csv("test_cancer_data_y.csv", engine='c', memory_map=True)
    except Exception:
        return

    x_train = tr_x_raw.values.T
    y_train = tr_y_raw.values.T
    x_test = te_x_raw.values.T
    y_test = te_y_raw.values.T

    dim, m = x_train.shape
    w_t = np.zeros((1, dim))
    b = 0.0
    iters = 190500
    alpha_m = 0.000000065 / m
    
    x_train_flat = x_train.T
    
    for _ in range(iters):
        z = np.dot(w_t, x_train) + b
        a = 1.0 / (1.0 + np.exp(-z))
        dz = a - y_train
        w_t -= alpha_m * np.dot(dz, x_train_flat)
        b -= alpha_m * np.sum(dz)
        
    a_test = 1.0 / (1.0 + np.exp(-(np.dot(w_t, x_test) + b)))
    accuracy = np.mean((a_test > 0.5) == y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# OPTIMIZATION SUMMARY
# 1. Vectorized the entire training and prediction process using NumPy to eliminate slow Python loops.
# 2. Removed the computationally expensive logarithmic cost function calculation from the iteration loop.
# 3. Streamlined data ingestion using the optimized C engine for pandas and memory mapping for reduced I/O overhead.
# 4. Pre-calculated training constants (alpha/m) and pre-transposed input matrices to reduce redundant arithmetic.
# 5. Minimized memory footprint by avoiding dictionary-based parameter storage and intermediate list conversions.
# 6. Replaced manual metric calculation loops with efficient NumPy broadcasting to compute accuracy directly from arrays.