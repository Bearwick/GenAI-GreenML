# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: assisted

import numpy as np
import pandas as pd

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def initialize(dim):
    return np.zeros((dim, 1)), 0.0

def optimize(w, b, X, Y, num_iters, alpha, print_cost=False):
    m = X.shape[1]
    for i in range(num_iters):
        A = 1 / (1 + np.exp(-(np.dot(w.T, X) + b)))
        
        dz = A - Y
        dw = (1 / m) * np.dot(X, dz.T)
        db = (1 / m) * np.sum(dz)

        w -= alpha * dw
        b -= alpha * db

        if print_cost and i % 10000 == 0:
            cost = (-1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))
            print(f"Cost after iteration {i}: {cost:f}")

    return w, b

def predict(w, b, X):
    A = sigmoid(np.dot(w.T, X) + b)
    return (A > 0.5).astype(float)

def calculate_metrics(Y_true, Y_pred, set_name):
    accuracy = (100 - np.mean(np.abs(Y_pred - Y_true)) * 100)
    
    tp = np.sum((Y_pred == 1) & (Y_true == 1))
    tn = np.sum((Y_pred == 0) & (Y_true == 0))
    fp = np.sum((Y_pred == 1) & (Y_true == 0))
    fn = np.sum((Y_pred == 0) & (Y_true == 1))
    
    tpr = (tp / (tp + fn) * 100) if (tp + fn) > 0 else 0
    fpr = (fp / (fp + tn) * 100) if (fp + tn) > 0 else 0
    precision = (tp / (tp + fp) * 100) if (tp + fp) > 0 else 0
    
    print(f"\n{set_name} accuracy: {accuracy} %")
    print(f"On {set_name} set:\nTrue Positive:  {tp}")
    print(f"True Negative:  {tn}")
    print(f"False Negative:  {fn}")
    print(f"False Positive:  {fp}")
    print(f"True Positive Rate / Recall: {tpr:.2f}%")
    print(f"Precision: {precision:.2f}%")
    print(f"False Positive Rate / Fallout: {fpr:.2f}%")
    return accuracy / 100.0

def run_model():
    X_train = pd.read_csv("cancer_data.csv").values.T
    Y_train = pd.read_csv("cancer_data_y.csv").values.T
    X_test = pd.read_csv("test_cancer_data.csv").values.T
    Y_test = pd.read_csv("test_cancer_data_y.csv").values.T

    w, b = initialize(X_train.shape[0])
    w, b = optimize(w, b, X_train, Y_train, num_iters=190500, alpha=0.000000065, print_cost=False)

    y_pred_train = predict(w, b, X_train)
    y_pred_test = predict(w, b, X_test)

    calculate_metrics(Y_train, y_pred_train, "training")
    accuracy_test = calculate_metrics(Y_test, y_pred_test, "Test")
    
    print(f"ACCURACY={accuracy_test:.6f}")

if __name__ == "__main__":
    run_model()

# Applied Optimizations:
# 1. Vectorized Metrics: Replaced manual loops and list conversions for TP, TN, FP, FN with NumPy boolean indexing, significantly reducing runtime and memory overhead.
# 2. Eliminated Redundant Computation: Inlined the propagation step within the optimization loop to avoid function call overhead and removed the cost calculation from iterations where it wasn't required.
# 3. Memory Footprint Reduction: Removed the storage of the 'costs' list and the 'params'/'grads' dictionaries which were bloating memory across 190,500 iterations.
# 4. Streamlined I/O: Simplified data loading and immediate conversion to NumPy arrays to minimize data movement.
# 5. Simplified Logic: Vectorized the prediction thresholding using NumPy comparison instead of an explicit loop.
# 6. Green Principles: Reduced energy consumption by minimizing CPU cycles (vectorization) and removing unnecessary visualizations/logging.
