# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import numpy as np
import pandas as pd

def load_data(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    return df.to_numpy()

def train_model(X, Y, iters, alpha):
    n_feat, m = X.shape
    w = np.zeros((n_feat, 1))
    b = 0.0
    lr_m = alpha / m
    for _ in range(iters):
        z = np.dot(w.T, X) + b
        A = 1.0 / (1.0 + np.exp(-z))
        dz = A - Y
        w -= lr_m * np.dot(X, dz.T)
        b -= lr_m * np.sum(dz)
    return w, b

def predict(w, b, X):
    z = np.dot(w.T, X) + b
    A = 1.0 / (1.0 + np.exp(-z))
    return (A > 0.5).astype(float)

def run():
    np.random.seed(42)
    try:
        X_train = load_data("cancer_data.csv").T
        Y_train = load_data("cancer_data_y.csv").T
        X_test = load_data("test_cancer_data.csv").T
        Y_test = load_data("test_cancer_data_y.csv").T
    except Exception:
        return
    
    w, b = train_model(X_train, Y_train, 190500, 0.000000065)
    preds = predict(w, b, X_test)
    accuracy = np.mean(preds == Y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run()

# Optimization Summary
# 1. Vectorized all computations using NumPy to replace inefficient Python for-loops in training, prediction, and metric evaluation.
# 2. Combined forward and backward propagation into a single lean training loop, reducing memory access and overhead.
# 3. Eliminated redundant cost function calculations and intermediate dictionary creations within the optimization loop.
# 4. Pre-calculated constant factors (alpha/m) outside the iteration loop to minimize repetitive arithmetic operations.
# 5. Removed all visualization libraries, plotting code, and diagnostic logging to significantly reduce CPU and memory footprint.
# 6. Implemented robust data loading with fallback parsing and avoided redundant data re-loading by passing data directly.
# 7. Optimized memory usage by working with NumPy arrays and avoiding unnecessary intermediate data structures or copies.