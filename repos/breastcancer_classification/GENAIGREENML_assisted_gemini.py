# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: assisted

import numpy as np
import pandas as pd

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def initialize_params(dim):
    return np.zeros((dim, 1)), 0.0

def optimize(w, b, X, Y, num_iters, alpha, print_cost=False):
    m = X.shape[1]
    
    for i in range(num_iters):
        A = 1 / (1 + np.exp(-(np.dot(w.T, X) + b)))
        
        dz = A - Y
        dw = np.dot(X, dz.T) / m
        db = np.sum(dz) / m
        
        w -= alpha * dw
        b -= alpha * db

        if print_cost and i % 10000 == 0:
            cost = -np.mean(Y * np.log(A) + (1 - Y) * np.log(1 - A))
            print(f"Cost after iteration {i}: {cost:f}")

    return w, b

def get_metrics(w, b, X, Y, set_name):
    m = X.shape[1]
    A = sigmoid(np.dot(w.T, X) + b)
    y_pred = (A > 0.5).astype(float)
    
    accuracy = (1 - np.mean(np.abs(y_pred - Y))) * 100
    
    tp = np.sum((y_pred == 1) & (Y == 1))
    tn = np.sum((y_pred == 0) & (Y == 0))
    fp = np.sum((y_pred == 1) & (Y == 0))
    fn = np.sum((y_pred == 0) & (Y == 1))
    
    tpr = (tp / (tp + fn) * 100) if (tp + fn) > 0 else 0
    fpr = (fp / (fp + tn) * 100) if (fp + tn) > 0 else 0
    precision = (tp / (tp + fp) * 100) if (tp + fp) > 0 else 0
    
    print(f"\nOn {set_name} set:")
    print(f"True Positive:  {tp}")
    print(f"True Negative:  {tn}")
    print(f"False Negative: {fn}")
    print(f"False Positive: {fp}")
    print(f"True Positive Rate / Recall: {tpr:.2f}%")
    print(f"Precision: {precision:.2f}%")
    print(f"False Positive Rate / Fallout: {fpr:.2f}%")
    print(f"{set_name.lower()} accuracy: {accuracy}%")
    
    return accuracy / 100

def run_model():
    try:
        X_train = pd.read_csv("cancer_data.csv", header=None).values.T
        Y_train = pd.read_csv("cancer_data_y.csv", header=None).values.T
        X_test = pd.read_csv("test_cancer_data.csv", header=None).values.T
        Y_test = pd.read_csv("test_cancer_data_y.csv", header=None).values.T
    except FileNotFoundError:
        return

    w, b = initialize_params(X_train.shape[0])
    
    w, b = optimize(w, b, X_train, Y_train, num_iters=190500, alpha=0.000000065, print_cost=True)
    
    get_metrics(w, b, X_train, Y_train, "Training")
    test_acc = get_metrics(w, b, X_test, Y_test, "Test")
    
    print(f"ACCURACY={test_acc:.6f}")

if __name__ == "__main__":
    run_model()

"""
OPTIMIZATIONS APPLIED:
1. Vectorization: Removed manual Python loops for prediction and confusion matrix calculations using NumPy boolean indexing, significantly reducing CPU cycles.
2. Computational Efficiency: Moved the expensive log-based cost calculation inside a conditional block so it only executes when printing is required, saving significant floating-point operations over 190,500 iterations.
3. Memory Footprint: Eliminated redundant list structures (like 'costs') and dictionary packing/unpacking within the optimization loop to reduce memory overhead and garbage collection frequency.
4. I/O Optimization: Streamlined data loading by converting pandas DataFrames directly to NumPy arrays and minimizing unnecessary data copies or intermediate transpositions.
5. Inlining: Simplified the propagation step by integrating it directly into the optimization loop, reducing function call overhead.
"""
