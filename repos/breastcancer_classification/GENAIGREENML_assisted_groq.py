# Generated by generate_llm_code_2.py
# LLM: groq
# Mode: assisted

import numpy as np
import pandas as pd

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def initialize(dim):
    return np.zeros((dim, 1)), 0.0

def propagate(w, b, X, Y):
    m = X.shape[1]
    A = sigmoid(np.dot(w.T, X) + b)
    cost = -np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / m
    dw = np.dot(X, (A - Y).T) / m
    db = np.sum(A - Y) / m
    return {"dw": dw, "db": db}, cost

def optimize(w, b, X, Y, num_iters, alpha, print_cost=False):
    for i in range(num_iters):
        grads, cost = propagate(w, b, X, Y)
        w -= alpha * grads["dw"]
        b -= alpha * grads["db"]
        if print_cost and i % 100 == 0:
            print(f"Cost after iteration {i}: {cost:.6f}")
    return {"w": w, "b": b}

def predict(w, b, X):
    A = sigmoid(np.dot(w.T, X) + b)
    return (A > 0.5).astype(int)

def evaluate(y_pred, Y):
    tp = np.sum((y_pred == 1) & (Y == 1))
    tn = np.sum((y_pred == 0) & (Y == 0))
    fp = np.sum((y_pred == 1) & (Y == 0))
    fn = np.sum((y_pred == 0) & (Y == 1))
    accuracy = np.mean(y_pred == Y)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "fpr": fpr,
        "tp": tp,
        "tn": tn,
        "fp": fp,
        "fn": fn
    }

def model(X_train, Y_train, X_test, Y_test, num_iters=2000, alpha=0.5, print_cost=False):
    w, b = initialize(X_train.shape[0])
    params = optimize(w, b, X_train, Y_train, num_iters, alpha, print_cost)
    w, b = params["w"], params["b"]
    y_pred_train = predict(w, b, X_train)
    y_pred_test = predict(w, b, X_test)
    train_metrics = evaluate(y_pred_train, Y_train)
    test_metrics = evaluate(y_pred_test, Y_test)
    print(f"TRAIN ACCURACY={train_metrics['accuracy']:.6f}")
    print(f"TEST ACCURACY={test_metrics['accuracy']:.6f}")
    return params, train_metrics, test_metrics

def main():
    # Load datasets
    X_train = pd.read_csv("cancer_data.csv", header=None).values.T
    Y_train = pd.read_csv("cancer_data_y.csv", header=None).values.T
    X_test = pd.read_csv("test_cancer_data.csv", header=None).values.T
    Y_test = pd.read_csv("test_cancer_data_y.csv", header=None).values.T

    # Train model
    params, train_metrics, test_metrics = model(
        X_train, Y_train, X_test, Y_test,
        num_iters=190500, alpha=6.5e-8, print_cost=False
    )

if __name__ == "__main__":
    main()
# Optimizations applied:
# 1. Vectorized prediction and evaluation to avoid Python loops.
# 2. Removed matplotlib and plotting to reduce I/O overhead.
# 3. Simplified cost storage; only necessary metrics printed.
# 4. Used header=None to skip header parsing, reducing memory.
# 5. Eliminated unnecessary intermediate variables and prints.
# 6. Maintained reproducibility by using deterministic operations.