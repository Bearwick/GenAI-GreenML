# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load the breast cancer dataset (matches the 30-feature header provided)
data = load_breast_cancer()
X, y = data.data, data.target

# Lightweight preprocessing: standard scaling for logistic regression convergence
scaler = StandardScaler()

# Use stratified k-fold for robust evaluation on small dataset
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

accuracies = []
for train_idx, test_idx in skf.split(X, y):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Fit scaler on training data only to prevent data leakage
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Logistic Regression: lightweight, interpretable, excellent for this task
    # Using saga solver for efficiency, L2 regularization for generalization
    model = LogisticRegression(
        C=1.0,
        solver='lbfgs',
        max_iter=500,
        random_state=42,
        n_jobs=1  # Single core for energy efficiency
    )
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    accuracies.append(accuracy_score(y_test, y_pred))

accuracy = np.mean(accuracies)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Model choice: Logistic Regression — ideal for binary classification on 30 features
#    with 569 samples. Trains in milliseconds on CPU, no GPU needed.
# 2. Preprocessing: StandardScaler only — minimal overhead, sufficient for linear models.
# 3. Evaluation: 5-fold stratified CV provides robust accuracy estimate without
#    wasting data on a held-out set for this small dataset.
# 4. No deep learning, no ensemble methods — the dataset is small and well-separable,
#    making simple linear models both accurate (~96-97%) and energy-efficient.
# 5. Single-threaded execution (n_jobs=1) to minimize energy consumption.
# 6. LBFGS solver chosen for efficiency on small datasets with L2 penalty.