# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

data = load_breast_cancer()
X, y = data.data, data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = LogisticRegression(
    penalty='l2',
    C=1.0,
    solver='liblinear',
    tol=1e-4,
    max_iter=100,
    random_state=42
)

model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Algorithm Choice: Logistic Regression was selected as a highly energy-efficient linear model. 
#    It offers a minimal memory footprint and low CPU cycle requirements compared to ensemble methods or neural networks.
# 2. Solver Selection: The 'liblinear' solver is optimized for small-to-medium datasets, 
#    providing fast convergence and reducing the total execution time.
# 3. Scaling for Efficiency: StandardScaler was used to normalize feature scales, 
#    which accelerates the convergence of the gradient-based optimization, further saving energy.
# 4. Resource Management: The solution utilizes scikit-learn's optimized C-based implementations, 
#    minimizing Python-level overhead and avoiding the need for GPU acceleration.
# 5. Model Complexity: By using a single linear model instead of an ensemble (like Random Forest or XGBoost), 
#    we significantly reduce the number of floating-point operations (FLOPs) during both training and inference.