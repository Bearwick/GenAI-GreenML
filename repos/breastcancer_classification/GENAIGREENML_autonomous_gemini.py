# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: autonomous

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def execute_pipeline():
    X, y = load_breast_cancer(return_X_y=True)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    
    model = LogisticRegression(
        penalty='l2',
        C=1.0,
        solver='lbfgs',
        max_iter=1000,
        tol=1e-4
    )
    
    model.fit(X_train, y_train)
    
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    execute_pipeline()

"""
JUSTIFICATION FOR ENERGY-EFFICIENT DESIGN:
1. Algorithm Selection: Logistic Regression was chosen over deep learning or complex ensembles. For tabular datasets with ~30 features, linear models provide high performance with minimal floating-point operations (FLOPs).
2. Solver Efficiency: The 'lbfgs' solver is utilized as it uses limited-memory second-order optimization. This converges in significantly fewer iterations than the manual gradient descent approach (which required 190,000+ iterations), drastically reducing CPU time and energy consumption.
3. Preprocessing: StandardScaler ensures that the loss surface is well-conditioned. This allows the optimizer to find the global minimum faster, further decreasing computational overhead.
4. Minimal Resource Footprint: The solution relies on highly optimized C-extensions via scikit-learn/numpy and runs entirely on a single CPU core without the need for high-power accelerators like GPUs.
"""
