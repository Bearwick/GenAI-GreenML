# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import pandas as pd


def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))


def model(X_train, Y_train, X_test, Y_test, num_iters=190500, alpha=0.000000065):
    m = X_train.shape[1]
    w = np.zeros((X_train.shape[0], 1))
    b = 0.0

    for i in range(num_iters):
        A = sigmoid(np.dot(w.T, X_train) + b)
        dw = (1.0 / m) * np.dot(X_train, (A - Y_train).T)
        db = (1.0 / m) * np.sum(A - Y_train)
        w -= alpha * dw
        b -= alpha * db

    A_test = sigmoid(np.dot(w.T, X_test) + b)
    y_prediction_test = (A_test > 0.5).astype(np.float64)

    accuracy = 1.0 - np.mean(np.abs(y_prediction_test - Y_test))
    return accuracy


def main():
    train_x = np.array(pd.read_csv("cancer_data.csv"))
    train_y = np.array(pd.read_csv("cancer_data_y.csv"))
    test_x = np.array(pd.read_csv("test_cancer_data.csv"))
    test_y = np.array(pd.read_csv("test_cancer_data_y.csv"))

    accuracy = model(train_x.T, train_y.T, test_x.T, test_y.T, num_iters=190500, alpha=0.000000065)
    print(f"ACCURACY={accuracy:.6f}")


main()

# OPTIMIZATION SUMMARY
# 1. Removed all prints, plots, and saving of model artifacts.
# 2. Removed redundant dictionary packing/unpacking for grads and params.
# 3. Inlined propagate and optimize into a single training loop to avoid repeated dict creation and function call overhead.
# 4. Replaced per-element prediction loop with vectorized comparison (A > 0.5).
# 5. Removed confusion matrix loops (not needed for accuracy output).
# 6. Loaded test data once inside model instead of creating separate function; reduced data movement.
# 7. Removed cost tracking list since we no longer need to record or plot costs.
# 8. Used in-place subtraction (w -= ...) to reduce memory allocation.
# 9. Kept same algorithm, hyperparameters, and dataset to preserve original behavior.