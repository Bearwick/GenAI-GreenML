# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, r2_score

dataset_headers_str = "17.99,10.38,122.8,1001,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871,1.095,0.9053,8.589,153.4,0.006399,0.04904,0.05373,0.01587,0.03003,0.006193,25.38,17.33,184.6,2019,0.1622,0.6656,0.7119,0.2654,0.4601,0.1189"

def is_numeric_string(s):
    try:
        float(str(s))
        return True
    except Exception:
        return False

def make_unique(cols):
    seen = {}
    unique_cols = []
    for c in cols:
        if c in seen:
            seen[c] += 1
            unique_cols.append(f"{c}_{seen[c]}")
        else:
            seen[c] = 0
            unique_cols.append(c)
    return unique_cols

def read_csv_robust(file_path):
    sep = ','
    decimal = '.'
    try:
        df = pd.read_csv(file_path)
    except Exception:
        df = pd.read_csv(file_path, sep=';', decimal=',')
        sep = ';'
        decimal = ','
    def looks_wrong(df_local):
        if df_local.shape[1] == 1:
            col0 = str(df_local.columns[0])
            if ';' in col0:
                return True
            if len(df_local) > 0:
                val = df_local.iloc[0, 0]
                if isinstance(val, str) and ';' in val:
                    return True
        if any(';' in str(c) for c in df_local.columns):
            return True
        return False
    if looks_wrong(df):
        try_df = pd.read_csv(file_path, sep=';', decimal=',')
        if try_df.shape[1] > df.shape[1]:
            df = try_df
            sep = ';'
            decimal = ','
    cols = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
    df.columns = make_unique(cols)
    df = df.loc[:, [c for c in df.columns if c and not str(c).lower().startswith('unnamed')]]
    dataset_headers = [x.strip() for x in dataset_headers_str.split(',') if x.strip()]
    cols_str = [str(c).strip() for c in df.columns]
    headerless = False
    if len(cols_str) > 0:
        numeric_ratio = sum(is_numeric_string(c) for c in cols_str) / len(cols_str)
        if numeric_ratio > 0.5:
            headerless = True
        if len(dataset_headers) == len(cols_str):
            if all(cols_str[i] == dataset_headers[i] for i in range(len(cols_str))):
                headerless = True
    if headerless:
        df2 = pd.read_csv(file_path, header=None, sep=sep, decimal=decimal)
        if df2.shape[1] >= df.shape[1]:
            df = df2
            df.columns = [f"col_{i}" for i in range(df.shape[1])]
    return df

target_keywords = ['target', 'label', 'class', 'y', 'diagnosis', 'outcome', 'response', 'result', 'status']

def has_target_column(columns):
    for col in columns:
        col_lower = str(col).lower()
        if any(k == col_lower or k in col_lower for k in target_keywords):
            return True
    return False

def select_target(df_local):
    for col in df_local.columns:
        col_lower = str(col).lower()
        if any(k == col_lower or k in col_lower for k in target_keywords):
            return col
    n = len(df_local)
    nunique = df_local.nunique(dropna=True)
    thresh = max(2, min(20, int(0.2 * n)))
    candidates = [c for c in df_local.columns if 1 < nunique[c] <= thresh]
    if candidates:
        candidates.sort(key=lambda c: nunique[c])
        return candidates[0]
    return df_local.columns[-1]

csv_files = [f for f in os.listdir('.') if f.lower().endswith('.csv')]
if not csv_files:
    raise FileNotFoundError("No CSV file found in current directory.")

dfs = {}
for f in csv_files:
    try:
        dfs[f] = read_csv_robust(f)
    except Exception:
        continue

if not dfs:
    raise FileNotFoundError("No readable CSV file found.")

base_file = max(dfs.keys(), key=lambda f: dfs[f].shape[0] * dfs[f].shape[1])
df = dfs[base_file]

if not has_target_column(df.columns):
    n_rows = df.shape[0]
    candidate_labels = []
    for f, d in dfs.items():
        if f == base_file:
            continue
        if d.shape[0] == n_rows and d.shape[1] == 1:
            nunique = d.iloc[:, 0].nunique(dropna=True)
            if nunique > 1 and nunique <= max(10, int(0.1 * n_rows)):
                candidate_labels.append((nunique, f, d))
    if candidate_labels:
        candidate_labels.sort(key=lambda x: x[0])
        label_df = candidate_labels[0][2].reset_index(drop=True)
        df = df.reset_index(drop=True)
        df['target'] = label_df.iloc[:, 0].values

df.replace([np.inf, -np.inf], np.nan, inplace=True)

assert df.shape[0] > 0
assert df.shape[1] > 0

for col in df.columns:
    if df[col].dtype == object:
        series = df[col]
        series_str = series.astype(str)
        series_str = series_str.where(~series.isna(), np.nan)
        series_str = series_str.str.strip()
        converted = pd.to_numeric(series_str.str.replace(',', '.', regex=False), errors='coerce')
        if converted.notna().sum() >= max(1, int(0.6 * len(series_str))):
            df[col] = converted
        else:
            df[col] = series_str
    else:
        df[col] = pd.to_numeric(df[col], errors='coerce')

df = df.dropna(axis=1, how='all')
assert df.shape[1] > 0

target_col = select_target(df)

df = df[df[target_col].notna()]
assert len(df) > 0

y = df[target_col]
X = df.drop(columns=[target_col])

if X.shape[1] == 0:
    X = pd.DataFrame({'index': np.arange(len(df))})

nunique_X = X.nunique(dropna=True)
constant_cols = [c for c in X.columns if nunique_X[c] <= 1]
if constant_cols:
    X = X.drop(columns=constant_cols)

id_cols = [c for c in X.columns if 'id' in str(c).lower()]
if id_cols:
    X = X.drop(columns=id_cols)

if X.shape[1] == 0:
    X = pd.DataFrame({'index': np.arange(len(df))})

numeric_features = X.select_dtypes(include=['number', 'bool']).columns.tolist()
categorical_features = [c for c in X.columns if c not in numeric_features]

if categorical_features:
    high_card = [c for c in categorical_features if X[c].nunique(dropna=True) > max(50, int(0.5 * len(X)))]
    if high_card:
        X = X.drop(columns=high_card)
        numeric_features = X.select_dtypes(include=['number', 'bool']).columns.tolist()
        categorical_features = [c for c in X.columns if c not in numeric_features]

if X.shape[1] == 0:
    X = pd.DataFrame({'index': np.arange(len(df))})
    numeric_features = ['index']
    categorical_features = []

y_numeric = pd.api.types.is_numeric_dtype(y)
y_unique = y.nunique(dropna=True)
if (not y_numeric) or (y_unique <= max(10, int(0.1 * len(y)))):
    task = 'classification'
else:
    task = 'regression'

if task == 'classification':
    if not y_numeric:
        le = LabelEncoder()
        y_used = le.fit_transform(y.astype(str))
    else:
        y_num = pd.to_numeric(y, errors='coerce')
        mask = y_num.notna()
        if mask.sum() < len(y_num):
            X = X.loc[mask]
            y_num = y_num.loc[mask]
        y_used = y_num.values
else:
    y_num = pd.to_numeric(y, errors='coerce')
    mask = y_num.notna()
    if mask.sum() < len(y_num):
        X = X.loc[mask]
        y_num = y_num.loc[mask]
    y_used = y_num.values

X = X.reset_index(drop=True)
y_used = np.asarray(y_used)
assert len(y_used) > 0

n = len(y_used)
if n < 2:
    X_train, X_test, y_train, y_test = X, X, y_used, y_used
else:
    test_size = 0.2 if n > 10 else 0.5
    stratify = None
    if task == 'classification':
        unique_classes, counts = np.unique(y_used, return_counts=True)
        if len(unique_classes) > 1 and counts.min() >= 2 and int(n * test_size) >= len(unique_classes):
            stratify = y_used
    X_train, X_test, y_train, y_test = train_test_split(X, y_used, test_size=test_size, random_state=42, stratify=stratify)

assert len(y_train) > 0 and len(y_test) > 0

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler(with_mean=False))
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])
transformers = []
if numeric_features:
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    transformers.append(('cat', categorical_transformer, categorical_features))
preprocessor = ColumnTransformer(transformers=transformers, remainder='drop') if transformers else 'passthrough'

if task == 'classification':
    if len(np.unique(y_train)) < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])

try:
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
except Exception:
    if task == 'classification':
        fallback_model = DummyClassifier(strategy='most_frequent')
    else:
        fallback_model = DummyRegressor(strategy='mean')
    clf = Pipeline(steps=[('preprocess', preprocessor), ('model', fallback_model)])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    if np.isnan(r2):
        r2 = 0.0
    accuracy = max(0.0, min(1.0, r2))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Lightweight logistic/ridge models chosen for CPU-efficient baselines with deterministic train/test split.
# - Robust CSV loading handles alternate delimiters, headerless numeric headers, and separate label files when present.
# - Preprocessing uses simple imputers and minimal scaling/one-hot encoding via ColumnTransformer for reproducibility.
# - Constant/ID-like and high-cardinality categorical features are pruned to reduce feature size and energy use.
# - Regression output uses clipped R2 as a bounded [0,1] accuracy proxy for stable reporting.