# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import sys
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _load_dataset() -> pd.DataFrame:
    candidates = [
        "data.csv",
        "dataset.csv",
        "train.csv",
        "breast_cancer.csv",
        "BreastCancer.csv",
        "wdbc.csv",
        "wdbc.data",
        "input.csv",
    ]
    for p in candidates:
        if os.path.exists(p):
            return pd.read_csv(p)

    for name in os.listdir("."):
        if name.lower().endswith(".csv"):
            try:
                df = pd.read_csv(name)
                if df.shape[0] > 10 and df.shape[1] >= 5:
                    return df
            except Exception:
                continue

    raise FileNotFoundError("No suitable CSV dataset found in the current directory.")


def _infer_target_column(df: pd.DataFrame) -> str:
    lower_cols = {c.lower(): c for c in df.columns}

    for key in ("target", "label", "class", "diagnosis", "y"):
        if key in lower_cols:
            return lower_cols[key]

    if "id" in lower_cols and df.shape[1] > 2:
        candidate_cols = [c for c in df.columns if c != lower_cols["id"]]
    else:
        candidate_cols = list(df.columns)

    for c in candidate_cols:
        s = df[c]
        if s.dtype == object or str(s.dtype).startswith("category"):
            nun = s.nunique(dropna=True)
            if 2 <= nun <= 20:
                return c

    best_col = None
    best_nunique = None
    for c in candidate_cols:
        nun = df[c].nunique(dropna=True)
        if best_nunique is None or nun < best_nunique:
            best_nunique = nun
            best_col = c
    return best_col


def _encode_target(y: pd.Series) -> np.ndarray:
    if y.dtype == object or str(y.dtype).startswith("category"):
        vals = pd.Series(y).astype(str).str.strip()
        vlow = vals.str.lower()
        mapping = {
            "m": 1, "malignant": 1, "1": 1, "true": 1, "yes": 1, "pos": 1, "positive": 1,
            "b": 0, "benign": 0, "0": 0, "false": 0, "no": 0, "neg": 0, "negative": 0,
        }
        mapped = vlow.map(mapping)
        if mapped.isna().any():
            uniq = vlow.dropna().unique()
            uniq_sorted = sorted(uniq.tolist())
            if len(uniq_sorted) == 2:
                fallback = {uniq_sorted[0]: 0, uniq_sorted[1]: 1}
                mapped = vlow.map(fallback)
            else:
                mapped = pd.factorize(vlow)[0]
        return mapped.to_numpy(dtype=int)
    else:
        yy = pd.to_numeric(y, errors="coerce")
        if yy.isna().any():
            yy = pd.Series(pd.factorize(y)[0], index=y.index)
        uniq = np.unique(yy.dropna().to_numpy())
        if len(uniq) == 2 and set(uniq.tolist()) != {0, 1}:
            minv, maxv = float(np.min(uniq)), float(np.max(uniq))
            yy = (yy == maxv).astype(int)
        return yy.to_numpy(dtype=int)


def main() -> None:
    df = _load_dataset()

    target_col = _infer_target_column(df)
    y_raw = df[target_col]
    X_raw = df.drop(columns=[target_col])

    for col in list(X_raw.columns):
        if X_raw[col].dtype == object or str(X_raw[col].dtype).startswith("category"):
            X_raw = X_raw.drop(columns=[col])

    X = X_raw.apply(pd.to_numeric, errors="coerce")
    y = _encode_target(y_raw)

    valid = ~pd.isna(y)
    X = X.loc[valid].to_numpy(dtype=np.float32, copy=False)
    y = y[valid.to_numpy()]

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y if len(np.unique(y)) > 1 else None,
    )

    pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("var", VarianceThreshold(threshold=0.0)),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
            ("clf", LogisticRegression(solver="liblinear", max_iter=200, C=1.0, random_state=42)),
        ]
    )

    pipe.fit(X_train, y_train)
    pred = pipe.predict(X_test)
    accuracy = accuracy_score(y_test, pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Used a small linear model (LogisticRegression with liblinear) for strong accuracy on tabular data at low CPU cost.
# - Avoided deep learning/embeddings; kept compute and memory minimal and predictable on CPU.
# - Pipeline is fully reproducible: median imputation + variance filter + standardization + classifier.
# - Converted features to float32 to reduce memory bandwidth and improve CPU cache efficiency.
# - Dropped non-numeric feature columns to avoid expensive one-hot encoding unless necessary for small-scale tasks.
# - Stratified split when possible to stabilize accuracy estimates without extra compute.