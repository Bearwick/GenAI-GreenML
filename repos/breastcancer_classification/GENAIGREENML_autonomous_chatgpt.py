# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor


RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _read_csv_robust(path):
    # Try default parsing first, then fall back to common European CSV format.
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(dfx):
        if dfx is None or dfx.shape[0] == 0:
            return True
        if dfx.shape[1] == 1:
            sample = dfx.iloc[:5, 0].astype(str).str.contains(r"[;,]").mean()
            if sample > 0.2:
                return True
        return False

    if _looks_wrong(df):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if not _looks_wrong(df2):
                df = df2
            else:
                df = df if df is not None else df2
        except Exception:
            pass

    if df is None:
        raise FileNotFoundError(f"Could not read file: {path}")

    df.columns = _normalize_columns(df.columns)
    df = df.loc[:, ~df.columns.astype(str).str.match(r"^Unnamed:\s*\d+$", na=False)]
    return df


def _to_1d_target(arr):
    if isinstance(arr, pd.DataFrame):
        if arr.shape[1] == 0:
            return np.array([])
        s = arr.iloc[:, 0]
    elif isinstance(arr, pd.Series):
        s = arr
    else:
        arr = np.asarray(arr)
        if arr.ndim == 2 and arr.shape[1] >= 1:
            s = arr[:, 0]
        else:
            s = arr
        return np.asarray(s).ravel()

    return s.to_numpy().ravel()


def _coerce_numeric_columns(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _choose_target_from_df(df):
    # Prefer a non-constant numeric column as target; otherwise use the last column.
    candidates = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        nun = s.nunique(dropna=True)
        if nun >= 2:
            candidates.append((c, nun, s.isna().mean()))
    if candidates:
        # Prefer columns with fewer NaNs; tie-breaker: fewer unique values (often label-like).
        candidates.sort(key=lambda x: (x[2], x[1]))
        return candidates[0][0]
    if len(df.columns) > 0:
        return df.columns[-1]
    return None


def _is_probably_classification(y):
    y_ser = pd.Series(y)
    y_num = pd.to_numeric(y_ser, errors="coerce")
    unique = pd.Series(y_num).dropna().unique()
    if unique.size == 0:
        return False
    if unique.size <= 20:
        # If all values are close to integers, treat as classification.
        if np.all(np.isclose(unique, np.round(unique))):
            return True
    # Also treat object/string small cardinality as classification.
    y_obj_unique = y_ser.dropna().unique()
    if y_obj_unique.size <= 20:
        return True
    return False


def _regression_score_to_accuracy(y_true, y_pred):
    # Convert regression goodness into [0,1] bounded proxy:
    # accuracy = max(0, 1 - MAE / (IQR + eps))
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    mae = float(np.mean(np.abs(yt - yp)))
    q75, q25 = np.percentile(yt, [75, 25])
    iqr = float(q75 - q25)
    denom = iqr if iqr > 1e-12 else float(np.std(yt) + 1e-12)
    acc = 1.0 - (mae / (denom + 1e-12))
    if not np.isfinite(acc):
        acc = 0.0
    return float(np.clip(acc, 0.0, 1.0))


def main():
    # Prefer typical filenames; fall back to any single CSV in working directory.
    preferred = [
        ("cancer_data.csv", "cancer_data_y.csv"),
        ("train.csv", None),
        ("data.csv", None),
        ("dataset.csv", None),
    ]

    X_df = None
    y = None

    for x_path, y_path in preferred:
        if os.path.exists(x_path):
            X_df = _read_csv_robust(x_path)
            if y_path is not None and os.path.exists(y_path):
                y_df = _read_csv_robust(y_path)
                y = _to_1d_target(y_df)
            break

    if X_df is None:
        csvs = [f for f in os.listdir(".") if f.lower().endswith(".csv")]
        csvs.sort()
        if len(csvs) >= 1:
            X_df = _read_csv_robust(csvs[0])

    if X_df is None:
        raise FileNotFoundError("No CSV dataset found in current directory.")

    X_df.columns = _normalize_columns(X_df.columns)
    X_df = X_df.loc[:, ~X_df.columns.astype(str).str.match(r"^Unnamed:\s*\d+$", na=False)]
    assert X_df.shape[0] > 0 and X_df.shape[1] > 0

    if y is None:
        target_col = _choose_target_from_df(X_df)
        if target_col is None:
            raise ValueError("Could not infer a target column.")
        y = _to_1d_target(X_df[target_col])
        X_df = X_df.drop(columns=[target_col])

    y = _to_1d_target(y)
    assert y.size == X_df.shape[0]

    # Replace inf with NaN early.
    X_df = X_df.replace([np.inf, -np.inf], np.nan)
    y_series = pd.Series(y).replace([np.inf, -np.inf], np.nan)

    # Drop rows with missing target.
    keep = ~y_series.isna()
    X_df = X_df.loc[keep].reset_index(drop=True)
    y_series = y_series.loc[keep].reset_index(drop=True)
    assert X_df.shape[0] > 0

    # Identify column types conservatively.
    numeric_cols = []
    categorical_cols = []
    for c in X_df.columns:
        # Try coercion on a sample; if mostly numeric, treat as numeric.
        s = X_df[c]
        s_num = pd.to_numeric(s, errors="coerce")
        frac_num = float(s_num.notna().mean())
        if frac_num >= 0.9:
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)

    X_df = _coerce_numeric_columns(X_df, numeric_cols)

    # Decide task type.
    is_classification = _is_probably_classification(y_series.values)
    y_for_model = y_series.copy()

    if is_classification:
        # For numeric labels, cast to int if close to integer.
        y_num = pd.to_numeric(y_for_model, errors="coerce")
        if y_num.notna().all():
            if np.all(np.isclose(y_num.values, np.round(y_num.values))):
                y_for_model = pd.Series(np.round(y_num.values).astype(int))
        else:
            y_for_model = y_for_model.astype(str)

        # If still <2 classes, fallback to regression proxy path.
        if pd.Series(y_for_model).nunique(dropna=True) < 2:
            is_classification = False

    if not is_classification:
        y_num = pd.to_numeric(y_for_model, errors="coerce")
        # If cannot coerce, use a trivial constant regressor.
        if y_num.isna().all():
            # Still do a split to respect evaluation contract.
            X_train, X_test, y_train, y_test = train_test_split(
                X_df, np.zeros(X_df.shape[0]), test_size=0.2, random_state=RANDOM_STATE
            )
            assert X_train.shape[0] > 0 and X_test.shape[0] > 0
            dummy = DummyRegressor(strategy="mean")
            dummy.fit(np.zeros((X_train.shape[0], 1)), y_train)
            preds = dummy.predict(np.zeros((X_test.shape[0], 1)))
            accuracy = _regression_score_to_accuracy(y_test, preds)
            print(f"ACCURACY={accuracy:.6f}")
            return
        y_for_model = y_num

    X_train, X_test, y_train, y_test = train_test_split(
        X_df, y_for_model.values, test_size=0.2, random_state=RANDOM_STATE, stratify=y_for_model.values if is_classification and pd.Series(y_for_model.values).nunique() > 1 else None
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if is_classification:
        # Use a small, CPU-friendly linear model with a fast solver.
        model = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            random_state=RANDOM_STATE,
        )
        clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

        # If y_train has <2 classes, use a dummy classifier.
        if pd.Series(y_train).nunique(dropna=True) < 2:
            clf = Pipeline(
                steps=[
                    ("preprocess", preprocessor),
                    ("model", DummyClassifier(strategy="most_frequent", random_state=RANDOM_STATE)),
                ]
            )

        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Light regularized linear regression.
        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])

        # If too few samples, fall back to dummy.
        if X_train.shape[0] < 5:
            pipe = Pipeline(
                steps=[
                    ("preprocess", preprocessor),
                    ("model", DummyRegressor(strategy="mean")),
                ]
            )

        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = _regression_score_to_accuracy(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression with liblinear; Ridge) for strong CPU baselines without heavy ensembles.
# - ColumnTransformer+Pipeline ensures single-pass preprocessing and reproducibility; avoids manual feature engineering loops.
# - Robust CSV parsing fallback (default then sep=';' decimal=',') reduces reruns/debug cycles and wasted compute.
# - Conservative schema inference: normalizes headers, drops 'Unnamed' columns, infers target if missing, and proceeds with available features.
# - Numeric coercion with errors='coerce' + safe imputation prevents expensive failure modes and avoids computing stats on object dtypes.
# - Regression fallback maps MAE scaled by IQR into a bounded [0,1] "accuracy" proxy for stable reporting on unknown targets.