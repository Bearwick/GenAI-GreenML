# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "cancer_data.csv"


def _normalize_columns(cols):
    new_cols = []
    for c in cols:
        c2 = str(c)
        c2 = re.sub(r"\s+", " ", c2.strip())
        if c2.lower().startswith("unnamed:"):
            new_cols.append(None)
        else:
            new_cols.append(c2)
    return new_cols


def _read_csv_robust(path):
    # Attempt 1: default parsing
    try:
        df1 = pd.read_csv(path)
    except Exception:
        df1 = None

    # Attempt 2: European CSV conventions
    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
    except Exception:
        df2 = None

    # Choose the "better-looking" dataframe
    candidates = [d for d in [df1, df2] if isinstance(d, pd.DataFrame)]
    if not candidates:
        raise FileNotFoundError(f"Could not read dataset at path: {path}")

    def score_df(d):
        # Prefer more columns and more numeric content
        if d is None or d.empty:
            return -1
        d = d.copy()
        d.columns = [c if c is not None else "" for c in d.columns]
        # numeric score: attempt to coerce each col to numeric and count non-nan
        numeric_non_nan = 0
        for col in d.columns:
            s = pd.to_numeric(d[col], errors="coerce")
            numeric_non_nan += int(s.notna().sum())
        return (d.shape[1] * 10) + numeric_non_nan

    best = max(candidates, key=score_df)
    return best


def _make_unique_columns(cols):
    seen = {}
    out = []
    for c in cols:
        if c is None or c == "":
            c = "col"
        base = c
        n = seen.get(base, 0)
        if n == 0:
            out.append(base)
        else:
            out.append(f"{base}__{n}")
        seen[base] = n + 1
    return out


def _pick_target_and_features(df):
    # Prefer an obvious label column if present
    cols_lower = {c.lower(): c for c in df.columns}
    for key in ["target", "label", "class", "diagnosis", "y"]:
        if key in cols_lower:
            target = cols_lower[key]
            features = [c for c in df.columns if c != target]
            return target, features

    # Otherwise choose a good numeric target: non-constant with few unique values (classification-friendly)
    numeric_candidates = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() < max(10, int(0.2 * len(df))):
            continue
        nun = s.dropna().nunique()
        if nun <= 1:
            continue
        # Score: prefer small cardinality (2-10), else fallback to higher cardinality
        cls_like = 1 if 2 <= nun <= 10 else 0
        numeric_candidates.append((cls_like, -nun, c))

    if numeric_candidates:
        numeric_candidates.sort(reverse=True)
        target = numeric_candidates[0][2]
        features = [c for c in df.columns if c != target]
        return target, features

    # Fallback: last column as target
    target = df.columns[-1]
    features = [c for c in df.columns if c != target]
    return target, features


def _safe_accuracy_proxy_regression(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    ss_tot = float(np.sum((y_true - float(np.mean(y_true))) ** 2))
    if ss_tot <= 0.0:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    # Bound into [0,1] for a stable "accuracy" proxy
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    if not os.path.exists(DATASET_PATH):
        # Try also relative to script directory
        script_dir = os.path.dirname(os.path.abspath(__file__))
        alt_path = os.path.join(script_dir, DATASET_PATH)
        if os.path.exists(alt_path):
            path = alt_path
        else:
            raise FileNotFoundError(DATASET_PATH)
    else:
        path = DATASET_PATH

    df = _read_csv_robust(path)

    # Normalize column names and drop "Unnamed" columns
    norm_cols = _normalize_columns(df.columns)
    keep_mask = [c is not None for c in norm_cols]
    df = df.loc[:, keep_mask].copy()
    norm_cols = [c for c in norm_cols if c is not None]
    df.columns = _make_unique_columns(norm_cols)

    # If data accidentally loaded as a single column with comma-separated values, split it
    if df.shape[1] == 1 and isinstance(df.iloc[0, 0], str) and ("," in df.iloc[0, 0]):
        split = df.iloc[:, 0].astype(str).str.split(",", expand=True)
        split.columns = _make_unique_columns([f"f{i}" for i in range(split.shape[1])])
        df = split

    # Coerce numeric-looking strings safely later via transformers; here just ensure non-empty
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.dropna(axis=0, how="all")
    df = df.dropna(axis=1, how="all")
    assert not df.empty, "Dataset is empty after basic cleaning."

    target_col, feature_cols = _pick_target_and_features(df)

    # Ensure we have at least 1 feature; if not, create a constant feature
    if len(feature_cols) == 0:
        df["__const__"] = 1.0
        feature_cols = ["__const__"]

    X = df[feature_cols].copy()
    y_raw = df[target_col].copy()

    # Determine task type: classification if low cardinality or non-numeric labels
    y_num = pd.to_numeric(y_raw, errors="coerce")
    y_is_mostly_numeric = (y_num.notna().mean() >= 0.95)

    if not y_is_mostly_numeric:
        # Classification with categorical labels
        y = y_raw.astype(str).fillna("NA")
        task = "classification"
    else:
        # Numeric target: decide classification if few unique values
        nun = y_num.dropna().nunique()
        if 2 <= nun <= 10:
            y = y_num
            task = "classification"
        else:
            y = y_num
            task = "regression"

    # Remove rows with missing target for modeling stability
    valid_y_mask = y.notna()
    X = X.loc[valid_y_mask].copy()
    y = y.loc[valid_y_mask].copy()
    assert len(X) > 1, "Not enough samples after removing missing target."

    # Identify numeric and categorical feature columns robustly
    numeric_features = []
    categorical_features = []
    for c in X.columns:
        s = pd.to_numeric(X[c], errors="coerce")
        if s.notna().mean() >= 0.8:
            numeric_features.append(c)
        else:
            categorical_features.append(c)

    # Preprocess: impute + scale numeric; impute + one-hot categorical
    numeric_transformer = Pipeline(steps=[
        ("to_num", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    rng = 42
    if task == "classification":
        # If classification target is numeric but has <2 classes, fallback
        y_unique = pd.Series(y).dropna().unique()
        if len(y_unique) < 2:
            task = "regression"

    if task == "classification":
        # Stratify when possible (>=2 classes with enough samples)
        stratify = None
        try:
            y_counts = pd.Series(y).value_counts()
            if y_counts.min() >= 2 and y_counts.shape[0] >= 2:
                stratify = y
        except Exception:
            stratify = None

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=rng, stratify=stratify
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split produced empty set."

        # Lightweight linear model; saga handles sparse one-hot efficiently
        clf = LogisticRegression(
            solver="saga",
            penalty="l2",
            C=1.0,
            max_iter=200,
            n_jobs=1,
            random_state=rng,
        )

        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y.astype(float), test_size=0.2, random_state=rng
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split produced empty set."

        reg = Ridge(alpha=1.0, random_state=rng)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _safe_accuracy_proxy_regression(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) for strong CPU efficiency and small memory footprint.
# - ColumnTransformer+Pipeline ensures single-pass, reproducible preprocessing without redundant work.
# - OneHotEncoder with sparse output and LogisticRegression(saga, n_jobs=1) keeps computation efficient on CPU.
# - Robust CSV parsing fallback (default then sep=';' & decimal=',') reduces manual intervention and avoids re-runs.
# - Defensive schema handling: normalizes headers, drops unnamed columns, auto-selects target/features to prevent hard failures.
# - Regression fallback uses a bounded R2-derived proxy in [0,1] to provide a stable ACCURACY value when classification is ill-posed.