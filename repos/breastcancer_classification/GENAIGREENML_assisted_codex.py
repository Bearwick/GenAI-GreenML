# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd

np.random.seed(0)

DATASET_HEADERS = "17.99,10.38,122.8,1001,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871,1.095,0.9053,8.589,153.4,0.006399,0.04904,0.05373,0.01587,0.03003,0.006193,25.38,17.33,184.6,2019,0.1622,0.6656,0.7119,0.2654,0.4601,0.1189"


def parse_dataset_headers(headers_str):
    return [float(h.strip()) for h in headers_str.split(",") if h.strip()]


HEADER_VALUES = parse_dataset_headers(DATASET_HEADERS)
EXPECTED_FEATURE_COLS = len(HEADER_VALUES)


def _columns_look_numeric(cols):
    try:
        for c in cols:
            float(str(c).strip())
        return True
    except Exception:
        return False


def _match_header_values(cols, header_values):
    if header_values is None or len(cols) != len(header_values):
        return False
    try:
        col_vals = np.array([float(str(c).strip()) for c in cols], dtype=float)
        hv = np.array(header_values, dtype=float)
        return np.allclose(col_vals, hv, rtol=1e-06, atol=1e-08)
    except Exception:
        return False


def read_csv_robust(path, expected_n_cols=None, header_values=None):
    def _read(sep=None, decimal=None, header="infer"):
        if sep is None:
            return pd.read_csv(path, header=header)
        return pd.read_csv(path, sep=sep, decimal=decimal, header=header)

    df = _read()
    sep = None
    decimal = None

    def needs_sep_fallback(df_local):
        if df_local.shape[1] != 1:
            return False
        if expected_n_cols is not None and expected_n_cols > 1:
            return True
        col = df_local.columns[0]
        if isinstance(col, str) and ";" in col:
            return True
        if df_local.shape[0] > 0:
            val = df_local.iloc[0, 0]
            if isinstance(val, str) and ";" in val:
                return True
        return False

    if needs_sep_fallback(df):
        df_alt = _read(sep=";", decimal=",")
        if df_alt.shape[1] > 1:
            df = df_alt
            sep = ";"
            decimal = ","

    if _match_header_values(df.columns, header_values) or _columns_look_numeric(df.columns):
        df = _read(sep=sep, decimal=decimal, header=None)

    return df


def load_features(path):
    df = read_csv_robust(path, expected_n_cols=EXPECTED_FEATURE_COLS, header_values=HEADER_VALUES)
    return df.to_numpy(dtype=float, copy=False)


def load_labels(path):
    df = read_csv_robust(path, expected_n_cols=1)
    return df.to_numpy(dtype=float, copy=False)


def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))


def initialize(dim):
    w = np.zeros((dim, 1), dtype=float)
    b = 0.0
    return w, b


def _propagate_internal(w, b, X, Y, inv_m, compute_cost):
    A = sigmoid(np.dot(w.T, X) + b)
    cost = None
    if compute_cost:
        cost = -np.sum(Y * np.log(A) + (1.0 - Y) * np.log(1.0 - A)) * inv_m
        cost = float(np.squeeze(cost))
    diff = A - Y
    dw = np.dot(X, diff.T) * inv_m
    db = np.sum(diff) * inv_m
    return dw, db, cost


def propagate(w, b, X, Y):
    inv_m = 1.0 / X.shape[1]
    dw, db, cost = _propagate_internal(w, b, X, Y, inv_m, True)
    grads = {"dw": dw, "db": db}
    return grads, cost


def optimize(w, b, X, Y, num_iters, alpha, print_cost=False):
    inv_m = 1.0 / X.shape[1]
    costs = []
    dw = np.zeros_like(w)
    db = 0.0
    for i in range(num_iters):
        need_cost = i % 100 == 0
        dw, db, cost = _propagate_internal(w, b, X, Y, inv_m, need_cost)
        w -= alpha * dw
        b -= alpha * db
        if need_cost:
            costs.append(cost)
    params = {"w": w, "b": b}
    grads = {"dw": dw, "db": db}
    return params, grads, costs


def predict(w, b, X):
    A = sigmoid(np.dot(w.T, X) + b)
    return (A > 0.5).astype(int)


def model(X_train, Y_train, num_iters=2000, alpha=0.5, print_cost=False):
    w, b = initialize(X_train.shape[0])
    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iters, alpha, print_cost)
    w = parameters["w"]
    b = parameters["b"]
    X_test = load_features("test_cancer_data.csv").T
    Y_test = load_labels("test_cancer_data_y.csv").T
    y_prediction_train = predict(w, b, X_train)
    y_prediction_test = predict(w, b, X_test)
    train_accuracy = 100.0 - np.mean(np.abs(y_prediction_train - Y_train)) * 100.0
    test_accuracy = 100.0 - np.mean(np.abs(y_prediction_test - Y_test)) * 100.0
    return {
        "costs": costs,
        "Y_prediction_test": y_prediction_test,
        "Y_prediction_train": y_prediction_train,
        "w": w,
        "b": b,
        "learning_rate": alpha,
        "num_iterations": num_iters,
        "train_accuracy": train_accuracy,
        "test_accuracy": test_accuracy,
    }


def __init__():
    train_x = load_features("cancer_data.csv").T
    train_y = load_labels("cancer_data_y.csv").T
    results = model(train_x, train_y, num_iters=190500, alpha=0.000000065, print_cost=True)
    accuracy = results["test_accuracy"]
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    __init__()


# Optimization Summary
# Removed plotting and logging to cut I/O overhead.
# Implemented robust CSV parsing with header detection to avoid data loss and unnecessary copies.
# Vectorized prediction and accuracy computation to eliminate Python loops.
# Reduced per-iteration work by computing costs only when recorded and precomputing the inverse sample count.