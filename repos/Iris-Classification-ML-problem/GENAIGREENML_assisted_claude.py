# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F

SEED = 4096
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
np.random.seed(SEED)

file_path = 'iris.data'
data = np.genfromtxt(file_path, delimiter=',', dtype=None, encoding=None)

features = np.array([[float(row[i]) for i in range(4)] for row in data], dtype=np.float32)
class_names = [row[4] for row in data]
unique_classes = sorted(set(class_names))
class_map = {name: idx for idx, name in enumerate(unique_classes)}
labels = np.array([class_map[c] for c in class_names], dtype=np.int64)

n = len(labels)
shuffled_indices = np.random.permutation(n)
features = features[shuffled_indices]
labels = labels[shuffled_indices]

mu = features.mean(axis=0)
span = features.max(axis=0) - features.min(axis=0)
features = (features - mu) / span

num_train = int(n * 0.6)
x_train = features[:num_train]
y_train = labels[:num_train]
x_test = features[num_train:]
y_test = labels[num_train:]

x_train_t = torch.from_numpy(x_train)
y_train_t = torch.from_numpy(y_train).long()
x_test_t = torch.from_numpy(x_test)
y_test_t = torch.from_numpy(y_test).long()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
x_train_t = x_train_t.to(device)
y_train_t = y_train_t.to(device)
x_test_t = x_test_t.to(device)
y_test_t = y_test_t.to(device)


class IrisNN(nn.Module):
    def __init__(self):
        super(IrisNN, self).__init__()
        self.fn1 = nn.Linear(4, 6)
        self.fn2 = nn.Linear(6, 3)

    def forward(self, x):
        x = F.relu(self.fn1(x))
        x = self.fn2(x)
        return x


model = IrisNN().to(device)
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)

max_epochs = 200
for epoch in range(max_epochs):
    model.train()
    optimizer.zero_grad()
    score = model(x_train_t)
    loss = loss_fn(score, y_train_t)
    loss.backward()
    optimizer.step()

model.eval()
with torch.no_grad():
    score = model(x_test_t)
    predictions = score.argmax(dim=1)
    num_correct = (predictions == y_test_t).sum().item()

accuracy = num_correct / len(y_test_t)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Replaced pandas with numpy genfromtxt to avoid heavy pandas dependency overhead
# Removed Dataset/DataLoader; data fits in memory, so direct tensor operations are used
# Moved all data to device once upfront, eliminating per-epoch data transfer
# Removed per-epoch evaluation; evaluate only once after training to save compute
# Removed redundant test forward pass during training loop
# Eliminated unnecessary intermediate variables and loops
# Removed all prints, plots, model saving per requirements
# Used argmax instead of max with keepdim for simpler prediction extraction
# Single batch training (data is small, 90 samples) avoids DataLoader overhead