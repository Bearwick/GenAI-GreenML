# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import glob
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression


def _find_dataset_file() -> str:
    candidates = []
    patterns = [
        "*.csv",
        "*.tsv",
        "*.txt",
        "*.data",
    ]
    for pat in patterns:
        candidates.extend(glob.glob(pat))
        candidates.extend(glob.glob(os.path.join("data", pat)))
        candidates.extend(glob.glob(os.path.join("dataset", pat)))
        candidates.extend(glob.glob(os.path.join("input", pat)))
    candidates = [c for c in candidates if os.path.isfile(c)]
    for c in sorted(candidates):
        if os.path.basename(c).lower() in {"submission.csv", "sample_submission.csv"}:
            continue
        if os.path.getsize(c) == 0:
            continue
        return c
    raise FileNotFoundError("No dataset file found in current directory or common subfolders.")


def _read_tabular(path: str):
    ext = os.path.splitext(path)[1].lower()
    if ext == ".tsv":
        delim = "\t"
    elif ext in {".txt", ".data"}:
        delim = None
    else:
        delim = ","

    # Use numpy structured reading with dtype=str for low overhead parsing without pandas
    # This keeps CPU-only execution lightweight for small-to-medium tabular files.
    try:
        if delim is None:
            data = np.genfromtxt(
                path,
                dtype=str,
                delimiter=None,
                encoding="utf-8",
                autostrip=True,
                invalid_raise=False,
            )
        else:
            data = np.genfromtxt(
                path,
                dtype=str,
                delimiter=delim,
                encoding="utf-8",
                autostrip=True,
                invalid_raise=False,
            )
    except Exception:
        data = np.genfromtxt(
            path,
            dtype=str,
            delimiter=",",
            encoding="utf-8",
            autostrip=True,
            invalid_raise=False,
        )

    if data.ndim == 1:
        data = data.reshape(1, -1)
    return data


def _split_header_and_rows(raw: np.ndarray):
    # Heuristic: treat first row as header if it contains any non-numeric token or duplicates are unlikely
    first = raw[0]
    def _is_number(s):
        try:
            float(str(s))
            return True
        except Exception:
            return False

    non_numeric = sum(not _is_number(x) for x in first)
    has_alpha = any(any(ch.isalpha() for ch in str(x)) for x in first)
    header_like = has_alpha or non_numeric >= max(1, len(first) // 3)

    if header_like and raw.shape[0] >= 2:
        header = [str(x) if str(x) != "" else f"col_{i}" for i, x in enumerate(first)]
        rows = raw[1:]
    else:
        header = [f"col_{i}" for i in range(raw.shape[1])]
        rows = raw

    return header, rows


def _infer_target_column(header, rows):
    # Choose last column as target by default; if a column name suggests target/label, prefer it.
    lowered = [h.lower() for h in header]
    for key in ("target", "label", "class", "y"):
        if key in lowered:
            return lowered.index(key)
    return len(header) - 1


def _coerce_numeric_columns(X_str: np.ndarray):
    n_rows, n_cols = X_str.shape
    X_num = np.empty((n_rows, n_cols), dtype=float)
    is_numeric = np.zeros(n_cols, dtype=bool)

    for j in range(n_cols):
        col = X_str[:, j]
        col_clean = np.where((col == "") | (col == "nan") | (col == "NaN") | (col == "None"), "nan", col)
        ok = 0
        total = len(col_clean)
        for v in col_clean:
            try:
                float(v)
                ok += 1
            except Exception:
                pass
        # Consider numeric if most values parse as float
        if total > 0 and ok / total >= 0.85:
            is_numeric[j] = True
            out = np.full(total, np.nan, dtype=float)
            for i, v in enumerate(col_clean):
                try:
                    out[i] = float(v)
                except Exception:
                    out[i] = np.nan
            X_num[:, j] = out
        else:
            X_num[:, j] = np.nan

    return is_numeric


def _encode_target(y_str: np.ndarray):
    y = np.asarray(y_str).astype(str)
    y = np.where((y == "") | (y == "nan") | (y == "NaN") | (y == "None"), "__MISSING__", y)

    # If y is numeric-like, map to int classes via unique values
    def _is_float_like(s):
        try:
            float(s)
            return True
        except Exception:
            return False

    numeric_like = np.mean([_is_float_like(v) for v in y]) >= 0.95
    if numeric_like:
        y_float = np.array([float(v) if _is_float_like(v) else np.nan for v in y], dtype=float)
        uniq = np.unique(y_float[~np.isnan(y_float)])
        mapping = {val: i for i, val in enumerate(uniq.tolist())}
        y_enc = np.array([mapping.get(v, -1) if not np.isnan(v) else -1 for v in y_float], dtype=int)
        mask = y_enc != -1
        return y_enc[mask], mask
    else:
        uniq = np.unique(y)
        mapping = {val: i for i, val in enumerate(uniq.tolist())}
        y_enc = np.array([mapping[v] for v in y], dtype=int)
        return y_enc, np.ones_like(y_enc, dtype=bool)


def main():
    path = _find_dataset_file()
    raw = _read_tabular(path)
    header, rows = _split_header_and_rows(raw)
    target_idx = _infer_target_column(header, rows)

    y_str = rows[:, target_idx]
    X_str = np.delete(rows, target_idx, axis=1)

    y, y_mask = _encode_target(y_str)
    X_str = X_str[y_mask]

    # Identify numeric vs categorical using a lightweight parsing heuristic
    numeric_mask = _coerce_numeric_columns(X_str)
    n_rows, n_cols = X_str.shape

    numeric_indices = [i for i in range(n_cols) if numeric_mask[i]]
    categorical_indices = [i for i in range(n_cols) if not numeric_mask[i]]

    # Build efficient preprocessing using simple imputers + one-hot encoding for categoricals
    # Keep sparse output to reduce memory and CPU for linear models.
    preprocess = ColumnTransformer(
        transformers=[
            (
                "num",
                Pipeline(
                    steps=[
                        ("imputer", SimpleImputer(strategy="median")),
                    ]
                ),
                numeric_indices,
            ),
            (
                "cat",
                Pipeline(
                    steps=[
                        ("imputer", SimpleImputer(strategy="most_frequent")),
                        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
                    ]
                ),
                categorical_indices,
            ),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Use logistic regression with a fast, CPU-friendly solver suitable for small-to-medium sparse data
    clf = LogisticRegression(
        max_iter=200,
        solver="liblinear",
        n_jobs=1,
    )

    model = Pipeline(
        steps=[
            ("preprocess", preprocess),
            ("clf", clf),
        ]
    )

    # Stratify when possible to preserve class distribution without heavy compute
    stratify = y if (len(np.unique(y)) > 1 and len(y) >= 10) else None
    X_train, X_test, y_train, y_test = train_test_split(
        X_str,
        y,
        test_size=0.2,
        random_state=42,
        stratify=stratify,
    )

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Avoided heavy dependencies (no pandas) and deep learning to minimize CPU cycles and memory use.
# - Used a deterministic preprocessing pipeline (imputation + one-hot) for reproducibility and low overhead.
# - Selected logistic regression with liblinear for efficient CPU training on small-scale tabular data.
# - Kept categorical encoding sparse to reduce memory footprint and speed up linear algebra operations.
# - Limited iterations and enforced single-threading (n_jobs=1) for predictable, energy-aware execution.