# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# Dataset selection: load_breast_cancer provides a small-scale, high-quality tabular dataset suitable for CPU-only training
data = load_breast_cancer()
X, y = data.data, data.target

# Split strategy: 80/20 split with a fixed random state ensures reproducible results with minimal memory overhead
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Pipeline implementation: Encapsulating scaling and estimation prevents data leakage and streamlines the transformation process
# StandardScaler: Normalizes features to unit variance, significantly reducing the number of iterations required for convergence
# LogisticRegression: A linear model that is extremely energy-efficient compared to ensembles or neural networks
# Solver selection: 'liblinear' is highly optimized for small datasets and executes efficiently on a single CPU core
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LogisticRegression(solver='liblinear', penalty='l2', C=1.0, random_state=42))
])

# Training: The fit method uses coordinate descent for l2 penalty, which is computationally inexpensive
pipeline.fit(X_train, y_train)

# Inference: Predictions on small batches are nearly instantaneous and require negligible electrical power
y_pred = pipeline.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Model Architecture: Logistic Regression was selected as it provides high interpretability and minimal FLOPs, lowering carbon emissions per training run.
# 2. Computational Efficiency: Avoided deep learning and gradient boosting to prevent unnecessary GPU/multi-core CPU utilization.
# 3. Preprocessing: StandardScaler ensures the optimization surface is well-conditioned, leading to faster convergence and less CPU time.
# 4. Memory Management: Used a pipeline approach to process data in-place without creating redundant copies of large arrays.
# 5. Environmental Impact: The solution is designed to run on low-power hardware, making it suitable for edge deployment or green data centers.