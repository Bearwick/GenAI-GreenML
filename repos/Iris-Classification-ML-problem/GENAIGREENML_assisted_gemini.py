# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

SEED = 4096
torch.manual_seed(SEED)
np.random.seed(SEED)
device = torch.device('cpu')

try:
    df = pd.read_csv('iris.data', header=None)
except Exception:
    df = pd.read_csv('iris.data', header=None, sep=';', decimal=',')

if df.shape[1] == 5:
    df.columns = ['SLength', 'SWidth', 'PLength', 'PWidth', 'class']

df['class'] = df['class'].astype('category').cat.codes
data_values = df.values.astype(np.float32)

indices = np.random.permutation(len(data_values))
data_values = data_values[indices]

x_raw = data_values[:, :4]
y_raw = data_values[:, 4].astype(np.int64)

mu = x_raw.mean(axis=0)
span = x_raw.max(axis=0) - x_raw.min(axis=0)
x_scaled = (x_raw - mu) / span

num_train = int(len(data_values) * 0.6)
x_train = torch.from_numpy(x_scaled[:num_train]).to(device)
y_train = torch.from_numpy(y_raw[:num_train]).to(device)
x_test = torch.from_numpy(x_scaled[num_train:]).to(device)
y_test = torch.from_numpy(y_raw[num_train:]).to(device)

class IrisNN(nn.Module):
    def __init__(self):
        super(IrisNN, self).__init__()
        self.fn1 = nn.Linear(4, 6)
        self.fn2 = nn.Linear(6, 3)
        
    def forward(self, x):
        return self.fn2(F.relu(self.fn1(x)))

model = IrisNN().to(device)
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)

for epoch in range(200):
    model.train()
    optimizer.zero_grad(set_to_none=True)
    output = model(x_train)
    loss = loss_fn(output, y_train)
    loss.backward()
    optimizer.step()

model.eval()
with torch.inference_mode():
    test_output = model(x_test)
    predictions = test_output.argmax(dim=1)
    correct = (predictions == y_test).sum().item()
    accuracy = correct / len(y_test)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Forced CPU execution to eliminate GPU-CPU PCIe data transfer overhead, which is inefficient for small datasets like Iris.
# 2. Removed DataLoader and Dataset class abstractions to reduce Python object overhead and iterator latency.
# 3. Streamlined data preprocessing by using NumPy array slicing and in-place operations instead of redundant DataFrame copies.
# 4. Replaced optimizer.zero_grad() with optimizer.zero_grad(set_to_none=True) to reduce memory bandwidth usage.
# 5. Switched from torch.no_grad() to torch.inference_mode() for more efficient internal tensor handling during evaluation.
# 6. Eliminated redundant data conversions by transforming the entire dataset to tensors once prior to the training loop.
# 7. Removed all logging, printing, and temporary variable allocations inside the training loop to minimize CPU cycle consumption.
# 8. Used argmax(dim=1) for simplified prediction logic compared to multiple max/eq/view_as calls.