# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F

SEED = 4096


def set_reproducible(seed: int) -> None:
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def load_and_prepare_iris(file_path: str):
    df = pd.read_csv(
        file_path,
        header=None,
        names=["SLength", "SWidth", "PLength", "PWidth", "class"],
    )
    df["class"] = df["class"].astype("category").cat.codes

    idx = np.random.permutation(len(df))
    x = df.iloc[idx, :4].to_numpy(dtype=np.float32, copy=True)
    y = df.iloc[idx, -1].to_numpy(dtype=np.int64, copy=True)

    mu = x.mean(axis=0)
    span = x.max(axis=0) - x.min(axis=0)
    x = (x - mu) / span

    n = x.shape[0]
    num_train = int(n * 0.6)

    x_train = x[:num_train]
    y_train = y[:num_train]
    x_test = x[n - (n - num_train) :]
    y_test = y[n - (n - num_train) :]

    return x_train, y_train, x_test, y_test


class IrisNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fn1 = nn.Linear(4, 6)
        self.fn2 = nn.Linear(6, 3)

    def forward(self, x):
        x = F.relu(self.fn1(x))
        return self.fn2(x)


def accuracy_from_logits(logits: torch.Tensor, y: torch.Tensor) -> float:
    preds = logits.argmax(dim=1)
    return (preds == y).float().mean().item()


def train_epoch(model, optimizer, loss_fn, x_train, y_train, device):
    model.train()
    optimizer.zero_grad(set_to_none=True)
    logits = model(x_train)
    loss = loss_fn(logits, y_train)
    loss.backward()
    optimizer.step()
    return loss.detach(), accuracy_from_logits(logits.detach(), y_train)


@torch.no_grad()
def evaluate(model, loss_fn, x_test, y_test, device):
    model.eval()
    logits = model(x_test)
    loss = loss_fn(logits, y_test)
    return loss, accuracy_from_logits(logits, y_test)


def main():
    set_reproducible(SEED)

    file_path = "iris.data"
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Missing dataset file: {file_path}")

    x_train, y_train, x_test, y_test = load_and_prepare_iris(file_path)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    x_train_t = torch.from_numpy(x_train).to(device)
    y_train_t = torch.from_numpy(y_train).to(device)
    x_test_t = torch.from_numpy(x_test).to(device)
    y_test_t = torch.from_numpy(y_test).to(device)

    model = IrisNN().to(device)
    loss_fn = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)

    max_epochs = 200
    for _ in range(max_epochs):
        train_epoch(model, optimizer, loss_fn, x_train_t, y_train_t, device)
        loss, acc = evaluate(model, loss_fn, x_test_t, y_test_t, device)

    accuracy = acc
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# Removed DataLoader/Dataset and per-batch iteration; dataset is tiny, so full-batch training reduces Python overhead and data movement.
# Eliminated redundant device transfers by moving train/test tensors to device once and reusing them across epochs.
# Used optimizer.zero_grad(set_to_none=True) to reduce memory writes and improve performance.
# Replaced max(1, keepdim=True)[1] + view logic with argmax + mean for a simpler, vectorized accuracy computation.
# Avoided extra intermediate prints/tests and parameter dumps to reduce runtime and I/O overhead.
# Ensured reproducibility with deterministic flags and seeded random sources while keeping the same task and model structure.