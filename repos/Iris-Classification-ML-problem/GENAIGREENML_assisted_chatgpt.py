# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
from typing import Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F


SEED = 4096


def set_reproducible(seed: int = SEED) -> None:
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.use_deterministic_algorithms(True, warn_only=True)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def read_csv_robust(path: str, headers: list) -> pd.DataFrame:
    df = pd.read_csv(path, header=None, names=headers)
    expected_cols = len(headers)

    looks_wrong = (
        df.shape[1] != expected_cols
        or df.isna().all(axis=None)
        or (df.dtypes == "object").sum() == expected_cols
    )
    if looks_wrong:
        df = pd.read_csv(path, header=None, names=headers, sep=";", decimal=",")
    return df


def prepare_data(file_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    headers = ["SLength", "SWidth", "PLength", "PWidth", "class"]
    df = read_csv_robust(file_path, headers=headers)

    if "class" not in df.columns:
        df.columns = headers[: df.shape[1]]

    df = df.dropna()

    df["class"] = df["class"].astype("category").cat.codes.astype(np.int64)

    n = len(df)
    idx = np.random.permutation(n)
    df = df.iloc[idx].reset_index(drop=True)

    feature_cols = [c for c in df.columns if c != "class"][:4]
    x = df.loc[:, feature_cols].to_numpy(dtype=np.float32, copy=False)
    y = df.loc[:, "class"].to_numpy(dtype=np.int64, copy=False)

    mu = x.mean(axis=0)
    span = x.max(axis=0) - x.min(axis=0)
    span = np.where(span == 0.0, 1.0, span).astype(np.float32, copy=False)
    x = (x - mu) / span

    num_train = int(n * 0.6)
    x_train = x[:num_train]
    y_train = y[:num_train]
    x_test = x[n - (n - num_train) :]
    y_test = y[n - (n - num_train) :]

    return x_train, y_train, x_test, y_test


class IrisNN(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.fn1 = nn.Linear(4, 6)
        self.fn2 = nn.Linear(6, 3)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = F.relu(self.fn1(x))
        x = self.fn2(x)
        return x


def accuracy_on_tensors(model: nn.Module, x: torch.Tensor, y: torch.Tensor) -> float:
    model.eval()
    with torch.no_grad():
        logits = model(x)
        preds = logits.argmax(dim=1)
        return (preds == y).float().mean().item()


def train_model(
    x_train: np.ndarray,
    y_train: np.ndarray,
    x_test: np.ndarray,
    y_test: np.ndarray,
    max_epochs: int = 200,
) -> float:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    xtr = torch.from_numpy(x_train).to(device=device, dtype=torch.float32, non_blocking=True)
    ytr = torch.from_numpy(y_train).to(device=device, dtype=torch.long, non_blocking=True)
    xte = torch.from_numpy(x_test).to(device=device, dtype=torch.float32, non_blocking=True)
    yte = torch.from_numpy(y_test).to(device=device, dtype=torch.long, non_blocking=True)

    model = IrisNN().to(device)
    loss_fn = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)

    model.train()
    for _ in range(max_epochs):
        optimizer.zero_grad(set_to_none=True)
        logits = model(xtr)
        loss = loss_fn(logits, ytr)
        loss.backward()
        optimizer.step()

    return accuracy_on_tensors(model, xte, yte)


def main() -> None:
    set_reproducible(SEED)
    x_train, y_train, x_test, y_test = prepare_data("iris.data")
    accuracy = train_model(x_train, y_train, x_test, y_test, max_epochs=200)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced DataLoader/Dataset iteration with single-tensor full-batch training/evaluation to eliminate Python loop overhead and reduce data movement.
# - Removed intermediate debug forward-pass and all non-required printing to reduce unnecessary computation and I/O.
# - Used optimizer.zero_grad(set_to_none=True) to lower memory writes and improve runtime.
# - Performed one-time NumPy->Torch conversion and kept tensors on the target device to avoid repeated host-device transfers per batch.
# - Added robust CSV parsing fallback (default then sep=';' and decimal=',') to prevent costly mis-parses and retries later in the pipeline.
# - Derived feature columns from df.columns (excluding 'class') to avoid hard assumptions about schema while preserving the same 4-feature intent.
# - Ensured reproducibility with fixed seeds and deterministic settings where feasible.
# - Avoided extra copies by using to_numpy(..., copy=False) where possible and vectorized normalization; guarded against zero-span division.