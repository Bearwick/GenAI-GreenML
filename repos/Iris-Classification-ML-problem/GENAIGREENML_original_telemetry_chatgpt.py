# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: original_telemetry

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F

SEED = 4096
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
np.random.seed(SEED)

file_path = "iris.data"
df = pd.read_csv(
    file_path,
    header=None,
    names=["SLength", "SWidth", "PLength", "PWidth", "class"],
)

df["class"] = df["class"].astype("category")
df["class"] = df["class"].cat.codes

n = len(df.index)
shuffled_indices = np.random.permutation(n)
df = df.iloc[shuffled_indices]

x = df.iloc[:, :4].values.astype(np.float32)
y = df.iloc[:, -1].values.astype(np.int64)

mu = x.mean(axis=0)
span = x.max(axis=0) - x.min(axis=0)


def rescale(inputs):
    return (inputs - mu) / span


x = rescale(x)

num_train = int(n * 0.6)
num_test = n - num_train

x_train = x[:num_train]
y_train = y[:num_train]
x_test = x[-num_test:]
y_test = y[-num_test:]


class NpDataset(Dataset):
    def __init__(self, data, label):
        assert len(data) == len(label)
        self.data = torch.from_numpy(data)
        self.label = torch.from_numpy(label).long()

    def __getitem__(self, index):
        return self.data[index], self.label[index]

    def __len__(self):
        return len(self.label)


train_dataset = NpDataset(x_train, y_train)
test_dataset = NpDataset(x_test, y_test)

train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=False)
test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class IrisNN(nn.Module):
    def __init__(self):
        super(IrisNN, self).__init__()
        self.fn1 = nn.Linear(4, 6)
        self.fn2 = nn.Linear(6, 3)

    def forward(self, x_in):
        x_out = F.relu(self.fn1(x_in))
        x_out = self.fn2(x_out)
        return x_out


model = IrisNN()
model.to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)


def train():
    model.train()
    for xb, yb in train_dataloader:
        xb = xb.to(device)
        yb = yb.to(device)
        batch_n = xb.size(0)

        optimizer.zero_grad()
        score = model(xb)
        loss = loss_fn(score, yb)

        loss.backward()
        optimizer.step()

        predictions = score.max(1, keepdim=True)[1]
        num_correct = predictions.eq(yb.view_as(predictions)).sum().item()

    acc = num_correct / batch_n
    return loss, acc


def evaluate():
    model.eval()
    with torch.no_grad():
        for xb, yb in test_dataloader:
            xb = xb.to(device)
            yb = yb.to(device)
            batch_n = xb.size(0)
            score = model(xb)
            loss = loss_fn(score, yb)
            predictions = score.max(1, keepdim=True)[1]
            num_correct = predictions.eq(yb.view_as(predictions)).sum().item()

    acc = num_correct / batch_n
    return loss, acc


max_epochs = 200
for _epoch in range(max_epochs):
    train()
    evaluate()

_, accuracy = evaluate()
print(f"ACCURACY={accuracy:.6f}")