# Generated by generate_llm_code.py
# LLM: gemini
# Mode: original_telemetry

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F

SEED = 4096
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
np.random.seed(SEED)

file_path = 'iris.data'
df = pd.read_csv(
    file_path,
    header=None,
    names=['SLength', 'SWidth', 'PLength', 'PWidth', 'class'],
)

df['class'] = df['class'].astype('category')
df['class'] = df['class'].cat.codes

n = len(df.index)
shuffled_indices = np.random.permutation(n)
df = df.iloc[shuffled_indices]

x = df.iloc[:, :4].values.astype(np.float32)
y = df.iloc[:, -1].values.astype(np.int64)

mu = x.mean(axis=0)
span = x.max(axis=0) - x.min(axis=0)

def rescale(inputs):
    return (inputs - mu) / span

x = rescale(x)

num_train = int(n * .6)
num_test = n - num_train

x_train = x[:num_train]
y_train = y[:num_train]
x_test = x[-num_test:]
y_test = y[-num_test:]

class NpDataset(Dataset):
    def __init__(self, data, label):
        assert len(data) == len(label)
        self.data = torch.from_numpy(data)
        self.label = torch.from_numpy(label).long()
        
    def __getitem__(self, index):
        return self.data[index], self.label[index]
    
    def __len__(self):
        return len(self.label)

train_dataset = NpDataset(x_train, y_train)
test_dataset = NpDataset(x_test, y_test)

train_dataloader = DataLoader(
    train_dataset,
    batch_size=128,
    shuffle=False
)
test_dataloader = DataLoader(
    test_dataset,
    batch_size=128,
    shuffle=False
)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class IrisNN(nn.Module):
    def __init__(self):
        super(IrisNN, self).__init__()
        self.fn1 = nn.Linear(4, 6)
        self.fn2 = nn.Linear(6, 3)
        
    def forward(self, x):
        x = F.relu(self.fn1(x))
        x = self.fn2(x)
        return x
    
model = IrisNN()
model.to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)

def train():
    model.train()
    total_correct = 0
    total_samples = 0
    for x_batch, y_batch in train_dataloader:
        x_batch = x_batch.to(device)
        y_batch = y_batch.to(device)
        
        optimizer.zero_grad()
        score = model(x_batch)
        loss = loss_fn(score, y_batch)
        
        loss.backward()
        optimizer.step()
        
        predictions = score.max(1, keepdim=True)[1]
        total_correct += predictions.eq(y_batch.view_as(predictions)).sum().item()
        total_samples += y_batch.size(0)
    
    acc = total_correct / total_samples
    return loss, acc

def evaluate():
    model.eval()
    total_correct = 0
    total_samples = 0
    with torch.no_grad():
        for x_batch, y_batch in test_dataloader:
            x_batch = x_batch.to(device)
            y_batch = y_batch.to(device)
            score = model(x_batch)
            loss = loss_fn(score, y_batch)
            predictions = score.max(1, keepdim=True)[1]
            total_correct += predictions.eq(y_batch.view_as(predictions)).sum().item()
            total_samples += y_batch.size(0)
        
    acc = total_correct / total_samples
    return loss, acc

max_epochs = 200
final_accuracy = 0.0
for epoch in range(max_epochs):
    train()
    _, final_accuracy = evaluate()

print(f"ACCURACY={final_accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Fixed accuracy calculation in train/evaluate functions to accumulate correct predictions across all batches. 
# 2. Removed all print statements inside the training loop and data processing.
# 3. Removed interactive code snippets used for debugging (e.g., printing model weights or checking first batch scores).
# 4. Standardized variable names in loops (e.g., using x_batch instead of shadowing outer x).
# 5. Added specific ACCURACY output format as requested.