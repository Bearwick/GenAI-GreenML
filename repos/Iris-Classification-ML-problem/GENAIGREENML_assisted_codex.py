# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

SEED = 4096

def set_seed(seed):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def is_numeric(val):
    try:
        float(val)
        return True
    except (TypeError, ValueError):
        return False

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
        sep_used = None
        decimal_used = "."
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
        sep_used = ";"
        decimal_used = ","
    else:
        if df.shape[1] <= 1:
            df = pd.read_csv(path, sep=";", decimal=",")
            sep_used = ";"
            decimal_used = ","
    cols = list(df.columns)
    if cols:
        numeric_cols = sum(is_numeric(c) for c in cols)
        if numeric_cols >= len(cols) / 2:
            read_kwargs = {"header": None}
            if sep_used is not None:
                read_kwargs["sep"] = sep_used
                read_kwargs["decimal"] = decimal_used
            df = pd.read_csv(path, **read_kwargs)
    return df

def preprocess(df):
    feature_df = df.iloc[:, :-1]
    label_series = df.iloc[:, -1]
    feature_df = feature_df.apply(pd.to_numeric, errors="coerce")
    valid_mask = ~feature_df.isna().any(axis=1)
    feature_df = feature_df[valid_mask]
    label_series = label_series[valid_mask]
    labels = label_series.astype("category").cat.codes
    x = feature_df.to_numpy(dtype=np.float32, copy=False)
    y = labels.to_numpy(dtype=np.int64, copy=False)
    if (y < 0).any():
        mask = y >= 0
        x = x[mask]
        y = y[mask]
    return x, y

def shuffle_and_scale(x, y):
    indices = np.random.permutation(len(y))
    x = x[indices]
    y = y[indices]
    mu = x.mean(axis=0)
    span = x.max(axis=0) - x.min(axis=0)
    span[span == 0] = 1.0
    x -= mu
    x /= span
    return x, y

class IrisNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.fn1 = nn.Linear(input_dim, hidden_dim)
        self.fn2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = F.relu(self.fn1(x))
        return self.fn2(x)

def train_model(model, x_train, y_train, epochs, optimizer, loss_fn):
    model.train()
    for _ in range(epochs):
        optimizer.zero_grad()
        outputs = model(x_train)
        loss = loss_fn(outputs, y_train)
        loss.backward()
        optimizer.step()

def evaluate_model(model, x_test, y_test):
    model.eval()
    with torch.no_grad():
        outputs = model(x_test)
        preds = outputs.argmax(dim=1)
        num_correct = preds.eq(y_test).sum().item()
        accuracy = num_correct / y_test.size(0)
    return accuracy

def main():
    set_seed(SEED)
    df = read_csv_robust("iris.data")
    x, y = preprocess(df)
    del df
    x, y = shuffle_and_scale(x, y)
    input_dim = x.shape[1]
    output_dim = int(y.max()) + 1 if y.size > 0 else 0
    n = len(y)
    num_train = int(n * 0.6)
    num_test = n - num_train
    x_train = x[:num_train]
    y_train = y[:num_train]
    x_test = x[-num_test:]
    y_test = y[-num_test:]
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    x_train_t = torch.from_numpy(x_train).to(device)
    y_train_t = torch.from_numpy(y_train).to(device)
    x_test_t = torch.from_numpy(x_test).to(device)
    y_test_t = torch.from_numpy(y_test).to(device)
    model = IrisNN(input_dim, 6, output_dim).to(device)
    loss_fn = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)
    train_model(model, x_train_t, y_train_t, 200, optimizer, loss_fn)
    accuracy = evaluate_model(model, x_test_t, y_test_t)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed Dataset/DataLoader overhead by training on full-batch tensors.
# - Moved data to the target device once and skipped per-epoch evaluation/logging to reduce redundant work.
# - Normalized features in-place and filtered non-numeric rows efficiently to minimize extra allocations.