# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import torch
import torch.nn as nn

SEED = 4096
torch.manual_seed(SEED)
use_cuda = torch.cuda.is_available()
if use_cuda:
    torch.cuda.manual_seed_all(SEED)
np.random.seed(SEED)

def prepare_data(file_path):
    data = np.genfromtxt(file_path, delimiter=",", dtype=str, autostrip=True)
    if data.ndim == 1:
        data = data.reshape(1, -1)
    data = data[(data[:, -1] != "") & (data[:, -1] != "nan")]
    x = data[:, :4].astype(np.float32)
    labels = data[:, -1]
    _, y = np.unique(labels, return_inverse=True)
    y = y.astype(np.int64)
    perm = np.random.permutation(len(x))
    x = x[perm]
    y = y[perm]
    mu = x.mean(axis=0, dtype=np.float32)
    span = np.ptp(x, axis=0)
    x -= mu
    x /= span
    split = int(len(x) * 0.6)
    return x[:split], y[:split], x[split:], y[split:]

def train_model(model, X, y, loss_fn, optimizer, epochs):
    model.train()
    for _ in range(epochs):
        optimizer.zero_grad()
        outputs = model(X)
        loss = loss_fn(outputs, y)
        loss.backward()
        optimizer.step()

def evaluate_model(model, X, y):
    model.eval()
    with torch.inference_mode():
        logits = model(X)
        preds = logits.argmax(dim=1)
        return (preds == y).sum().item() / y.numel()

file_path = "iris.data"
x_train, y_train, x_test, y_test = prepare_data(file_path)

device = torch.device("cuda" if use_cuda else "cpu")

X_train = torch.from_numpy(x_train).to(device)
y_train_t = torch.from_numpy(y_train).to(device)
X_test = torch.from_numpy(x_test).to(device)
y_test_t = torch.from_numpy(y_test).to(device)

model = nn.Sequential(
    nn.Linear(4, 6),
    nn.ReLU(),
    nn.Linear(6, 3),
).to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)

train_model(model, X_train, y_train_t, loss_fn, optimizer, epochs=200)
accuracy = evaluate_model(model, X_test, y_test_t)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Replaced pandas with NumPy-only loading to reduce dependency overhead and data movement.
# Used full-batch tensors to avoid Dataset/DataLoader overhead and repeated device transfers.
# Normalized features in-place and used np.ptp to minimize temporary arrays and extra passes.
# Moved all data to the device once and used inference_mode for efficient evaluation.
# Removed per-epoch evaluation/printing while keeping the training loop and model architecture intact.