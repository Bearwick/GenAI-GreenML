# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

SEED = 4096
np.random.seed(SEED)
torch.manual_seed(SEED)
use_cuda = torch.cuda.is_available()
if use_cuda:
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

FILE_PATH = "iris.data"
TRAIN_RATIO = 0.6
BATCH_SIZE = 128
EPOCHS = 200
HIDDEN_UNITS = 6

DATASET_HEADERS = []
env_headers = os.environ.get("DATASET_HEADERS")
if env_headers:
    parsed = [h.strip() for h in env_headers.split(",") if h.strip()]
    if parsed:
        DATASET_HEADERS = parsed

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
        sep_used = ","
        decimal_used = "."
        if df.shape[1] == 1:
            df = pd.read_csv(path, sep=";", decimal=",")
            sep_used = ";"
            decimal_used = ","
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
        sep_used = ";"
        decimal_used = ","
    if df.shape[1] > 0:
        colnames = list(df.columns)
        numeric_name_count = pd.to_numeric(pd.Series(colnames), errors="coerce").notna().sum()
        if numeric_name_count >= len(colnames) - 1:
            df = pd.read_csv(path, sep=sep_used, decimal=decimal_used, header=None)
    if df.shape[1] == 1:
        df = pd.read_csv(path, sep=";", decimal=",", header=None)
    df = df.dropna(how="all")
    if DATASET_HEADERS and len(DATASET_HEADERS) == df.shape[1]:
        df.columns = list(DATASET_HEADERS)
    return df

def prepare_data(df, train_ratio):
    label_col = df.columns[-1]
    df[label_col] = df[label_col].astype("category").cat.codes
    x = df.iloc[:, :-1].to_numpy(dtype=np.float32)
    y = df.iloc[:, -1].to_numpy(dtype=np.int64)
    n = x.shape[0]
    perm = np.random.permutation(n)
    x = x[perm]
    y = y[perm]
    mu = x.mean(axis=0, dtype=np.float32)
    span = x.max(axis=0) - x.min(axis=0)
    x = (x - mu) / span
    x = x.astype(np.float32, copy=False)
    num_train = int(n * train_ratio)
    x_train = x[:num_train]
    y_train = y[:num_train]
    x_test = x[num_train:]
    y_test = y[num_train:]
    return x_train, y_train, x_test, y_test

class IrisNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.fn1 = nn.Linear(input_dim, hidden_dim)
        self.fn2 = nn.Linear(hidden_dim, output_dim)
    def forward(self, x):
        return self.fn2(F.relu(self.fn1(x)))

def iterate_batches(x, y, batch_size):
    n = x.shape[0]
    for start in range(0, n, batch_size):
        end = start + batch_size
        yield x[start:end], y[start:end]

def train_model(model, x_train, y_train, loss_fn, optimizer, epochs, batch_size):
    for _ in range(epochs):
        model.train()
        for xb, yb in iterate_batches(x_train, y_train, batch_size):
            optimizer.zero_grad(set_to_none=True)
            scores = model(xb)
            loss = loss_fn(scores, yb)
            loss.backward()
            optimizer.step()

def evaluate_model(model, x_test, y_test, batch_size):
    model.eval()
    accuracy = 0.0
    with torch.no_grad():
        for xb, yb in iterate_batches(x_test, y_test, batch_size):
            scores = model(xb)
            pred = scores.argmax(dim=1)
            accuracy = (pred == yb).sum().item() / yb.size(0)
    return accuracy

def main():
    df = read_csv_robust(FILE_PATH)
    x_train, y_train, x_test, y_test = prepare_data(df, TRAIN_RATIO)
    device = torch.device("cuda" if use_cuda else "cpu")
    input_dim = x_train.shape[1]
    max_label = int(y_train.max()) if y_train.size else -1
    if y_test.size:
        max_label = max(max_label, int(y_test.max()))
    num_classes = max_label + 1
    model = IrisNN(input_dim, HIDDEN_UNITS, num_classes).to(device)
    loss_fn = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)
    x_train_t = torch.from_numpy(x_train).to(device)
    y_train_t = torch.from_numpy(y_train).to(device)
    x_test_t = torch.from_numpy(x_test).to(device)
    y_test_t = torch.from_numpy(y_test).to(device)
    train_model(model, x_train_t, y_train_t, loss_fn, optimizer, EPOCHS, BATCH_SIZE)
    accuracy = evaluate_model(model, x_test_t, y_test_t, BATCH_SIZE)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced DataLoader/Dataset with lightweight tensor slicing to cut iteration overhead.
# - Moved tensors to the target device once to avoid repeated transfers each epoch.
# - Removed per-epoch evaluation and unused logging to eliminate redundant computation.
# - Normalized features in a single pass while keeping float32 to reduce memory usage.
# - Added robust CSV loading with delimiter fallback and header detection for reliable parsing.