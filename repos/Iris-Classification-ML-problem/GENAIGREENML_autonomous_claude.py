# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer

# Load dataset
df = pd.read_csv("data.csv")

# Identify target column (assume last column is the target)
target_col = df.columns[-1]
feature_cols = [c for c in df.columns if c != target_col]

X = df[feature_cols].copy()
y = df[target_col].copy()

# Encode target if categorical
if y.dtype == object or y.dtype.name == "category":
    le_target = LabelEncoder()
    y = le_target.fit_transform(y)

# Separate numeric and categorical features
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = X.select_dtypes(include=["object", "category"]).columns.tolist()

# Impute numeric features with median (efficient and robust)
if numeric_cols:
    num_imputer = SimpleImputer(strategy="median")
    X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])

# Encode categorical features with LabelEncoder (lightweight vs one-hot for high cardinality)
label_encoders = {}
if categorical_cols:
    cat_imputer = SimpleImputer(strategy="most_frequent")
    X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])
    for col in categorical_cols:
        le = LabelEncoder()
        X[col] = le.fit_transform(X[col].astype(str))
        label_encoders[col] = le

# Scale numeric features for consistency
scaler = StandardScaler()
X[numeric_cols] = scaler.fit_transform(X[numeric_cols])

# Convert to numpy arrays
X_arr = X.values.astype(np.float32)
y_arr = y if isinstance(y, np.ndarray) else y.values

# Train/test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X_arr, y_arr, test_size=0.2, random_state=42, stratify=y_arr
)

# RandomForest: good balance of accuracy and efficiency for small-scale tasks
# n_estimators=100 is a reasonable default; n_jobs=-1 for CPU parallelism
model = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1,
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. RandomForestClassifier chosen: efficient on CPU, no GPU needed, handles mixed features well
# 2. LabelEncoding for categoricals: O(n) memory vs exponential for one-hot with high cardinality
# 3. Median imputation: robust to outliers, O(n log n) computation
# 4. StandardScaler on numeric features: helps tree ensembles marginally, essential if switching models
# 5. float32 used to reduce memory footprint by 50% compared to float64
# 6. n_jobs=-1 leverages all CPU cores for parallel tree construction
# 7. min_samples_split=5, min_samples_leaf=2: light regularization to prevent overfitting and reduce tree depth
# 8. No deep learning: avoids unnecessary energy consumption for small-scale tabular data
# 9. Stratified split ensures balanced class representation in train/test sets