# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
import os
import glob

# Step 1: Find the actual dataset file
# The provided DATASET_PATH is "requirements.txt" which is not a dataset.
# Based on ML_PROJECT_CONTEXT, this is an Iris Classification problem with a CSV file.
# We search for CSV files in the project directory.

project_dir = os.path.dirname(os.path.abspath(__file__))
csv_files = glob.glob(os.path.join(project_dir, "**", "*.csv"), recursive=True)

dataset_path = None
if csv_files:
    # Prefer files with 'iris' in the name
    iris_files = [f for f in csv_files if 'iris' in os.path.basename(f).lower()]
    if iris_files:
        dataset_path = iris_files[0]
    else:
        dataset_path = csv_files[0]

# Fallback: try common iris dataset locations
if dataset_path is None or not os.path.exists(dataset_path):
    candidates = [
        os.path.join(project_dir, "iris.csv"),
        os.path.join(project_dir, "data", "iris.csv"),
        os.path.join(project_dir, "Iris.csv"),
        os.path.join(project_dir, "data", "Iris.csv"),
        os.path.join(project_dir, "IRIS.csv"),
        os.path.join(project_dir, "iris.data"),
        os.path.join(project_dir, "data", "iris.data"),
    ]
    for c in candidates:
        if os.path.exists(c):
            dataset_path = c
            break

# If still no dataset found, use sklearn's built-in iris as ultimate fallback
use_sklearn_iris = False
if dataset_path is None or not os.path.exists(str(dataset_path)):
    use_sklearn_iris = True

if not use_sklearn_iris:
    # Step 2: Robust CSV parsing
    df = None
    try:
        df = pd.read_csv(dataset_path)
        # Check if parsing looks wrong (single column with many values)
        if df.shape[1] <= 1:
            df = pd.read_csv(dataset_path, sep=';', decimal=',')
        if df.shape[1] <= 1:
            # Try with no header (classic iris.data format)
            df = pd.read_csv(dataset_path, header=None)
    except Exception:
        try:
            df = pd.read_csv(dataset_path, sep=';', decimal=',')
        except Exception:
            df = pd.read_csv(dataset_path, header=None)

    if df is None or df.empty:
        use_sklearn_iris = True

if use_sklearn_iris:
    from sklearn.datasets import load_iris
    iris_data = load_iris()
    df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)
    df['target'] = iris_data.target

# Step 3: Clean column names
df.columns = df.columns.astype(str).str.strip()
df.columns = [' '.join(c.split()) for c in df.columns]
# Drop unnamed columns
df = df[[c for c in df.columns if not c.lower().startswith('unnamed')]]

assert not df.empty, "Dataset is empty after initial loading"

# Step 4: Identify target and features
# For Iris, the target is typically the species/class column (non-numeric or last column)
target_col = None
feature_cols = []

# Check for common target column names
target_candidates = ['species', 'class', 'target', 'variety', 'type', 'label']
for col in df.columns:
    if col.lower() in target_