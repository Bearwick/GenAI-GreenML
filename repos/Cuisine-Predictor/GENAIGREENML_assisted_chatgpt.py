# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import json
from collections import defaultdict, Counter
from pathlib import Path

import numpy as np
from unidecode import unidecode


SEED = 42


def _load_json(path: str):
    return json.loads(Path(path).read_text(encoding="utf-8"))


def _normalize_ingredient(s: str) -> str:
    return unidecode(s.lower())


def build_model(train_data):
    class_doc_count = Counter()
    class_token_count = Counter()
    ingredient_class_df = defaultdict(Counter)

    for doc in train_data:
        c = doc["cuisine"]
        class_doc_count[c] += 1
        ing_list = doc["ingredients"]
        class_token_count[c] += len(ing_list)
        doc_id = doc["id"]
        for ing in ing_list:
            ingredient_class_df[_normalize_ingredient(ing)][c] += 1

    classes = list(class_token_count.keys())
    vocab_size = len(ingredient_class_df)
    n_docs = len(train_data)
    prior_prob = {c: class_doc_count[c] / n_docs for c in classes}

    return {
        "classes": classes,
        "vocab_size": vocab_size,
        "prior_prob": prior_prob,
        "class_token_count": dict(class_token_count),
        "ingredient_class_df": ingredient_class_df,
    }


def naive_based(queryset, model):
    classes = model["classes"]
    vocab_size = model["vocab_size"]
    prior_prob = model["prior_prob"]
    class_token_count = model["class_token_count"]
    ingredient_class_df = model["ingredient_class_df"]

    qdoc_class = {}
    for doc in queryset:
        best_c = None
        best_p = None

        ingredients = doc["ingredients"]
        for c in classes:
            denom = vocab_size + class_token_count[c]
            p_ingredient = 1.0
            for ing in ingredients:
                norm_ing = _normalize_ingredient(ing)
                length = ingredient_class_df.get(norm_ing, {}).get(c, 0)
                p_ingredient *= (1.0 + length) / denom

            p = p_ingredient * prior_prob[c]
            if best_p is None or p > best_p:
                best_p = p
                best_c = c

        qdoc_class[doc["id"]] = best_c

    return qdoc_class


def empirical_accuracy(train_data, model):
    empirical_dict = naive_based(train_data, model)
    correct = 0
    for doc in train_data:
        if doc["cuisine"] == empirical_dict[doc["id"]]:
            correct += 1
    return correct / len(train_data) if train_data else 0.0


def main():
    np.random.seed(SEED)

    train_data = _load_json("train.json")
    model = build_model(train_data)

    accuracy = empirical_accuracy(train_data, model)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced per-ingredient storage of doc-id lists with per-class document-frequency counts (Counter), preserving len(list) behavior while cutting memory and list-append overhead.
# - Collapsed multiple passes over training data into a single pass to reduce redundant computation and data movement.
# - Avoided building an explicit vocab list; used vocab_size from the ingredient dictionary size to prevent extra allocations.
# - Removed unused imports and all non-required side effects (no test.json inference, no file writes, no extra prints).
# - Used local variable bindings inside the classifier loop to reduce repeated global/dict lookups.
# - Set a fixed seed (numpy) to improve reproducibility even though the pipeline is deterministic.