# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import json
import math
from unidecode import unidecode


def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def build_model(train_data):
    class_dcount = {}
    class_tcount = {}
    token_class_df = {}
    vocab_set = set()

    for doc in train_data:
        cuisine = doc["cuisine"]
        ingredients = doc["ingredients"]

        class_dcount[cuisine] = class_dcount.get(cuisine, 0) + 1
        class_tcount[cuisine] = class_tcount.get(cuisine, 0) + len(ingredients)

        for ing in ingredients:
            ing_norm = unidecode(ing).lower()
            vocab_set.add(ing_norm)
            per_class = token_class_df.get(ing_norm)
            if per_class is None:
                per_class = {}
                token_class_df[ing_norm] = per_class
            per_class[cuisine] = per_class.get(cuisine, 0) + 1

    classes = tuple(class_dcount.keys())
    vocab_size = len(vocab_set)
    total_docs = len(train_data)

    log_prior = {c: math.log(class_dcount[c] / total_docs) for c in classes}
    log_denom = {c: math.log(vocab_size + class_tcount[c]) for c in classes}

    return classes, log_prior, log_denom, token_class_df


def predict_doc(doc, classes, log_prior, log_denom, token_class_df):
    ingredients = doc["ingredients"]
    best_class = None
    best_score = -float("inf")

    for c in classes:
        score = log_prior[c]
        denom = log_denom[c]
        for ing in ingredients:
            ing_norm = unidecode(ing).lower()
            df_c = token_class_df.get(ing_norm, {}).get(c, 0)
            score += math.log1p(df_c) - denom
        if score > best_score:
            best_score = score
            best_class = c

    return best_class


def accuracy_on_train(train_data, classes, log_prior, log_denom, token_class_df):
    correct = 0
    total = len(train_data)

    for doc in train_data:
        pred = predict_doc(doc, classes, log_prior, log_denom, token_class_df)
        if pred == doc["cuisine"]:
            correct += 1

    return correct / total if total else 0.0


def main():
    train_data = load_json("train.json")
    classes, log_prior, log_denom, token_class_df = build_model(train_data)
    accuracy = accuracy_on_train(train_data, classes, log_prior, log_denom, token_class_df)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Removed pandas/Excel output and all non-essential I/O to reduce runtime and energy use.
# - Built a lightweight model representation: token->class document frequency counts (ints) instead of storing full doc-id lists, reducing memory and data movement.
# - Avoided redundant passes by aggregating class doc counts, class token counts, vocabulary, and token-class counts in a single loop over training data.
# - Switched from multiplying many tiny probabilities to log-space scoring (log priors + sum log likelihoods) to prevent underflow and reduce expensive multiplications.
# - Precomputed per-class constant denominators in log-space (log(vocab_size + class_tcount[c])) to avoid recomputing inside inner loops.
# - Kept computation local (no globals) and used simple dict/set structures to minimize overhead and improve reproducibility.