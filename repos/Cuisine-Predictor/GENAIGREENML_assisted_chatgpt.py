# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import json
import math
import os
import random
from collections import Counter, defaultdict


SEED = 42
random.seed(SEED)
os.environ.setdefault("PYTHONHASHSEED", str(SEED))


def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def build_model(train_data):
    class_dcount = Counter()
    class_tcount = Counter()
    ingredient_class_df = defaultdict(Counter)
    vocab_set = set()

    for doc in train_data:
        c = doc["cuisine"]
        ingredients = [ing.lower() for ing in doc["ingredients"]]
        class_dcount[c] += 1
        class_tcount[c] += len(ingredients)
        for ing in ingredients:
            vocab_set.add(ing)
            ingredient_class_df[ing][c] += 1

    total_docs = len(train_data)
    classes = sorted(class_dcount.keys())
    vocab_size = len(vocab_set)

    log_prior = {c: math.log(class_dcount[c] / total_docs) for c in classes}
    log_denom = {c: math.log(vocab_size + class_tcount[c]) for c in classes}

    return {
        "classes": classes,
        "vocab_size": vocab_size,
        "log_prior": log_prior,
        "log_denom": log_denom,
        "ingredient_class_df": ingredient_class_df,
    }


def predict(model, queryset):
    classes = model["classes"]
    log_prior = model["log_prior"]
    log_denom = model["log_denom"]
    ingredient_class_df = model["ingredient_class_df"]

    predictions = {}
    for doc in queryset:
        ingredients = [ing.lower() for ing in doc["ingredients"]]
        best_c = None
        best_score = None

        for c in classes:
            score = log_prior[c] - (len(ingredients) * log_denom[c])
            for ing in ingredients:
                score += math.log(1 + ingredient_class_df.get(ing, {}).get(c, 0))
            if best_score is None or score > best_score:
                best_score = score
                best_c = c

        predictions[doc["id"]] = best_c
    return predictions


def accuracy_on_train(model, train_data):
    preds = predict(model, train_data)
    correct = 0
    for doc in train_data:
        if preds.get(doc["id"]) == doc["cuisine"]:
            correct += 1
    return correct / len(train_data) if train_data else 0.0


def main():
    train_data = load_json("train.json")
    model = build_model(train_data)

    test_data = load_json("test.json")
    _ = predict(model, test_data)

    accuracy = accuracy_on_train(model, train_data)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed redundant passes over training data by building class counts, token counts, and ingredient-class document frequencies in a single loop.
# - Replaced storing full document-id lists per ingredient/class with integer document-frequency counts to cut memory and data movement while preserving probability math.
# - Switched to log-probabilities to avoid expensive/unstable repeated multiplications and reduce underflow; argmax remains identical.
# - Precomputed per-class constant terms (log priors and denominators) to avoid recomputation inside inner loops.
# - Avoided constructing an explicit vocabulary list; used a set for uniqueness and only kept its size, minimizing allocations.
# - Ensured reproducibility by fixing the seed and PYTHONHASHSEED without affecting model logic.
# - Removed file outputs, interactive behavior, prints, and plots; kept only the required final accuracy print.