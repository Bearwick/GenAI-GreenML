# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import glob
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score

from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.dummy import DummyClassifier


def _find_dataset_file():
    candidates = []
    for ext in ("csv", "tsv", "txt", "data"):
        candidates.extend(glob.glob(os.path.join(".", f"*.{ext}")))
    candidates = [p for p in candidates if os.path.isfile(p)]

    if not candidates:
        raise FileNotFoundError("No dataset file found in current directory (expected .csv/.tsv/.txt/.data).")

    def score_path(p):
        name = os.path.basename(p).lower()
        score = 0
        if "train" in name:
            score -= 5
        if "test" in name:
            score -= 5
        if "sample" in name:
            score -= 5
        if "submission" in name:
            score -= 10
        if name.endswith(".csv"):
            score += 2
        if name.endswith(".tsv"):
            score += 1
        try:
            size = os.path.getsize(p)
        except OSError:
            size = 0
        return (score, size)

    candidates.sort(key=score_path, reverse=True)
    return candidates[0]


def _read_table(path):
    _, ext = os.path.splitext(path.lower())
    if ext == ".csv":
        return pd.read_csv(path)
    if ext == ".tsv":
        return pd.read_csv(path, sep="\t")
    try:
        return pd.read_csv(path)
    except Exception:
        return pd.read_csv(path, sep=None, engine="python")


def _infer_target_column(df):
    # Prefer explicit target-like names; otherwise use the last column.
    target_candidates = [
        "target", "label", "y", "class", "outcome", "response", "churn", "default",
        "fraud", "diagnosis", "survived", "species", "sentiment"
    ]
    cols_lower = {c.lower(): c for c in df.columns}
    for key in target_candidates:
        if key in cols_lower:
            return cols_lower[key]

    for suffix in ("_target", "_label", "label_", "target_"):
        for c in df.columns:
            if c.lower().endswith(suffix) or c.lower().startswith(suffix):
                return c

    return df.columns[-1]


def _build_preprocessor(X):
    num_cols = X.select_dtypes(include=[np.number, "bool"]).columns.tolist()
    cat_cols = [c for c in X.columns if c not in num_cols]

    numeric_pipeline = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
    ])

    categorical_pipeline = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_pipeline, num_cols),
            ("cat", categorical_pipeline, cat_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3
    )
    return preprocessor


def _pick_model(X, y):
    # Prefer very light linear models; choose based on dimensionality and class count.
    n_classes = len(pd.unique(y))
    if n_classes <= 1:
        return DummyClassifier(strategy="most_frequent")

    # If target is highly imbalanced or binary, logistic regression is usually strong and efficient.
    # Use saga for sparse + large features; liblinear for smaller problems.
    try:
        n_samples, n_features = X.shape
    except Exception:
        n_samples, n_features = len(X), X.shape[1]

    if n_features > 2000 or n_samples > 50000:
        return LogisticRegression(
            solver="saga",
            max_iter=200,
            n_jobs=1,
            random_state=42
        )

    # LinearSVC can be faster than LR on some sparse high-dim data, but doesn't provide probabilities.
    if 200 < n_features <= 2000:
        return LinearSVC(random_state=42)

    # For very small feature spaces, LR is stable and accurate.
    return LogisticRegression(
        solver="liblinear",
        max_iter=300,
        random_state=42
    )


def main():
    path = _find_dataset_file()
    df = _read_table(path)

    # Drop empty columns to reduce processing and memory.
    df = df.dropna(axis=1, how="all")
    if df.shape[1] < 2:
        raise ValueError("Dataset must contain at least one feature column and one target column.")

    target_col = _infer_target_column(df)
    y = df[target_col]
    X = df.drop(columns=[target_col])

    # If index-like columns exist, drop simple monotonically increasing IDs to reduce noise/compute.
    for c in list(X.columns):
        if X[c].dtype.kind in ("i", "u") and X[c].nunique(dropna=False) == len(X):
            # Heuristic: likely ID; remove for energy-efficient training and generalization.
            X = X.drop(columns=[c])

    # Ensure y is 1D and handle missing labels.
    y = y.copy()
    mask = ~pd.isna(y)
    X = X.loc[mask].reset_index(drop=True)
    y = y.loc[mask].reset_index(drop=True)

    # Split with stratification when possible for stable accuracy estimates.
    stratify = None
    if len(pd.unique(y)) > 1:
        vc = y.value_counts(dropna=False)
        if (vc.min() >= 2) and (len(vc) <= len(y) // 2):
            stratify = y

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=stratify
    )

    preprocessor = _build_preprocessor(X_train)

    # Build model after preprocessing to use transformed dimensionality if needed.
    # Fit-transform a tiny sample to estimate feature count cheaply.
    sample = X_train.iloc[: min(len(X_train), 256)]
    Xt_sample = preprocessor.fit_transform(sample)
    model = _pick_model(Xt_sample, y_train.iloc[: len(sample)])

    # Recreate preprocessor to fit on full training data (avoid leakage from sampling).
    preprocessor = _build_preprocessor(X_train)

    clf = Pipeline(steps=[
        ("prep", preprocessor),
        ("model", model),
    ])

    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Uses lightweight, CPU-friendly linear models (LogisticRegression/LinearSVC) chosen heuristically to avoid oversized methods.
# - OneHotEncoder with sparse output keeps memory low for categorical features; no embeddings/deep learning used.
# - SimpleImputer (median/most_frequent) provides deterministic preprocessing with minimal compute.
# - Drops all-empty columns and likely ID columns to reduce dimensionality and training cost.
# - Uses a single train/test split and avoids cross-validation to minimize compute while keeping reproducibility (random_state=42).
# - Avoids plotting, interactive inputs, and model persistence to keep runtime and I/O energy low.