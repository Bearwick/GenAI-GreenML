# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import json
import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.naive_bayes import MultinomialNB
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score
from sklearn.linear_model import Ridge


RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed_columns(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _load_dataset(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"Dataset not found at path: {path}")

    # Prefer JSON parsing for .json; fall back to pandas readers if needed.
    df = None
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, list):
            df = pd.DataFrame(data)
        elif isinstance(data, dict):
            # Attempt common patterns: {"data":[...]} or {"train":[...]} else normalize keys.
            for k in ("data", "train", "records", "items"):
                if k in data and isinstance(data[k], list):
                    df = pd.DataFrame(data[k])
                    break
            if df is None:
                try:
                    df = pd.json_normalize(data)
                except Exception:
                    df = pd.DataFrame([data])
        else:
            df = None
    except Exception:
        df = None

    if df is None or df.empty:
        # Robust CSV fallback per requirements (even if file is .json)
        try:
            df = pd.read_csv(path)
        except Exception:
            df = None
        if df is None or df.shape[1] <= 1:
            try:
                df = pd.read_csv(path, sep=";", decimal=",")
            except Exception as e:
                raise RuntimeError(f"Failed to load dataset from {path}: {e}")

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed_columns(df)
    return df


def _pick_target_and_features(df):
    cols = list(df.columns)

    # Prefer common labels if present, but never hard-fail.
    candidate_targets = []
    for name in cols:
        low = str(name).strip().lower()
        if low in ("cuisine", "label", "target", "y", "class"):
            candidate_targets.append(name)

    if candidate_targets:
        target_col = candidate_targets[0]
    else:
        # Prefer non-constant low-cardinality object column as classification target
        obj_cols = [c for c in cols if df[c].dtype == "object"]
        obj_nonconst = []
        for c in obj_cols:
            nun = df[c].nunique(dropna=True)
            if nun >= 2 and nun <= max(2, min(50, int(0.2 * len(df)) + 2)):
                obj_nonconst.append((nun, c))
        if obj_nonconst:
            obj_nonconst.sort(key=lambda x: x[0])
            target_col = obj_nonconst[0][1]
        else:
            # Fallback: choose a numeric non-constant column (regression)
            num_candidates = []
            for c in cols:
                s = pd.to_numeric(df[c], errors="coerce")
                nun = s.nunique(dropna=True)
                if nun >= 2:
                    num_candidates.append((nun, c))
            target_col = num_candidates[0][1] if num_candidates else cols[-1]

    feature_cols = [c for c in cols if c != target_col]
    if not feature_cols:
        # If only target exists, create a trivial feature to keep pipeline running.
        df["_bias_feature_"] = 1
        feature_cols = ["_bias_feature_"]
    return target_col, feature_cols


def _safe_text_from_ingredients(x):
    # Accept list-like ingredients or strings; convert to lightweight tokenizable text.
    if isinstance(x, list) or isinstance(x, tuple):
        parts = []
        for item in x:
            if item is None:
                continue
            s = str(item).strip().lower()
            if s:
                parts.append(s)
        return " ".join(parts)
    if x is None or (isinstance(x, float) and not np.isfinite(x)):
        return ""
    return str(x).strip().lower()


def _build_preprocessor(df, feature_cols):
    # Identify ingredient-like columns to textify (lists/strings) and keep CPU-friendly bag-of-words.
    n = len(df)
    text_cols = []
    cat_cols = []
    num_cols = []

    # Heuristic: if any value is list/tuple in a column, treat as text ingredients.
    for c in feature_cols:
        s = df[c]
        dtype = s.dtype
        is_text = False

        # Sample a few non-null entries to detect list-like or rich text.
        sample = s.dropna()
        if len(sample) > 0:
            sample_vals = sample.iloc[: min(50, len(sample))].tolist()
            for v in sample_vals:
                if isinstance(v, (list, tuple)):
                    is_text = True
                    break
            if not is_text and dtype == "object":
                # If object with moderate-to-long strings, treat as text
                str_lens = [len(str(v)) for v in sample_vals[: min(30, len(sample_vals))]]
                if str_lens and (np.mean(str_lens) >= 12):
                    is_text = True

        if is_text:
            text_cols.append(c)
        else:
            if pd.api.types.is_numeric_dtype(dtype):
                num_cols.append(c)
            else:
                cat_cols.append(c)

    transformers = []

    if text_cols:
        # Combine multiple text columns into a single sparse token space by concatenating per row.
        # Using CountVectorizer + MultinomialNB is typically very efficient on CPU for such tasks.
        def _concat_text_cols(X):
            # X is a DataFrame with text_cols
            out = []
            for i in range(len(X)):
                parts = []
                for col in X.columns:
                    parts.append(_safe_text_from_ingredients(X.iloc[i][col]))
                out.append(" ".join([p for p in parts if p]))
            return np.array(out, dtype=object)

        from sklearn.preprocessing import FunctionTransformer

        text_pipe = Pipeline(
            steps=[
                ("to_text", FunctionTransformer(_concat_text_cols, validate=False)),
                ("vect", CountVectorizer(lowercase=True, min_df=1, token_pattern=r"(?u)\b[a-zA-Z][a-zA-Z]+\b")),
            ]
        )
        transformers.append(("text", text_pipe, text_cols))

    if cat_cols:
        cat_pipe = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
            ]
        )
        transformers.append(("cat", cat_pipe, cat_cols))

    if num_cols:
        num_pipe = Pipeline(
            steps=[
                ("to_num", FunctionTransformer(lambda X: pd.DataFrame(X).apply(pd.to_numeric, errors="coerce"), validate=False)),
                ("imputer", SimpleImputer(strategy="median")),
            ]
        )
        transformers.append(("num", num_pipe, num_cols))

    if not transformers:
        # Ensure at least one transformer exists
        from sklearn.preprocessing import FunctionTransformer
        passthrough = Pipeline(steps=[("const", FunctionTransformer(lambda X: np.ones((len(X), 1)), validate=False))])
        transformers.append(("const", passthrough, feature_cols[:1]))

    preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)
    return preprocessor


def main():
    df = _load_dataset("train.json")
    assert df is not None and not df.empty, "Dataset is empty after loading."

    # Normalize column names already done; now pick target/features defensively.
    target_col, feature_cols = _pick_target_and_features(df)

    y_raw = df[target_col]
    X = df[feature_cols].copy()

    # Decide task type robustly.
    # Prefer classification if target is object or has low number of unique values.
    y_is_numeric = pd.api.types.is_numeric_dtype(y_raw.dtype)
    if y_is_numeric:
        y_num = pd.to_numeric(y_raw, errors="coerce")
        nun = y_num.nunique(dropna=True)
        task = "classification" if (nun >= 2 and nun <= 20) else "regression"
    else:
        nun = y_raw.nunique(dropna=True)
        task = "classification" if nun >= 2 else "regression"

    # Drop rows with missing target (safe)
    valid_mask = ~pd.isna(y_raw)
    X = X.loc[valid_mask].reset_index(drop=True)
    y_raw = y_raw.loc[valid_mask].reset_index(drop=True)
    assert len(X) > 0, "No samples remain after removing missing target."

    preprocessor = _build_preprocessor(df.loc[valid_mask].reset_index(drop=True), feature_cols)

    if task == "classification":
        y = y_raw.astype(str)
        # If still degenerate, fallback to regression-like path but with dummy accuracy.
        if y.nunique(dropna=True) < 2:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=RANDOM_STATE, shuffle=True
            )
            assert len(X_train) > 0 and len(X_test) > 0, "Train/test split produced empty set."
            model = Pipeline(
                steps=[
                    ("preprocess", preprocessor),
                    ("clf", DummyClassifier(strategy="most_frequent")),
                ]
            )
            model.fit(X_train, y_train)
            preds = model.predict(X_test)
            accuracy = float(accuracy_score(y_test, preds))
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=RANDOM_STATE, shuffle=True, stratify=y if y.nunique() > 1 else None
            )
            assert len(X_train) > 0 and len(X_test) > 0, "Train/test split produced empty set."

            clf = MultinomialNB(alpha=0.5)
            model = Pipeline(steps=[("preprocess", preprocessor), ("clf", clf)])
            model.fit(X_train, y_train)
            preds = model.predict(X_test)
            accuracy = float(accuracy_score(y_test, preds))
    else:
        y = pd.to_numeric(y_raw, errors="coerce")
        valid_mask2 = np.isfinite(y.to_numpy())
        X = X.loc[valid_mask2].reset_index(drop=True)
        y = y.loc[valid_mask2].reset_index(drop=True)
        assert len(X) > 1, "Not enough numeric target samples for regression."

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE, shuffle=True
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split produced empty set."

        # Lightweight ridge regression; score mapped into [0,1] using bounded transform.
        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        model = Pipeline(steps=[("preprocess", preprocessor), ("reg", reg)])
        model.fit(X_train, y_train)
        r2 = float(model.score(X_test, y_test))
        # Convert to stable [0,1] proxy: clamp r2 to [-1,1], then map to [0,1]
        r2c = max(-1.0, min(1.0, r2))
        accuracy = 0.5 * (r2c + 1.0)

        # Fallback if NaN
        if not np.isfinite(accuracy):
            dummy = DummyRegressor(strategy="mean")
            model2 = Pipeline(steps=[("preprocess", preprocessor), ("reg", dummy)])
            model2.fit(X_train, y_train)
            r2 = float(model2.score(X_test, y_test))
            r2c = max(-1.0, min(1.0, r2))
            accuracy = 0.5 * (r2c + 1.0)
            if not np.isfinite(accuracy):
                accuracy = 0.0

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses MultinomialNB + CountVectorizer for ingredient text: sparse bag-of-words is CPU-friendly and energy-efficient.
# - Keeps preprocessing in a single ColumnTransformer/Pipeline to avoid redundant passes and ensure reproducibility.
# - Robust schema handling: auto-detects target/features, supports list-like ingredients, drops unnamed columns, and handles missing/degenerate targets.
# - Uses SimpleImputer and safe numeric coercion to prevent costly failures; avoids heavy models/ensembles and deep learning.
# - Regression fallback uses Ridge (lightweight) and maps R^2 to a bounded [0,1] "accuracy" proxy for a stable single metric output.