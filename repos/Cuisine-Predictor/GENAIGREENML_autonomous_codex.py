# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")

path = "train.json"
df = None
try:
    df = pd.read_json(path)
except Exception:
    try:
        df = pd.read_json(path, lines=True)
    except Exception:
        try:
            df = pd.read_csv(path)
            if df.shape[1] == 1:
                df_alt = pd.read_csv(path, sep=';', decimal=',')
                if df_alt.shape[1] > 1:
                    df = df_alt
        except Exception:
            df = pd.read_csv(path, sep=';', decimal=',')

if isinstance(df, pd.Series):
    df = df.to_frame()
if not isinstance(df, pd.DataFrame):
    df = pd.DataFrame(df)

df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
df = df.loc[:, ~df.columns.str.match(r'^Unnamed')]

assert df.shape[0] > 0 and df.shape[1] > 0

def normalize_cell(x):
    if isinstance(x, (list, tuple, set)):
        return ' '.join([str(i) for i in x])
    if isinstance(x, dict):
        return ' '.join([str(k) + ':' + str(v) for k, v in x.items()])
    return x

for col in df.columns:
    sample = df[col].dropna().head(3)
    needs_norm = False
    for v in sample:
        if isinstance(v, (list, tuple, set, dict)):
            needs_norm = True
            break
    if needs_norm:
        df[col] = df[col].apply(normalize_cell)

def safe_nunique(s):
    try:
        return s.nunique(dropna=True)
    except Exception:
        return s.astype(str).nunique(dropna=True)

target_col = None
preferred = ['target', 'label', 'class', 'cuisine', 'y']
lower_map = {c.lower(): c for c in df.columns}
for name in preferred:
    if name in lower_map:
        target_col = lower_map[name]
        break
if target_col is None:
    n_rows = len(df)
    candidate_cols = [c for c in df.columns if safe_nunique(df[c]) > 1]
    max_card = max(2, min(20, int(0.5 * n_rows))) if n_rows > 0 else 2
    low_card = [c for c in candidate_cols if safe_nunique(df[c]) <= max_card]
    if low_card:
        target_col = sorted(low_card, key=lambda c: safe_nunique(df[c]))[0]
    else:
        numeric_candidates = []
        for c in candidate_cols:
            if pd.to_numeric(df[c], errors='coerce').notna().sum() > 0:
                numeric_candidates.append(c)
        if numeric_candidates:
            target_col = numeric_candidates[0]
        else:
            target_col = candidate_cols[0] if candidate_cols else df.columns[0]

y = df[target_col]
X = df.drop(columns=[target_col]) if target_col in df.columns else df.copy()

if X.shape[1] == 0:
    X = pd.DataFrame({'__dummy__': np.zeros(len(df))})

mask = ~pd.isna(y)
X = X.loc[mask]
y = y.loc[mask]

assert len(X) > 0

numeric_cols = []
for col in X.columns:
    coerced = pd.to_numeric(X[col], errors='coerce')
    if coerced.notna().mean() > 0.8:
        numeric_cols.append(col)
        X[col] = coerced

for col in numeric_cols:
    X[col] = X[col].replace([np.inf, -np.inf], np.nan)

n_rows = len(X)
id_like = []
if n_rows > 0:
    for col in numeric_cols:
        nun = safe_nunique(X[col])
        if nun / n_rows > 0.95:
            id_like.append(col)
if id_like:
    X = X.drop(columns=id_like)
    numeric_cols = [c for c in numeric_cols if c not in id_like]

text_cols = [c for c in X.columns if c not in numeric_cols]
text_col = None
if len(text_cols) > 0:
    X[text_cols] = X[text_cols].fillna('').astype(str)
    X['__combined_text__'] = X[text_cols].agg(' '.join, axis=1)
    text_col = '__combined_text__'
    X = X.drop(columns=text_cols)

if X.shape[1] == 0:
    X = pd.DataFrame({'__dummy__': np.zeros(len(df))})
    numeric_cols = ['__dummy__']
    text_col = None

feature_cols = list(X.columns)
assert len(feature_cols) > 0 and len(X) > 0

y_numeric = pd.to_numeric(y, errors='coerce')
if y_numeric.notna().mean() > 0.9:
    nun_y = y_numeric.nunique()
    if nun_y > 15 and nun_y > 0.05 * len(y_numeric):
        task = 'regression'
        y = y_numeric
    else:
        task = 'classification'
else:
    task = 'classification'

if task == 'regression':
    y = pd.to_numeric(y, errors='coerce')
    mask = y.notna()
    X = X.loc[mask]
    y = y.loc[mask]

assert len(X) > 0

if len(X) < 2:
    X_train, X_test, y_train, y_test = X, X, y, y
else:
    test_size = 0.2 if len(X) >= 5 else 0.5
    stratify = None
    if task == 'classification' and y.nunique() > 1:
        class_counts = y.value_counts()
        n_classes = class_counts.shape[0]
        if class_counts.min() >= 2 and int(round(test_size * len(X))) >= n_classes:
            stratify = y
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )

assert len(X_train) > 0 and len(X_test) > 0

transformers = []
if text_col is not None:
    transformers.append(('text', CountVectorizer(max_features=5000), text_col))
if len(numeric_cols) > 0:
    if task == 'classification' and text_col is not None:
        num_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', MinMaxScaler())
        ])
    else:
        num_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler(with_mean=False))
        ])
    transformers.append(('num', num_pipeline, numeric_cols))
if len(transformers) == 0:
    transformers.append(('num', 'passthrough', feature_cols))

preprocessor = ColumnTransformer(transformers, remainder='drop')

if task == 'classification':
    if y.nunique() < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        if text_col is not None:
            model = MultinomialNB()
        else:
            model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    if len(y) < 2 or y.nunique() < 2:
        model = DummyRegressor(strategy='mean')
    else:
        model = Ridge(alpha=1.0)

pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('model', model)
])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    if not np.isfinite(r2):
        r2 = 0.0
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Selected lightweight linear/naive models and capped text features to keep CPU and energy usage low.
# - Used deterministic preprocessing (imputation, scaling, bag-of-words) inside sklearn pipelines for reproducibility.
# - Collapsed non-numeric columns into one text field to simplify unknown schemas and avoid heavy feature engineering.
# - Regression quality is reported via a bounded R2-to-[0,1] mapping for a stable accuracy proxy.