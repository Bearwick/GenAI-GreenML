# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import json
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, r2_score
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")
np.random.seed(42)

def find_dataset_file():
    search_dirs = ['.', '/mnt/data']
    exts = ['.csv', '.tsv', '.data', '.txt', '.json']
    candidates = []
    for d in search_dirs:
        if not os.path.isdir(d):
            continue
        for f in os.listdir(d):
            if f.startswith('.'):
                continue
            path = os.path.join(d, f)
            if os.path.isfile(path):
                ext = os.path.splitext(f)[1].lower()
                if ext in exts:
                    try:
                        size = os.path.getsize(path)
                    except Exception:
                        size = 0
                    candidates.append((size, path))
    if not candidates:
        return None
    candidates.sort(reverse=True)
    return candidates[0][1]

def read_csv_with_fallback(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
        return df
    if df.shape[1] == 1:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > 1:
                df = df_alt
        except Exception:
            pass
    return df

def read_json_with_fallback(path):
    try:
        df = pd.read_json(path)
    except Exception:
        try:
            df = pd.read_json(path, lines=True)
        except Exception:
            with open(path, 'r') as f:
                data = json.load(f)
            df = pd.json_normalize(data)
    if not isinstance(df, pd.DataFrame):
        df = pd.DataFrame(df)
    return df

def normalize_columns(df):
    new_cols = []
    counts = {}
    for c in df.columns:
        c = ' '.join(str(c).strip().split())
        if c.lower().startswith('unnamed') or c == '':
            new_cols.append(None)
            continue
        if c in counts:
            counts[c] += 1
            c = f"{c}_{counts[c]}"
        else:
            counts[c] = 0
        new_cols.append(c)
    valid_idx = [i for i, c in enumerate(new_cols) if c is not None]
    df = df.iloc[:, valid_idx]
    df.columns = [new_cols[i] for i in valid_idx]
    return df

def stringify_value(x):
    if pd.isna(x):
        return ''
    if isinstance(x, (list, tuple, set)):
        return ' '.join([str(i) for i in x])
    if isinstance(x, dict):
        return ' '.join([str(k) for k in x.keys()] + [str(v) for v in x.values()])
    return str(x)

def safe_nunique(series):
    try:
        return series.nunique(dropna=True)
    except Exception:
        return series.dropna().apply(stringify_value).nunique()

def safe_to_numeric(series):
    try:
        return pd.to_numeric(series, errors='coerce')
    except Exception:
        return pd.Series([np.nan] * len(series), index=series.index)

def choose_target_column(df):
    n_rows = len(df)
    candidate_names = ['target', 'label', 'class', 'y', 'output', 'outcome', 'result', 'cuisine']
    lower_map = {c.lower(): c for c in df.columns}
    for name in candidate_names:
        if name in lower_map:
            col = lower_map[name]
            if safe_nunique(df[col]) > 1:
                return col
    low_unique = []
    for c in df.columns:
        nunique = safe_nunique(df[c])
        if nunique > 1 and nunique <= max(20, int(0.2 * n_rows)):
            low_unique.append((nunique, c))
    if low_unique:
        low_unique.sort()
        return low_unique[0][1]
    numeric_candidates = []
    for c in df.columns:
        series = safe_to_numeric(df[c])
        nunique = series.nunique(dropna=True)
        if nunique > 1:
            numeric_candidates.append((nunique, c))
    if numeric_candidates:
        numeric_candidates.sort(reverse=True)
        return numeric_candidates[0][1]
    return df.columns[0]

dataset_path = find_dataset_file()
if dataset_path is None:
    raise FileNotFoundError("No dataset file found")

ext = os.path.splitext(dataset_path)[1].lower()
if ext == '.json':
    df = read_json_with_fallback(dataset_path)
elif ext == '.tsv':
    df = pd.read_csv(dataset_path, sep='\t')
else:
    df = read_csv_with_fallback(dataset_path)

if df is None or len(df) == 0:
    raise ValueError("Dataset is empty")

df = normalize_columns(df)
df = df.replace([np.inf, -np.inf], np.nan)
assert df.shape[0] > 0 and df.shape[1] > 0

target_col = choose_target_column(df)
df = df[df[target_col].notna()].copy()
assert len(df) > 0

y_raw = df[target_col]
task = 'classification'
y = None

if y_raw.dtype == 'object' or str(y_raw.dtype).startswith('category'):
    y_num = safe_to_numeric(y_raw)
    if y_num.notna().mean() > 0.9:
        mask = y_num.notna()
        df = df.loc[mask].copy()
        y_num = y_num.loc[mask]
        if y_num.nunique(dropna=True) <= max(20, int(0.05 * len(y_num))):
            task = 'classification'
            y = y_num.astype(str)
        else:
            task = 'regression'
            y = y_num
    else:
        task = 'classification'
        y = y_raw.apply(stringify_value).astype(str)
else:
    y_num = safe_to_numeric(y_raw)
    mask = y_num.notna()
    df = df.loc[mask].copy()
    y_num = y_num.loc[mask]
    if y_num.nunique(dropna=True) <= max(20, int(0.05 * len(y_num))):
        task = 'classification'
        y = y_num.astype(str)
    else:
        task = 'regression'
        y = y_num

assert len(df) > 0

X = df.drop(columns=[target_col])
X = X.dropna(axis=1, how='all')

numeric_cols = []
for c in X.columns:
    series = safe_to_numeric(X[c])
    non_na_ratio = series.notna().mean()
    if pd.api.types.is_numeric_dtype(X[c]) or non_na_ratio > 0.8:
        numeric_cols.append(c)
        X[c] = series

obj_cols = [c for c in X.columns if c not in numeric_cols]
for c in obj_cols:
    X[c] = X[c].apply(stringify_value)

text_cols = []
cat_cols = []
n_rows = len(X)
for c in obj_cols:
    s = X[c].fillna('').astype(str)
    avg_len = s.str.len().mean() if n_rows > 0 else 0
    nunique = s.nunique(dropna=True)
    if avg_len > 30 or nunique > max(50, int(0.5 * n_rows)):
        text_cols.append(c)
    else:
        cat_cols.append(c)

text_col = None
if text_cols:
    X['__combined_text__'] = X[text_cols].fillna('').agg(' '.join, axis=1)
    X = X.drop(columns=text_cols)
    text_col = '__combined_text__'

constant_cols = [c for c in X.columns if safe_nunique(X[c]) <= 1]
if len(X.columns) - len(constant_cols) >= 1:
    X = X.drop(columns=constant_cols)

numeric_cols = [c for c in numeric_cols if c in X.columns]
cat_cols = [c for c in cat_cols if c in X.columns]
if text_col not in X.columns:
    text_col = None

if X.shape[1] == 0:
    X['__dummy__'] = 0
    numeric_cols = ['__dummy__']
    cat_cols = []
    text_col = None

transformers = []
if numeric_cols:
    num_transformer = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler(with_mean=False))
    ])
    transformers.append(('num', num_transformer, numeric_cols))
if cat_cols:
    cat_transformer = Pipeline([
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    transformers.append(('cat', cat_transformer, cat_cols))
if text_col:
    text_transformer = TfidfVectorizer(max_features=1000)
    transformers.append(('text', text_transformer, text_col))

preprocessor = ColumnTransformer(transformers, sparse_threshold=0.3)

if task == 'classification':
    if pd.Series(y).nunique() < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    if pd.Series(y).nunique() < 2:
        model = DummyRegressor(strategy='mean')
    else:
        model = Ridge(alpha=1.0)

n_samples = len(X)
test_size = 0.2 if n_samples >= 5 else 0.5
stratify = None
if task == 'classification' and pd.Series(y).nunique() > 1:
    class_counts = pd.Series(y).value_counts()
    if class_counts.min() >= 2 and n_samples >= 4:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=42, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0

if task == 'classification' and pd.Series(y_train).nunique() < 2:
    model = DummyClassifier(strategy='most_frequent')

clf = Pipeline([
    ('preprocess', preprocessor),
    ('model', model)
])

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    if len(y_test) < 2 or pd.Series(y_test).nunique() < 2:
        accuracy = 1.0
    else:
        r2 = r2_score(y_test, y_pred)
        if np.isnan(r2):
            r2 = 0.0
        accuracy = float(np.clip(r2, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# - Used lightweight linear models with dummy fallbacks to keep CPU usage low while remaining robust to edge cases.
# - Applied a reproducible ColumnTransformer pipeline with simple imputation, scaling, and one-hot encoding for categorical data.
# - Collapsed detected text fields into a single TF-IDF vector with capped features to limit dimensionality and energy cost.
# - Regression uses a clipped R2 score in [0,1] as a stable accuracy proxy for uniform reporting.