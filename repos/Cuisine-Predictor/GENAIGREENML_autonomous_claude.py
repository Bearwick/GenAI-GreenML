# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

# Load dataset
df = pd.read_csv("train.csv")
df_test = None
try:
    df_test = pd.read_csv("test.csv")
except FileNotFoundError:
    pass

# Identify target column - assume last column or common names
target_col = None
common_target_names = ['target', 'label', 'class', 'y', 'outcome', 'Survived', 'survived',
                       'Price', 'price', 'SalePrice', 'saleprice']
for name in common_target_names:
    if name in df.columns:
        target_col = name
        break

if target_col is None:
    target_col = df.columns[-1]

y = df[target_col].copy()
X = df.drop(columns=[target_col]).copy()

# Determine if classification or regression
is_classification = True
if y.dtype in ['float64', 'float32']:
    unique_ratio = y.nunique() / len(y)
    if unique_ratio > 0.1 and y.nunique() > 20:
        is_classification = False

# Encode target if classification and non-numeric
label_enc_target = None
if is_classification and y.dtype == 'object':
    label_enc_target = LabelEncoder()
    y = pd.Series(label_enc_target.fit_transform(y))

# Drop columns with too many unique values (likely IDs) or constant columns
cols_to_drop = []
for col in X.columns:
    if X[col].nunique() == len(X) and X[col].dtype in ['int64', 'int32', 'float64']:
        cols_to_drop.append(col)
    elif X[col].nunique() <= 1:
        cols_to_drop.append(col)
    elif col.lower() in ['id', 'index', 'unnamed: 0']:
        cols_to_drop.append(col)

X = X.drop(columns=cols_to_drop, errors='ignore')

# Separate numeric and categorical columns
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()

# Handle high-cardinality categorical columns
high_card_cols = []
for col in categorical_cols:
    if X[col].nunique() > 50:
        high_card_cols.append(col)

categorical_cols = [c for c in categorical_cols if c not in high_card_cols]
X = X.drop(columns=high_card_cols, errors='ignore')

# Impute numeric columns
if numeric_cols:
    num_imputer = SimpleImputer(strategy='median')
    X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])

# Encode categorical columns with label encoding (lightweight)
label_encoders = {}
for col in categorical_cols:
    X[col] = X[col].fillna('missing')
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    label_encoders[col] = le

# Scale numeric features
scaler = StandardScaler()
if numeric_cols:
    X[numeric_cols] = scaler.fit_transform(X[numeric_cols])

# Split data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if is_classification else None)

if is_classification:
    # Try multiple lightweight class