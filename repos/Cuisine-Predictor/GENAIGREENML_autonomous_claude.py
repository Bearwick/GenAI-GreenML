# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import json
import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

# Robust file reading: try train.json first
data = None
for candidate in ["train.json", "data/train.json", "../train.json"]:
    if os.path.exists(candidate):
        with open(candidate, "r") as f:
            data = json.load(f)
        break

# If no JSON found, try CSV fallback
if data is None:
    csv_candidates = [f for f in os.listdir(".") if f.endswith(".csv")]
    if not csv_candidates:
        for d in ["data", ".."]:
            if os.path.isdir(d):
                csv_candidates = [os.path.join(d, f) for f in os.listdir(d) if f.endswith(".csv")]
                if csv_candidates:
                    break
    if csv_candidates:
        csv_path = csv_candidates[0]
        try:
            df = pd.read_csv(csv_path)
        except Exception:
            df = pd.read_csv(csv_path, sep=";", decimal=",")
        df.columns = [c.strip() for c in df.columns]
        df = df[[c for c in df.columns if not c.startswith("Unnamed")]]
        # Try to find text + label columns for a naive bayes text classifier
        # or fall back to generic numeric classification
        text_cols = df.select_dtypes(include=["object"]).columns.tolist()
        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if len(text_cols) >= 2:
            target_col = text_cols[-1]
            feature_col = text_cols[0]
            le = LabelEncoder()
            df = df.dropna(subset=[feature_col, target_col])
            y = le.fit_transform(df[target_col].astype(str))
            X_text = df[feature_col].astype(str)
            X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=42)
            vec = CountVectorizer(max_features=5000)
            X_train_v = vec.fit_transform(X_train)
            X_test_v = vec.transform(X_test)
            model = MultinomialNB()
            model.fit(X_train_v, y_train)
            accuracy = accuracy_score(y_test, model.predict(X_test_v))
            print(f"ACCURACY={accuracy:.6f}")
            exit()
        elif len(num_cols) >= 2:
            target_col = num_cols[-1]
            feature_cols = [c for c in num_cols if c != target_col]
            df = df[feature_cols + [target_col]].apply(pd.to_numeric, errors="coerce").dropna()
            assert len(df) > 0
            from sklearn.linear_model import LogisticRegression
            X = df[feature_cols].values
            y = df[target_col].values
            unique_classes = np.unique(y)
            if len(unique_classes) < 2:
                accuracy = 1.0
                print(f"ACCURACY={accuracy:.6f}")
                exit()
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            lr = LogisticRegression(max_iter=1000, random_state=42)
            lr.fit(X_train, y_train)
            accuracy = accuracy_score(y_test, lr.predict(X_test))
            print(f"ACCURACY={accuracy:.6f