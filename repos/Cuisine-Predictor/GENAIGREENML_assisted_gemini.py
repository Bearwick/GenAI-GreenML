# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import json
from unidecode import unidecode

def solve():
    train_file = "train.json"
    test_file = "test.json"

    try:
        with open(train_file, "r") as f:
            train_data = json.load(f)
    except (FileNotFoundError, IOError):
        return

    cuisine_dcounts = {}
    cuisine_tcounts = {}
    ing_freqs = {}
    
    memo = {}
    for doc in train_data:
        c = doc['cuisine']
        cuisine_dcounts[c] = cuisine_dcounts.get(c, 0) + 1
        ings = doc['ingredients']
        cuisine_tcounts[c] = cuisine_tcounts.get(c, 0) + len(ings)
        for ing in ings:
            if ing not in memo:
                memo[ing] = unidecode(ing.lower())
            clean = memo[ing]
            if clean not in ing_freqs:
                ing_freqs[clean] = {}
            c_dict = ing_freqs[clean]
            c_dict[c] = c_dict.get(c, 0) + 1
    
    vocab_len = len(ing_freqs)
    cuisines = list(cuisine_dcounts.keys())
    total_train = len(train_data)
    
    priors = {c: cuisine_dcounts[c] / total_train for c in cuisines}
    inv_dens = {c: 1.0 / (vocab_len + cuisine_tcounts[c]) for c in cuisines}

    def predict(dataset):
        preds = {}
        for doc in dataset:
            ings = doc['ingredients']
            n = len(ings)
            scores = {c: priors[c] * (inv_dens[c] ** n) for c in cuisines}
            for ing in ings:
                if ing in ing_freqs:
                    c_freqs = ing_freqs[ing]
                    for c, freq in c_freqs.items():
                        scores[c] *= (1 + freq)
            preds[doc['id']] = max(scores, key=scores.get)
        return preds

    try:
        with open(test_file, "r") as f:
            test_queryset = json.load(f)
        predict(test_queryset)
    except (FileNotFoundError, IOError):
        pass

    train_preds = predict(train_data)
    correct = sum(1 for doc in train_data if train_preds[doc['id']] == doc['cuisine'])
    accuracy = correct / total_train
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# OPTIMIZATION SUMMARY
# 1. Combined multiple data passes (document counts, token counts, and corpus building) into a single loop.
# 2. Replaced the memory-heavy inverted index (storing lists of document IDs) with simple integer counts.
# 3. Memoized the unidecode string processing to avoid redundant transformations of common ingredients.
# 4. Pre-calculated class priors and Laplace-smoothing denominators as constant factors to reduce floating-point divisions.
# 5. Optimized the Naive Bayes probability calculation by using an inverted lookup (only iterating over classes where an ingredient actually appears).
# 6. Used the exponentiation operator for base denominator probabilities to avoid repeated multiplications in the inner loop.
# 7. Removed redundant file I/O by loading the dataset into memory once and eliminating multiple re-reads.
# 8. Eliminated the pandas dependency and Excel file generation to reduce overhead and memory footprint.
# 9. Scoped variables locally within a main function to improve lookup speed and facilitate garbage collection.