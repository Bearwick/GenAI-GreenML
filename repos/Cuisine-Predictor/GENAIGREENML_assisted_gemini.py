# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import json
import os
import math
from collections import Counter, defaultdict
from unidecode import unidecode

def load_json(path):
    if not os.path.exists(path):
        return None
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def preprocess_ingredients(ingredients):
    return [unidecode(ing.lower()) for ing in ingredients]

def train_model(data):
    cuisine_dcount = Counter()
    cuisine_tcount = Counter()
    cuisine_ing_counts = defaultdict(Counter)
    vocab = set()
    
    for doc in data:
        cuisine = doc['cuisine']
        cuisine_dcount[cuisine] += 1
        processed_ings = preprocess_ingredients(doc['ingredients'])
        doc['ingredients'] = processed_ings
        
        cuisine_tcount[cuisine] += len(processed_ings)
        for ing in processed_ings:
            cuisine_ing_counts[cuisine][ing] += 1
            vocab.add(ing)
            
    vocab_size = len(vocab)
    classes = list(cuisine_dcount.keys())
    total_docs = len(data)
    
    log_priors = {c: math.log(cuisine_dcount[c] / total_docs) for c in classes}
    log_denominators = {c: math.log(cuisine_tcount[c] + vocab_size) for c in classes}
    
    ing_logs = {
        c: {ing: math.log(count + 1) for ing, count in cuisine_ing_counts[c].items()}
        for c in classes
    }
    
    return classes, log_priors, log_denominators, ing_logs

def predict(doc_ingredients, classes, log_priors, log_denominators, ing_logs):
    best_class = None
    max_score = -float('inf')
    
    ing_count = len(doc_ingredients)
    
    for c in classes:
        score = log_priors[c] - (ing_count * log_denominators[c])
        current_ing_logs = ing_logs[c]
        for ing in doc_ingredients:
            score += current_ing_logs.get(ing, 0)
            
        if score > max_score:
            max_score = score
            best_class = c
            
    return best_class

def main():
    train_data = load_json("train.json")
    if not train_data:
        return

    classes, log_priors, log_denominators, ing_logs = train_model(train_data)
    
    test_data = load_json("test.json")
    if test_data:
        for doc in test_data:
            processed_ings = preprocess_ingredients(doc['ingredients'])
            predict(processed_ings, classes, log_priors, log_denominators, ing_logs)
            
    correctly_classified = 0
    for doc in train_data:
        prediction = predict(doc['ingredients'], classes, log_priors, log_denominators, ing_logs)
        if prediction == doc['cuisine']:
            correctly_classified += 1
            
    accuracy = correctly_classified / len(train_data)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# 1. Switched from probability multiplication to log-probability summation to improve runtime performance and numerical stability.
# 2. Replaced redundant nested dictionary lookups with pre-calculated log-probability tables (ing_logs).
# 3. Optimized the Naive Bayes score formula to move the denominator calculation (log_denominators) outside the ingredient loop.
# 4. Reduced memory footprint by storing integer counts in collections.Counter instead of lists of document IDs.
# 5. Pre-processed ingredients (unidecode/lower) once during training and stored the results, avoiding repeated string operations during evaluation.
# 6. Eliminated redundant loops by merging vocabulary building and frequency counting into a single pass over the dataset.
# 7. Implemented efficient lookups using sets and dictionaries instead of list searches.
# 8. Removed file I/O operations (Excel saving) that were not part of the required execution flow, reducing disk overhead.
# 9. Used standard math.log for scalar operations which is faster and more lightweight than importing large libraries like NumPy for this specific task.