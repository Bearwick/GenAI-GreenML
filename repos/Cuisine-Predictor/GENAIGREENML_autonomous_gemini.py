# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

X, y = load_breast_cancer(return_X_y=True)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = LogisticRegression(
    penalty='l2',
    C=1.0,
    solver='lbfgs',
    max_iter=100,
    tol=1e-4,
    random_state=42
)

model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Model Selection: Logistic Regression was chosen for its O(n) complexity, providing the highest energy efficiency and lowest CPU overhead for binary classification.
# 2. Convergence Optimization: StandardScaler was used to normalize features, ensuring the solver reaches the global minimum in fewer iterations, thus saving compute cycles.
# 3. Hardware Efficiency: The solution relies on Scikit-learn's optimized C-based L-BFGS solver, which runs efficiently on single-core CPU environments without requiring GPU/TPU.
# 4. Resource Footprint: Minimal memory usage is achieved by avoiding deep learning frameworks, large ensemble trees, or high-dimensional embeddings.
# 5. Pipeline Design: Used a deterministic preprocessing approach (StandardScaler) which is computationally cheaper than non-linear transformations or complex feature engineering.
# 6. Hyperparameter Tuning: Used default L2 regularization to prevent overfitting without the need for expensive cross-validation search cycles on this scale.