# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import json
import math
from unidecode import unidecode

def file_read():
    with open("train.json", "r") as handle:
        data = json.load(handle)
    return data

def build_model(data):
    class_dcount = {}
    class_tcount = {}
    corpus = {}
    for doc in data:
        c = doc['cuisine']
        class_dcount[c] = class_dcount.get(c, 0) + 1
        ingredients = doc['ingredients']
        class_tcount[c] = class_tcount.get(c, 0) + len(ingredients)
        for ing in ingredients:
            ing_key = unidecode(ing.lower())
            if ing_key not in corpus:
                corpus[ing_key] = {}
            if c not in corpus[ing_key]:
                corpus[ing_key][c] = 0
            corpus[ing_key][c] += 1
    total_docs = len(data)
    classes = list(class_dcount.keys())
    log_prior = {}
    for c in classes:
        log_prior[c] = math.log(class_dcount[c] / total_docs)
    vocab_size = len(corpus)
    log_likelihood = {}
    for ing, class_counts in corpus.items():
        log_likelihood[ing] = {}
        for c in classes:
            count = class_counts.get(c, 0)
            log_likelihood[ing][c] = math.log((1 + count) / (vocab_size + class_tcount[c]))
    log_default = {}
    for c in classes:
        log_default[c] = math.log(1.0 / (vocab_size + class_tcount[c]))
    return classes, log_prior, log_likelihood, log_default

def naive_based(queryset, classes, log_prior, log_likelihood, log_default):
    qdoc_class = {}
    for doc in queryset:
        best_class = None
        best_score = -float('inf')
        ingredients = [unidecode(ing.lower()) for ing in doc['ingredients']]
        for c in classes:
            score = log_prior[c]
            for ing_key in ingredients:
                ll = log_likelihood.get(ing_key)
                if ll is not None and c in ll:
                    score += ll[ing_key][c]
                else:
                    score += log_default[c]
            if score > best_score:
                best_score = score
                best_class = c
        qdoc_class[doc['id']] = best_class
    return qdoc_class

data = file_read()
classes, log_prior, log_likelihood, log_default = build_model(data)
empirical_dict = naive_based(data, classes, log_prior, log_likelihood, log_default)
correctly_classified = sum(1 for doc in data if doc['cuisine'] == empirical_dict[doc['id']])
accuracy = correctly_classified / len(empirical_dict)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Replaced per-document ID lists with integer counts to reduce memory footprint significantly.
# 2. Precomputed log-probabilities (log_prior, log_likelihood, log_default) once during model build,
#    avoiding repeated multiplication and division during classification.
# 3. Used log-space addition instead of probability multiplication to avoid numerical underflow
#    and reduce floating-point operations.
# 4. Pre-normalized ingredient strings once per document in classification loop.
# 5. Removed redundant file reads: reuse training data already in memory for empirical accuracy.
# 6. Removed all file output, plots, and unnecessary global variables.
# 7. Combined vocab_maker and prior_probability into a single build_model pass for fewer iterations.
# 8. Eliminated intermediate data structures (vocab list, separate class loops) to reduce overhead.