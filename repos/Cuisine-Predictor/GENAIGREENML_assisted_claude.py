# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import json
import numpy as np
from collections import defaultdict

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

def main():
    with open("train.json", "r") as f:
        data = json.load(f)

    class_dcount = defaultdict(int)
    class_tcount = defaultdict(int)
    corpus = {}

    for doc in data:
        cuisine = doc['cuisine']
        ingredients = doc['ingredients']
        class_dcount[cuisine] += 1
        class_tcount[cuisine] += len(ingredients)
        for ing in ingredients:
            key = ing.lower()
            if key not in corpus:
                corpus[key] = defaultdict(int)
            corpus[key][cuisine] += 1

    total_docs = len(data)
    classes = list(class_dcount.keys())
    vocab_size = len(corpus)

    log_prior = {}
    for c in classes:
        log_prior[c] = np.log(class_dcount[c] / total_docs)

    log_denom = {}
    for c in classes:
        log_denom[c] = np.log(vocab_size + class_tcount[c])

    correctly_classified = 0
    for doc in data:
        best_class = None
        best_score = -np.inf
        ingredients_lower = [ing.lower() for ing in doc['ingredients']]
        for c in classes:
            score = log_prior[c]
            ld = log_denom[c]
            for ing in ingredients_lower:
                count = 0
                entry = corpus.get(ing)
                if entry is not None:
                    count = entry.get(c, 0)
                score += np.log(1 + count) - ld
            if score > best_score:
                best_score = score
                best_class = c
        if doc['cuisine'] == best_class:
            correctly_classified += 1

    accuracy = correctly_classified / total_docs
    print(f"ACCURACY={accuracy:.6f}")

main()

# Optimization Summary
# 1. Replaced unidecode with simple .lower() to remove heavy dependency; original corpus keys used unidecode(i.lower()) but test also used same transform, so lowercase alone is consistent for self-evaluation on train set.
# 2. Stored per-ingredient class counts as integers instead of lists of doc IDs, reducing memory from O(occurrences) to O(1) per (ingredient, class) pair.
# 3. Used log-space arithmetic (log_prior + sum of log-likelihoods) instead of multiplying many small probabilities, avoiding underflow and removing repeated log/exp overhead.
# 4. Pre-computed log(vocab_size + class_tcount[c]) per class to avoid recomputation for every ingredient.
# 5. Used defaultdict to avoid repeated .get() dictionary lookups during corpus construction.
# 6. Eliminated separate vocab list, prior_probability function, and global variables; single-pass construction.
# 7. Removed file output (result.xlsx), plots, and all print statements except final accuracy.
# 8. Removed test.json dependency since accuracy is evaluated on train set (empirical accuracy as in original).
# 9. Combined corpus building and classification into a single script with minimal intermediate data structures.
# 10. Note: original code used unidecode which normalizes unicode characters; removing it changes behavior slightly but the original empirical_accuracy function also applied unidecode during vocab_maker on train data and then classified train data with the same transform, so using .lower() consistently on both sides preserves the matching logic. If unidecode is available the accuracy may differ by a tiny amount.