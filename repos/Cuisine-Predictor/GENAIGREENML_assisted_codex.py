# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import json
import random
from unidecode import unidecode

random.seed(42)

qdoc_class = {}


def load_json(path):
    with open(path, "r") as f:
        return json.load(f)


def infer_schema(records):
    if not records:
        raise ValueError("Empty dataset")
    keys = list(records[0].keys())
    headers = globals().get("DATASET_HEADERS")
    if headers:
        for h in headers:
            if h not in keys:
                keys.append(h)
    lower_map = {k.lower(): k for k in keys}
    cuisine_key = lower_map.get("cuisine")
    ingredients_key = lower_map.get("ingredients") or lower_map.get("ingredient")
    id_key = lower_map.get("id")
    if cuisine_key is None:
        for k in keys:
            if "cuisine" in k.lower():
                cuisine_key = k
                break
    if ingredients_key is None:
        for k in keys:
            if "ingredient" in k.lower():
                ingredients_key = k
                break
    if id_key is None:
        for k in keys:
            if k.lower().endswith("id"):
                id_key = k
                break
    actual_keys_lower = {k.lower(): k for k in records[0].keys()}

    def resolve(k):
        if k is None:
            return None
        if k in records[0]:
            return k
        return actual_keys_lower.get(k.lower())

    cuisine_key = resolve(cuisine_key)
    ingredients_key = resolve(ingredients_key)
    id_key = resolve(id_key)
    if cuisine_key is None or ingredients_key is None or id_key is None:
        raise KeyError("Required keys not found in dataset")
    return cuisine_key, ingredients_key, id_key


def build_model(data, cuisine_key, ingredients_key):
    class_doc_count = {}
    class_token_count = {}
    corpus = {}
    normalize = unidecode
    for doc in data:
        cuisine = doc.get(cuisine_key)
        ingredients = doc.get(ingredients_key, [])
        class_doc_count[cuisine] = class_doc_count.get(cuisine, 0) + 1
        class_token_count[cuisine] = class_token_count.get(cuisine, 0) + len(ingredients)
        for ing in ingredients:
            key = normalize(ing.lower())
            counts = corpus.get(key)
            if counts is None:
                counts = {}
                corpus[key] = counts
            counts[cuisine] = counts.get(cuisine, 0) + 1
    vocab_len = len(corpus)
    total_docs = len(data)
    class_info = []
    for c in class_doc_count:
        denom = vocab_len + class_token_count[c]
        prior = class_doc_count[c] / total_docs if total_docs else 0.0
        class_info.append((c, denom, prior))
    return corpus, class_info


def predict_class(ingredients, corpus, class_info):
    corpus_get = corpus.get
    ing_counts_list = [corpus_get(ing, {}) for ing in ingredients]
    best_class = None
    best_prob = -1.0
    for c, denom, prior in class_info:
        p = 1.0
        for counts in ing_counts_list:
            p *= (1.0 + counts.get(c, 0)) / denom
        p *= prior
        if p > best_prob:
            best_prob = p
            best_class = c
    return best_class


def predict(queryset, ingredients_key, id_key, corpus, class_info):
    predictions = {}
    for doc in queryset:
        ingredients = doc.get(ingredients_key, [])
        predictions[doc.get(id_key)] = predict_class(ingredients, corpus, class_info)
    return predictions


def accuracy_from_model(data, cuisine_key, ingredients_key, id_key, corpus, class_info):
    predictions = predict(data, ingredients_key, id_key, corpus, class_info)
    total = len(predictions)
    if total == 0:
        return 0.0
    correct = 0
    for doc in data:
        if doc.get(cuisine_key) == predictions.get(doc.get(id_key)):
            correct += 1
    return correct / total


def main():
    train_data = load_json("train.json")
    cuisine_key, ingredients_key, id_key = infer_schema(train_data)
    corpus, class_info = build_model(train_data, cuisine_key, ingredients_key)
    test_data = load_json("test.json")
    global qdoc_class
    qdoc_class = predict(test_data, ingredients_key, id_key, corpus, class_info)
    accuracy = accuracy_from_model(train_data, cuisine_key, ingredients_key, id_key, corpus, class_info)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Eliminated redundant training data reads by reusing loaded JSON for both modeling and accuracy.
# - Replaced per-ingredient ID lists with count storage to reduce memory footprint.
# - Precomputed class priors and denominators once to avoid repeated calculations in loops.
# - Avoided per-document probability dictionaries by tracking the best class on the fly.
# - Inferred schema from dataset keys to avoid hardcoded column assumptions.