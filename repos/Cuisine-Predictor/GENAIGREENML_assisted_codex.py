# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import json
from unidecode import unidecode

qdoc_class = {}

def load_json(path):
    with open(path, "r") as f:
        return json.load(f)

def build_model(data):
    class_doc_counts = {}
    class_term_counts = {}
    corpus = {}
    normalize = unidecode
    for doc in data:
        cuisine = doc["cuisine"]
        class_doc_counts[cuisine] = class_doc_counts.get(cuisine, 0) + 1
        class_term_counts[cuisine] = class_term_counts.get(cuisine, 0) + len(doc["ingredients"])
        for ing in doc["ingredients"]:
            ing_norm = normalize(ing.lower())
            class_counts = corpus.get(ing_norm)
            if class_counts is None:
                class_counts = {}
                corpus[ing_norm] = class_counts
            class_counts[cuisine] = class_counts.get(cuisine, 0) + 1
    classes = list(class_doc_counts.keys())
    num_classes = len(classes)
    class_index = {c: i for i, c in enumerate(classes)}
    total_docs = len(data)
    prior_probs = [class_doc_counts[c] / total_docs for c in classes]
    vocab_size = len(corpus)
    denoms = [vocab_size + class_term_counts[c] for c in classes]
    default_probs = [1.0 / d for d in denoms]
    for ing, class_counts in corpus.items():
        probs = default_probs.copy()
        for c, count in class_counts.items():
            idx = class_index[c]
            probs[idx] = (1.0 + count) / denoms[idx]
        corpus[ing] = probs
    return {
        "classes": classes,
        "num_classes": num_classes,
        "prior_probs": prior_probs,
        "default_probs": default_probs,
        "corpus": corpus,
    }

def make_predictor(model):
    classes = model["classes"]
    num_classes = model["num_classes"]
    prior_probs = model["prior_probs"]
    default_probs = model["default_probs"]
    corpus = model["corpus"]
    corpus_get = corpus.get
    class_range = range(num_classes)
    def predict(ingredients):
        probs = [1.0] * num_classes
        for ing in ingredients:
            ing_probs = corpus_get(ing)
            if ing_probs is None:
                ing_probs = default_probs
            for i in class_range:
                probs[i] *= ing_probs[i]
        best_idx = 0
        best_prob = probs[0] * prior_probs[0]
        for i in range(1, num_classes):
            p = probs[i] * prior_probs[i]
            if p > best_prob:
                best_prob = p
                best_idx = i
        return classes[best_idx]
    return predict

def classify_dataset(dataset, predict):
    results = {}
    for doc in dataset:
        results[doc["id"]] = predict(doc["ingredients"])
    return results

def compute_accuracy(dataset, predict):
    correct = 0
    total = len(dataset)
    for doc in dataset:
        if predict(doc["ingredients"]) == doc["cuisine"]:
            correct += 1
    return correct / total if total else 0.0

def main():
    global qdoc_class
    train_data = load_json("train.json")
    model = build_model(train_data)
    predictor = make_predictor(model)
    accuracy = compute_accuracy(train_data, predictor)
    del train_data
    test_data = load_json("test.json")
    qdoc_class = classify_dataset(test_data, predictor)
    del test_data
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Merged class and token counting into one pass to avoid redundant scans of training data.
# - Stored ingredient-class counts and precomputed probability tables to eliminate repeated divisions.
# - Switched to list-based class probabilities to reduce dictionary lookups during prediction.
# - Loaded training data once and reused it for accuracy to minimize I/O overhead.
# - Released datasets after use to lower peak memory footprint.