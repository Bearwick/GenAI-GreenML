# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import json
import os
import random
from unidecode import unidecode

random.seed(0)

def load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return []
    if isinstance(data, dict):
        for v in data.values():
            if isinstance(v, list):
                return v
        return []
    return data if isinstance(data, list) else []

def read_csv_fallback(path):
    try:
        import pandas as pd
    except Exception:
        return None
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            df = pd.read_csv(path, sep=";", decimal=",")
        return df
    except Exception:
        try:
            return pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            return None

def infer_schema(records):
    headers = globals().get("DATASET_HEADERS")
    if headers:
        headers = list(headers)
    if not records:
        return None, None, None
    sample = records[0]
    if not isinstance(sample, dict):
        return None, None, None
    keys = [k for k in (headers or sample.keys()) if k in sample]
    if not keys:
        keys = list(sample.keys())
    ingredients_key = None
    for k in keys:
        if isinstance(sample.get(k), list):
            ingredients_key = k
            break
    id_key = None
    for k in keys:
        if "id" in k.lower():
            id_key = k
            break
    if id_key is None:
        for k in keys:
            if k != ingredients_key and isinstance(sample.get(k), (int, str)):
                id_key = k
                break
    label_key = None
    for k in keys:
        kl = k.lower()
        if "cuisine" in kl or "label" in kl or "class" in kl:
            label_key = k
            break
    if label_key is None:
        for k in keys:
            if k != ingredients_key and k != id_key and isinstance(sample.get(k), str):
                label_key = k
                break
    return ingredients_key, id_key, label_key

def build_model(data, ingredients_key, label_key):
    corpus = {}
    class_tcount = {}
    class_dcount = {}
    decode = unidecode
    norm_cache = {}
    for doc in data:
        c = doc[label_key]
        ingredients = doc[ingredients_key]
        class_dcount[c] = class_dcount.get(c, 0) + 1
        class_tcount[c] = class_tcount.get(c, 0) + len(ingredients)
        for ing in ingredients:
            token = norm_cache.get(ing)
            if token is None:
                token = decode(ing.lower())
                norm_cache[ing] = token
            counts = corpus.get(token)
            if counts is None:
                counts = {}
                corpus[token] = counts
            counts[c] = counts.get(c, 0) + 1
    total_docs = len(data)
    prior_prob = {c: class_dcount[c] / total_docs for c in class_dcount} if total_docs else {}
    classes = list(class_tcount.keys())
    class_info = [(c, class_tcount[c], prior_prob.get(c, 0.0)) for c in classes]
    return {"corpus": corpus, "class_info": class_info}

def predict_class(ingredients, model):
    corpus = model["corpus"]
    class_info = model["class_info"]
    corpus_get = corpus.get
    best_class = None
    best_prob = -1.0
    for c, denom, prior in class_info:
        p = 1.0
        for ing in ingredients:
            counts = corpus_get(ing)
            length = 0 if counts is None else counts.get(c, 0)
            p *= (length + 1) / denom
        p *= prior
        if p > best_prob:
            best_prob = p
            best_class = c
    return best_class

def predict_dataset(data, model, ingredients_key, id_key):
    predictions = {}
    if not data or ingredients_key is None or id_key is None:
        return predictions
    predict = predict_class
    for doc in data:
        predictions[doc[id_key]] = predict(doc[ingredients_key], model)
    return predictions

def compute_accuracy(data, model, ingredients_key, label_key):
    if not data or ingredients_key is None or label_key is None:
        return 0.0
    correct = 0
    total = len(data)
    predict = predict_class
    for doc in data:
        if predict(doc[ingredients_key], model) == doc[label_key]:
            correct += 1
    return correct / total if total else 0.0

qdoc_class = {}

def main():
    global qdoc_class
    train_data = load_json("train.json")
    ingredients_key, id_key, label_key = infer_schema(train_data)
    if train_data and ingredients_key is not None and label_key is not None:
        model = build_model(train_data, ingredients_key, label_key)
    else:
        model = {"corpus": {}, "class_info": []}
    test_data = load_json("test.json") if os.path.exists("test.json") else []
    if test_data and (ingredients_key is None or id_key is None):
        ing_k, id_k, _ = infer_schema(test_data)
        if ingredients_key is None:
            ingredients_key = ing_k
        if id_key is None:
            id_key = id_k
    qdoc_class = predict_dataset(test_data, model, ingredients_key, id_key)
    accuracy = compute_accuracy(train_data, model, ingredients_key, label_key)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Consolidated class and term counting into a single pass and cached ingredient normalization to reduce repeated work.
# - Stored ingredient counts instead of document ID lists to lower memory usage.
# - Reused loaded training data for accuracy calculation, eliminating redundant file reads.
# - Cached per-class constants and avoided intermediate prediction dictionaries during accuracy computation.
# - Added schema inference and safe file loading to ensure reliable execution across input variations.