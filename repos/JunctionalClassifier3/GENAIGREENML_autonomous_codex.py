# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score

def find_dataset():
    candidates = []
    search_paths = ['.', '/kaggle/input', '/data', '/datasets']
    for base in search_paths:
        if not os.path.exists(base):
            continue
        for root, dirs, files in os.walk(base):
            depth = root[len(base):].count(os.sep)
            if depth > 2:
                dirs[:] = []
                continue
            for f in files:
                lf = f.lower()
                if lf.endswith('.csv') or lf.endswith('.tsv') or lf.endswith('.data'):
                    path = os.path.join(root, f)
                    try:
                        size = os.path.getsize(path)
                    except OSError:
                        continue
                    candidates.append((size, path))
        if candidates:
            break
    if not candidates:
        raise FileNotFoundError("No dataset file found.")
    candidates.sort(reverse=True)
    return candidates[0][1]

def load_dataframe(path):
    try:
        return pd.read_csv(path, sep=None, engine='python')
    except Exception:
        try:
            return pd.read_csv(path)
        except Exception:
            return pd.read_csv(path, sep='\t')

path = find_dataset()
df = load_dataframe(path)

accuracy = None

if df.shape[1] < 2:
    accuracy = 0.0
else:
    target_col = df.columns[-1]
    df = df.dropna(subset=[target_col])
    if len(df) < 2:
        accuracy = 1.0
    else:
        y = df[target_col]
        X = df.drop(columns=[target_col])
        y_numeric = pd.to_numeric(y, errors='coerce')
        if y_numeric.notna().mean() > 0.9:
            y = y_numeric
        X = X.dropna(axis=1, how='all')
        if X.shape[1] > 0:
            nunique = X.nunique(dropna=True)
            X = X.loc[:, nunique > 1]
        n_unique = y.nunique(dropna=True)
        if y.dtype == object or str(y.dtype).startswith('category') or n_unique <= 20:
            task = 'classification'
        else:
            task = 'regression'
        test_size = 0.2
        if len(df) * test_size < 1:
            test_size = 0.5
        stratify = None
        if task == 'classification' and n_unique > 1:
            if y.value_counts().min() > 1:
                stratify = y
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=stratify
        )
        if X.shape[1] == 0 or (task == 'classification' and y_train.nunique() < 2):
            if task == 'classification':
                majority = y_train.mode().iloc[0]
                y_pred = np.full(len(y_test), majority)
                accuracy = accuracy_score(y_test, y_pred)
            else:
                mean_val = y_train.mean()
                y_pred = np.full(len(y_test), mean_val)
                if len(y_test) < 2:
                    accuracy = 0.0
                else:
                    accuracy = r2_score(y_test, y_pred)
        else:
            categorical_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()
            numeric_cols = [c for c in X.columns if c not in categorical_cols]
            numeric_transformer = Pipeline(steps=[
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler(with_mean=False))
            ])
            categorical_transformer = Pipeline(steps=[
                ('imputer', SimpleImputer(strategy='most_frequent')),
                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
            ])
            preprocessor = ColumnTransformer(
                transformers=[
                    ('num', numeric_transformer, numeric_cols),
                    ('cat', categorical_transformer, categorical_cols)
                ],
                remainder='drop'
            )
            if task == 'classification':
                model = LogisticRegression(max_iter=200, solver='liblinear')
            else:
                model = Ridge(alpha=1.0)
            clf = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
            if task == 'classification':
                accuracy = accuracy_score(y_test, y_pred)
            else:
                if len(y_test) < 2:
                    accuracy = 0.0
                else:
                    accuracy = r2_score(y_test, y_pred)

if accuracy is None:
    accuracy = 0.0

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Lightweight linear models are used for efficiency and to avoid deep-learning overhead.
# The preprocessing pipeline uses simple imputations and one-hot encoding for reproducibility.
# Model complexity and iterations are kept modest to reduce CPU time and energy use.