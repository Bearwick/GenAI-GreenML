# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

# Load dataset
df = pd.read_csv("input_data.csv")

# Identify target column - assume last column is target
target_col = df.columns[-1]
X = df.drop(columns=[target_col])
y = df[target_col]

# Encode target if categorical
if y.dtype == 'object' or y.dtype.name == 'category':
    le_target = LabelEncoder()
    y = le_target.fit_transform(y)

# Separate numeric and categorical columns
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()

# Impute missing values for numeric columns with median (robust to outliers)
if numeric_cols:
    num_imputer = SimpleImputer(strategy='median')
    X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])

# Encode categorical columns with LabelEncoder (lightweight)
label_encoders = {}
if categorical_cols:
    cat_imputer = SimpleImputer(strategy='most_frequent')
    X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])
    for col in categorical_cols:
        le = LabelEncoder()
        X[col] = le.fit_transform(X[col].astype(str))
        label_encoders[col] = le

# Drop columns with zero variance
variances = X.var()
zero_var_cols = variances[variances == 0].index.tolist()
if zero_var_cols:
    X = X.drop(columns=zero_var_cols)

# Scale features - StandardScaler is efficient and helps linear models
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train/test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Determine number of classes and samples for model selection
n_classes = len(np.unique(y))
n_samples = X_train.shape[0]
n_features = X_train.shape[1]

# Try multiple lightweight models and pick the best
models = {}

# Logistic Regression - very efficient, good baseline
models['lr'] = LogisticRegression(
    max_iter=1000, random_state=42, solver='lbfgs', multi_class='auto', C=1.0
)

# Random Forest - efficient ensemble, handles nonlinearity well
n_estimators_rf = min(200, max(50, n_samples // 10))
models['rf'] = RandomForestClassifier(
    n_estimators=n_estimators_rf, max_depth=min(20, n_features),
    min_samples_split=5, min_samples_leaf=2,
    random_state=42, n_jobs=-1
)

# Gradient Boosting - often best accuracy, kept small for efficiency
n_estimators_gb = min(200, max(50, n_samples // 10))
models['gb'] = GradientBoostingClassifier(
    n_estimators=n_estimators_gb, max_depth=5, learning_rate=0.1,
    min_samples_split=5, min_samples_leaf=2,
    subsample=0.8, random_state=42
)

best_accuracy = 0.0
best_model_name = None

for name, model in models.items():
    model.fit(X_train, y_train)