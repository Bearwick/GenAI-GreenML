# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
import pickle
import os
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def load_data_source(file_path, is_labeled=True):
    if not os.path.exists(file_path):
        return None, None
    try:
        df = pd.read_csv(file_path, sep=',', skipinitialspace=True)
    except Exception:
        df = pd.read_csv(file_path, sep=';', decimal=',', skipinitialspace=True)
    
    df = df.dropna(axis=1, how='all')
    df = df.apply(pd.to_numeric, errors='coerce').dropna()
    
    if df.empty:
        return None, None
        
    data = df.values
    if is_labeled:
        x = data[:, :-1]
        y = np.sign(data[:, -1])
        return x, y
    return data, None

def run_pipeline():
    model_path = 'dict.pickle'
    input_csv = 'input.csv'
    train_csv = '14k.csv'
    
    clf = None
    if os.path.exists(model_path):
        try:
            with open(model_path, 'rb') as f:
                clf = pickle.load(f)
        except Exception:
            clf = None

    query_features, _ = load_data_source(input_csv, is_labeled=False)
    if query_features is not None and clf is not None:
        predictions = clf.predict(query_features)
        print(predictions.tolist())

    accuracy = 0.0
    features, labels = load_data_source(train_csv, is_labeled=True)
    if features is not None and labels is not None:
        x_train, x_test, y_train, y_test = train_test_split(
            features, labels, test_size=0.3, random_state=42
        )
        eval_clf = SVC(kernel='linear', random_state=42)
        eval_clf.fit(x_train, y_train)
        y_pred = eval_clf.predict(x_test)
        accuracy = accuracy_score(y_test, y_pred)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary
# 1. Replaced manual row-by-row CSV parsing and string splitting with pandas.read_csv for significant speedup and lower energy consumption.
# 2. Implemented vectorized label transformation using numpy.sign instead of iterative if-else logic.
# 3. Eliminated redundant file I/O by removing unnecessary model saving (saveModel) after prediction tasks.
# 4. Reduced memory footprint by using NumPy arrays and Pandas DataFrames instead of Python lists of lists.
# 5. Optimized data preprocessing by using dropna(axis=1, how='all') to handle trailing delimiters efficiently.
# 6. Set random_state=42 for deterministic behavior and reproducibility without increasing computational cost.
# 7. Removed global variable dependencies and consolidated logic into modular functions to improve memory management.
# 8. Optimized CSV parsing with skipinitialspace and automated delimiter detection to prevent redundant cleaning steps.