# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
import pickle
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import os

def load_and_preprocess(file_path, is_training=True):
    try:
        df = pd.read_csv(file_path, sep=None, engine='python')
    except Exception:
        df = pd.read_csv(file_path, sep=';', decimal=',')

    df = df.dropna(axis=1, how='all')
    
    data = df.to_numpy(dtype=np.float64)
    
    if is_training:
        X = data[:, :-1]
        y_raw = data[:, -1]
        y = np.where(y_raw > 0, 1, np.where(y_raw < 0, -1, 0))
        return X, y
    return data

def main():
    seed = 42
    accuracy = 0.0

    if os.path.exists('14k.csv'):
        X, y = load_and_preprocess('14k.csv', is_training=True)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)
        
        clf = SVC(kernel='linear', random_state=seed)
        clf.fit(X_train, y_train)
        
        y_pred = clf.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        clf = SVC(kernel='linear', random_state=seed)
        if os.path.exists('dict.pickle'):
            with open('dict.pickle', 'rb') as f:
                clf = pickle.load(f)

    if os.path.exists('input.csv'):
        p_features = load_and_preprocess('input.csv', is_training=False)
        if hasattr(clf, "support_vectors_"):
            predictions = clf.predict(p_features)
            print(predictions)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# 1. Replaced manual CSV parsing and row-by-row list appending with pandas.read_csv for faster, vectorized data ingestion.
# 2. Implemented an automated separator detection (sep=None) with a robust fallback to handle different CSV formats efficiently.
# 3. Utilized NumPy's vectorized operations (np.where) for label transformation instead of conditional loops, significantly reducing CPU cycles.
# 4. Replaced redundant data structures and global variables with local, memory-efficient NumPy arrays to reduce the memory footprint.
# 5. Fixed the random seed for train_test_split and SVC to ensure reproducibility and stable evaluation results without unnecessary iterations.
# 6. Streamlined the workflow by eliminating redundant model saving/loading calls and unused utility functions.
# 7. Removed all visualization, logging, and interactive overhead to minimize energy consumption during execution.