# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import pickle
import random
import numpy as np
import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

DATASET_PATH = "dict.pickle"
SEED = 42

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)

def read_csv_with_fallback(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] == 1:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > 1:
                df = df_alt
        return df
    except Exception:
        try:
            return pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            return None

def apply_headers(df):
    headers = globals().get("DATASET_HEADERS")
    if headers is None or len(headers) != df.shape[1]:
        return df
    unnamed = True
    for c in df.columns:
        if isinstance(c, str):
            if not c.isdigit():
                unnamed = False
                break
        elif not isinstance(c, (int, np.integer)):
            unnamed = False
            break
    if unnamed:
        df = df.copy()
        df.columns = list(headers)
    return df

def extract_from_dataframe(df):
    if df is None or df.shape[1] < 2:
        return None, None
    df = apply_headers(df)
    cols = list(df.columns)
    label_col = None
    for c in cols:
        if str(c).strip().lower() in {"label", "labels", "target", "class", "y", "output"}:
            label_col = c
            break
    if label_col is None:
        label_col = cols[-1]
    X = df.drop(columns=[label_col]).to_numpy()
    y = df[label_col].to_numpy()
    if X.size == 0:
        return None, None
    return X, y

def extract_xy(obj):
    if obj is None:
        return None, None
    if isinstance(obj, pd.DataFrame):
        return extract_from_dataframe(obj)
    if hasattr(obj, "data") and hasattr(obj, "target"):
        try:
            return np.asarray(obj.data), np.asarray(obj.target)
        except Exception:
            pass
    if isinstance(obj, dict):
        key_pairs = [("data", "target"), ("X", "y"), ("features", "labels"), ("inputs", "outputs")]
        for kx, ky in key_pairs:
            if kx in obj and ky in obj:
                return np.asarray(obj[kx]), np.asarray(obj[ky])
        for k in ("df", "dataframe", "frame"):
            if k in obj and isinstance(obj[k], pd.DataFrame):
                return extract_from_dataframe(obj[k])
        if "data" in obj and isinstance(obj["data"], (np.ndarray, pd.DataFrame)):
            return extract_xy(obj["data"])
        for val in obj.values():
            X, y = extract_xy(val)
            if X is not None and y is not None:
                return X, y
    if isinstance(obj, (list, tuple)) and len(obj) == 2:
        try:
            return np.asarray(obj[0]), np.asarray(obj[1])
        except Exception:
            pass
    if isinstance(obj, np.ndarray):
        if obj.ndim == 2 and obj.shape[1] > 1:
            return obj[:, :-1], obj[:, -1]
    return None, None

def is_model(obj):
    return hasattr(obj, "predict") and hasattr(obj, "fit")

def parse_object(obj):
    model = obj if is_model(obj) else None
    X, y = extract_xy(obj)
    if X is None or y is None:
        if isinstance(obj, (list, tuple)):
            for item in obj:
                if model is None and is_model(item):
                    model = item
                if X is None or y is None:
                    X_tmp, y_tmp = extract_xy(item)
                    if X_tmp is not None and y_tmp is not None:
                        X, y = X_tmp, y_tmp
        elif isinstance(obj, dict):
            for item in obj.values():
                if model is None and is_model(item):
                    model = item
                if X is None or y is None:
                    X_tmp, y_tmp = extract_xy(item)
                    if X_tmp is not None and y_tmp is not None:
                        X, y = X_tmp, y_tmp
    return X, y, model

def load_data(path):
    if not os.path.exists(path):
        return None, None, None
    ext = os.path.splitext(path)[1].lower()
    if ext in {".csv", ".txt"}:
        df = read_csv_with_fallback(path)
        if df is None:
            return None, None, None
        X, y = extract_from_dataframe(df)
        return X, y, None
    try:
        with open(path, "rb") as f:
            obj = pickle.load(f)
    except Exception:
        return None, None, None
    return parse_object(obj)

def normalize_labels(y):
    y_arr = np.asarray(y)
    if y_arr.ndim > 1:
        y_arr = y_arr.ravel()
    if np.issubdtype(y_arr.dtype, np.number):
        y_sign = np.sign(y_arr)
        y_sign[y_sign == -0.0] = 0.0
        return y_sign.astype(int)
    return y_arr

def prepare_features(X):
    X_arr = np.asarray(X)
    if X_arr.ndim == 1:
        X_arr = X_arr.reshape(-1, 1)
    if not np.issubdtype(X_arr.dtype, np.number):
        X_arr = pd.DataFrame(X_arr).apply(pd.to_numeric, errors="coerce").to_numpy()
    return X_arr.astype(float, copy=False)

def clean_data(X, y):
    if X is None or y is None:
        return None, None
    X_arr = prepare_features(X)
    y_arr = normalize_labels(y)
    if X_arr.shape[0] != len(y_arr):
        n = min(X_arr.shape[0], len(y_arr))
        X_arr = X_arr[:n]
        y_arr = y_arr[:n]
    mask = None
    if np.issubdtype(X_arr.dtype, np.number):
        mask = np.isfinite(X_arr).all(axis=1)
    if np.issubdtype(y_arr.dtype, np.number):
        mask = mask & np.isfinite(y_arr) if mask is not None else np.isfinite(y_arr)
    if mask is not None:
        X_arr = X_arr[mask]
        y_arr = y_arr[mask]
    return X_arr, y_arr

def safe_train_test_split(X, y):
    n_samples = len(y)
    if n_samples < 2:
        return X, X, y, y
    stratify = None
    unique, counts = np.unique(y, return_counts=True)
    if unique.size > 1 and counts.min() >= 2:
        stratify = y
    return train_test_split(X, y, test_size=0.3, random_state=SEED, stratify=stratify)

def train_and_evaluate(X, y):
    if len(y) < 2:
        return 0.0
    X_train, X_test, y_train, y_test = safe_train_test_split(X, y)
    if len(y_test) == 0:
        return 0.0
    if np.unique(y_train).size < 2:
        fill_value = y_train[0] if len(y_train) > 0 else 0
        pred = np.full_like(y_test, fill_value)
        return accuracy_score(y_test, pred)
    model = SVC(kernel="linear", random_state=SEED)
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    return accuracy_score(y_test, pred)

def evaluate_model(model, X, y):
    X_arr, y_arr = clean_data(X, y)
    if X_arr is None or y_arr is None or len(y_arr) == 0:
        return 0.0
    pred = model.predict(X_arr)
    return accuracy_score(y_arr, pred)

def load_input_data_for_model(model):
    path = "input.csv"
    if not os.path.exists(path):
        return None, None
    df = read_csv_with_fallback(path)
    if df is None:
        return None, None
    df = apply_headers(df)
    n_features = getattr(model, "n_features_in_", None)
    if n_features is None and hasattr(model, "support_vectors_"):
        n_features = model.support_vectors_.shape[1]
    if n_features is not None:
        if df.shape[1] == n_features + 1:
            X = df.iloc[:, :n_features].to_numpy()
            y = df.iloc[:, n_features].to_numpy()
            return X, y
        if df.shape[1] == n_features:
            return df.to_numpy(), None
    return extract_from_dataframe(df)

def main():
    set_seed(SEED)
    X, y, model = load_data(DATASET_PATH)
    accuracy = 0.0
    if X is not None and y is not None:
        X, y = clean_data(X, y)
        if X is not None and y is not None and len(y) > 0 and X.shape[0] == len(y):
            accuracy = train_and_evaluate(X, y)
    elif model is not None:
        X_in, y_in = load_input_data_for_model(model)
        if X_in is not None and y_in is not None:
            accuracy = evaluate_model(model, X_in, y_in)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced manual CSV parsing loops with vectorized pandas/numpy loading to cut redundant processing.
# - Consolidated data extraction and cleaning into single-pass functions to reduce memory copies.
# - Avoided unnecessary model serialization and removed unused components to minimize I/O overhead.
# - Added deterministic seeding and stratified splitting to ensure reproducible results with minimal extra work.
# - Used conditional data loading and evaluation paths to prevent unnecessary computation when inputs are absent.