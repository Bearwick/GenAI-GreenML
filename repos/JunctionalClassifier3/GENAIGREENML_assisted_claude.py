# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import csv
import numpy as np
from sklearn import svm, metrics
from sklearn.model_selection import train_test_split

def load_csv(filename):
    features = []
    labels = []
    with open(filename, newline='') as csvfile:
        reader = csv.reader(csvfile)
        next(reader)  # skip header
        for row in reader:
            cols = [c for c in row if c != '']
            vals = list(map(float, cols))
            label_val = vals.pop()
            if label_val > 0:
                label_val = 1.0
            elif label_val < 0:
                label_val = -1.0
            labels.append(label_val)
            features.append(vals)
    return np.array(features, dtype=np.float64), np.array(labels, dtype=np.float64)

def train_and_test():
    X, y = load_csv('14k.csv')
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
    clf = svm.LinearSVC(dual='auto', max_iter=10000)
    clf.fit(X_train, y_train)
    prediction = clf.predict(X_test)
    accuracy = metrics.accuracy_score(y_test, prediction)
    print(f"ACCURACY={accuracy:.6f}")

train_and_test()

# OPTIMIZATION SUMMARY
# Replaced svm.SVC(kernel='linear') with LinearSVC which is significantly faster for linear kernels
# Converted features/labels directly to numpy arrays instead of nested Python lists to reduce memory overhead
# Removed redundant global mutable state (features, labels, pFeatures lists)
# Removed pickle save/load, predict, takeInput, and unittest code as they are not needed for training/testing
# Simplified CSV parsing by using csv.reader directly without manual splitting
# Skipped header using next(reader) instead of a boolean flag check each iteration
# Removed all plots, prints, model saving, and interactive inputs per requirements
# Used dual='auto' in LinearSVC to let sklearn pick the optimal solver based on data shape