# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

dataset = load_breast_cancer()
X, y = dataset.data, dataset.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = LogisticRegression(
    penalty='l2',
    C=1.0,
    solver='lbfgs',
    max_iter=100,
    random_state=42,
    n_jobs=1
)
model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Data Selection: Used the Breast Cancer Wisconsin dataset via sklearn, which is small and resides in memory, minimizing energy-intensive I/O operations and network calls.
# 2. Algorithm Choice: Selected Logistic Regression, a linear model that is significantly more energy-efficient than tree-based ensembles or neural networks for small-scale tabular data.
# 3. Computational Efficiency: Used the 'lbfgs' solver with a low max_iter limit (100) to ensure rapid convergence and minimal CPU usage.
# 4. Resource Management: Set n_jobs=1 to avoid the energy overhead and system complexity of multiprocessing for a dataset that fits easily into a single core's cache.
# 5. Preprocessing: Used StandardScaler to normalize features, which accelerates model convergence and reduces the number of iterations needed for training.
# 6. Minimalist Pipeline: Avoided complex feature engineering or high-dimensional embeddings that would increase the memory footprint and carbon intensity of the training process.