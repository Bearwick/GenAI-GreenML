# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.utils.multiclass import type_of_target


def _find_dataset_path() -> str:
    candidates = [
        "data.csv",
        "dataset.csv",
        "train.csv",
        "Train.csv",
        "Data.csv",
        "Dataset.csv",
        "input.csv",
    ]
    for c in candidates:
        if os.path.isfile(c):
            return c
    csvs = [f for f in os.listdir(".") if os.path.isfile(f) and f.lower().endswith(".csv")]
    if len(csvs) == 1:
        return csvs[0]
    if len(csvs) > 1:
        # Prefer common training file naming
        for name in ["train.csv", "data.csv", "dataset.csv"]:
            for f in csvs:
                if f.lower() == name:
                    return f
        return csvs[0]
    raise FileNotFoundError("No CSV dataset found in current directory.")


def _read_csv(path: str) -> pd.DataFrame:
    # Lightweight, robust read: try default; fallback to common separators
    try:
        return pd.read_csv(path)
    except Exception:
        try:
            return pd.read_csv(path, sep=";")
        except Exception:
            return pd.read_csv(path, sep="\t")


def _choose_target_column(df: pd.DataFrame) -> str:
    # Prefer typical label column names; else fallback to last column
    lower_map = {c.lower(): c for c in df.columns}
    preferred = [
        "target",
        "label",
        "y",
        "class",
        "outcome",
        "species",
        "survived",
        "churn",
        "default",
        "diagnosis",
    ]
    for p in preferred:
        if p in lower_map:
            return lower_map[p]
    return df.columns[-1]


def _sanitize_df(df: pd.DataFrame) -> pd.DataFrame:
    # Remove duplicate columns to avoid expanding feature space unnecessarily
    df = df.loc[:, ~df.columns.duplicated()].copy()
    # Drop entirely empty columns (saves compute and prevents pipeline issues)
    df = df.dropna(axis=1, how="all")
    return df


def main() -> None:
    path = _find_dataset_path()
    df = _read_csv(path)
    df = _sanitize_df(df)

    target_col = _choose_target_column(df)
    if target_col not in df.columns:
        raise ValueError("Target column not found after sanitization.")

    y = df[target_col]
    X = df.drop(columns=[target_col])

    # Remove rows where y is missing (keeps pipeline simple and stable)
    mask = ~pd.isna(y)
    X = X.loc[mask].reset_index(drop=True)
    y = y.loc[mask].reset_index(drop=True)

    # If y is text/categorical, keep as is; if numeric with many unique values, treat as classification only if discrete
    y_type = type_of_target(y)
    if y_type not in ("binary", "multiclass"):
        # Attempt to coerce to categorical if it's integer-like with limited unique values
        if pd.api.types.is_numeric_dtype(y):
            y_unique = pd.Series(y).dropna().unique()
            if len(y_unique) <= max(20, int(0.02 * len(y)) + 2):
                y = pd.Series(y).astype("Int64").astype(str)
            else:
                # Fallback: binarize by median (keeps classification requirement; energy-light)
                median = pd.to_numeric(y, errors="coerce").median()
                y = (pd.to_numeric(y, errors="coerce") >= median).astype(int)
        else:
            y = pd.Series(y).astype(str)

    # Identify feature types
    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = [c for c in X.columns if c not in numeric_cols]

    numeric_pipeline = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_pipeline = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_pipeline, numeric_cols),
            ("cat", categorical_pipeline, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y if type_of_target(y) in ("binary", "multiclass") else None,
    )

    # Logistic regression is small, fast on CPU, and works well for tabular data with one-hot features.
    # Use solver suitable for sparse high-dimensional matrices.
    model = LogisticRegression(
        solver="saga",
        penalty="l2",
        C=1.0,
        max_iter=300,
        n_jobs=1,
        random_state=42,
    )

    clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Chosen model: LogisticRegression (linear) to minimize compute and memory vs. tree ensembles or deep nets.
# - Solver: 'saga' handles sparse one-hot matrices efficiently on CPU; n_jobs=1 avoids excess parallel overhead.
# - Preprocessing: ColumnTransformer + SimpleImputer + StandardScaler/OneHotEncoder is fully reproducible and lightweight.
# - Sparse one-hot output reduces RAM for high-cardinality categoricals; handle_unknown='ignore' prevents costly failures.
# - Dropped fully-empty and duplicated columns to reduce feature space and unnecessary computation.
# - Avoided plotting, interactive inputs, and model serialization to keep runtime and I/O energy usage low.