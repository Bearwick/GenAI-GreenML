# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import glob
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        normed.append(c2)
    return normed


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", str(c).strip())]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _safe_read_csv(path):
    # Robust CSV parsing: try default, then fallback to European-style separators/decimals
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(dfx):
        if dfx is None:
            return True
        if dfx.shape[0] == 0 or dfx.shape[1] == 0:
            return True
        # Heuristic: if there's only 1 column but many commas/semicolons in sample rows, parsing likely wrong
        if dfx.shape[1] == 1:
            s = dfx.iloc[:10, 0].astype(str)
            if s.str.contains(r"[;,]").mean() > 0.6:
                return True
        return False

    if _looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            # Last resort: try python engine with separator inference
            df = pd.read_csv(path, sep=None, engine="python")

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)
    return df


def _coerce_numeric_inplace(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _choose_target(df):
    # Prefer a non-constant numeric column; else a low-cardinality object column
    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    # Ensure numeric cols are truly numeric (coerce objects that look numeric later in selection)
    # Here we also consider object columns that can be coerced mostly to numeric as candidates
    object_cols = [c for c in df.columns if df[c].dtype == "object" or pd.api.types.is_string_dtype(df[c])]

    # Try numeric columns first (non-constant, enough non-NaN)
    best = None
    best_score = -1
    for c in numeric_cols:
        s = df[c]
        nn = s.notna().sum()
        if nn < max(10, int(0.2 * len(df))):
            continue
        nunique = s.nunique(dropna=True)
        if nunique <= 1:
            continue
        score = nn + min(nunique, 100)  # simple tie-breaker
        if score > best_score:
            best, best_score = c, score
    if best is not None:
        return best, "numeric"

    # Try coercible numeric among object columns
    for c in object_cols:
        coerced = pd.to_numeric(df[c], errors="coerce")
        nn = coerced.notna().sum()
        if nn < max(10, int(0.2 * len(df))):
            continue
        nunique = coerced.nunique(dropna=True)
        if nunique <= 1:
            continue
        return c, "coerced_numeric"

    # Else: classification target from low-cardinality columns
    best = None
    best_score = -1
    for c in df.columns:
        nunique = df[c].nunique(dropna=True)
        if nunique < 2:
            continue
        # prefer modest number of classes
        if 2 <= nunique <= max(20, int(0.1 * len(df))):
            score = -abs(nunique - 2)  # prefer binary
            if score > best_score:
                best, best_score = c, score
    if best is not None:
        return best, "categorical"

    # Fallback: last column
    return df.columns[-1], "unknown"


def _pick_dataset_path():
    # Prefer common dataset filenames; else first CSV in cwd
    candidates = []
    for name in ["14k.csv", "data.csv", "dataset.csv", "train.csv"]:
        if os.path.exists(name):
            candidates.append(name)
    if candidates:
        return candidates[0]
    csvs = sorted(glob.glob("*.csv"))
    if csvs:
        return csvs[0]
    raise FileNotFoundError("No CSV dataset found in the current directory.")


def _bounded_regression_accuracy(y_true, y_pred):
    # Stable proxy in [0,1]: 1 / (1 + NMAE), where NMAE = MAE / (IQR + eps)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    mae = np.mean(np.abs(yt - yp))
    q75, q25 = np.percentile(yt, [75, 25])
    iqr = float(q75 - q25)
    denom = iqr if iqr > 1e-12 else (np.std(yt) if np.std(yt) > 1e-12 else (np.mean(np.abs(yt)) if np.mean(np.abs(yt)) > 1e-12 else 1.0))
    nmae = mae / (denom + 1e-12)
    acc = 1.0 / (1.0 + nmae)
    if not np.isfinite(acc):
        return 0.0
    return float(np.clip(acc, 0.0, 1.0))


def main():
    path = _pick_dataset_path()
    df = _safe_read_csv(path)

    # Normalize column names already done in reader; apply again defensively
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Drop fully empty rows/cols
    df = df.dropna(axis=0, how="all").dropna(axis=1, how="all")
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col, target_kind = _choose_target(df)

    # Prepare y and X
    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # If no features left, create a trivial feature
    if X.shape[1] == 0:
        X = pd.DataFrame({"__bias__": np.ones(len(df), dtype=float)})

    # Attempt to coerce object columns that are mostly numeric into numeric to reduce one-hot cost
    for c in list(X.columns):
        if X[c].dtype == "object" or pd.api.types.is_string_dtype(X[c]):
            coerced = pd.to_numeric(X[c], errors="coerce")
            # If many values become numeric, keep numeric; else keep as categorical
            if coerced.notna().mean() >= 0.8 and coerced.nunique(dropna=True) > 1:
                X[c] = coerced

    # Determine column types
    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    # Coerce numeric columns explicitly
    X = _coerce_numeric_inplace(X, numeric_features)

    # Decide task: classification if y has small number of distinct values; else regression
    task = "classification"
    y = y_raw

    if target_kind in ("numeric", "coerced_numeric") or pd.api.types.is_numeric_dtype(y_raw):
        y_num = pd.to_numeric(y_raw, errors="coerce")
        nunique = y_num.nunique(dropna=True)
        # If very low unique count, still treat as classification
        if nunique >= 2 and nunique <= 20:
            # classification on discretized/label values
            y = y_num
            task = "classification"
        else:
            y = y_num
            task = "regression"
    else:
        # categorical
        task = "classification"

    # Drop rows with missing target
    mask_y = pd.notna(y)
    X = X.loc[mask_y].reset_index(drop=True)
    y = y.loc[mask_y].reset_index(drop=True)

    assert len(X) > 0

    # If classification and too many unique labels (high cardinality), fallback to regression if possible
    if task == "classification":
        y_for_classes = y
        # If numeric labels but huge unique, fallback to regression
        if pd.api.types.is_numeric_dtype(y_for_classes):
            if y_for_classes.nunique(dropna=True) > max(50, int(0.2 * len(y_for_classes))):
                task = "regression"
                y = pd.to_numeric(y_for_classes, errors="coerce")
        else:
            if y_for_classes.nunique(dropna=True) > max(50, int(0.2 * len(y_for_classes))):
                # high-cardinality categorical: fallback to regression using codes
                task = "regression"
                y = y_for_classes.astype("category").cat.codes.astype(float)

    # Preprocessors
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Split
    stratify = None
    if task == "classification":
        try:
            if pd.Series(y).nunique(dropna=True) >= 2:
                stratify = y
        except Exception:
            stratify = None

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=stratify
    )
    assert len(X_train) > 0 and len(X_test) > 0

    if task == "classification":
        # If <2 classes after split, fallback to regression proxy
        if pd.Series(y_train).nunique(dropna=True) < 2 or pd.Series(y_test).nunique(dropna=True) < 2:
            task = "regression"

    if task == "classification":
        # Lightweight linear classifier
        clf = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            C=1.0,
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        # Ensure numeric y
        y_train_num = pd.to_numeric(y_train, errors="coerce")
        y_test_num = pd.to_numeric(y_test, errors="coerce")
        # Drop NaN y rows if any appeared
        train_mask = pd.notna(y_train_num)
        test_mask = pd.notna(y_test_num)
        X_train2 = X_train.loc[train_mask].reset_index(drop=True)
        y_train2 = y_train_num.loc[train_mask].reset_index(drop=True)
        X_test2 = X_test.loc[test_mask].reset_index(drop=True)
        y_test2 = y_test_num.loc[test_mask].reset_index(drop=True)
        if len(X_train2) == 0 or len(X_test2) == 0:
            accuracy = 0.0
        else:
            model.fit(X_train2, y_train2)
            y_pred = model.predict(X_test2)
            accuracy = _bounded_regression_accuracy(y_test2, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression liblinear / Ridge) for CPU efficiency and fast convergence.
# - ColumnTransformer+Pipeline ensures single-pass, reproducible preprocessing and avoids repeated conversions.
# - Minimal feature engineering: median/mode imputation, standard scaling for numeric stability, one-hot for categoricals.
# - Robust CSV loading with fallback separators/decimals reduces manual intervention and prevents expensive rework.
# - Schema-agnostic target selection picks a non-constant column; falls back safely if classification is ill-posed.
# - Regression fallback uses a bounded accuracy proxy in [0,1]: ACCURACY = 1 / (1 + NMAE), NMAE normalized by IQR.