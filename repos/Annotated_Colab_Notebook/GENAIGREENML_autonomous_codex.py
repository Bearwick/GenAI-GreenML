# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier
from sklearn.metrics import accuracy_score, r2_score

np.random.seed(42)

dataset_path = "dataset_adult.arff"

def load_dataset(path):
    df = None
    if str(path).lower().endswith(".arff"):
        try:
            from scipy.io import arff
            data, meta = arff.loadarff(path)
            df = pd.DataFrame(data)
        except Exception:
            df = None
    if df is None:
        try:
            df = pd.read_csv(path)
        except Exception:
            df = None
        if df is not None and df.shape[1] <= 1:
            try:
                df_alt = pd.read_csv(path, sep=";", decimal=",")
                if df_alt.shape[1] > df.shape[1]:
                    df = df_alt
            except Exception:
                pass
        if df is None:
            try:
                df = pd.read_csv(path, sep=";", decimal=",")
            except Exception:
                df = pd.DataFrame()
    return df

df = load_dataset(dataset_path)
if df is None:
    df = pd.DataFrame()

df.columns = [c.decode("utf-8") if isinstance(c, (bytes, bytearray, np.bytes_)) else c for c in df.columns]

for col in df.columns:
    if df[col].dtype == object:
        df[col] = df[col].apply(lambda x: x.decode("utf-8") if isinstance(x, (bytes, bytearray, np.bytes_)) else x)

df.replace("?", np.nan, inplace=True)

clean_cols = []
for c in df.columns:
    c = " ".join(str(c).strip().split())
    clean_cols.append(c)
df.columns = clean_cols
df = df.loc[:, [c for c in df.columns if not c.lower().startswith("unnamed")]]

new_cols = []
counts = {}
for c in df.columns:
    if c in counts:
        counts[c] += 1
        new_cols.append(f"{c}_{counts[c]}")
    else:
        counts[c] = 0
        new_cols.append(c)
df.columns = new_cols

def select_target(df):
    cols = list(df.columns)
    if not cols:
        return None
    lower = [c.lower() for c in cols]
    preferred = None
    for key in ["target", "label", "class", "y", "output"]:
        for c, lc in zip(cols, lower):
            if lc == key or lc.endswith(key) or key in lc:
                preferred = c
                break
        if preferred is not None:
            break
    if preferred is None:
        numeric_candidates = []
        for c in cols:
            series = df[c]
            if pd.api.types.is_numeric_dtype(series):
                numeric_candidates.append(c)
            else:
                converted = pd.to_numeric(series, errors="coerce")
                if converted.notna().mean() > 0.9:
                    numeric_candidates.append(c)
        for c in numeric_candidates:
            if df[c].nunique(dropna=True) > 1:
                preferred = c
                break
    if preferred is None:
        preferred = cols[-1]
    if df[preferred].nunique(dropna=True) <= 1:
        for c in cols:
            if df[c].nunique(dropna=True) > 1:
                preferred = c
                break
    return preferred

target_col = select_target(df)
if target_col is None:
    df["target"] = 0
    target_col = "target"

y = df[target_col] if target_col in df.columns else pd.Series([0] * len(df))
X = df.drop(columns=[target_col]) if target_col in df.columns else df.copy()

if not X.empty:
    X = X.loc[:, X.notna().any()]

def determine_task(y):
    if pd.api.types.is_bool_dtype(y):
        return True
    if pd.api.types.is_numeric_dtype(y):
        uniq = y.nunique(dropna=True)
        if uniq <= 20 and uniq / max(len(y), 1) < 0.2:
            return True
        return False
    converted = pd.to_numeric(y, errors="coerce")
    non_na_ratio = converted.notna().mean() if len(converted) > 0 else 0
    uniq = converted.nunique(dropna=True)
    if non_na_ratio > 0.9 and uniq > 20 and uniq / max(len(y), 1) > 0.2:
        return False
    return True

classification_flag = determine_task(y)

if classification_flag:
    mask = y.notna()
    X = X.loc[mask]
    y = y.loc[mask].astype(str)
    y = y.str.strip()
    y = y.replace(["nan", "NaN", "None"], np.nan)
    mask = y.notna()
    X = X.loc[mask]
    y = y.loc[mask]
else:
    y = pd.to_numeric(y, errors="coerce")
    mask = y.notna()
    X = X.loc[mask]
    y = y.loc[mask]

numeric_cols = []
categorical_cols = []
for col in X.columns:
    if pd.api.types.is_numeric_dtype(X[col]):
        numeric_cols.append(col)
    else:
        converted = pd.to_numeric(X[col], errors="coerce")
        non_na_ratio = converted.notna().mean() if len(converted) > 0 else 0
        if non_na_ratio > 0.8:
            X[col] = converted
            numeric_cols.append(col)
        else:
            categorical_cols.append(col)

for col in numeric_cols:
    X[col] = pd.to_numeric(X[col], errors="coerce")

X.replace([np.inf, -np.inf], np.nan, inplace=True)

drop_cols = []
for col in numeric_cols:
    if X[col].notna().sum() == 0:
        drop_cols.append(col)
for col in categorical_cols:
    if X[col].notna().sum() == 0:
        drop_cols.append(col)
if drop_cols:
    X = X.drop(columns=drop_cols)
    numeric_cols = [c for c in numeric_cols if c not in drop_cols]
    categorical_cols = [c for c in categorical_cols if c not in drop_cols]

if X.shape[1] == 0:
    X = pd.DataFrame({"__dummy__": np.zeros(len(y))})
    numeric_cols = ["__dummy__"]
    categorical_cols = []

assert len(X) > 0 and len(y) > 0

transformers = []
if numeric_cols:
    num_pipeline = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False))
    ])
    transformers.append(("num", num_pipeline, numeric_cols))
if categorical_cols:
    cat_pipeline = Pipeline([
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])
    transformers.append(("cat", cat_pipeline, categorical_cols))
preprocess = ColumnTransformer(transformers, remainder="drop")

if classification_flag:
    n_classes = y.nunique()
    if n_classes >= 2:
        model = LogisticRegression(max_iter=200, solver="liblinear", n_jobs=1, random_state=42)
    else:
        model = DummyClassifier(strategy="most_frequent")
else:
    model = Ridge(alpha=1.0)

pipeline = Pipeline([
    ("preprocess", preprocess),
    ("model", model)
])

n_samples = len(y)
if n_samples < 2:
    X_train, X_test, y_train, y_test = X, X, y, y
else:
    test_size = 0.2 if n_samples >= 5 else 0.5
    stratify = None
    if classification_flag and y.nunique() >= 2:
        vc = y.value_counts()
        if vc.min() >= 2 and n_samples >= 4:
            stratify = y
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )

assert len(X_train) > 0 and len(X_test) > 0

pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

if classification_flag:
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Chose lightweight linear models with minimal preprocessing to reduce CPU usage.
# - Used a reproducible ColumnTransformer pipeline with simple imputation and one-hot encoding.
# - For regression, mapped R2 to a bounded [0,1] accuracy proxy via (r2+1)/2.