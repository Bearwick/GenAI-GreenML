# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

from scipy.io import arff
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

file_path = "dataset_adult.arff"
data, meta = arff.loadarff(file_path)
df = pd.DataFrame(data)

for col in df.select_dtypes([object]).columns:
    df[col] = df[col].str.decode('utf-8')

df_encoded = pd.get_dummies(df, drop_first=True)

X = df_encoded.drop('income_>50K', axis=1)
y = df_encoded['income_>50K']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

clf = RandomForestClassifier(
    n_estimators=50,
    max_depth=20,
    min_samples_leaf=4,
    max_features="sqrt",
    n_jobs=-1,
    random_state=42,
)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Reduced n_estimators from default 100 to 50 to cut training time and energy roughly in half.
# 2. Set max_depth=20 and min_samples_leaf=4 to limit tree complexity, reducing memory and compute.
# 3. Explicitly set max_features="sqrt" for efficient feature subsampling (already default but explicit).
# 4. Enabled n_jobs=-1 to use all CPU cores, reducing wall-clock time and active energy window.
# 5. Removed all prints, plots, model saving, and interactive inputs per requirements.