# Generated by generate_llm_code_2.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from scipy.io import arff
from sklearn.model_selection import train_test_split
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.preprocessing import OrdinalEncoder
from sklearn.metrics import accuracy_score

data, meta = arff.loadarff("dataset_adult.arff")
df = pd.DataFrame(data)

for col in df.select_dtypes([object]).columns:
    df[col] = df[col].str.decode('utf-8')

target_col = df.columns[-1]
X = df.drop(columns=[target_col]).copy()
y = df[target_col]

categorical_columns = X.select_dtypes(include=['object']).columns
X[categorical_columns] = OrdinalEncoder().fit_transform(X[categorical_columns])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = HistGradientBoostingClassifier(
    max_iter=100,
    early_stopping=True,
    random_state=42
)
model.fit(X_train, y_train)

accuracy = accuracy_score(y_test, model.predict(X_test))
print(f"ACCURACY={accuracy:.6f}")

"""
JUSTIFICATION FOR ENERGY-EFFICIENT DESIGN:
1. Model Selection: HistGradientBoostingClassifier was chosen over Random Forest because it uses histogram-based binning, which significantly reduces the computational complexity and memory usage during the split-finding process.
2. Encoding Strategy: OrdinalEncoder is utilized instead of One-Hot Encoding to prevent a high-dimensional sparse feature space. This keeps the memory footprint small and reduces the number of operations required per tree iteration.
3. Training Optimization: Early stopping is enabled to automatically terminate the training process once the model stops improving, preventing wasted CPU cycles and energy.
4. Computationally Lightweight: The solution avoids deep learning frameworks and specialized hardware (GPUs), utilizing highly optimized C++/Cython-based Scikit-Learn implementations that run efficiently on standard CPUs.
"""