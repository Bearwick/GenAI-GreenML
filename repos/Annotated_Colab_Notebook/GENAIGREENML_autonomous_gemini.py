# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from scipy.io import arff
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import sys

# Dataset path provided in prompt
DATA_PATH = 'dataset_adult.arff'

def load_data(path):
    try:
        # ARFF files often used in UCI datasets like Adult
        data, meta = arff.loadarff(path)
        df = pd.DataFrame(data)
        # Decode bytes to strings for pandas objects
        for col in df.columns:
            if df[col].dtype == object:
                df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
        return df
    except Exception:
        try:
            # Fallback to standard CSV parsing
            return pd.read_csv(path, skipinitialspace=True)
        except Exception:
            try:
                # Robust fallback for alternative delimiters
                return pd.read_csv(path, sep=';', decimal=',', skipinitialspace=True)
            except Exception:
                return pd.DataFrame()

def preprocess_and_train():
    df = load_data(DATA_PATH)
    
    if df.empty:
        return 0.0

    # Normalize column names: strip whitespace, remove Unnamed, collapse internal spaces
    df.columns = [str(c).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    
    # Identify target: assume last column for classification tasks
    target_col = df.columns[-1]
    
    # Clean features: coerce potential numeric columns that were parsed as strings
    X = df.drop(columns=[target_col])
    y = df[target_col]

    # Energy-efficient choice: minimize feature set by identifying numeric/categorical
    numeric_features = []
    categorical_features = []
    
    for col in X.columns:
        # Robustly determine if column is numeric
        if pd.api.types.is_numeric_dtype(X[col]):
            numeric_features.append(col)
        else:
            # Check if it's a string representation of a number
            converted = pd.to_numeric(X[col], errors='coerce')
            if converted.notnull().mean() > 0.8:
                X[col] = converted
                numeric_features.append(col)
            else:
                categorical_features.append(col)

    # Ensure dataset is valid
    if X.empty or len(y.unique()) < 1:
        return 0.0

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Define Preprocessing Pipeline
    # Numeric: Median imputation (robust to outliers) and Scaling (required for Linear Models)
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    # Categorical: Most frequent imputation and OneHotEncoding
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ])

    # Model: Logistic Regression (extremely CPU efficient and low energy consumption)
    clf = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', LogisticRegression(max_iter=500, solver='lbfgs', n_jobs=1))
    ])

    # Final check on target diversity
    if len(np.unique(y_train)) < 2:
        # Trivial baseline if target lacks variance
        y_pred = [y_train.iloc[0]] * len(y_test)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
    
    return accuracy

if __name__ == "__main__":
    final_acc = preprocess_and_train()
    print(f"ACCURACY={final_acc:.6f}")

# Optimization Summary:
# 1. Model Choice: Logistic Regression was selected as it provides a strong baseline with minimal CPU cycles compared to Gradient Boosting or Deep Learning.
# 2. Preprocessing: Used Scikit-learn Pipelines to avoid data leakage and minimize redundant transformations.
# 3. Energy Efficiency: Leveraged the 'lbfgs' solver which is memory-efficient and converges quickly for medium datasets.
# 4. Robustness: Implemented multi-stage CSV/ARFF parsing and dynamic column identification to handle schema uncertainty.
# 5. Resource Management: Single-threaded execution (n_jobs=1) avoids overhead of process spawning for small-to-medium tasks.
# 6. Fallback logic: Included checks for single-class targets to prevent pipeline failure and ensure end-to-end execution.