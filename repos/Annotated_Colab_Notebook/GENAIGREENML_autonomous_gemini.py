# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

from scipy.io import arff
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder

data, meta = arff.loadarff("dataset_adult.arff")
df = pd.DataFrame(data)

for col in df.select_dtypes([object]).columns:
    df[col] = df[col].str.decode('utf-8')

target_col = df.columns[-1]
X = df.drop(columns=[target_col])
y = df[target_col]

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

for col in X.select_dtypes(include=['object']).columns:
    X[col] = X[col].astype('category').cat.codes

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = HistGradientBoostingClassifier(
    max_iter=100,
    early_stopping=True,
    validation_fraction=0.1,
    n_iter_no_change=10,
    random_state=42
)

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

"""
Design Justification for Energy Efficiency:
1. HistGradientBoostingClassifier: Uses histogram-based binning which reduces the computational complexity of finding optimal splits from O(n log n) to O(n), significantly lowering CPU cycles.
2. Early Stopping: The training process terminates automatically when the validation score plateaus, preventing unnecessary iterations and associated energy waste.
3. Memory Optimization: Integer encoding (cat.codes) is used instead of One-Hot Encoding. This keeps the feature space compact, reduces the memory footprint, and improves cache efficiency during model training.
4. Resource Appropriateness: By using a gradient-boosted tree approach instead of a Deep Learning model, the solution achieves high performance on tabular data while remaining lightweight enough for CPU-only environments.
"""