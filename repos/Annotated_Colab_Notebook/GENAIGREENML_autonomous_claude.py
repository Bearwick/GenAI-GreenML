# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
import os

# Load dataset - try common dataset locations
train_path = "train.csv"
test_path = "test.csv"

if os.path.exists(train_path):
    df = pd.read_csv(train_path)
else:
    raise FileNotFoundError("Cannot find train.csv")

# Detect target column - assume last column or common names
target_candidates = ['target', 'Target', 'label', 'Label', 'class', 'Class', 'y', 'Survived', 'survived', 'outcome', 'price_range', 'Species', 'species']
target_col = None

for col in target_candidates:
    if col in df.columns:
        target_col = col
        break

if target_col is None:
    target_col = df.columns[-1]

# Separate features and target
y = df[target_col].copy()
X = df.drop(columns=[target_col])

# Drop ID-like columns (unique values close to number of rows)
id_cols = []
for col in X.columns:
    if X[col].dtype in ['int64', 'float64']:
        if X[col].nunique() > 0.9 * len(X) and X[col].nunique() == len(X):
            id_cols.append(col)
    elif col.lower() in ['id', 'index', 'row_id']:
        id_cols.append(col)

X = X.drop(columns=id_cols)

# Encode target if categorical
le_target = LabelEncoder()
if y.dtype == 'object' or y.dtype.name == 'category':
    y = le_target.fit_transform(y)
else:
    y = y.values

# Identify numeric and categorical columns
numeric_cols = X.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()

# Build preprocessing pipeline - lightweight and efficient
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False, max_categories=20))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_cols),
        ('cat', categorical_transformer, categorical_cols)
    ],
    remainder='drop'
)

# Preprocess data
X_processed = preprocessor.fit_transform(X)

# Determine number of classes for model selection
n_classes = len(np.unique(y))
n_samples = X_processed.shape[0]
n_features = X_processed.shape[1]

# Use cross-validation for robust evaluation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Choose lightweight models appropriate for dataset size
# RandomForest: good balance of accuracy and efficiency for small/medium datasets
# GradientBoosting: often best accuracy for tabular data, kept small for efficiency
models = {
    'rf': RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    ),
    'gb': Grad