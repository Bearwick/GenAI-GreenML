# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: assisted

import pandas as pd
from scipy.io import arff
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

data, _ = arff.loadarff("dataset_adult.arff")
df = pd.DataFrame(data)

for col in df.columns:
    if df[col].dtype == object:
        df[col] = df[col].str.decode('utf-8').astype('category')

categorical_cols = df.select_dtypes(['category']).columns
for col in categorical_cols:
    df[col] = df[col].cat.codes

X = df.drop('income', axis=1)
y = df['income']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATIONS APPLIED:
# 1. Memory Efficiency: Converted object strings to pandas 'category' type and then to integer codes. This drastically reduces memory footprint compared to One-Hot Encoding.
# 2. Computational Speed: Used Label Encoding (cat.codes) instead of One-Hot Encoding. This keeps the feature space small, allowing the Random Forest to train faster and consume less energy.
# 3. Parallelization: Enabled n_jobs=-1 in RandomForestClassifier to utilize all available CPU cores, reducing total execution runtime.
# 4. Minimized Data Movement: Avoided creating multiple intermediate DataFrame copies (like df_encoded) to save RAM and reduce overhead.
# 5. Vectorized Operations: Used vectorized string decoding and category casting instead of explicit Python loops where possible.
