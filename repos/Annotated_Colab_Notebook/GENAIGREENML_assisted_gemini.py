# Generated by generate_llm_code_2.py
# LLM: gemini
# Mode: assisted

from scipy.io import arff
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

def run_optimized_pipeline(file_path="dataset_adult.arff"):
    raw_data, _ = arff.loadarff(file_path)
    df = pd.DataFrame(raw_data)

    object_columns = df.select_dtypes(['object']).columns
    for col in object_columns:
        df[col] = df[col].str.decode('utf-8')

    df = pd.get_dummies(df, drop_first=True, dtype='int8')

    target_col = 'income_>50K'
    X = df.drop(target_col, axis=1)
    y = df[target_col]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_optimized_pipeline()

"""
APPLIED OPTIMIZATIONS:
1. Memory Footprint Reduction: Specified dtype='int8' in pd.get_dummies to reduce the memory 
   usage of the sparse encoded dataframe compared to default float64/int64.
2. Parallel Processing: Enabled n_jobs=-1 in RandomForestClassifier to utilize all available 
   CPU cores, reducing total execution runtime and energy consumption during the training phase.
3. Efficient Data Loading: Used vectorized pandas string decoding instead of manual iterative 
   processing where possible.
4. Minimal I/O and Overhead: Removed expensive operations such as classification_report, 
   head() previews, and redundant prints which add unnecessary computational and I/O overhead.
5. Optimized Feature Selection: Dropped target column directly from the encoded dataframe to 
   avoid creating multiple large intermediate copies of the dataset.
"""