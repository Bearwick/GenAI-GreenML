# Generated by generate_llm_code.py
# LLM: groq
# Mode: assisted

from scipy.io import arff
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load dataset
data, _ = arff.loadarff("dataset_adult.arff")
df = pd.DataFrame(data)
for col in df.select_dtypes([object]).columns:
    df[col] = df[col].str.decode("utf-8")

# One‑hot encode and convert to numpy arrays to avoid unnecessary DataFrame overhead
df_enc = pd.get_dummies(df, drop_first=True)
X = df_enc.drop("income_>50K", axis=1).values
y = df_enc["income_>50K"].values

# Train‑test split with a fixed seed for reproducibility
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Train a lightweight RandomForest with a reduced number of trees
clf = RandomForestClassifier(
    n_estimators=50,  # fewer trees reduce compute time
    random_state=42,
    n_jobs=-1,        # parallelize over CPU cores
)
clf.fit(X_train, y_train)

# Evaluate and print accuracy
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# Optimization explanations
# 1. Removed interactive prints and classification report to lower I/O overhead.
# 2. Converted DataFrame to dense numpy arrays before training, reducing DataFrame
#    bookkeeping costs during model fit.
# 3. Dropped unnecessary DataFrame columns after encoding to keep only feature matrix
#    and target vector.
# 4. Limited RandomForest to 50 estimators; this shortens training time while keeping
#    comparable predictive power for this benchmark task.
# 5. Enabled multi‑core training via n_jobs=-1 to amortize CPU usage.
# 6. Fixed random_state for reproducible results across runs.