# Generated by generate_llm_code_2.py
# LLM: groq
# Mode: assisted

import pandas as pd
from scipy.io import arff
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

def load_and_prepare(path: str):
    data, _ = arff.loadarff(path)
    df = pd.DataFrame(data)
    for col in df.select_dtypes([object]).columns:
        df[col] = df[col].str.decode("utf-8")
    df_encoded = pd.get_dummies(df, drop_first=True)
    X = df_encoded.drop("income_>50K", axis=1)
    y = df_encoded["income_>50K"]
    return X, y

def split_data(X, y):
    return train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

def train_model(X_train, y_train):
    clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    clf.fit(X_train, y_train)
    return clf

def evaluate(clf, X_test, y_test):
    y_pred = clf.predict(X_test)
    return accuracy_score(y_test, y_pred)

def main():
    X, y = load_and_prepare("dataset_adult.arff")
    X_train, X_test, y_train, y_test = split_data(X, y)
    clf = train_model(X_train, y_train)
    accuracy = evaluate(clf, X_test, y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()
# Optimization summary:
# 1. Removed dataset preview printing to save I/O time.
# 2. Kept a single decode loop; vectorized decode would add overhead for small categorical columns.
# 3. Eliminated the classification report to reduce computation.
# 4. Used n_jobs=-1 in RandomForest to leverage all CPU cores, speeding up training.
# 5. Added stratified splitting to maintain class balance without extra processing.
# 6. Consolidated logic into functions for readability and potential reuse, reducing global namespace clutter.