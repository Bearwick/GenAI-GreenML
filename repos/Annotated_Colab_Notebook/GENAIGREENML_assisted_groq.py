# Generated by generate_llm_code.py
# LLM: groq
# Mode: assisted

from scipy.io import arff
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import joblib

# Load ARFF file into a DataFrame
data, meta = arff.loadarff("dataset_adult.arff")
df = pd.DataFrame(data)

# Decode bytes in categorical columns
for col in df.select_dtypes([object]).columns:
    df[col] = df[col].str.decode('utf-8')

# One‑hot encode categorical features, dropping the first level to avoid multicollinearity
df_encoded = pd.get_dummies(df, drop_first=True)

# Separate features and target
X = df_encoded.drop('income_>50K', axis=1)
y = df_encoded['income_>50K']

# Convert to float32 to reduce memory footprint
X = X.astype('float32')
y = y.astype('float32')

# Train‑test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, shuffle=True
)

# Train a lightweight Random Forest
clf = RandomForestClassifier(
    n_estimators=50,
    max_features='sqrt',
    n_jobs=-1,
    random_state=42,
    verbose=0
)
clf.fit(X_train, y_train)

# Predict and evaluate
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# Persist the trained model for future reuse
joblib.dump(clf, "adult_income_rf.joblib")

# ---------- Optimizations Applied ----------
# 1. Reduced the number of trees (n_estimators=50) to speed up training while maintaining reasonable accuracy.
# 2. Set max_features='sqrt' (default) to limit feature combinations per split, reducing computation.
# 3. Used n_jobs=-1 to leverage all CPU cores for parallel tree construction, lowering wall‑time.
# 4. Converted feature matrix and target vector to float32, cutting memory usage by roughly half.
# 5. Dropped the first dummy level in one‑hot encoding (drop_first=True) to avoid redundant columns and save space.
# 6. Avoided unnecessary intermediate prints, visualizations, and interactive input; only the final accuracy is printed.
# 7. Serialized the model with joblib to prevent retraining on subsequent runs, saving energy on future predictions.