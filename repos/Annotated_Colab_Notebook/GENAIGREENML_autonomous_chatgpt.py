# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression


def _find_target_column(df: pd.DataFrame) -> str:
    # Try common target names first
    lowered = {c.lower(): c for c in df.columns}
    for cand in ("target", "label", "y", "class", "outcome"):
        if cand in lowered:
            return lowered[cand]

    # Otherwise pick the last column (common in many tabular datasets)
    return df.columns[-1]


def _load_dataset() -> pd.DataFrame:
    # Keep IO simple and CPU-light: prefer a single CSV in the working directory.
    candidates = []
    for fn in os.listdir("."):
        if fn.lower().endswith(".csv"):
            candidates.append(fn)

    if not candidates:
        raise FileNotFoundError("No .csv dataset found in the current directory.")

    # Prefer files named like "train.csv", "data.csv", etc. if present.
    preferred = ("train.csv", "data.csv", "dataset.csv", "df.csv")
    for p in preferred:
        for c in candidates:
            if c.lower() == p:
                return pd.read_csv(c)

    # Otherwise load the first CSV found (deterministic sort).
    candidates.sort()
    return pd.read_csv(candidates[0])


def _split_features_target(df: pd.DataFrame, target_col: str):
    X = df.drop(columns=[target_col])
    y = df[target_col]
    return X, y


def _make_preprocessor(X: pd.DataFrame) -> ColumnTransformer:
    # Lightweight preprocessing: imputers + scaling for numeric and one-hot for categorical.
    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = [c for c in X.columns if c not in numeric_cols]

    numeric_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            (
                "onehot",
                OneHotEncoder(handle_unknown="ignore", sparse_output=True, dtype=np.float32),
            ),
        ]
    )

    return ColumnTransformer(
        transformers=[
            ("num", numeric_pipe, numeric_cols),
            ("cat", categorical_pipe, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
        n_jobs=None,
    )


def _coerce_target(y: pd.Series) -> pd.Series:
    # Ensure target is 1D and usable for classification.
    # If numeric with many unique values, discretization isn't safe; assume it's classification if few uniques.
    if y.dtype.kind in "biufc":
        # If floats, try to reduce to int if near-integer labels to avoid accidental regression-like target.
        y_non_null = y.dropna()
        if len(y_non_null) > 0:
            # If values are integers or close to integers, cast.
            if np.all(np.isclose(y_non_null.values, np.round(y_non_null.values), atol=1e-9)):
                y = y.round().astype("Int64")
    return y


def main():
    df = _load_dataset()
    df = df.copy()

    target_col = _find_target_column(df)

    # Drop rows where target is missing to keep training stable and lightweight.
    df = df.loc[~df[target_col].isna()].reset_index(drop=True)

    X, y = _split_features_target(df, target_col)
    y = _coerce_target(y)

    # Stratify if classification labels are not too sparse
    y_for_strat = None
    try:
        y_counts = y.value_counts(dropna=True)
        if (len(y_counts) >= 2) and (y_counts.min() >= 2):
            y_for_strat = y
    except Exception:
        y_for_strat = None

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y_for_strat,
    )

    preprocessor = _make_preprocessor(X_train)

    # Energy-efficient model choice: LogisticRegression is fast, convex, and strong for tabular baselines.
    # Use saga to handle sparse one-hot efficiently; keep iterations modest and deterministic.
    clf = LogisticRegression(
        solver="saga",
        penalty="l2",
        C=1.0,
        max_iter=300,
        tol=1e-3,
        n_jobs=1,
        random_state=42,
    )

    model = Pipeline(
        steps=[
            ("prep", preprocessor),
            ("clf", clf),
        ]
    )

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()


# OPTIMIZATION SUMMARY
# - Uses a small, CPU-efficient linear classifier (LogisticRegression) instead of deep models to minimize compute/energy.
# - Employs a reproducible, single-pass preprocessing pipeline (imputation + scaling + sparse one-hot) via ColumnTransformer.
# - Sparse one-hot encoding reduces memory and compute for categorical features; 'saga' handles sparse data efficiently.
# - Keeps training deterministic (fixed random_state) and bounded (modest max_iter, relaxed tol) to avoid wasted cycles.
# - Avoids plots, interactive inputs, model saving, and excessive logging to reduce unnecessary overhead.