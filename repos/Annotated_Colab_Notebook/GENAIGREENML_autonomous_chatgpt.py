# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings
from typing import Tuple, Optional

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c = "" if c is None else str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        normed.append(c)
    return normed


def _drop_unnamed(df: pd.DataFrame) -> pd.DataFrame:
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _read_arff_fallback(path: str) -> pd.DataFrame:
    # Minimal ARFF loader for common cases (Adult dataset-like): reads @attribute names and @data rows
    attributes = []
    data_rows = []
    in_data = False
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        for raw in f:
            line = raw.strip()
            if not line or line.startswith("%"):
                continue
            low = line.lower()
            if low.startswith("@attribute"):
                # @attribute name type
                # name can be quoted or unquoted; split carefully
                parts = line.split(None, 2)
                if len(parts) >= 2:
                    name = parts[1].strip()
                    if (name.startswith("'") and name.endswith("'")) or (name.startswith('"') and name.endswith('"')):
                        name = name[1:-1]
                    attributes.append(name)
            elif low.startswith("@data"):
                in_data = True
                continue
            elif in_data:
                if line.startswith("{") and line.endswith("}"):
                    # Sparse ARFF not supported; skip
                    continue
                # Split CSV-like (Adult ARFF is comma-separated)
                row = [x.strip() for x in line.split(",")]
                data_rows.append(row)

    if not data_rows:
        return pd.DataFrame()

    # Align row lengths to attributes where possible
    max_len = max(len(r) for r in data_rows)
    if attributes and len(attributes) != max_len:
        # If attributes mismatch, fall back to generic column names
        cols = [f"col_{i}" for i in range(max_len)]
    else:
        cols = attributes if attributes else [f"col_{i}" for i in range(max_len)]

    fixed_rows = []
    for r in data_rows:
        if len(r) < len(cols):
            r = r + [None] * (len(cols) - len(r))
        elif len(r) > len(cols):
            r = r[: len(cols)]
        fixed_rows.append(r)

    df = pd.DataFrame(fixed_rows, columns=cols)
    return df


def load_dataset(path: str) -> pd.DataFrame:
    ext = os.path.splitext(path)[1].lower()
    df = pd.DataFrame()

    if ext == ".arff":
        df = _read_arff_fallback(path)
    else:
        # Robust CSV parsing fallback
        try:
            df = pd.read_csv(path)
        except Exception:
            df = pd.DataFrame()
        # If parsing looks wrong (single column with separators), retry
        if df is not None and not df.empty and df.shape[1] == 1:
            col0 = str(df.columns[0])
            sample = ""
            try:
                sample = str(df.iloc[0, 0])
            except Exception:
                sample = ""
            if (";" in col0) or (";" in sample):
                try:
                    df = pd.read_csv(path, sep=";", decimal=",")
                except Exception:
                    pass

    if df is None:
        df = pd.DataFrame()

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)
    return df


def _to_numeric_if_possible(s: pd.Series) -> pd.Series:
    # Coerce to numeric if it seems mostly numeric; else keep as object
    if s.dtype.kind in "biufc":
        return pd.to_numeric(s, errors="coerce")
    # Try conversion; if many values become numeric, accept
    sn = pd.to_numeric(s.astype(str).str.replace(",", ".", regex=False), errors="coerce")
    non_na = s.notna().sum()
    if non_na == 0:
        return s
    ratio = sn.notna().sum() / max(non_na, 1)
    if ratio >= 0.85:
        return sn
    return s


def choose_target_and_features(df: pd.DataFrame) -> Tuple[str, list]:
    cols = list(df.columns)
    assert len(cols) > 0, "Empty dataset after loading."

    # Normalize missing markers
    for c in cols:
        if df[c].dtype == object:
            df[c] = df[c].replace(["?", "NA", "N/A", "null", "None", ""], np.nan)

    # Prefer likely label columns by name (robust, not strict)
    lowered = {c: str(c).strip().lower() for c in cols}
    preferred = [c for c in cols if lowered[c] in ("class", "target", "label", "y", "income")]
    if not preferred:
        preferred = [c for c in cols if any(k in lowered[c] for k in ("class", "target", "label", "income"))]

    candidate_targets = preferred[:] if preferred else []

    # If none, prefer last column as many datasets place label last
    if not candidate_targets and cols:
        candidate_targets = [cols[-1]]

    # If still problematic, search for a non-constant column (prefer low-cardinality)
    if not candidate_targets:
        candidate_targets = cols[:]

    # Determine best target: non-constant, reasonably low cardinality for classification
    best = None
    best_score = -1
    for c in candidate_targets:
        ser = df[c]
        n_unique = ser.nunique(dropna=True)
        if n_unique <= 1:
            continue
        # Score: prefer <=50 unique and object-like; else numeric with few uniques
        score = 0
        if ser.dtype == object:
            score += 3
        if 2 <= n_unique <= 50:
            score += 3
        elif n_unique > 50:
            score -= 1
        # Penalize extremely high missingness
        miss = ser.isna().mean()
        score -= miss
        if score > best_score:
            best_score = score
            best = c

    if best is None:
        # Fallback: choose first column
        best = cols[0]

    target = best
    features = [c for c in cols if c != target]
    if not features:
        # If only one column exists, fabricate a constant feature to allow pipeline to run
        df["__const__"] = 1
        features = ["__const__"]
    return target, features


def build_and_evaluate(df: pd.DataFrame) -> float:
    target, features = choose_target_and_features(df)

    # Coerce numeric columns safely
    for c in features + [target]:
        df[c] = _to_numeric_if_possible(df[c])

    # Remove infs
    df = df.replace([np.inf, -np.inf], np.nan)

    # Drop rows with missing target
    df = df.dropna(subset=[target])
    assert not df.empty, "No data left after dropping missing target."

    X = df[features]
    y = df[target]

    # Determine task type
    is_classification = False
    if y.dtype == object:
        is_classification = True
    else:
        # If numeric but low unique count, treat as classification
        n_unique = y.nunique(dropna=True)
        if 2 <= n_unique <= 20:
            is_classification = True

    # Handle too-small datasets
    if len(df) < 5:
        # Trivial baseline
        if is_classification:
            accuracy = 0.0
        else:
            accuracy = 0.0
        return float(accuracy)

    # Split with defensive handling
    test_size = 0.2
    random_state = 42

    if is_classification:
        # If only 1 class, fallback to regression-like score proxy
        classes = pd.Series(y).dropna().unique()
        if len(classes) < 2:
            is_classification = False

    if is_classification:
        # Try stratify; if it fails, proceed without
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state, stratify=y
            )
        except Exception:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state
            )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )

    assert len(X_train) > 0 and len(X_test) > 0, "Train/test split produced empty set."

    # Identify column types from training data
    numeric_features = [c for c in features if pd.api.types.is_numeric_dtype(X_train[c])]
    categorical_features = [c for c in features if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if is_classification:
        # Lightweight linear classifier; small iterations for CPU efficiency
        clf = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            C=1.0,
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        # If training classes collapse due to split, fallback to DummyClassifier
        if pd.Series(y_train).nunique(dropna=True) < 2:
            model = Pipeline(steps=[("preprocess", preprocessor), ("model", DummyClassifier(strategy="most_frequent"))])

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        return float(accuracy)
    else:
        # Regression fallback; convert R^2 into bounded [0,1] accuracy proxy
        reg = Ridge(alpha=1.0, random_state=random_state)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])

        # If y is non-numeric, coerce for regression
        y_train_num = pd.to_numeric(y_train, errors="coerce")
        y_test_num = pd.to_numeric(y_test, errors="coerce")

        # If coercion fails badly, use dummy regressor on encoded target index
        if y_train_num.isna().mean() > 0.5 or y_test_num.isna().mean() > 0.5:
            # Encode target categories deterministically
            y_all = pd.Series(pd.concat([y_train.astype(str), y_test.astype(str)], axis=0))
            codes, uniques = pd.factorize(y_all, sort=True)
            y_train_num = pd.Series(codes[: len(y_train)], index=y_train.index).astype(float)
            y_test_num = pd.Series(codes[len(y_train) :], index=y_test.index).astype(float)

        # Drop rows with NaN in y for fitting/scoring
        train_mask = ~y_train_num.isna()
        test_mask = ~y_test_num.isna()
        if train_mask.sum() < 2 or test_mask.sum() < 1:
            dummy = Pipeline(steps=[("preprocess", preprocessor), ("model", DummyRegressor(strategy="mean"))])
            dummy.fit(X_train, y_train_num.fillna(y_train_num.mean()))
            preds = dummy.predict(X_test)
            yt = y_test_num.fillna(y_train_num.mean()).to_numpy(dtype=float)
        else:
            model.fit(X_train.loc[train_mask], y_train_num.loc[train_mask])
            preds = model.predict(X_test.loc[test_mask])
            yt = y_test_num.loc[test_mask].to_numpy(dtype=float)

        yp = np.asarray(preds, dtype=float)
        if yt.size == 0:
            return 0.0
        ss_res = float(np.sum((yt - yp) ** 2))
        ss_tot = float(np.sum((yt - float(np.mean(yt))) ** 2))
        r2 = 1.0 - (ss_res / ss_tot) if ss_tot > 0 else 0.0
        accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))
        return float(accuracy)


def main():
    path = "dataset_adult.arff"
    df = load_dataset(path)

    # If ARFF loader returns empty, attempt generic CSV reads as last resort
    if df.empty:
        try:
            df = pd.read_csv(path)
        except Exception:
            try:
                df = pd.read_csv(path, sep=";", decimal=",")
            except Exception:
                df = pd.DataFrame()

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    assert df is not None and not df.empty, "Failed to load dataset or dataset is empty."

    accuracy = build_and_evaluate(df)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used lightweight models (LogisticRegression with liblinear; Ridge regression fallback) for CPU-friendly training.
# - Employed a single sklearn Pipeline + ColumnTransformer to avoid redundant preprocessing and ensure reproducibility.
# - Applied minimal feature engineering: median imputation + standardization for numeric, most-frequent + one-hot for categoricals.
# - Implemented robust schema handling: column normalization, unnamed column dropping, dynamic target selection, numeric coercion.
# - Added ARFF parsing without heavy dependencies to keep footprint small and energy-efficient.
# - Regression fallback uses bounded accuracy proxy: ACCURACY = clamp((R^2 + 1)/2, 0..1) for stable reporting.