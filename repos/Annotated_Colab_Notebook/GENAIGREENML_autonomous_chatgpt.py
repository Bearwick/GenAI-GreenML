# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")

RANDOM_STATE = 42


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        normed.append(c2)
    return normed


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _try_read_csv(path):
    df = None
    # Attempt 1: default
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d):
        if d is None or d.empty:
            return True
        # Heuristic: single column with many separators indicates wrong delimiter
        if d.shape[1] == 1:
            sample = ""
            try:
                sample = "\n".join([str(x) for x in d.iloc[:5, 0].tolist()])
            except Exception:
                sample = ""
            if sample.count(";") > sample.count(","):
                return True
        return False

    if _looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            pass
    return df


def _try_read_arff(path):
    try:
        from scipy.io import arff
        data, meta = arff.loadarff(path)
        df = pd.DataFrame(data)
        # Decode bytes columns
        for col in df.columns:
            if df[col].dtype == object:
                try:
                    df[col] = df[col].apply(lambda x: x.decode("utf-8") if isinstance(x, (bytes, bytearray)) else x)
                except Exception:
                    pass
        return df
    except Exception:
        return None


def _load_dataset():
    candidates = [
        "dataset_adult.arff",
        "adult.arff",
        "dataset.arff",
        "data.arff",
        "dataset.csv",
        "data.csv",
        "train.csv",
    ]
    env_path = os.environ.get("DATASET_PATH")
    if env_path:
        candidates = [env_path] + candidates

    for p in candidates:
        if not p or not os.path.exists(p):
            continue
        low = p.lower()
        if low.endswith(".arff"):
            df = _try_read_arff(p)
            if df is not None and not df.empty:
                return df
        elif low.endswith(".csv") or low.endswith(".txt"):
            df = _try_read_csv(p)
            if df is not None and not df.empty:
                return df

    # Final fallback: search current directory
    for fname in os.listdir("."):
        low = fname.lower()
        if low.endswith(".arff"):
            df = _try_read_arff(fname)
            if df is not None and not df.empty:
                return df
        if low.endswith(".csv") or low.endswith(".txt"):
            df = _try_read_csv(fname)
            if df is not None and not df.empty:
                return df

    raise FileNotFoundError("No dataset file found (tried common names and current directory).")


def _choose_target(df):
    cols = list(df.columns)
    if len(cols) == 0:
        return None

    # Prefer common target names
    preferred = ["target", "label", "y", "class", "income", "outcome"]
    lower_map = {c: str(c).strip().lower() for c in cols}
    for key in preferred:
        for c in cols:
            if lower_map[c] == key or lower_map[c].endswith(" " + key) or lower_map[c].startswith(key + " "):
                return c

    # Prefer binary/low-cardinality object columns as classification targets
    obj_cols = [c for c in cols if df[c].dtype == object]
    best = None
    best_card = None
    for c in obj_cols:
        vc = df[c].dropna()
        if vc.empty:
            continue
        n = vc.nunique(dropna=True)
        if n >= 2 and n <= 20:
            if best is None or n < best_card:
                best = c
                best_card = n
    if best is not None:
        return best

    # Else prefer numeric with variability
    num_cols = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() > 0:
            num_cols.append(c)
    for c in num_cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.nunique(dropna=True) >= 2:
            return c

    # Fall back to last column
    return cols[-1]


def _bounded_regression_score(y_true, y_pred):
    # Map R^2 to [0,1] to keep a stable "accuracy proxy" for regression.
    try:
        r2 = r2_score(y_true, y_pred)
        if not np.isfinite(r2):
            r2 = -1.0
    except Exception:
        r2 = -1.0
    score = (r2 + 1.0) / 2.0
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    df = _load_dataset()

    # Normalize/clean columns
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Defensive: ensure non-empty
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _choose_target(df)
    if target_col is None or target_col not in df.columns:
        # Fallback: use last column
        target_col = df.columns[-1]

    y_raw = df[target_col].copy()
    X = df.drop(columns=[target_col])

    # If no features left, use trivial baseline
    if X.shape[1] == 0:
        # Try classification if possible
        y_tmp = y_raw.copy()
        if y_tmp.dtype == object:
            y_tmp = y_tmp.astype(str)
        else:
            y_tmp = pd.to_numeric(y_tmp, errors="coerce")
        y_tmp = y_tmp.dropna()
        if y_tmp.nunique() >= 2:
            # Accuracy of predicting majority class
            maj = y_tmp.value_counts().idxmax()
            accuracy = float((y_tmp == maj).mean())
        else:
            accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Identify numeric/categorical feature columns robustly
    numeric_features = []
    categorical_features = []

    for c in X.columns:
        s_num = pd.to_numeric(X[c], errors="coerce")
        # Consider numeric if it has at least some numeric values and not too many non-numeric tokens
        non_na = X[c].notna().sum()
        if non_na == 0:
            # Treat empty column as categorical to avoid numeric stats issues
            categorical_features.append(c)
            continue
        num_na = s_num.notna().sum()
        if num_na / max(non_na, 1) >= 0.7:
            numeric_features.append(c)
            X[c] = s_num
        else:
            categorical_features.append(c)
            # Ensure consistent string type for OHE
            X[c] = X[c].astype(str).where(X[c].notna(), other=np.nan)

    # Decide task type
    is_classification = False
    y = y_raw.copy()
    if y.dtype == object:
        y = y.astype(str).where(y.notna(), other=np.nan)
        is_classification = True
    else:
        y_num = pd.to_numeric(y, errors="coerce")
        # If numeric with low cardinality, treat as classification
        nunq = y_num.nunique(dropna=True)
        if nunq >= 2 and nunq <= 20:
            is_classification = True
            y = y_num
        else:
            is_classification = False
            y = y_num

    # Drop rows with missing target
    valid_mask = pd.Series(y).notna()
    X = X.loc[valid_mask].reset_index(drop=True)
    y = pd.Series(y).loc[valid_mask].reset_index(drop=True)

    assert X.shape[0] > 1

    # If classification but <2 classes after cleaning, fallback to regression proxy (or trivial)
    if is_classification:
        if pd.Series(y).nunique(dropna=True) < 2:
            is_classification = False
            y = pd.to_numeric(y, errors="coerce")

    # Split
    stratify = y if is_classification and pd.Series(y).nunique() >= 2 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=stratify
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    # Preprocess
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, [c for c in numeric_features if c in X.columns]),
            ("cat", categorical_transformer, [c for c in categorical_features if c in X.columns]),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if is_classification:
        # Lightweight linear classifier; 'liblinear' is CPU-friendly for small/medium sparse data
        model = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            random_state=RANDOM_STATE,
        )
        clf = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Lightweight linear regression; works well with sparse one-hot; deterministic and CPU-friendly
        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        reg = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])
        # For regression, ensure numeric y
        y_train_num = pd.to_numeric(pd.Series(y_train), errors="coerce")
        y_test_num = pd.to_numeric(pd.Series(y_test), errors="coerce")
        # Drop NaNs introduced by coercion
        tr_mask = y_train_num.notna().to_numpy()
        te_mask = y_test_num.notna().to_numpy()
        X_train2 = X_train.loc[tr_mask].reset_index(drop=True)
        X_test2 = X_test.loc[te_mask].reset_index(drop=True)
        y_train2 = y_train_num.loc[tr_mask].reset_index(drop=True)
        y_test2 = y_test_num.loc[te_mask].reset_index(drop=True)

        # If regression data becomes too small, return trivial score
        if X_train2.shape[0] < 2 or X_test2.shape[0] < 1:
            accuracy = 0.0
        else:
            reg.fit(X_train2, y_train2)
            y_pred = reg.predict(X_test2)
            accuracy = _bounded_regression_score(y_test2, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) instead of heavy ensembles to reduce CPU/time/energy.
# - ColumnTransformer + Pipeline avoids repeated preprocessing and ensures reproducibility with fixed random_state.
# - Robust schema handling: normalizes headers, drops Unnamed columns, auto-selects target, and infers numeric vs categorical defensively.
# - Efficient categorical handling via sparse OneHotEncoder(handle_unknown="ignore") to minimize memory/compute.
# - Robust CSV parsing fallback (default, then sep=';' and decimal=',') and ARFF support for broad compatibility.
# - Regression fallback prints a bounded accuracy proxy in [0,1] using (R^2+1)/2 clipped, ensuring stable end-to-end execution.