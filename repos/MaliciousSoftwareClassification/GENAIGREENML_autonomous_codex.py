# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import glob
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score

csv_files = sorted(glob.glob("*.csv"))
if not csv_files:
    raise FileNotFoundError("No CSV file found.")

selected = None
for name in csv_files:
    lower = name.lower()
    if "train" in lower or "dataset" in lower or "data" in lower:
        selected = name
        break
if selected is None:
    selected = csv_files[0]

df = pd.read_csv(selected)

if "label" in df.columns:
    y = df["label"]
    X = df.drop(columns=["label"])
else:
    y = df.iloc[:, -1]
    X = df.iloc[:, :-1]

if "id" in X.columns:
    X = X.drop(columns=["id"])

X = X.apply(pd.to_numeric, errors="coerce")
X = X.replace([np.inf, -np.inf], np.nan)

all_nan_cols = X.columns[X.isna().all()].tolist()
if all_nan_cols:
    X = X.drop(columns=all_nan_cols)

if X.shape[1] == 0:
    raise ValueError("No valid features available.")

if y.dtype == object or str(y.dtype).startswith("category"):
    le = LabelEncoder()
    y = le.fit_transform(y.astype(str))
else:
    y = pd.to_numeric(y, errors="coerce")
    if np.any(pd.isna(y)):
        y = y.fillna(y.mode().iloc[0])
    y = y.values

unique_classes = np.unique(y)
stratify = y if unique_classes.size > 1 else None

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=stratify
)

min_value = np.nanmin(X_train.values)
if np.isnan(min_value):
    min_value = 0.0

if min_value >= 0:
    model = Pipeline(
        [("imputer", SimpleImputer(strategy="median")), ("nb", MultinomialNB())]
    )
else:
    model = Pipeline(
        [
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
            ("lr", LogisticRegression(max_iter=200, solver="liblinear")),
        ]
    )

model.fit(X_train, y_train)
pred = model.predict(X_test)
accuracy = accuracy_score(y_test, pred)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# Used lightweight Naive Bayes for non-negative count features and Logistic Regression as a fallback.
# Simple median imputation avoids complex preprocessing while ensuring robustness to missing values.
# The pipeline is CPU-friendly with minimal memory overhead and avoids heavy models.