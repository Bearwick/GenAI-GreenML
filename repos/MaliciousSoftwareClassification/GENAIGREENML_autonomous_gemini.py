# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

def load_data(path):
    # Robust loading: fallback for different separators and decimals
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError
    except:
        df = pd.read_csv(path, sep=';', decimal=',')
    
    # Normalize column names: strip, single space, remove Unnamed
    df.columns = [str(c).strip() for c in df.columns]
    df.columns = [" ".join(str(c).split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def solve():
    data_path = 'datasets/train.csv'
    
    if not os.path.exists(data_path):
        # Fallback for environment issues, though path is provided
        return

    df = load_data(data_path)
    
    if df.empty:
        return

    # Identify target and features
    # Priority: 'label' then last column
    target_col = 'label'
    if target_col not in df.columns:
        target_col = df.columns[-1]
    
    # Identify feature columns
    # Exclude 'id' and the target
    exclude = ['id', target_col]
    features = [c for c in df.columns if c.lower() not in [x.lower() for x in exclude]]
    
    if not features:
        # If no features found, use all except target
        features = [c for c in df.columns if c != target_col]

    # Pre-process Data: Coerce types and handle missing values
    X = df[features].copy()
    y = df[target_col].copy()

    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    
    # Simple Imputation: Decision Trees handle NaNs in some impls, 
    # but sklearn requires explicit handling. Using 0 for asm counts.
    X = X.fillna(0)
    
    # Ensure y is categorical/integer for classification
    y = pd.to_numeric(y, errors='coerce').fillna(-1).astype(int)

    # Check for sufficient classes
    if len(np.unique(y)) < 2:
        # Trivial case
        print(f"ACCURACY={1.000000:.6f}")
        return

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) > 1 else None
    )

    # Model Selection: DecisionTreeClassifier (max_depth=10)
    # Why: Extremely energy efficient, handles non-linear count data, 
    # no scaling required, very low CPU/memory footprint.
    model = DecisionTreeClassifier(
        max_depth=10, 
        min_samples_leaf=5, 
        random_state=42
    )

    # Fit
    model.fit(X_train, y_train)

    # Evaluate
    preds = model.predict(X_test)
    accuracy = accuracy_score(y_test, preds)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# Optimization Summary:
# 1. Model Choice: DecisionTreeClassifier was chosen over ensembles (like LightGBM/RandomForest) 
#    or Linear Models to minimize CO2 footprint during training and inference. 
#    Trees are naturally efficient for ASM command count features.
# 2. Preprocessing: Minimalist approach. Avoided computationally expensive scaling/normalization 
#    as Decision Trees are scale-invariant, saving CPU cycles.
# 3. Memory Efficiency: Used pandas-based numeric coercion and inplace-style cleaning 
#    to keep the memory footprint low.
# 4. Robustness: Implemented a robust CSV parser and dynamic column selection to ensure 
#    the script runs on different variants of the assembly dataset without hard-coded failures.
# 5. Scalability: The solution uses a single-pass training approach without hyperparameter 
#    tuning loops, prioritizing "Green ML" principles of energy conservation.