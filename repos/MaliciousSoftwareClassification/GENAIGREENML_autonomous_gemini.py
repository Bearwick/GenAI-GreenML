# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

def load_data_robust(filepath):
    """Robustly loads CSV with fallback delimiters and encoding."""
    try:
        df = pd.read_csv(filepath)
    except Exception:
        try:
            df = pd.read_csv(filepath, sep=';', decimal=',')
        except Exception:
            return pd.DataFrame()
    
    # Normalize column names
    df.columns = [str(c).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def run_pipeline():
    # File paths (assuming standard local structure based on prompt context)
    train_path = 'train.csv'
    
    # Fallback to local if path logic fails
    if not os.path.exists(train_path):
        # Trivial dataframe for runtime safety if file is missing in env
        # In real scenario, this would be provided
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = load_data_robust(train_path)
    if df.empty:
        print(f"ACCURACY={0.0:.6f}")
        return

    # Identify target and features
    # Priority: 'label' then last column
    target_col = 'label' if 'label' in df.columns else df.columns[-1]
    
    # Drop ID-like columns
    id_cols = [c for c in df.columns if c.lower() in ['id', 'uuid', 'index']]
    feature_cols = [c for c in df.columns if c != target_col and c not in id_cols]

    # Defensive check: ensure data exists
    if not feature_cols:
        print(f"ACCURACY={0.0:.6f}")
        return

    # Separate X and y
    X = df[feature_cols].copy()
    y = df[target_col].copy()

    # Pre-processing: Coerce numeric and handle missing
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')

    # Drop samples where target is NaN
    mask = y.notna()
    X = X[mask]
    y = y[mask]

    # Check if we have enough data to train
    if len(X) < 10 or len(np.unique(y)) < 2:
        # Trivial accuracy if insufficient data
        print(f"ACCURACY={0.0:.6f}")
        return

    # Train/Test Split (Fixed seed for reproducibility)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) > 1 else None
    )

    # Energy-efficient Pipeline:
    # 1. Simple Imputation (Median is robust for assembly instruction counts)
    # 2. Standard Scaling (Helps some tree variants/convergence, though RF is invariant)
    # 3. Small Random Forest (20 trees, max depth 10) - Highly CPU efficient vs GB/DL
    
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    preprocessor = ColumnTransformer(
        transformers=[('num', numeric_transformer, feature_cols)]
    )

    model = RandomForestClassifier(
        n_estimators=20, 
        max_depth=10, 
        random_state=42, 
        n_jobs=-1 # Utilize CPU cores efficiently
    )

    clf = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', model)
    ])

    # Fit model
    clf.fit(X_train, y_train)

    # Evaluate
    preds = clf.predict(X_test)
    accuracy = accuracy_score(y_test, preds)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary:
# 1. Replaced high-energy LightGBM (50,000 rounds) with a lightweight Random Forest (20 estimators).
# 2. Removed 5-fold cross-validation in favor of a single 80/20 split to reduce CPU cycles by 80%.
# 3. Implemented robust CSV parsing with fallback delimiters and whitespace normalization to prevent crash.
# 4. Used sklearn Pipeline to consolidate preprocessing (imputation/scaling) into a single pass.
# 5. Enabled n_jobs=-1 in Random Forest to maximize CPU throughput and minimize wall-clock execution time.
# 6. Added defensive logic for non-numeric coercion and NaN handling to ensure end-to-end execution on dirty data.
# 7. Constrained tree depth to 10 to prevent overfitting and keep the model memory-footprint minimal.
# 8. Used standard library and core ML packages (pandas/sklearn/numpy) to minimize dependency-loading overhead.