# Generated by generate_llm_code.py
# LLM: gemini
# Mode: original_telemetry

import os
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

data_path = './datasets//'
train = pd.read_csv(os.path.join(data_path, 'train.csv'))
test = pd.read_csv(os.path.join(data_path, 'test.csv'))

ycol = 'label'
features = [x for x in train.columns if x not in [ycol, 'id']]

NFOLD = 5
num_class = 9
random_state = 2021
KF = StratifiedKFold(n_splits=NFOLD, shuffle=True, random_state=random_state)

def custom_accuracy_eval(y_hat, data, num_class=num_class):
    y_true = data.get_label()
    y_hat = y_hat.reshape(num_class, -1).T
    return 'accuracy', accuracy_score(y_true, np.argmax(y_hat, axis=1)), True

params_lgb = {
    'boosting':'gbdt',
    'objective':'multiclass',
    'num_class': num_class,
    'metric':'multi_logloss',
    'first_metric_only':True,
    'force_row_wise': True,
    'random_state':random_state,
    'learning_rate':0.05,
    'subsample':0.8,
    'subsample_freq':3,
    'colsample_bytree':0.8,
    'max_depth':6,
    'num_leaves':31,
    'n_jobs':-1,
    'verbose': -1,
}

oof_lgb = np.zeros([len(train), num_class])
predictions_lgb = np.zeros([len(test), num_class])

for fold_, (trn_idx, val_idx) in enumerate(KF.split(train[features], train[ycol])):
    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=train.iloc[trn_idx][ycol])
    val_data = lgb.Dataset(train.iloc[val_idx][features], label=train.iloc[val_idx][ycol], reference=trn_data)

    train_kwargs = dict(
        params=params_lgb,
        train_set=trn_data,
        valid_sets=[trn_data, val_data],
        valid_names=('train', 'val'),
        num_boost_round=50000,
        feval=custom_accuracy_eval,
    )

    try:
        clf_lgb = lgb.train(
            **train_kwargs,
            early_stopping_rounds=200,
            verbose_eval=False,
        )
    except TypeError:
        clf_lgb = lgb.train(
            **train_kwargs,
            callbacks=[lgb.early_stopping(200), lgb.log_evaluation(False)],
        )

    oof_lgb[val_idx] = clf_lgb.predict(train.iloc[val_idx][features], num_iteration=clf_lgb.best_iteration)
    predictions_lgb[:] += (clf_lgb.predict(test[features], num_iteration=clf_lgb.best_iteration) / NFOLD)

accuracy = accuracy_score(train[ycol], np.argmax(oof_lgb, axis=1))
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Removed all visualization libraries (matplotlib, seaborn) and plotting functions.
# 2. Removed interactive prints and console output tracking fold progress.
# 3. Stripped CSV saving operations for submissions and evaluation metrics.
# 4. Maintained the K-Fold cross-validation logic and the LightGBM training structure.
# 5. Kept the version-handling try-except block for LightGBM's training API.
# 6. Removed redundant model fitting at the end of the script intended for learning curves.
# 7. Standardized the output to only provide the required accuracy metric.