# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold


def train_cv(X, y, params, n_splits, num_class, random_state):
    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    oof = np.zeros((X.shape[0], num_class))
    num_boost_round = 50000
    early_stop = 200
    for trn_idx, val_idx in kf.split(X, y):
        X_trn = X[trn_idx]
        y_trn = y[trn_idx]
        X_val = X[val_idx]
        y_val = y[val_idx]
        train_data = lgb.Dataset(X_trn, label=y_trn)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
        try:
            model = lgb.train(
                params=params,
                train_set=train_data,
                valid_sets=[train_data, val_data],
                num_boost_round=num_boost_round,
                early_stopping_rounds=early_stop,
                verbose_eval=False,
            )
        except TypeError:
            model = lgb.train(
                params=params,
                train_set=train_data,
                valid_sets=[train_data, val_data],
                num_boost_round=num_boost_round,
                callbacks=[lgb.early_stopping(early_stop, verbose=False)],
            )
        oof[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)
    return oof


def main():
    data_path = './datasets//'
    train_path = os.path.join(data_path, 'train.csv')
    train_df = pd.read_csv(train_path, usecols=lambda c: c != 'id')
    ycol = 'label'
    y = train_df.pop(ycol).to_numpy()
    X = train_df.to_numpy()
    del train_df
    num_class = 9
    random_state = 2021
    np.random.seed(random_state)
    params_lgb = {
        'boosting': 'gbdt',
        'objective': 'multiclass',
        'num_class': num_class,
        'metric': 'multi_logloss',
        'first_metric_only': True,
        'force_row_wise': True,
        'random_state': random_state,
        'learning_rate': 0.05,
        'subsample': 0.8,
        'subsample_freq': 3,
        'colsample_bytree': 0.8,
        'max_depth': 6,
        'num_leaves': 31,
        'n_jobs': -1,
        'verbose': -1,
    }
    oof = train_cv(X, y, params_lgb, 5, num_class, random_state)
    accuracy = float(np.mean(np.argmax(oof, axis=1) == y))
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Removed plotting, extra metrics, and file outputs to cut unnecessary computation and I/O.
# - Excluded the unused ID column at load time and extracted NumPy arrays once to reduce data movement.
# - Reused sliced train/validation arrays per fold to avoid redundant indexing and kept training logs silent.
# - Computed only the required accuracy metric for the final output.