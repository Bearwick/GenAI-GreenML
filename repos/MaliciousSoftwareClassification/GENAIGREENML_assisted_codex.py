# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import random
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

SEED = 2021
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)

DATASET_HEADERS = "id,asm_commands_add,asm_commands_call,asm_commands_cdq,asm_commands_cld,asm_commands_cli,asm_commands_cmc,asm_commands_cmp,asm_commands_cwd,asm_commands_daa,asm_commands_dd,asm_commands_dec,asm_commands_dw,asm_commands_endp,asm_commands_faddp,asm_commands_fchs,asm_commands_fdiv,asm_commands_fdivr,asm_commands_fistp,asm_commands_fld,asm_commands_fstp,asm_commands_fword,asm_commands_fxch,asm_commands_imul,asm_commands_in,asm_commands_inc,asm_commands_ins,asm_commands_jb,asm_commands_je,asm_commands_jg,asm_commands_jl,asm_commands_jmp,asm_commands_jnb,asm_commands_jno,asm_commands_jo,asm_commands_jz,asm_commands_lea,asm_commands_mov,asm_commands_mul,asm_commands_not,asm_commands_or,asm_commands_out,asm_commands_outs,asm_commands_pop,asm_commands_push,asm_commands_rcl,asm_commands_rcr,asm_commands_rep,asm_commands_ret,asm_commands_rol,asm_commands_ror,asm_commands_sal,asm_commands_sar,asm_commands_sbb,asm_commands_scas,asm_commands_shl,asm_commands_shr,asm_commands_sidt,asm_commands_stc,asm_commands_std,asm_commands_sti,asm_commands_stos,asm_commands_sub,asm_commands_test,asm_commands_wait,asm_commands_xchg,asm_commands_xor,line_count_asm,size_asm,label"
EXPECTED_HEADERS = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]


def _needs_fallback(df, expected_cols):
    if df.shape[1] == 1:
        return True
    if any(";" in str(c) for c in df.columns):
        return True
    if expected_cols is not None and df.shape[1] < expected_cols - 1:
        return True
    return False


def read_csv_robust(path, expected_cols):
    df = pd.read_csv(path)
    if _needs_fallback(df, expected_cols):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


data_path = "./datasets"
train_path = os.path.join(data_path, "train.csv")
test_path = os.path.join(data_path, "test.csv")

expected_train_cols = len(EXPECTED_HEADERS)
expected_test_cols = expected_train_cols - (1 if "label" in EXPECTED_HEADERS else 0)

train = read_csv_robust(train_path, expected_train_cols)
test = read_csv_robust(test_path, expected_test_cols)

ycol = "label" if "label" in train.columns else train.columns[-1]
id_col = "id" if "id" in train.columns else train.columns[0]
if id_col not in test.columns:
    id_col = test.columns[0]

features = [c for c in train.columns if c not in {ycol, id_col}]

X = train[features].to_numpy()
y = train[ycol].to_numpy()
X_test = test.reindex(columns=features, fill_value=0).to_numpy()
test_ids = test[id_col].to_numpy()

del train, test

num_class = int(np.unique(y).size)
NFOLD = 5

params_lgb = {
    "boosting": "gbdt",
    "objective": "multiclass",
    "num_class": num_class,
    "metric": "multi_logloss",
    "first_metric_only": True,
    "force_row_wise": True,
    "random_state": SEED,
    "learning_rate": 0.05,
    "subsample": 0.8,
    "subsample_freq": 3,
    "colsample_bytree": 0.8,
    "max_depth": 6,
    "num_leaves": 31,
    "n_jobs": -1,
    "verbose": -1,
}

kf = StratifiedKFold(n_splits=NFOLD, shuffle=True, random_state=SEED)

oof = np.zeros((X.shape[0], num_class))
preds = np.zeros((X_test.shape[0], num_class))

for trn_idx, val_idx in kf.split(X, y):
    trn_data = lgb.Dataset(X[trn_idx], label=y[trn_idx])
    val_data = lgb.Dataset(X[val_idx], label=y[val_idx], reference=trn_data)
    train_kwargs = dict(
        params=params_lgb,
        train_set=trn_data,
        valid_sets=[val_data],
        num_boost_round=50000,
    )
    try:
        model = lgb.train(**train_kwargs, early_stopping_rounds=200, verbose_eval=False)
    except TypeError:
        model = lgb.train(**train_kwargs, callbacks=[lgb.early_stopping(200, verbose=False)])
    best_iter = model.best_iteration
    if best_iter is None or best_iter <= 0:
        best_iter = model.current_iteration()
    oof[val_idx] = model.predict(X[val_idx], num_iteration=best_iter)
    preds += model.predict(X_test, num_iteration=best_iter)

preds /= NFOLD

oof_pred = np.argmax(oof, axis=1)
accuracy = accuracy_score(y, oof_pred)
precision = precision_score(y, oof_pred, average="macro")
recall = recall_score(y, oof_pred, average="macro")
f1 = f1_score(y, oof_pred, average="macro")
auc = roc_auc_score(y, oof, average="macro", multi_class="ovo")

learning_rate = params_lgb["learning_rate"]
valid_accuracy_score = accuracy

evaluation = pd.DataFrame(
    {
        "Model": ["LightGBM"],
        "学习率": [learning_rate],
        "准确率": [accuracy],
        "精确率": [precision],
        "召回率": [recall],
        "F1 值": [f1],
        "AUC值": [auc],
        "5折交叉验证的score": [valid_accuracy_score],
    }
)
evaluation.to_csv("evaluation.csv", index=False)

submit_df = pd.DataFrame({id_col: test_ids, "label": np.argmax(preds, axis=1)})
submit_df.to_csv("./submit.csv", index=False)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Removed visualization and extra model training to cut runtime and energy use.
# Pre-extracted numpy arrays to avoid repeated DataFrame slicing during CV.
# Reduced validation overhead by evaluating only on the validation split without logging.
# Avoided redundant metric computations by reusing predictions for all scores.
# Released large DataFrames after extraction to lower memory footprint.