# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import sys
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _find_dataset_path() -> str:
    candidates = [
        "data.csv",
        "dataset.csv",
        "train.csv",
        "Train.csv",
        "input.csv",
        "Input.csv",
    ]
    for p in candidates:
        if os.path.isfile(p):
            return p
    csvs = [f for f in os.listdir(".") if f.lower().endswith(".csv")]
    if len(csvs) == 1:
        return csvs[0]
    if "DATASET_PATH" in os.environ and os.path.isfile(os.environ["DATASET_PATH"]):
        return os.environ["DATASET_PATH"]
    raise FileNotFoundError("No dataset CSV found. Place a single .csv in the working directory or set DATASET_PATH.")


def main() -> None:
    path = _find_dataset_path()

    df = pd.read_csv(path)

    if "label" not in df.columns:
        raise ValueError("Expected 'label' column in dataset.")

    y = df["label"]
    drop_cols = [c for c in ["label", "id"] if c in df.columns]
    X = df.drop(columns=drop_cols, errors="ignore")

    # Coerce to numeric with minimal overhead; non-numeric becomes NaN then imputed by median.
    X = X.apply(pd.to_numeric, errors="coerce")
    med = X.median(numeric_only=True)
    X = X.fillna(med)

    # Ensure contiguous float32 arrays to reduce memory bandwidth/CPU cache pressure.
    X = np.ascontiguousarray(X.to_numpy(dtype=np.float32))
    y = y.to_numpy()

    stratify = y if (pd.Series(y).nunique() > 1 and pd.Series(y).value_counts().min() >= 2) else None
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=stratify,
    )

    # Lightweight linear model; robust scaler handles count-like skewed features well.
    # Using 'saga' scales efficiently for medium feature counts and supports multinomial.
    clf = Pipeline(
        steps=[
            ("scaler", RobustScaler(with_centering=True, with_scaling=True, unit_variance=False)),
            (
                "model",
                LogisticRegression(
                    solver="saga",
                    penalty="l2",
                    C=1.0,
                    max_iter=500,
                    tol=1e-3,
                    n_jobs=1,
                    random_state=42,
                ),
            ),
        ]
    )

    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Chosen model: LogisticRegression (linear) for strong baseline accuracy with minimal compute and memory.
# - CPU efficiency: float32 contiguous feature matrix reduces RAM and improves cache locality.
# - RobustScaler: handles heavy-tailed/count features cheaply and improves linear separability without complex transforms.
# - No deep learning/embeddings: unnecessary for tabular instruction-count features; avoids high energy use.
# - Simple median imputation: deterministic, fast, and avoids iterative/expensive imputers.
# - Single-threaded training (n_jobs=1): predictable CPU usage; reduces energy spikes on shared/limited environments.