# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier


DATASET_PATH = "datasets/train.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        normed.append(c2)
    return normed


def _read_csv_robust(path):
    # Try default parsing first
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    # Fallback: semicolon-separated with comma decimal
    if df is None:
        df = pd.read_csv(path, sep=";", decimal=",")
        return df

    # Heuristic: if single column and looks like a separator issue, retry fallback
    if df.shape[1] == 1:
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df2.shape[1] > df.shape[1]:
                df = df2
        except Exception:
            pass

    return df


def _drop_unnamed(df):
    cols = []
    for c in df.columns:
        if isinstance(c, str) and c.strip().lower().startswith("unnamed:"):
            continue
        cols.append(c)
    return df[cols]


def _safe_numeric_coerce(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _pick_target(df):
    # Prefer 'label' if present
    for cand in ["label", "Label", "target", "Target", "y", "Y"]:
        if cand in df.columns:
            return cand

    # Otherwise: choose a numeric column with >1 unique non-null values, and not an obvious ID
    numeric_candidates = []
    for c in df.columns:
        if c.lower() in {"id", "index"}:
            continue
        if pd.api.types.is_numeric_dtype(df[c]):
            nun = df[c].dropna().nunique()
            if nun > 1:
                numeric_candidates.append((nun, c))
    if numeric_candidates:
        numeric_candidates.sort(reverse=True)
        return numeric_candidates[0][1]

    # Otherwise: choose any column with >1 unique values
    for c in df.columns:
        if c.lower() in {"id", "index"}:
            continue
        if df[c].dropna().nunique() > 1:
            return c

    # Last resort: first column
    return df.columns[0]


def _is_classification_target(y):
    # Classification if few unique values or non-numeric dtype
    y_non_null = y.dropna()
    if y_non_null.empty:
        return False
    if not pd.api.types.is_numeric_dtype(y_non_null):
        return True
    nun = y_non_null.nunique()
    # If integer-like with small cardinality, treat as classification
    if nun <= 50:
        vals = y_non_null.values
        if np.all(np.isfinite(pd.to_numeric(y_non_null, errors="coerce").values)):
            # integer-like check
            if np.all(np.abs(vals - np.round(vals)) < 1e-9):
                return True
    return False


def _bounded_regression_score(y_true, y_pred):
    # Convert regression quality into [0,1] accuracy proxy: 1/(1+normalized MAE)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = np.mean(np.abs(y_true - y_pred))
    scale = np.std(y_true)
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = np.mean(np.abs(y_true)) + 1e-9
    nmae = mae / (scale + 1e-12)
    score = 1.0 / (1.0 + nmae)
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Basic sanity
    assert df is not None and df.shape[0] > 0 and df.shape[1] > 0

    # Determine target
    target_col = _pick_target(df)

    # Split X/y
    y = df[target_col].copy()
    X = df.drop(columns=[target_col])

    # Drop obviously non-informative ID-like columns if present
    drop_candidates = [c for c in X.columns if c.strip().lower() in {"id", "index"}]
    if drop_candidates:
        X = X.drop(columns=drop_candidates)

    # Ensure not empty
    if X.shape[1] == 0:
        # If no features left, fallback to trivial classifier/regressor using dummy constant features
        X = pd.DataFrame({"__const__": np.ones(len(df), dtype=np.float32)})

    # Coerce potential numeric object columns safely: attempt on all columns, but keep categoricals as object
    # We'll decide types after coercion attempt on a copy.
    X_work = X.copy()
    for c in X_work.columns:
        if X_work[c].dtype == object:
            coerced = pd.to_numeric(X_work[c], errors="coerce")
            # If most values become numeric, keep numeric; else keep original object
            if coerced.notna().mean() >= 0.9:
                X_work[c] = coerced

    X = X_work

    # Prepare column lists
    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    # Coerce numeric columns and sanitize inf
    if numeric_features:
        X = _safe_numeric_coerce(X, numeric_features)
        for c in numeric_features:
            vals = X[c].to_numpy()
            if np.isfinite(vals).sum() == 0:
                X[c] = np.nan
            else:
                X.loc[~np.isfinite(vals), c] = np.nan

    # Decide task
    is_clf = _is_classification_target(y)

    # Train/test split (stratify if classification and possible)
    stratify = None
    if is_clf:
        y_non_null = y.dropna()
        if y_non_null.nunique() >= 2:
            stratify = y
        else:
            is_clf = False  # fallback to regression/trivial if only one class

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
    )

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    # Preprocess
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if is_clf:
        # Lightweight linear baseline for multiclass; use saga for sparse + L2; cap iterations for CPU efficiency
        model = LogisticRegression(
            max_iter=300,
            solver="saga",
            penalty="l2",
            C=1.0,
            n_jobs=1,
            random_state=RANDOM_STATE,
            multi_class="auto",
        )
        pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

        # If y is numeric but integer-coded, keep as-is; if object, fine.
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        # Regression fallback
        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

        # Coerce y to numeric; if that fails badly, use dummy classifier on binned y
        y_train_num = pd.to_numeric(y_train, errors="coerce")
        y_test_num = pd.to_numeric(y_test, errors="coerce")

        if y_train_num.notna().sum() < max(2, int(0.5 * len(y_train_num))):
            # Trivial baseline classification on original y (most frequent) to still output accuracy
            dummy = DummyClassifier(strategy="most_frequent")
            # Make sure preprocessing can run with X
            dummy_pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", dummy)])
            dummy_pipe.fit(X_train, y_train.astype(str))
            y_pred = dummy_pipe.predict(X_test)
            accuracy = accuracy_score(y_test.astype(str), y_pred)
        else:
            # Drop rows where y is NaN
            tr_mask = y_train_num.notna().to_numpy()
            te_mask = y_test_num.notna().to_numpy()

            X_train2 = X_train.loc[tr_mask]
            y_train2 = y_train_num.loc[tr_mask]
            X_test2 = X_test.loc[te_mask]
            y_test2 = y_test_num.loc[te_mask]

            assert X_train2.shape[0] > 0 and X_test2.shape[0] > 0

            pipe.fit(X_train2, y_train2)
            y_pred = pipe.predict(X_test2)
            accuracy = _bounded_regression_score(y_test2.values, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses robust CSV loading with a lightweight delimiter/decimal fallback to avoid expensive manual debugging loops.
# - Normalizes column names and drops 'Unnamed:' columns to reduce noise and memory use.
# - Minimal preprocessing via ColumnTransformer: median imputation + StandardScaler for numeric, most_frequent + OneHot for categoricals.
# - Chooses CPU-friendly linear models: LogisticRegression(saga) for sparse multiclass; Ridge for regression fallback.
# - Caps iterations and disables parallelism (n_jobs=1) to reduce peak CPU usage and improve reproducibility.
# - Defensive target selection avoids hard failures on schema mismatch; falls back to regression or trivial baseline if needed.
# - Regression fallback reports a bounded [0,1] "accuracy proxy" = 1/(1+normalized MAE) for stability across scales.