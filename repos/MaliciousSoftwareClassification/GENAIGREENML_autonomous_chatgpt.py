# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge


warnings.filterwarnings("ignore")


DATASET_PATH = "datasets/train.csv"
DATASET_HEADERS = [
    "id","asm_commands_add","asm_commands_call","asm_commands_cdq","asm_commands_cld","asm_commands_cli",
    "asm_commands_cmc","asm_commands_cmp","asm_commands_cwd","asm_commands_daa","asm_commands_dd",
    "asm_commands_dec","asm_commands_dw","asm_commands_endp","asm_commands_faddp","asm_commands_fchs",
    "asm_commands_fdiv","asm_commands_fdivr","asm_commands_fistp","asm_commands_fld","asm_commands_fstp",
    "asm_commands_fword","asm_commands_fxch","asm_commands_imul","asm_commands_in","asm_commands_inc",
    "asm_commands_ins","asm_commands_jb","asm_commands_je","asm_commands_jg","asm_commands_jl",
    "asm_commands_jmp","asm_commands_jnb","asm_commands_jno","asm_commands_jo","asm_commands_jz",
    "asm_commands_lea","asm_commands_mov","asm_commands_mul","asm_commands_not","asm_commands_or",
    "asm_commands_out","asm_commands_outs","asm_commands_pop","asm_commands_push","asm_commands_rcl",
    "asm_commands_rcr","asm_commands_rep","asm_commands_ret","asm_commands_rol","asm_commands_ror",
    "asm_commands_sal","asm_commands_sar","asm_commands_sbb","asm_commands_scas","asm_commands_shl",
    "asm_commands_shr","asm_commands_sidt","asm_commands_stc","asm_commands_std","asm_commands_sti",
    "asm_commands_stos","asm_commands_sub","asm_commands_test","asm_commands_wait","asm_commands_xchg",
    "asm_commands_xor","line_count_asm","size_asm","label"
]


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if isinstance(c, str) and c.strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _read_csv_robust(path):
    df1 = None
    try:
        df1 = pd.read_csv(path)
    except Exception:
        df1 = None

    def _looks_wrong(df):
        if df is None or df.shape[0] == 0:
            return True
        if df.shape[1] <= 1:
            return True
        first_col = str(df.columns[0])
        if "," in first_col and df.shape[1] == 1:
            return True
        return False

    if not _looks_wrong(df1):
        return df1

    df2 = None
    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
    except Exception:
        df2 = None

    if df2 is not None and df2.shape[1] > 1 and df2.shape[0] > 0:
        return df2

    if df1 is not None:
        return df1
    raise RuntimeError("Could not read dataset.")


def _choose_target_column(df, preferred="label"):
    cols_lower = {str(c).strip().lower(): c for c in df.columns}
    if preferred and preferred.lower() in cols_lower:
        return cols_lower[preferred.lower()]

    for candidate in ["target", "y", "class", "label"]:
        if candidate in cols_lower:
            return cols_lower[candidate]

    # Fallback: choose a non-constant numeric column (prefer last columns)
    numeric_candidates = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        nun = s.nunique(dropna=True)
        if nun is not None and nun >= 2:
            numeric_candidates.append((nun, c))
    if numeric_candidates:
        numeric_candidates.sort(key=lambda x: (x[0], str(x[1])))
        return numeric_candidates[-1][1]

    # Last resort: any non-constant column
    for c in reversed(list(df.columns)):
        try:
            if df[c].nunique(dropna=True) >= 2:
                return c
        except Exception:
            continue

    return df.columns[-1]


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    ss_res = np.sum((y_true - y_pred) ** 2)
    y_mean = np.mean(y_true) if y_true.size else 0.0
    ss_tot = np.sum((y_true - y_mean) ** 2)
    if not np.isfinite(ss_res) or not np.isfinite(ss_tot):
        return 0.0
    if ss_tot <= 0.0:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    r2 = float(np.clip(r2, -1.0, 1.0))
    return (r2 + 1.0) / 2.0


df = _read_csv_robust(DATASET_PATH)
df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)

# Coerce obviously numeric-looking columns (safe; leaves non-convertible as NaN)
for c in df.columns:
    if df[c].dtype == object:
        coerced = pd.to_numeric(df[c], errors="coerce")
        non_na_ratio = float(coerced.notna().mean()) if len(coerced) else 0.0
        if non_na_ratio >= 0.80:
            df[c] = coerced

assert df.shape[0] > 0 and df.shape[1] > 0

target_col = _choose_target_column(df, preferred="label")

# Build features from available subset; prefer DATASET_HEADERS excluding id/label if present, else all except target
preferred_features = [c for c in DATASET_HEADERS if c in df.columns and c not in ("label", "id")]
if preferred_features:
    feature_cols = [c for c in preferred_features if c != target_col]
else:
    feature_cols = [c for c in df.columns if c != target_col]

if not feature_cols:
    # last-resort: use any column other than target if possible
    feature_cols = [c for c in df.columns if c != target_col]

X = df[feature_cols].copy()
y = df[target_col].copy()

# Drop rows where target missing
mask = ~pd.isna(y)
X = X.loc[mask]
y = y.loc[mask]

assert X.shape[0] > 0

# Identify column types
numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
categorical_features = [c for c in X.columns if c not in numeric_features]

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=True, with_std=True)),
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Decide classification vs regression robustly
y_is_numeric = pd.api.types.is_numeric_dtype(y)
n_unique = int(pd.Series(y).nunique(dropna=True))

is_classification = False
if n_unique >= 2:
    if (not y_is_numeric) or (n_unique <= 50):
        is_classification = True

if is_classification:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if n_unique >= 2 else None
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    # Lightweight, CPU-friendly baseline; saga handles sparse OHE well
    clf = LogisticRegression(
        max_iter=200,
        solver="saga",
        n_jobs=1,
        multi_class="auto",
        C=1.0,
        tol=1e-3,
    )

    model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Regression fallback with bounded proxy score in [0,1]
    y_num = pd.to_numeric(y, errors="coerce")
    mask2 = ~pd.isna(y_num)
    X = X.loc[mask2]
    y_num = y_num.loc[mask2]
    assert X.shape[0] > 0

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_num, test_size=0.2, random_state=42
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    reg = Ridge(alpha=1.0, random_state=42)
    model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = float(_bounded_regression_score(y_test.values, y_pred))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) for strong baselines with minimal CPU/energy cost vs ensembles/deep nets.
# - ColumnTransformer+Pipeline ensures reproducible preprocessing and avoids redundant transformations.
# - Robust CSV parsing fallback (comma/semicolon + decimal) and column normalization handle schema inconsistencies efficiently.
# - Sparse OneHotEncoder for categoricals keeps memory/compute low; StandardScaler helps linear convergence without feature engineering.
# - Defensive target/feature selection prevents failures on missing/changed headers; regression fallback uses bounded R2 proxy mapped to [0,1].