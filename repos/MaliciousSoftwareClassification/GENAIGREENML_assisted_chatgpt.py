# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
from typing import Tuple

import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score


SEED = 2021
NFOLD = 5
NUM_CLASS = 9


def set_reproducibility(seed: int) -> None:
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)


def read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if _looks_misparsed(df):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _looks_misparsed(df: pd.DataFrame) -> bool:
    if df.shape[1] <= 2:
        return True
    if df.columns.size == 1:
        return True
    return any((";" in str(c)) for c in df.columns)


def infer_columns(df: pd.DataFrame, dataset_headers: str) -> Tuple[str, str]:
    expected = [h.strip() for h in dataset_headers.split(",") if h.strip()]
    label_col = "label" if "label" in df.columns else (expected[-1] if expected and expected[-1] in df.columns else "")
    id_col = "id" if "id" in df.columns else ("id" if "id" in expected else "")
    if not label_col:
        raise ValueError("Label column not found.")
    return label_col, id_col


def custom_accuracy_eval(y_hat: np.ndarray, data: lgb.Dataset, num_class: int = NUM_CLASS):
    y_true = data.get_label()
    y_hat = y_hat.reshape(num_class, -1).T
    return "accuracy", accuracy_score(y_true, np.argmax(y_hat, axis=1)), True


def main() -> None:
    set_reproducibility(SEED)

    dataset_headers = (
        "id,asm_commands_add,asm_commands_call,asm_commands_cdq,asm_commands_cld,asm_commands_cli,asm_commands_cmc,"
        "asm_commands_cmp,asm_commands_cwd,asm_commands_daa,asm_commands_dd,asm_commands_dec,asm_commands_dw,"
        "asm_commands_endp,asm_commands_faddp,asm_commands_fchs,asm_commands_fdiv,asm_commands_fdivr,asm_commands_fistp,"
        "asm_commands_fld,asm_commands_fstp,asm_commands_fword,asm_commands_fxch,asm_commands_imul,asm_commands_in,"
        "asm_commands_inc,asm_commands_ins,asm_commands_jb,asm_commands_je,asm_commands_jg,asm_commands_jl,asm_commands_jmp,"
        "asm_commands_jnb,asm_commands_jno,asm_commands_jo,asm_commands_jz,asm_commands_lea,asm_commands_mov,asm_commands_mul,"
        "asm_commands_not,asm_commands_or,asm_commands_out,asm_commands_outs,asm_commands_pop,asm_commands_push,asm_commands_rcl,"
        "asm_commands_rcr,asm_commands_rep,asm_commands_ret,asm_commands_rol,asm_commands_ror,asm_commands_sal,asm_commands_sar,"
        "asm_commands_sbb,asm_commands_scas,asm_commands_shl,asm_commands_shr,asm_commands_sidt,asm_commands_stc,asm_commands_std,"
        "asm_commands_sti,asm_commands_stos,asm_commands_sub,asm_commands_test,asm_commands_wait,asm_commands_xchg,asm_commands_xor,"
        "line_count_asm,size_asm,label"
    )

    data_dir = "./datasets"
    train_path = os.path.join(data_dir, "train.csv")
    test_path = os.path.join(data_dir, "test.csv")

    train = read_csv_robust(train_path)
    test = read_csv_robust(test_path)

    ycol, id_col = infer_columns(train, dataset_headers)

    features = [c for c in train.columns if c != ycol and (not id_col or c != id_col)]
    X = train[features]
    y = train[ycol].to_numpy()
    X_test = test[features]

    kf = StratifiedKFold(n_splits=NFOLD, shuffle=True, random_state=SEED)

    params_lgb = {
        "boosting": "gbdt",
        "objective": "multiclass",
        "num_class": NUM_CLASS,
        "metric": "multi_logloss",
        "first_metric_only": True,
        "force_row_wise": True,
        "random_state": SEED,
        "learning_rate": 0.05,
        "subsample": 0.8,
        "subsample_freq": 3,
        "colsample_bytree": 0.8,
        "max_depth": 6,
        "num_leaves": 31,
        "n_jobs": -1,
        "verbose": -1,
        "seed": SEED,
        "feature_fraction_seed": SEED,
        "bagging_seed": SEED,
        "data_random_seed": SEED,
        "deterministic": True,
    }

    oof = np.zeros((len(train), NUM_CLASS), dtype=np.float32)
    test_pred = np.zeros((len(test), NUM_CLASS), dtype=np.float32)

    early_stop_cb = lgb.early_stopping(stopping_rounds=200, verbose=False)

    for trn_idx, val_idx in kf.split(X, y):
        X_tr, X_val = X.iloc[trn_idx], X.iloc[val_idx]
        y_tr, y_val = y[trn_idx], y[val_idx]

        trn_data = lgb.Dataset(X_tr, label=y_tr, free_raw_data=True)
        val_data = lgb.Dataset(X_val, label=y_val, reference=trn_data, free_raw_data=True)

        clf = lgb.train(
            params=params_lgb,
            train_set=trn_data,
            valid_sets=[val_data],
            valid_names=["val"],
            num_boost_round=50000,
            feval=custom_accuracy_eval,
            callbacks=[early_stop_cb],
        )

        best_iter = clf.best_iteration or 50000
        oof[val_idx] = clf.predict(X_val, num_iteration=best_iter).astype(np.float32, copy=False)
        test_pred += (clf.predict(X_test, num_iteration=best_iter).astype(np.float32, copy=False) / NFOLD)

    accuracy = accuracy_score(y, np.argmax(oof, axis=1))
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused imports, metrics, visualization, and file-saving to reduce runtime and I/O energy cost.
# - Implemented robust CSV parsing with a fallback delimiter/decimal configuration to avoid repeated manual fixes.
# - Minimized redundant DataFrame slicing by caching X/y/X_test once and reusing them across folds.
# - Reduced memory footprint by storing OOF and test predictions as float32 (predictions do not require float64 precision).
# - Avoided per-iteration logging by using LightGBM callbacks with early stopping set to verbose=False.
# - Enabled deterministic behavior via fixed seeds and LightGBM deterministic settings for reproducible results.
# - Used early stopping callback object reused across folds to reduce repeated object creation overhead.