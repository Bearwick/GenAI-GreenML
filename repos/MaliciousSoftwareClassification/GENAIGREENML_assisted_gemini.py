# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import os
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

ycol = 'label'
features = [x for x in train.columns if x not in [ycol, 'id']]

X = train[features].astype(np.float32).values
y = train[ycol].values
X_test = test[features].astype(np.float32).values

NFOLD = 5
num_class = 9
random_state = 2021
KF = StratifiedKFold(n_splits=NFOLD, shuffle=True, random_state=random_state)

def custom_accuracy_eval(y_hat, data):
    y_true = data.get_label()
    y_hat = y_hat.reshape(num_class, -1).T
    return 'accuracy', accuracy_score(y_true, np.argmax(y_hat, axis=1)), True

params_lgb = {
    'boosting': 'gbdt',
    'objective': 'multiclass',
    'num_class': num_class,
    'metric': 'multi_logloss',
    'first_metric_only': True,
    'force_row_wise': True,
    'random_state': random_state,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'subsample_freq': 3,
    'colsample_bytree': 0.8,
    'max_depth': 6,
    'num_leaves': 31,
    'n_jobs': -1,
    'verbose': -1,
}

oof_lgb = np.zeros([len(train), num_class])
predictions_lgb = np.zeros([len(test), num_class])

for trn_idx, val_idx in KF.split(X, y):
    trn_data = lgb.Dataset(X[trn_idx], label=y[trn_idx])
    val_data = lgb.Dataset(X[val_idx], label=y[val_idx], reference=trn_data)

    clf_lgb = lgb.train(
        params=params_lgb,
        train_set=trn_data,
        valid_sets=[val_data],
        num_boost_round=50000,
        feval=custom_accuracy_eval,
        callbacks=[
            lgb.early_stopping(200),
            lgb.log_evaluation(0)
        ]
    )

    oof_lgb[val_idx] = clf_lgb.predict(X[val_idx], num_iteration=clf_lgb.best_iteration)
    predictions_lgb += clf_lgb.predict(X_test, num_iteration=clf_lgb.best_iteration) / NFOLD

accuracy = accuracy_score(y, np.argmax(oof_lgb, axis=1))
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Reduced memory footprint by casting feature DataFrames to float32 NumPy arrays, minimizing overhead during training.
# 2. Optimized data movement by extracting values once before the cross-validation loop rather than slicing the DataFrame repeatedly.
# 3. Eliminated redundant computations by removing feature importance aggregation and multiple metric calculations (Precision, Recall, F1, AUC) not required for the primary output.
# 4. Streamlined the training process by removing verbose logging and using efficient callbacks for early stopping.
# 5. Removed energy-intensive I/O operations, including saving trained models, generating large CSV files, and creating visualizations.
# 6. Improved execution speed by using direct NumPy indexing which is faster than Pandas .iloc within the high-frequency training loop.