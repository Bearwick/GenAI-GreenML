# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import os
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

def load_robust_csv(path):
    try:
        df = pd.read_csv(path, engine='c')
        if df.shape[1] <= 1:
            raise ValueError
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',', engine='c')
    return df

random_state = 2021
np.random.seed(random_state)
data_path = './datasets/'
train = load_robust_csv(os.path.join(data_path, 'train.csv'))
test = load_robust_csv(os.path.join(data_path, 'test.csv'))

ycol = 'label' if 'label' in train.columns else train.columns[-1]
features = [x for x in train.columns if x not in [ycol, 'id']]

for col in features:
    if train[col].dtype == 'float64':
        train[col] = train[col].astype('float32')
        test[col] = test[col].astype('float32')

NFOLD = 5
num_class = 9
KF = StratifiedKFold(n_splits=NFOLD, shuffle=True, random_state=random_state)

def custom_accuracy_eval(y_hat, data):
    y_true = data.get_label()
    y_hat = y_hat.reshape(num_class, -1).T
    return 'accuracy', accuracy_score(y_true, np.argmax(y_hat, axis=1)), True

params_lgb = {
    'boosting': 'gbdt',
    'objective': 'multiclass',
    'num_class': num_class,
    'metric': 'multi_logloss',
    'first_metric_only': True,
    'force_row_wise': True,
    'random_state': random_state,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'subsample_freq': 3,
    'colsample_bytree': 0.8,
    'max_depth': 6,
    'num_leaves': 31,
    'n_jobs': -1,
    'verbose': -1,
}

oof_lgb = np.zeros([len(train), num_class], dtype='float32')
predictions_lgb = np.zeros([len(test), num_class], dtype='float32')

X = train[features]
y = train[ycol]
X_test = test[features]

for trn_idx, val_idx in KF.split(X, y):
    X_trn, y_trn = X.iloc[trn_idx], y.iloc[trn_idx]
    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]
    
    trn_data = lgb.Dataset(X_trn, label=y_trn)
    val_data = lgb.Dataset(X_val, label=y_val, reference=trn_data)

    train_kwargs = dict(
        params=params_lgb,
        train_set=trn_data,
        valid_sets=[trn_data, val_data],
        valid_names=('train', 'val'),
        num_boost_round=50000,
        feval=custom_accuracy_eval,
    )

    try:
        clf_lgb = lgb.train(
            **train_kwargs,
            early_stopping_rounds=200,
            verbose_eval=False,
        )
    except TypeError:
        clf_lgb = lgb.train(
            **train_kwargs,
            callbacks=[lgb.early_stopping(200), lgb.log_evaluation(False)],
        )

    best_iter = clf_lgb.best_iteration
    oof_lgb[val_idx] = clf_lgb.predict(X_val, num_iteration=best_iter)
    predictions_lgb += (clf_lgb.predict(X_test, num_iteration=best_iter) / NFOLD)

oof_labels = np.argmax(oof_lgb, axis=1)
accuracy = accuracy_score(y, oof_labels)
precision = precision_score(y, oof_labels, average='macro')
recall = recall_score(y, oof_labels, average='macro')
f1 = f1_score(y, oof_labels, average='macro')
auc = roc_auc_score(y, oof_lgb, average='macro', multi_class='ovo')

test['label'] = np.argmax(predictions_lgb, axis=1)
test[['id', 'label']].to_csv('./submit.csv', index=False)

evaluation = pd.DataFrame({
    'Model': ['LightGBM'],
    '学习率': [params_lgb['learning_rate']],
    '准确率': [accuracy],
    '精确率': [precision],
    '召回率': [recall],
    'F1 值': [f1],
    'AUC值': [auc],
    '5折交叉验证的score': [accuracy]
})
evaluation.to_csv('evaluation.csv', index=False)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Memory Footprint Reduction: Converted float64 features to float32 to reduce memory usage and speed up computation.
# 2. Eliminated Redundant Computation: Removed the secondary model fit previously used for the learning curve visualization.
# 3. Removed Visualization Overhead: All plotting (Matplotlib/Seaborn) logic and IO-heavy image saving were removed to save energy and runtime.
# 4. Streamlined Data Movement: Utilized optimized pandas 'c' engine for CSV reading and minimized dataframe copying by indexing once per fold.
# 5. Efficient Resource Usage: Disabled verbose logging (verbose_eval=False) during training to reduce IO overhead and console interrupts.
# 6. Improved Reliability: Implemented robust CSV parsing with fallback delimiters to ensure script stability.
# 7. Optimized Metric Logic: Reused OOF predictions for all evaluation metrics (Accuracy, Precision, F1, AUC) instead of recalculating during loops.
# 8. Preservation of Accuracy: Maintained high-performance LightGBM configuration while trimming non-essential auxiliary tasks.