# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import os
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

def load_robust_csv(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError
    except:
        df = pd.read_csv(path, sep=';', decimal=',')
    return df

def downcast_df(df):
    fcols = df.select_dtypes('float').columns
    icols = df.select_dtypes('integer').columns
    df[fcols] = df[fcols].apply(pd.to_numeric, downcast='float')
    df[icols] = df[icols].apply(pd.to_numeric, downcast='integer')
    return df

train_path = 'datasets/train.csv'
test_path = 'datasets/test.csv'

train = load_robust_csv(train_path)
test = load_robust_csv(test_path)

train = downcast_df(train)
test = downcast_df(test)

ycol = 'label'
id_col = 'id'
features = [x for x in train.columns if x not in [ycol, id_col]]

NFOLD = 5
num_class = 9
random_state = 2021
KF = StratifiedKFold(n_splits=NFOLD, shuffle=True, random_state=random_state)

def custom_accuracy_eval(y_hat, data):
    y_true = data.get_label()
    y_hat = y_hat.reshape(num_class, -1).T
    return 'accuracy', accuracy_score(y_true, np.argmax(y_hat, axis=1)), True

params_lgb = {
    'boosting': 'gbdt',
    'objective': 'multiclass',
    'num_class': num_class,
    'metric': 'multi_logloss',
    'first_metric_only': True,
    'force_row_wise': True,
    'random_state': random_state,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'subsample_freq': 3,
    'colsample_bytree': 0.8,
    'max_depth': 6,
    'num_leaves': 31,
    'n_jobs': -1,
    'verbose': -1,
}

oof_lgb = np.zeros([len(train), num_class])
predictions_lgb = np.zeros([len(test), num_class])
importance_split = np.zeros(len(features))
importance_gain = np.zeros(len(features))

X = train[features].values
y = train[ycol].values
X_test = test[features].values

for trn_idx, val_idx in KF.split(X, y):
    trn_data = lgb.Dataset(X[trn_idx], label=y[trn_idx])
    val_data = lgb.Dataset(X[val_idx], label=y[val_idx], reference=trn_data)

    callbacks = [
        lgb.early_stopping(200),
        lgb.log_evaluation(0)
    ]

    clf_lgb = lgb.train(
        params=params_lgb,
        train_set=trn_data,
        valid_sets=[val_data],
        num_boost_round=50000,
        feval=custom_accuracy_eval,
        callbacks=callbacks
    )

    best_iter = clf_lgb.best_iteration
    oof_lgb[val_idx] = clf_lgb.predict(X[val_idx], num_iteration=best_iter)
    predictions_lgb += clf_lgb.predict(X_test, num_iteration=best_iter) / NFOLD
    
    importance_split += clf_lgb.feature_importance(importance_type='split') / NFOLD
    importance_gain += clf_lgb.feature_importance(importance_type='gain') / NFOLD

oof_labels = np.argmax(oof_lgb, axis=1)
accuracy = accuracy_score(y, oof_labels)
precision = precision_score(y, oof_labels, average='macro')
recall = recall_score(y, oof_labels, average='macro')
f1 = f1_score(y, oof_labels, average='macro')
auc = roc_auc_score(y, oof_lgb, average='macro', multi_class='ovo')

test['label'] = np.argmax(predictions_lgb, axis=1)
test[[id_col, 'label']].to_csv('submit.csv', index=False)

evaluation = pd.DataFrame({
    'Model': ['LightGBM'],
    '学习率': [params_lgb['learning_rate']],
    '准确率': [accuracy],
    '精确率': [precision],
    '召回率': [recall],
    'F1 值': [f1],
    'AUC值': [auc],
    '5折交叉验证的score': [accuracy]
})
evaluation.to_csv('evaluation.csv', index=False)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Memory Reduction: Implemented downcasting for numeric columns (int64 to int32/float64 to float32) to reduce RAM usage.
# 2. Eliminated Redundancy: Removed the secondary model training step previously used for generating learning curves.
# 3. Data Movement Efficiency: Converted DataFrames to NumPy arrays (X, y) before the cross-validation loop to avoid repetitive slicing/indexing overhead.
# 4. Streamlined Importance Calculation: Replaced per-fold DataFrame creation for feature importance with direct NumPy array accumulation.
# 5. Optimized Training: Disabled internal logging (verbose=-1) and removed external plot-related computations to save CPU cycles and energy.
# 6. Green Preprocessing: Replaced redundant accuracy calculations with a single pass over out-of-fold predictions.
# 7. Computation Efficiency: Used `force_row_wise=True` in LightGBM params for faster processing on typical CPU architectures.
# 8. Resource Cleanup: Removed all visualization libraries (Matplotlib, Seaborn) to minimize the environment footprint.