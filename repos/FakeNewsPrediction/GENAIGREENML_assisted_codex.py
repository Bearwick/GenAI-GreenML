# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

def load_data():
    fake_text = pd.read_csv("Fake.csv", usecols=["text"])["text"]
    real_text = pd.read_csv("True.csv", usecols=["text"])["text"]
    texts = pd.concat([fake_text, real_text], ignore_index=True)
    labels = np.concatenate(
        (np.zeros(len(fake_text), dtype=np.int8), np.ones(len(real_text), dtype=np.int8))
    )
    return texts, labels

def train_model():
    texts, labels = load_data()
    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, test_size=0.2, random_state=42
    )
    vectorizer = TfidfVectorizer(stop_words="english", max_df=0.7)
    X_train_tfidf = vectorizer.fit_transform(X_train)
    model = LogisticRegression()
    model.fit(X_train_tfidf, y_train)
    accuracy = model.score(vectorizer.transform(X_test), y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    train_model()

# OPTIMIZATION SUMMARY
# Loaded only the 'text' column to reduce disk I/O and memory footprint.
# Built label arrays with NumPy to avoid larger intermediate DataFrames.
# Added a fixed random_state for reproducible train/test splits.
# Used model.score to compute accuracy without storing prediction arrays.
# Removed model persistence and debug output to avoid unnecessary I/O.