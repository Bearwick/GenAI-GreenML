# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import random
import numpy as np
import pandas as pd
import warnings
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.exceptions import ConvergenceWarning
import joblib

SEED = 42
random.seed(SEED)
np.random.seed(SEED)

DATASET_HEADERS = "title,text,subject,date".split(",")
FAKE_PATH = "Fake.csv"
REAL_PATH = "True.csv"


def _normalize_name(name):
    return str(name).strip().lstrip("\ufeff").lower()


def _parsing_wrong(columns, expected_headers):
    cols_norm = [_normalize_name(c) for c in columns]
    expected_norm = [_normalize_name(h) for h in expected_headers]
    if len(cols_norm) <= 1:
        return True
    return not any(c in cols_norm for c in expected_norm)


def _detect_read_params(path, expected_headers):
    header = pd.read_csv(path, nrows=0)
    if _parsing_wrong(header.columns, expected_headers):
        header = pd.read_csv(path, nrows=0, sep=";", decimal=",")
        return header.columns, {"sep": ";", "decimal": ","}
    return header.columns, {}


def _select_text_column(columns, expected_headers):
    norm_map = {_normalize_name(c): c for c in columns}
    expected_norm = [_normalize_name(h) for h in expected_headers]
    for h_norm in expected_norm:
        if "text" in h_norm and h_norm in norm_map:
            return norm_map[h_norm]
    for h_norm in expected_norm:
        if h_norm in norm_map:
            return norm_map[h_norm]
    return columns[0]


def load_text_series(path, expected_headers):
    columns, params = _detect_read_params(path, expected_headers)
    text_col = _select_text_column(columns, expected_headers)
    df = pd.read_csv(path, usecols=[text_col], dtype={text_col: str}, **params)
    return df[text_col].fillna("")


def prepare_data(fake_path, real_path, headers):
    fake_text = load_text_series(fake_path, headers)
    real_text = load_text_series(real_path, headers)
    texts = pd.concat([fake_text, real_text], ignore_index=True)
    labels = np.concatenate(
        [np.zeros(len(fake_text), dtype=np.int8), np.ones(len(real_text), dtype=np.int8)]
    )
    return texts, labels


def train_and_evaluate(texts, labels, seed):
    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, test_size=0.2, random_state=seed
    )
    vectorizer = TfidfVectorizer(stop_words="english", max_df=0.7)
    X_train_tfidf = vectorizer.fit_transform(X_train)
    model = LogisticRegression(random_state=seed)
    model.fit(X_train_tfidf, y_train)
    X_test_tfidf = vectorizer.transform(X_test)
    preds = model.predict(X_test_tfidf)
    accuracy = float((preds == y_test).mean())
    return model, vectorizer, accuracy


def main():
    warnings.filterwarnings("ignore", category=ConvergenceWarning)
    texts, labels = prepare_data(FAKE_PATH, REAL_PATH, DATASET_HEADERS)
    model, vectorizer, accuracy = train_and_evaluate(texts, labels, SEED)
    joblib.dump(model, "fake_news_model.pkl")
    joblib.dump(vectorizer, "tfidf_vectorizer.pkl")
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Loaded only the necessary text column via header inspection to reduce I/O and memory use.
# - Avoided building an extra combined DataFrame by concatenating Series and compact label arrays.
# - Used deterministic seeds and random_state for reproducible splits and training.
# - Performed a lightweight header check for CSV parsing fallback to prevent redundant full reads.
# - Computed accuracy with vectorized NumPy operations to avoid extra metric overhead.