# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

def train_model():
    df_fake = pd.read_csv("Fake.csv", usecols=["text"])
    df_real = pd.read_csv("True.csv", usecols=["text"])

    df_fake["label"] = 0
    df_real["label"] = 1

    df = pd.concat([df_fake, df_real], ignore_index=True)

    X_train, X_test, y_train, y_test = train_test_split(
        df["text"], df["label"], test_size=0.2, random_state=42
    )

    vectorizer = TfidfVectorizer(
        stop_words="english",
        max_df=0.7,
        max_features=50000,
        sublinear_tf=True,
        dtype="float32",
    )
    X_train_tfidf = vectorizer.fit_transform(X_train)
    X_test_tfidf = vectorizer.transform(X_test)

    model = LogisticRegression(max_iter=200, solver="saga", random_state=42)
    model.fit(X_train_tfidf, y_train)

    accuracy = model.score(X_test_tfidf, y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    train_model()

# OPTIMIZATION SUMMARY
# 1. Load only the 'text' column via usecols to reduce memory and I/O overhead.
# 2. Set ignore_index=True in concat to avoid reindexing cost.
# 3. Added random_state=42 to train_test_split and model for reproducibility.
# 4. Limited TfidfVectorizer to max_features=50000 to reduce feature matrix size and speed up training.
# 5. Used dtype='float32' in TfidfVectorizer to halve memory for the sparse matrix.
# 6. Enabled sublinear_tf for potentially better generalization with less dominant high-freq terms.
# 7. Used solver='saga' in LogisticRegression for efficiency on large sparse datasets.
# 8. Removed all prints, plots, model saving, and interactive inputs.
# 9. Compute accuracy on test set and print in required format.