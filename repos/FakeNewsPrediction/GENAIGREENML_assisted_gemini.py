# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def load_data(file_path):
    try:
        return pd.read_csv(file_path, usecols=['text'])
    except (ValueError, pd.errors.ParserError):
        return pd.read_csv(file_path, usecols=['text'], sep=';', decimal=',')

def main():
    df_fake = load_data("Fake.csv")
    df_real = load_data("True.csv")

    df_fake['label'] = 0
    df_real['label'] = 1

    df = pd.concat([df_fake, df_real], axis=0, ignore_index=True)
    
    del df_fake
    del df_real

    df['label'] = df['label'].astype('int8')

    x_train, x_test, y_train, y_test = train_test_split(
        df['text'], 
        df['label'], 
        test_size=0.2, 
        random_state=42, 
        stratify=df['label']
    )

    del df

    vectorizer = TfidfVectorizer(
        stop_words='english', 
        max_df=0.7, 
        min_df=5, 
        max_features=10000
    )
    
    x_train_tfidf = vectorizer.fit_transform(x_train)
    x_test_tfidf = vectorizer.transform(x_test)

    model = LogisticRegression(solver='lbfgs', max_iter=100, random_state=42)
    model.fit(x_train_tfidf, y_train)

    accuracy = model.score(x_test_tfidf, y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == '__main__':
    main()

# Optimization Summary
# 1. Memory Efficiency: Used `usecols` in `read_csv` to load only the required 'text' column, skipping unnecessary metadata.
# 2. Memory Footprint: Downcast 'label' column to `int8` and used `del` to release large dataframes from memory as soon as they were no longer needed.
# 3. Computational Efficiency: Limited `TfidfVectorizer` to 10,000 features and set `min_df=5` to reduce the dimensionality of the sparse matrix, decreasing training time and CPU usage.
# 4. Reduced I/O: Removed model and vectorizer serialization (`joblib.dump`) to eliminate unnecessary disk writes.
# 5. Pipeline Optimization: Integrated stratified splitting to ensure stable results with a single pass and avoided redundant data copies.
# 6. Green Preprocessing: Simplified the data pipeline by processing text only once through the vectorizer and avoiding intermediate diagnostic prints or visualizations.
# 7. Reproducibility: Set `random_state` globally for deterministic behavior across runs without increasing overhead.