# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def train_model():
    df_fake = pd.read_csv("Fake.csv", usecols=['text'])
    df_fake["label"] = 0
    df_real = pd.read_csv("True.csv", usecols=['text'])
    df_real["label"] = 1

    df = pd.concat([df_fake, df_real], ignore_index=True, copy=False)
    del df_fake, df_real

    X_train, X_test, y_train, y_test = train_test_split(
        df['text'], 
        df['label'], 
        test_size=0.2, 
        random_state=42
    )
    del df

    vectorizer = TfidfVectorizer(
        stop_words='english', 
        max_df=0.7, 
        max_features=10000
    )
    
    X_train_tfidf = vectorizer.fit_transform(X_train)
    X_test_tfidf = vectorizer.transform(X_test)
    del X_train, X_test

    model = LogisticRegression(solver='lbfgs', max_iter=500)
    model.fit(X_train_tfidf, y_train)

    accuracy = accuracy_score(y_test, model.predict(X_test_tfidf))
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == '__main__':
    train_model()

# OPTIMIZATION SUMMARY
# 1. Reduced memory I/O by using 'usecols' in read_csv to load only the required 'text' column.
# 2. Minimized memory footprint by deleting large intermediate DataFrames (df_fake, df_real, df) as soon as they were no longer needed.
# 3. Enhanced computational efficiency by setting 'max_features=10000' in TfidfVectorizer, preventing an excessively large vocabulary and reducing training time.
# 4. Used 'copy=False' in pd.concat to avoid unnecessary memory duplication during the merging process.
# 5. Reduced energy consumption by removing redundant disk write operations (saving models with joblib).
# 6. Ensured reproducible and stable results across runs by defining a fixed 'random_state' in the train-test split.
# 7. Optimized solver performance by using the default 'lbfgs' with an appropriate 'max_iter' for binary classification.