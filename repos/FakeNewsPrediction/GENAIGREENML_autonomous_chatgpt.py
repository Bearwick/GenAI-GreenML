# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "Fake.csv"


def _read_csv_robust(path: str) -> pd.DataFrame:
    # Try default parsing first
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d: pd.DataFrame) -> bool:
        if d is None or d.empty:
            return True
        # If it became a single wide column, delimiter likely wrong
        if d.shape[1] == 1:
            col0 = str(d.columns[0])
            if any(sep in col0 for sep in [";", "\t", "|"]):
                return True
        # Extremely low column count can indicate parsing issues for known multi-col datasets
        if d.shape[1] == 1:
            return True
        return False

    if _looks_wrong(df):
        # Retry with common European CSV settings
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            # Final fallback: python engine (more permissive)
            df = pd.read_csv(path, sep=";", decimal=",", engine="python")

    return df


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols = []
    for c in df.columns:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        cols.append(c2)
    df = df.copy()
    df.columns = cols
    # Drop unnamed columns created by index serialization
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", c)]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _pick_target_and_task(df: pd.DataFrame):
    cols_lower = {c.lower(): c for c in df.columns}

    # Prefer common label names
    for cand in ["label", "target", "y", "class", "is_fake", "fake", "category"]:
        if cand in cols_lower:
            ycol = cols_lower[cand]
            y = df[ycol]
            # Determine if classification is viable
            nunique = y.nunique(dropna=True)
            if nunique >= 2:
                return ycol, "classification"

    # If there is a non-empty 'subject' column, treat as multiclass label
    if "subject" in cols_lower:
        ycol = cols_lower["subject"]
        y = df[ycol]
        if y.notna().sum() > 0 and y.nunique(dropna=True) >= 2:
            return ycol, "classification"

    # Otherwise, choose a numeric column with variance for regression
    numeric_candidates = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() > 0:
            # Prefer non-constant
            if np.isfinite(s.to_numpy(dtype=float, copy=False)).any():
                if s.nunique(dropna=True) >= 2:
                    numeric_candidates.append((c, s.notna().mean(), s.var(skipna=True)))
    if numeric_candidates:
        numeric_candidates.sort(key=lambda x: (x[1], x[2]), reverse=True)
        return numeric_candidates[0][0], "regression"

    # Fallback: use the last column as target (will degrade to trivial baseline later)
    return df.columns[-1], "unknown"


def _build_pipeline(feature_cols, text_cols, cat_cols, num_cols, task_type: str):
    transformers = []

    if text_cols:
        # Keep it lightweight: capped vocab, simple ngrams, no heavy normalization
        text_transformer = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="constant", fill_value="")),
                ("tfidf", TfidfVectorizer(lowercase=True, stop_words="english", max_features=20000, ngram_range=(1, 2))),
            ]
        )
        # ColumnTransformer requires selecting as 1D; provide as single column when possible.
        # For multiple text columns, we concatenate them earlier into a single 'text__combined'.
        transformers.append(("text", text_transformer, text_cols[0]))

    if cat_cols:
        cat_transformer = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
            ]
        )
        transformers.append(("cat", cat_transformer, cat_cols))

    if num_cols:
        num_transformer = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="median")),
            ]
        )
        transformers.append(("num", num_transformer, num_cols))

    preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    if task_type == "classification":
        # Efficient linear classifier for sparse TF-IDF on CPU
        model = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            C=2.0,
            class_weight=None,
            random_state=42,
        )
    else:
        # Lightweight linear regressor for fallback path
        model = Ridge(alpha=1.0, random_state=42)

    pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
    return pipe


def _regression_accuracy_proxy(y_true, y_pred) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    if ss_tot <= 0:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    # Bound into [0,1] for a stable "accuracy" proxy
    return float(np.clip(r2, 0.0, 1.0))


def main():
    df = _read_csv_robust(DATASET_PATH)
    df = _normalize_columns(df)

    assert df is not None and not df.empty, "Dataset is empty after loading."

    # Pick target and determine task type
    target_col, task_type = _pick_target_and_task(df)

    # Build features set from remaining columns
    feature_cols = [c for c in df.columns if c != target_col]
    if not feature_cols:
        # If no features, create a constant feature to keep pipeline runnable
        df = df.copy()
        df["_bias"] = 1
        feature_cols = ["_bias"]

    # If there are multiple obvious text columns, combine to one to avoid heavy FeatureUnion
    lower_map = {c.lower(): c for c in df.columns}
    text_candidates = []
    for cand in ["text", "title", "content", "body", "article"]:
        if cand in lower_map and lower_map[cand] in feature_cols:
            text_candidates.append(lower_map[cand])

    # If no obvious text column, detect object columns with long average length
    if not text_candidates:
        obj_cols = [c for c in feature_cols if df[c].dtype == "object"]
        scored = []
        for c in obj_cols:
            s = df[c].astype(str)
            avg_len = s.str.len().replace([np.inf, -np.inf], np.nan).fillna(0).mean()
            scored.append((c, avg_len))
        scored.sort(key=lambda x: x[1], reverse=True)
        if scored and scored[0][1] >= 30:
            text_candidates = [scored[0][0]]

    df_model = df.copy()

    # Combine multiple text fields if present (title+text is common)
    if len(text_candidates) >= 2:
        comb_col = "text__combined"
        parts = []
        for c in text_candidates:
            parts.append(df_model[c].astype(str).fillna(""))
        df_model[comb_col] = (parts[0] if len(parts) == 1 else (parts[0] + " " + parts[1]))
        for extra in parts[2:]:
            df_model[comb_col] = df_model[comb_col] + " " + extra
        # Drop originals from features to reduce preprocessing cost
        for c in text_candidates:
            if c in feature_cols:
                feature_cols.remove(c)
        feature_cols.append(comb_col)
        text_col = comb_col
    elif len(text_candidates) == 1:
        text_col = text_candidates[0]
    else:
        text_col = None

    # Define column groups
    text_cols = [text_col] if text_col is not None and text_col in df_model.columns else []
    remaining = [c for c in feature_cols if c not in text_cols]

    # Identify numeric and categorical among remaining
    num_cols = []
    cat_cols = []
    for c in remaining:
        # Attempt numeric coercion; if many values parse, treat as numeric
        s_num = pd.to_numeric(df_model[c], errors="coerce")
        ratio = float(s_num.notna().mean()) if len(s_num) else 0.0
        if ratio >= 0.8:
            df_model[c] = s_num.replace([np.inf, -np.inf], np.nan)
            num_cols.append(c)
        else:
            cat_cols.append(c)

    # Prepare target
    y_raw = df_model[target_col]
    if task_type == "classification":
        y = y_raw.astype(str).fillna("NA")
        if y.nunique(dropna=True) < 2:
            task_type = "regression"
    if task_type != "classification":
        y = pd.to_numeric(y_raw, errors="coerce").replace([np.inf, -np.inf], np.nan)

    # Drop rows with missing target for training stability
    valid_mask = y.notna() if task_type != "classification" else y.notna()
    df_model = df_model.loc[valid_mask].reset_index(drop=True)
    y = y.loc[valid_mask]
    assert not df_model.empty, "No valid rows after filtering missing target."

    X = df_model[feature_cols] if isinstance(feature_cols, list) else df_model[[feature_cols]]
    assert X.shape[0] > 1, "Not enough samples."

    # Train/test split
    if task_type == "classification":
        # Stratify if possible (saves accuracy variance without extra compute)
        stratify = y if y.nunique() > 1 and y.value_counts().min() >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

    assert len(X_train) > 0 and len(X_test) > 0, "Empty train/test split."

    # If no usable transformers (edge case), add a constant categorical
    if not text_cols and not cat_cols and not num_cols:
        df_model = df_model.copy()
        df_model["_const"] = "1"
        X = df_model[["_const"]]
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        cat_cols = ["_const"]
        feature_cols = ["_const"]

    pipe = _build_pipeline(feature_cols=feature_cols, text_cols=text_cols, cat_cols=cat_cols, num_cols=num_cols, task_type=task_type)

    # Fit & predict
    if task_type == "classification":
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # If regression target became all-NaN after coercion, use trivial baseline
        if pd.to_numeric(y_train, errors="coerce").notna().sum() < 2:
            accuracy = 0.0
        else:
            pipe.fit(X_train, y_train)
            y_pred = pipe.predict(X_test)
            accuracy = _regression_accuracy_proxy(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used TF-IDF + LogisticRegression (liblinear) as a strong, CPU-friendly baseline for text classification; linear models scale well with sparse features.
# - Capped TF-IDF vocabulary (max_features=20000) and limited n-grams (1,2) to reduce memory/compute while retaining useful signal.
# - Used ColumnTransformer + Pipeline for single-pass, reproducible preprocessing without redundant dataframe copies.
# - Robust schema handling: normalizes column names, drops 'Unnamed' columns, selects target defensively, and combines multiple text fields into one to avoid heavier unions.
# - Robust CSV parsing fallback (default then sep=';' & decimal=',') to handle locale-delimited files.
# - Numeric coercion uses errors='coerce' and inf->NaN to avoid failures and reduce wasted compute on invalid values.
# - Regression fallback uses Ridge (lightweight) and reports a bounded R^2 proxy clipped to [0,1] as ACCURACY for stability across datasets.