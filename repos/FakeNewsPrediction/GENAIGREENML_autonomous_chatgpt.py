# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings
from typing import List, Optional, Tuple

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder


warnings.filterwarnings("ignore")


DATASET_PATH = "Fake.csv"
DATASET_HEADERS = ["title", "text", "subject", "date"]


def _normalize_colname(c: str) -> str:
    c = "" if c is None else str(c)
    c = c.strip()
    c = re.sub(r"\s+", " ", c)
    return c


def _clean_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [_normalize_colname(c) for c in df.columns]
    drop_cols = [c for c in df.columns if c.lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _try_read_csv(path: str) -> pd.DataFrame:
    # Robust parsing fallback for different delimiters/decimals
    df1 = pd.read_csv(path)
    df1 = _clean_columns(df1)

    def looks_wrong(d: pd.DataFrame) -> bool:
        if d is None or d.empty:
            return True
        # if header collapsed into 1 column or most expected headers missing, try fallback
        if d.shape[1] <= 1:
            return True
        norm_cols = {_normalize_colname(c).lower() for c in d.columns}
        expected = {h.lower() for h in DATASET_HEADERS}
        hit = len(norm_cols & expected)
        if hit == 0 and d.shape[1] <= 2:
            return True
        return False

    if not looks_wrong(df1):
        return df1

    df2 = pd.read_csv(path, sep=";", decimal=",")
    df2 = _clean_columns(df2)
    if not looks_wrong(df2):
        return df2

    # If both "look wrong", return the less-wrong (more columns, more non-null)
    score1 = (df1.shape[1], int(df1.notna().sum().sum()))
    score2 = (df2.shape[1], int(df2.notna().sum().sum()))
    return df1 if score1 >= score2 else df2


def _pick_target_and_features(df: pd.DataFrame) -> Tuple[str, List[str], str]:
    cols = list(df.columns)
    lower_map = {c.lower(): c for c in cols}

    # Prefer known label-like columns if they exist (robust to unseen schemas)
    label_candidates = ["label", "target", "y", "class", "category", "is_fake", "fake", "truth"]
    for lc in label_candidates:
        if lc in lower_map:
            target = lower_map[lc]
            features = [c for c in cols if c != target]
            return target, features, "classification_auto"

    # If "subject" exists, likely a categorical target (can be used for classification baseline)
    if "subject" in lower_map:
        target = lower_map["subject"]
        features = [c for c in cols if c != target]
        return target, features, "classification_subject"

    # Otherwise, pick a non-constant numeric column as regression target if available
    numeric_cols = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() >= max(5, int(0.1 * len(df))):
            numeric_cols.append(c)

    def non_constant_numeric(c: str) -> bool:
        s = pd.to_numeric(df[c], errors="coerce")
        s = s.replace([np.inf, -np.inf], np.nan).dropna()
        if len(s) < 2:
            return False
        return float(s.nunique()) >= 2

    for c in numeric_cols:
        if non_constant_numeric(c):
            target = c
            features = [x for x in cols if x != target]
            return target, features, "regression_numeric"

    # Fallback: choose a non-constant object column as classification target
    for c in cols:
        if df[c].dtype == "object" or str(df[c].dtype).startswith("string"):
            nun = df[c].astype(str).nunique(dropna=True)
            if nun >= 2:
                target = c
                features = [x for x in cols if x != target]
                return target, features, "classification_object"

    # Last resort: pick first column as target and rest as features (still runnable)
    target = cols[0]
    features = cols[1:] if len(cols) > 1 else cols[:]
    return target, features, "fallback"


def _make_text_column(df: pd.DataFrame, cols: List[str]) -> Optional[str]:
    # Combine available text-like columns into one to keep vectorizer simple/CPU-friendly
    text_like = []
    for name in ["title", "text", "content", "body", "article", "headline", "summary", "description"]:
        c = next((col for col in cols if col.lower() == name), None)
        if c is not None:
            text_like.append(c)

    # If none of the common names exist, include object columns with relatively long content
    if not text_like:
        obj_cols = [c for c in cols if df[c].dtype == "object" or str(df[c].dtype).startswith("string")]
        # Heuristic: pick up to 2 columns with larger average string length
        lens = []
        for c in obj_cols:
            s = df[c].astype(str)
            lens.append((c, float(s.str.len().replace([np.inf, -np.inf], np.nan).fillna(0).mean())))
        lens.sort(key=lambda x: x[1], reverse=True)
        text_like = [c for c, _ in lens[:2] if _ > 10]

    if not text_like:
        return None

    new_col = "__combined_text__"
    # Build efficiently: fillna + astype(str) then join
    parts = []
    for c in text_like:
        parts.append(df[c].fillna("").astype(str))
    df[new_col] = parts[0]
    for p in parts[1:]:
        df[new_col] = df[new_col].str.cat(p, sep=" ", na_rep="")
    return new_col


def _build_pipeline(df: pd.DataFrame, target: str, features: List[str], mode: str):
    # Separate columns by dtype; coerce numeric only after selecting
    X = df[features].copy()
    y = df[target].copy()

    # Defensive: remove entirely empty feature columns
    nonempty = []
    for c in X.columns:
        if X[c].notna().sum() > 0:
            nonempty.append(c)
    X = X[nonempty]
    features = list(X.columns)

    # Create/choose text column and keep remaining categorical features small
    text_col = _make_text_column(df, features)
    if text_col is not None and text_col not in X.columns:
        X[text_col] = df[text_col]

    # Recompute types
    # Numeric candidates: those that can be substantially coerced
    numeric_cols = []
    categorical_cols = []
    for c in X.columns:
        if c == text_col:
            continue
        if pd.api.types.is_numeric_dtype(X[c]):
            numeric_cols.append(c)
        else:
            coerced = pd.to_numeric(X[c], errors="coerce")
            if coerced.notna().sum() >= max(5, int(0.2 * len(X))):
                X[c] = coerced
                numeric_cols.append(c)
            else:
                categorical_cols.append(c)

    transformers = []

    if text_col is not None:
        # Conservative TF-IDF settings to be CPU/energy-friendly
        transformers.append(
            ("txt", TfidfVectorizer(lowercase=True, stop_words="english", max_features=20000, ngram_range=(1, 2)),
             text_col)
        )

    if numeric_cols:
        transformers.append(
            ("num", Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="median")),
            ]), numeric_cols)
        )

    if categorical_cols:
        transformers.append(
            ("cat", Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("oh", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
            ]), categorical_cols)
        )

    pre = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    # Decide task
    y_is_numeric = pd.api.types.is_numeric_dtype(y) or pd.to_numeric(y, errors="coerce").notna().mean() > 0.9

    if mode.startswith("classification") and not y_is_numeric:
        y_str = y.astype(str).fillna("nan")
        n_classes = int(y_str.nunique(dropna=False))
        if n_classes < 2:
            # fallback to regression-like proxy if classification impossible
            model = Ridge(alpha=1.0, random_state=42)
            task = "regression_proxy"
            y_used = pd.to_numeric(y, errors="coerce")
        else:
            model = LogisticRegression(
                solver="liblinear",
                max_iter=200,
                random_state=42,
            )
            task = "classification"
            y_used = y_str
    else:
        # regression path
        model = Ridge(alpha=1.0, random_state=42)
        task = "regression"
        y_used = pd.to_numeric(y, errors="coerce")

    pipe = Pipeline(steps=[("pre", pre), ("model", model)])
    return pipe, X, y_used, task


def _regression_to_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() < 2:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    denom = np.sum((yt - yt.mean()) ** 2)
    if denom <= 0:
        return 0.0
    r2 = 1.0 - (np.sum((yt - yp) ** 2) / denom)
    # Bound to [0,1] as a stable accuracy proxy
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    if not os.path.exists(DATASET_PATH):
        # Minimal stdout requirement: do not print errors; create trivial accuracy
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _try_read_csv(DATASET_PATH)
    df = _clean_columns(df)

    # Defensive: ensure not empty
    df = df.dropna(how="all")
    assert df is not None and len(df) > 0

    # If expected headers exist, keep them (but don't require them)
    cols_lower = {c.lower() for c in df.columns}
    expected_lower = {h.lower() for h in DATASET_HEADERS}
    if len(cols_lower & expected_lower) >= 2:
        keep = [c for c in df.columns if c.lower() in expected_lower]
        if keep:
            df = df[keep].copy()

    target, features, mode = _pick_target_and_features(df)

    # Ensure target exists
    if target not in df.columns:
        target = df.columns[-1]
        features = [c for c in df.columns if c != target]

    # Ensure we have features
    if not features:
        # Create a constant dummy feature to keep pipeline runnable
        df["__dummy__"] = 0
        features = ["__dummy__"]

    pipe, X, y, task = _build_pipeline(df, target, features, mode)

    # Drop rows with missing y for regression, keep for classification (as string) but still drop empty
    if task.startswith("regression"):
        y_num = pd.to_numeric(y, errors="coerce").replace([np.inf, -np.inf], np.nan)
        good = y_num.notna()
        X = X.loc[good].copy()
        y = y_num.loc[good].copy()
    else:
        y = y.astype(str).replace({"nan": "nan"}).fillna("nan")
        good = y.notna()
        X = X.loc[good].copy()
        y = y.loc[good].copy()

    assert len(X) > 1 and len(y) == len(X)

    # Train/test split (stratify only if feasible)
    stratify = None
    if task == "classification":
        n_classes = int(pd.Series(y).nunique(dropna=False))
        if n_classes >= 2:
            vc = pd.Series(y).value_counts(dropna=False)
            if (vc.min() >= 2) and (len(y) >= 10):
                stratify = y

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=stratify
    )
    assert len(X_train) > 0 and len(X_test) > 0

    pipe.fit(X_train, y_train)
    preds = pipe.predict(X_test)

    if task == "classification":
        accuracy = float(accuracy_score(y_test, preds))
    else:
        accuracy = _regression_to_accuracy(y_test, preds)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight, CPU-friendly models (LogisticRegression with liblinear; Ridge as regression fallback) to minimize compute/energy.
# - Uses a single Pipeline + ColumnTransformer for reproducible preprocessing and to avoid repeated transformations.
# - Conservative TF-IDF (max_features=20000, ngram_range=(1,2), stop_words='english') to bound memory/CPU while retaining strong baseline quality.
# - Robust schema handling: normalizes column names, drops 'Unnamed:*', infers target/features safely, and combines text columns to reduce overhead.
# - Robust CSV parsing: retries with sep=';' and decimal=',' if initial parse likely incorrect.
# - Defensive numeric coercion and NaN/inf handling to prevent failures and keep pipeline end-to-end.
# - If regression is used, reports a bounded "accuracy proxy" in [0,1] computed from R^2 via (R^2+1)/2, clipped for stability.