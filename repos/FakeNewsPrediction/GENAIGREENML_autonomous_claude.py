# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
import os

# Load dataset - try common locations
data_path = None
for path in ["dataset.csv", "data.csv", "train.csv", "./data/dataset.csv"]:
    if os.path.exists(path):
        data_path = path
        break

if data_path is None:
    for root, dirs, files in os.walk("."):
        for f in files:
            if f.endswith(".csv"):
                data_path = os.path.join(root, f)
                break
        if data_path:
            break

df = pd.read_csv(data_path)

# Based on headers: title, text, subject, date
# This looks like a fake news dataset; subject is likely the target or we predict a label
# Check if there's a hidden label column or if subject is the classification target
# Common fake news datasets have title+text as features and a binary label

# Inspect columns to determine target
columns_lower = [c.lower().strip() for c in df.columns]

# If there's a label/class column beyond the 4 listed, use it
target_col = None
for col in df.columns:
    if col.lower().strip() in ['label', 'class', 'target', 'fake', 'real']:
        target_col = col
        break

if target_col is None:
    # Use 'subject' as the classification target
    target_col = 'subject'

# Drop rows with missing text
df = df.dropna(subset=['text', target_col]).reset_index(drop=True)

# Combine title and text for richer features (lightweight concatenation)
df['combined'] = df['title'].fillna('') + ' ' + df['text'].fillna('')

# Encode target
le = LabelEncoder()
y = le.fit_transform(df[target_col])

X_text = df['combined']

# Train/test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X_text, y, test_size=0.2, random_state=42, stratify=y
)

# TF-IDF vectorization - limited features for energy efficiency
# max_features=30000 balances accuracy and memory; sublinear_tf improves performance on text
tfidf = TfidfVectorizer(
    max_features=30000,
    sublinear_tf=True,
    ngram_range=(1, 2),
    min_df=3,
    max_df=0.95,
    strip_accents='unicode',
    dtype=np.float32  # float32 saves memory vs float64
)

X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

# Logistic Regression - lightweight, fast on CPU, excellent for text classification
# saga solver is efficient for large sparse datasets
model = LogisticRegression(
    C=1.0,
    solver='saga',
    max_iter=200,
    n_jobs=-1,
    random_state=42
)

model.fit(X_train_tfidf, y_train)

y_pred = model.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. TF-IDF with max_features=30000 and float32 dtype: limits memory footprint while capturing
#    important text features. Bigrams (ngram_range=(1,2)) add context without excessive cost.
# 2. sublinear_tf=True: applies log(1+tf) which improves text classification performance.
# 3. min_df=3, max_df=0.95: prunes rare and overly