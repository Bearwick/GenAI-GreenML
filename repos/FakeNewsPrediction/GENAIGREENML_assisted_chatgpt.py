# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression


RANDOM_SEED = 42


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] == 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _select_text_column(df: pd.DataFrame) -> str:
    cols_lower = {c.lower(): c for c in df.columns}
    if "text" in cols_lower:
        return cols_lower["text"]
    for c in df.columns:
        if pd.api.types.is_string_dtype(df[c]) and df[c].notna().any():
            return c
    return df.columns[0]


def train_and_evaluate() -> float:
    df_fake = _read_csv_robust("Fake.csv")
    df_real = _read_csv_robust("True.csv")

    fake_text_col = _select_text_column(df_fake)
    real_text_col = _select_text_column(df_real)

    df_fake = df_fake[[fake_text_col]].copy()
    df_real = df_real[[real_text_col]].copy()
    df_fake.columns = ["text"]
    df_real.columns = ["text"]

    df_fake["label"] = 0
    df_real["label"] = 1

    df = pd.concat([df_fake, df_real], ignore_index=True, copy=False)

    X_train, X_test, y_train, y_test = train_test_split(
        df["text"],
        df["label"],
        test_size=0.2,
        random_state=RANDOM_SEED,
        stratify=df["label"],
    )

    vectorizer = TfidfVectorizer(stop_words="english", max_df=0.7)
    X_train_tfidf = vectorizer.fit_transform(X_train)
    X_test_tfidf = vectorizer.transform(X_test)

    model = LogisticRegression(random_state=RANDOM_SEED, max_iter=1000, n_jobs=1)
    model.fit(X_train_tfidf, y_train)

    accuracy = model.score(X_test_tfidf, y_test)
    return float(accuracy)


if __name__ == "__main__":
    accuracy = train_and_evaluate()
    print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Removed all debug prints, plotting, interactive inputs, and model artifact saving to avoid unnecessary I/O and side effects.
# - Reduced memory footprint by selecting only the needed text column and label, and concatenating with ignore_index/copy=False.
# - Added robust CSV parsing fallback (default read_csv, then retry with sep=';' and decimal=',') to prevent costly downstream errors.
# - Ensured reproducibility with a fixed random seed, deterministic split, and fixed model random_state.
# - Used a single TF-IDF fit on training data and only transform on test data to avoid redundant computation.
# - Evaluated via model.score on sparse matrices to avoid extra predictions materialization and data movement.
# - Limited parallelism (n_jobs=1) to reduce energy spikes and improve run-to-run stability on shared environments.