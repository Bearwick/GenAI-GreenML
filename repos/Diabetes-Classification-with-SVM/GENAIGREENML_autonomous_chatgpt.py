# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = "" if c is None else str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df):
    keep = []
    for c in df.columns:
        if isinstance(c, str) and re.match(r"^Unnamed:\s*\d+$", c):
            continue
        if c == "":
            continue
        keep.append(c)
    return df[keep]


def _read_csv_robust(path):
    # Try default parsing first
    df1 = pd.read_csv(path)
    df1.columns = _normalize_columns(df1.columns)
    df1 = _drop_unnamed(df1)

    # Heuristic: if single column and many commas/semicolons in first line, retry with alternate settings
    if df1.shape[1] <= 1:
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            df2.columns = _normalize_columns(df2.columns)
            df2 = _drop_unnamed(df2)
            if df2.shape[1] > df1.shape[1]:
                return df2
        except Exception:
            pass
    return df1


def _pick_target(df, preferred_names=None):
    cols = list(df.columns)
    if preferred_names:
        lower_map = {str(c).strip().lower(): c for c in cols}
        for name in preferred_names:
            key = name.strip().lower()
            if key in lower_map:
                return lower_map[key]

    # Otherwise choose a likely target: non-constant numeric column, prefer low cardinality (classification)
    numeric_candidates = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() < 3:
            continue
        nunique = s.dropna().nunique()
        if nunique <= 1:
            continue
        numeric_candidates.append((c, nunique, s.notna().mean()))
    if not numeric_candidates:
        # Fallback to last column
        return cols[-1] if cols else None

    # Prefer binary/low-cardinality columns; tie-breaker by completeness
    numeric_candidates.sort(key=lambda x: (x[1] > 20, x[1], -x[2]))
    return numeric_candidates[0][0]


def _bounded_regression_score(y_true, y_pred):
    # Convert regression quality into a stable [0,1] "accuracy" proxy:
    # score = 1 / (1 + MAE / scale), scale uses robust IQR (or std fallback).
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = np.mean(np.abs(y_true - y_pred))
    q75, q25 = np.nanpercentile(y_true, [75, 25])
    scale = q75 - q25
    if not np.isfinite(scale) or scale <= 0:
        scale = np.nanstd(y_true)
    if not np.isfinite(scale) or scale <= 0:
        scale = 1.0
    return float(1.0 / (1.0 + (mae / scale)))


def main():
    path = "diabetes2.csv"
    if not os.path.exists(path):
        # Fallback to any CSV in cwd
        csvs = [f for f in os.listdir(".") if f.lower().endswith(".csv")]
        if csvs:
            path = csvs[0]

    df = _read_csv_robust(path)

    # Normalize column names (again, in case parsing fallback changed)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    assert df.shape[0] > 0 and df.shape[1] > 0, "Empty dataset after loading."

    # Choose target robustly
    target_col = _pick_target(df, preferred_names=["Outcome"])
    if target_col is None or target_col not in df.columns:
        target_col = df.columns[-1]

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # If no features left, create a constant feature to keep pipeline valid
    if X.shape[1] == 0:
        X = pd.DataFrame({"__bias__": np.ones(len(df), dtype=float)})

    # Identify numeric/categorical features robustly
    numeric_features = []
    categorical_features = []
    for c in X.columns:
        s = X[c]
        if pd.api.types.is_numeric_dtype(s):
            numeric_features.append(c)
        else:
            # try coercion; if many values become numeric, treat as numeric
            coerced = pd.to_numeric(s, errors="coerce")
            if coerced.notna().mean() >= 0.8:
                X[c] = coerced
                numeric_features.append(c)
            else:
                categorical_features.append(c)

    # If y is object, try numeric coercion; else keep as is
    y_num = pd.to_numeric(y_raw, errors="coerce")
    y_is_numeric_enough = y_num.notna().mean() >= 0.9

    # Prepare classification target if it looks like binary/low-cardinality
    task = "regression"
    y = y_raw.copy()

    if y_is_numeric_enough:
        y = y_num
        nunique = y.dropna().nunique()
        if 2 <= nunique <= 20:
            task = "classification"
            # If values aren't integers, map unique sorted values to 0..k-1 deterministically
            uniq = np.sort(y.dropna().unique())
            mapping = {val: i for i, val in enumerate(uniq)}
            y = y.map(mapping)
        else:
            task = "regression"
    else:
        # For non-numeric: treat as classification if multiple classes
        nunique = y.astype(str).nunique(dropna=True)
        if nunique >= 2:
            task = "classification"
            y = y.astype(str)
        else:
            task = "regression"
            y = pd.to_numeric(y_raw, errors="coerce")

    # Drop rows with missing target
    mask = pd.notna(y)
    X = X.loc[mask].reset_index(drop=True)
    y = y.loc[mask].reset_index(drop=True)

    assert len(X) > 1, "Not enough samples after target cleanup."

    # Split with safeguards (stratify only when feasible)
    stratify = None
    if task == "classification":
        y_for_strat = y
        # Avoid stratify if any class too small
        try:
            vc = pd.Series(y_for_strat).value_counts()
            if (vc.min() >= 2) and (vc.size >= 2):
                stratify = y_for_strat
        except Exception:
            stratify = None

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=stratify
    )

    assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

    # Preprocessing
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features if numeric_features else []),
            ("cat", categorical_transformer, categorical_features if categorical_features else []),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Model selection with safe fallback
    if task == "classification":
        # If only one class in train, fallback to trivial predictor accuracy
        y_train_series = pd.Series(y_train)
        if y_train_series.nunique(dropna=True) < 2:
            majority = y_train_series.mode().iloc[0]
            y_pred = np.full(shape=len(y_test), fill_value=majority)
            accuracy = float(accuracy_score(y_test, y_pred))
        else:
            clf = LogisticRegression(
                solver="liblinear",
                max_iter=300,
                random_state=RANDOM_STATE,
            )
            model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
    else:
        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight CPU-friendly models (LogisticRegression(liblinear) / Ridge) instead of SVM+SMOTE+search to reduce compute and energy.
# - Employs a single sklearn Pipeline + ColumnTransformer to avoid redundant preprocessing and ensure reproducibility.
# - Robust CSV loading fallback (default, then sep=';' with decimal=',') to reduce manual interventions and reruns.
# - Defensive schema handling: normalizes headers, drops 'Unnamed' columns, auto-selects a viable target, coerces numerics safely, and avoids hard failures.
# - For regression fallback, reports a bounded [0,1] proxy score: 1/(1+MAE/scale) using robust IQR scale for stability across datasets.