# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _load_dataset() -> pd.DataFrame:
    # Prefer local CSV to keep the solution self-contained and CPU-friendly.
    candidates = [
        "data.csv",
        "dataset.csv",
        "diabetes.csv",
        "pima-indians-diabetes.csv",
        "train.csv",
        os.path.join("data", "data.csv"),
        os.path.join("data", "dataset.csv"),
        os.path.join("data", "diabetes.csv"),
        os.path.join("data", "pima-indians-diabetes.csv"),
        os.path.join("data", "train.csv"),
    ]
    path = None
    for p in candidates:
        if os.path.exists(p):
            path = p
            break
    if path is None:
        # Fall back: first CSV in current directory (deterministic lexicographic order).
        csvs = sorted([f for f in os.listdir(".") if f.lower().endswith(".csv")])
        if csvs:
            path = csvs[0]
    if path is None:
        raise FileNotFoundError("No CSV dataset found in working directory.")

    df = pd.read_csv(path)

    expected = [
        "Pregnancies",
        "Glucose",
        "BloodPressure",
        "SkinThickness",
        "Insulin",
        "BMI",
        "DiabetesPedigreeFunction",
        "Age",
        "Outcome",
    ]
    missing = [c for c in expected if c not in df.columns]
    if missing:
        raise ValueError(f"Dataset missing expected columns: {missing}")
    return df[expected].copy()


def main() -> None:
    df = _load_dataset()

    target_col = "Outcome"
    feature_cols = [c for c in df.columns if c != target_col]

    X = df[feature_cols].copy()
    y = df[target_col].astype(int).to_numpy()

    # Treat biologically invalid zeros as missing for specific fields (common in this dataset).
    # This is cheaper than more complex outlier modeling and improves signal quality.
    zero_as_missing = ["Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI"]
    for c in zero_as_missing:
        X[c] = X[c].mask(X[c] == 0, np.nan)

    numeric_features = feature_cols

    preprocessor = ColumnTransformer(
        transformers=[
            (
                "num",
                Pipeline(
                    steps=[
                        ("imputer", SimpleImputer(strategy="median")),
                        ("scaler", StandardScaler(with_mean=True, with_std=True)),
                    ]
                ),
                numeric_features,
            )
        ],
        remainder="drop",
        n_jobs=1,
    )

    # Logistic Regression is a strong baseline for small tabular data:
    # fast on CPU, low memory, deterministic with fixed random_state, and energy-efficient.
    clf = LogisticRegression(
        solver="liblinear",
        max_iter=200,
        random_state=42,
    )

    model = Pipeline(
        steps=[
            ("preprocess", preprocessor),
            ("clf", clf),
        ]
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y,
    )

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Chosen model: LogisticRegression (liblinear) for high accuracy/compute ratio on small tabular data; avoids deep learning.
# - Preprocessing: ColumnTransformer+Pipeline for reproducibility; median imputation is simple and CPU-efficient.
# - Domain-aware missing handling: convert impossible zeros to NaN for select medical features to reduce noise cheaply.
# - StandardScaler improves optimization stability; lightweight compared to non-linear or ensemble models.
# - CPU efficiency: n_jobs=1 avoids parallel overhead; small max_iter; deterministic split for reproducibility.