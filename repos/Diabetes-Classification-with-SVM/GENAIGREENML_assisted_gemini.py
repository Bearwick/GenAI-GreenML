# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from scipy.stats import loguniform

data = pd.read_csv('diabetes2.csv')
X = data.drop(columns=['Outcome']).astype('float32')
y = data['Outcome']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

pipeline = Pipeline([
    ('scaler', MinMaxScaler()),
    ('svc', SVC(class_weight='balanced', random_state=42))
])

param_distributions = {
    'svc__C': loguniform(1e-3, 1e3),
    'svc__gamma': loguniform(1e-3, 1e3),
    'svc__kernel': ['linear', 'rbf']
}

search = RandomizedSearchCV(
    pipeline,
    param_distributions,
    n_iter=20,
    cv=5,
    random_state=42
)
search.fit(X_train, y_train)

y_pred = search.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Replaced SMOTE with the 'class_weight="balanced"' parameter in SVC. This handles class imbalance algorithmically, avoiding the computational and memory costs of generating and storing synthetic samples.
# 2. Utilized a Pipeline to bundle scaling and model training. This reduces data movement and ensures that preprocessing is only performed on necessary subsets during cross-validation.
# 3. Downcast input features to 'float32' to reduce the memory footprint and improve cache efficiency during matrix operations.
# 4. Reduced the number of iterations in RandomizedSearchCV from 50 to 20. For a small dataset, this significantly cuts total CPU cycles while still identifying high-performing hyperparameters.
# 5. Removed 'n_jobs=-1' to avoid the energy and time overhead caused by spawning multiple processes and context switching for such a small dataset.
# 6. Eliminated visualization libraries (Matplotlib, Seaborn) and multiple metric calculations (ROC, Precision-Recall) to reduce library import overhead and overall execution runtime.
# 7. Simplified the preprocessing flow by removing manual intermediate scaling and resampling steps, reducing redundant data copies.