# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
import scipy.stats as stats
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE

def run_optimized_pipeline():
    filepath = 'diabetes2.csv'
    try:
        df = pd.read_csv(filepath)
        if 'Outcome' not in df.columns:
            raise ValueError
    except (pd.errors.ParserError, ValueError, KeyError):
        df = pd.read_csv(filepath, sep=';', decimal=',')

    target_col = 'Outcome'
    if target_col not in df.columns:
        target_col = df.columns[-1]

    X = df.drop(columns=[target_col]).values.astype('float32')
    y = df[target_col].values

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    scaler = MinMaxScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    smote = SMOTE(random_state=42)
    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

    param_distributions = {
        'C': stats.loguniform(1e-3, 1e3),
        'gamma': stats.loguniform(1e-3, 1e3),
        'kernel': ['linear', 'rbf']
    }

    search = RandomizedSearchCV(
        SVC(cache_size=1000),
        param_distributions=param_distributions,
        n_iter=50,
        cv=5,
        n_jobs=-1,
        random_state=42
    )
    search.fit(X_train_res, y_train_res)

    y_pred = search.best_estimator_.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_optimized_pipeline()

# Optimization Summary
# 1. Reduced memory footprint by converting features to float32 immediately after loading.
# 2. Optimized data handling by using NumPy arrays instead of Pandas DataFrames for model training.
# 3. Enhanced SVM training efficiency by increasing the kernel cache size (cache_size=1000) to minimize re-computations.
# 4. Eliminated redundant metric calculations (AUC, ROC, F1, Precision-Recall) that were not required for the final output.
# 5. Removed all visualization libraries (matplotlib, seaborn) and plotting logic to save CPU cycles and reduce dependencies.
# 6. Streamlined preprocessing by combining operations and avoiding unnecessary intermediate data structures.
# 7. Implemented robust CSV parsing with a single-pass fallback mechanism to prevent execution failures and redundant IO.
# 8. Leveraged parallel processing (n_jobs=-1) for the hyperparameter search to reduce total execution time and maximize energy efficiency via 'race to sleep'.