# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE
from scipy.stats import loguniform


def main():
    data = pd.read_csv('diabetes2.csv')
    y = data.pop('Outcome').to_numpy()
    X = data.to_numpy()
    del data

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    del X, y

    scaler = MinMaxScaler(copy=False)
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    smote = SMOTE(random_state=42)
    X_train, y_train = smote.fit_resample(X_train, y_train)

    param_distributions = {
        'C': loguniform(1e-3, 1e3),
        'gamma': loguniform(1e-3, 1e3),
        'kernel': ['linear', 'rbf']
    }

    random_search = RandomizedSearchCV(
        SVC(),
        param_distributions=param_distributions,
        n_iter=50,
        cv=5,
        n_jobs=-1,
        random_state=42
    )
    random_search.fit(X_train, y_train)

    best_model = random_search.best_estimator_
    y_pred = best_model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# Eliminated plotting and unused metric calculations to avoid unnecessary CPU work.
# Used DataFrame.pop and NumPy arrays to reduce data copies and memory usage.
# Reused variables and set MinMaxScaler copy=False to limit intermediate allocations.
# Trimmed imports and kept fixed random_state values for reproducible results.