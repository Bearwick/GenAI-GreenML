# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE

import scipy.stats as stats


SEED = 42


def set_reproducibility(seed: int = SEED) -> None:
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)


def read_csv_robust(path: str, expected_headers=None) -> pd.DataFrame:
    df = pd.read_csv(path)
    if expected_headers is not None:
        expected = [h.strip() for h in expected_headers]
        cols = [c.strip() for c in df.columns.astype(str).tolist()]
        if len(cols) == 1 or (len(expected) > 0 and cols != expected and not set(expected).issubset(set(cols))):
            df = pd.read_csv(path, sep=";", decimal=",")
    return df


def infer_target_column(df: pd.DataFrame, dataset_headers) -> str:
    cols = df.columns.astype(str).tolist()
    if "Outcome" in cols:
        return "Outcome"
    if dataset_headers and dataset_headers[-1] in cols:
        return dataset_headers[-1]
    for c in reversed(cols):
        lc = str(c).strip().lower()
        if lc in {"outcome", "target", "label", "y"}:
            return c
    raise ValueError("Target column could not be inferred from dataset headers and df.columns.")


def main() -> None:
    set_reproducibility(SEED)

    dataset_headers = [h.strip() for h in "Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age,Outcome".split(",")]
    df = read_csv_robust("diabetes2.csv", expected_headers=dataset_headers)

    target_col = infer_target_column(df, dataset_headers)

    y = df[target_col]
    X = df.drop(columns=[target_col])

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=SEED
    )

    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    smote = SMOTE(random_state=SEED)
    X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)

    param_distributions = {
        "C": stats.loguniform(1e-3, 1e3),
        "gamma": stats.loguniform(1e-3, 1e3),
        "kernel": ["linear", "rbf"],
    }

    random_search = RandomizedSearchCV(
        estimator=SVC(),
        param_distributions=param_distributions,
        n_iter=50,
        cv=5,
        n_jobs=-1,
        random_state=SEED,
    )
    random_search.fit(X_resampled, y_resampled)

    best_model = random_search.best_estimator_
    y_pred = best_model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed all exploratory prints, reports, and plotting to reduce I/O overhead and runtime while keeping the same training/evaluation intent.
# - Computed only the required final metric (accuracy) instead of multiple unused metrics/curves to avoid redundant computation.
# - Implemented robust CSV parsing with a fast default read and a fallback (sep=';', decimal=',') only when header/schema looks wrong, minimizing unnecessary re-reads.
# - Ensured reproducibility with fixed seeds for Python and NumPy and consistent random_state usage across split, SMOTE, and RandomizedSearchCV.
# - Avoided unnecessary intermediate structures and kept data as NumPy arrays after scaling/SMOTE to reduce memory overhead and data movement.