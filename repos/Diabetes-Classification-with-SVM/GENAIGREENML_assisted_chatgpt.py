# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE
import scipy.stats as stats


def load_data(path: str) -> tuple[pd.DataFrame, pd.Series]:
    data = pd.read_csv(path)
    X = data.drop(columns=["Outcome"])
    y = data["Outcome"]
    return X, y


def split_data(X: pd.DataFrame, y: pd.Series, random_state: int = 42):
    return train_test_split(X, y, test_size=0.3, random_state=random_state)


def scale_data(X_train: pd.DataFrame, X_test: pd.DataFrame):
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    return X_train_scaled, X_test_scaled


def balance_data(X_train_scaled, y_train, random_state: int = 42):
    smote = SMOTE(random_state=random_state)
    return smote.fit_resample(X_train_scaled, y_train)


def tune_model(X_resampled, y_resampled, random_state: int = 42):
    param_distributions = {
        "C": stats.loguniform(1e-3, 1e3),
        "gamma": stats.loguniform(1e-3, 1e3),
        "kernel": ["linear", "rbf"],
    }
    search = RandomizedSearchCV(
        estimator=SVC(),
        param_distributions=param_distributions,
        n_iter=50,
        cv=5,
        n_jobs=1,
        random_state=random_state,
    )
    search.fit(X_resampled, y_resampled)
    return search.best_estimator_


def main() -> None:
    X, y = load_data("diabetes2.csv")
    X_train, X_test, y_train, y_test = split_data(X, y, random_state=42)
    X_train_scaled, X_test_scaled = scale_data(X_train, X_test)
    X_resampled, y_resampled = balance_data(X_train_scaled, y_train, random_state=42)
    model = tune_model(X_resampled, y_resampled, random_state=42)

    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# Removed unused imports (plots, extra metrics) to reduce startup overhead and dependency load.
# Reduced parallelism from n_jobs=-1 to n_jobs=1 to avoid high CPU contention and energy spikes while preserving results.
# Kept preprocessing and SMOTE only where needed; avoided computing decision_function/curves not used in final output.
# Structured code into small functions to minimize repeated work and keep data flow explicit (less accidental recomputation).
# Ensured reproducibility by keeping random_state fixed across split, SMOTE, and randomized search.