# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# Robust CSV loading
try:
    data = pd.read_csv('diabetes2.csv')
    if data.shape[1] < 2:
        data = pd.read_csv('diabetes2.csv', sep=';', decimal=',')
except Exception:
    data = pd.read_csv('diabetes2.csv', sep=';', decimal=',')

# Normalize column names
data.columns = [c.strip().replace('  ', ' ') for c in data.columns]
data = data[[c for c in data.columns if not c.startswith('Unnamed')]]

# Expected schema
expected_features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']
target_col = 'Outcome'

# Verify target exists; fallback if not
if target_col not in data.columns:
    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
    if len(numeric_cols) >= 2:
        target_col = numeric_cols[-1]
    else:
        target_col = data.columns[-1]

# Select available features
available_features = [c for c in expected_features if c in data.columns]
if len(available_features) == 0:
    available_features = [c for c in data.columns if c != target_col]

# Coerce all to numeric
for col in available_features + [target_col]:
    data[col] = pd.to_numeric(data[col], errors='coerce')

# Drop rows with NaN/inf in target or features
data = data.replace([np.inf, -np.inf], np.nan)
data = data.dropna(subset=available_features + [target_col])

assert len(data) > 0, "Dataset is empty after preprocessing"

X = data[list(available_features)]
y = data[target_col].astype(int)

# Check if classification is viable
n_classes = y.nunique()
if n_classes < 2:
    # Trivial baseline: predict the single class
    accuracy = 1.0
    print(f"ACCURACY={accuracy:.6f}")
    import sys
    sys.exit(0)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

assert len(X_train) > 0 and len(X_test) > 0, "Train or test set is empty"

# Impute remaining NaNs with median (computed on train only via pipeline approach)
from sklearn.impute import SimpleImputer

# Energy-efficient pipeline: impute -> scale -> logistic regression
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression(max_iter=500, solver='lbfgs', random_state=42))
])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Logistic Regression chosen over SVM+RandomizedSearchCV for energy efficiency:
#    - No hyperparameter search needed (avoids 50 iterations x 5 folds = 250 fits)
#    - Linear model trains in O(n*d) vs SVM's O(n^2) to O(n^3) for RBF kernel
#    - L-BFGS solver is efficient for small-scale problems
# 2. Removed SMOTE dependency (imblearn) to reduce external dependencies