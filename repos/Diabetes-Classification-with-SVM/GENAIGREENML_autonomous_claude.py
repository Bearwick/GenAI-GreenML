# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset - Pima Indians Diabetes dataset
df = pd.read_csv("dataset.csv")

# Separate features and target
X = df.drop("Outcome", axis=1).values
y = df["Outcome"].values

# Handle zero values that are likely missing (Glucose, BloodPressure, SkinThickness, Insulin, BMI)
# Replace zeros with NaN then impute with median - energy-efficient imputation strategy
cols_with_missing = [1, 2, 3, 4, 5]  # Glucose, BloodPressure, SkinThickness, Insulin, BMI
for col_idx in cols_with_missing:
    mask = X[:, col_idx] == 0
    if mask.any():
        median_val = np.median(X[~mask, col_idx])
        X[mask, col_idx] = median_val

# Use StratifiedKFold for robust evaluation on small dataset
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

accuracies = []
for train_idx, test_idx in skf.split(X, y):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # StandardScaler is lightweight and effective for logistic regression
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Logistic Regression: lightweight, interpretable, CPU-friendly
    # Using liblinear solver which is efficient for small datasets
    model = LogisticRegression(
        solver="liblinear",
        C=1.0,
        max_iter=200,
        random_state=42
    )
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(y_test, y_pred))

accuracy = np.mean(accuracies)

# Final model trained on all data for reporting
scaler_final = StandardScaler()
X_scaled = scaler_final.fit_transform(X)
model_final = LogisticRegression(solver="liblinear", C=1.0, max_iter=200, random_state=42)
model_final.fit(X_scaled, y)
y_pred_final = model_final.predict(X_scaled)
train_accuracy = accuracy_score(y, y_pred_final)

# Report cross-validated accuracy as the primary metric
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Used LogisticRegression with liblinear solver - extremely lightweight, fast on CPU, ideal for small binary classification tasks (~768 samples)
# 2. Replaced biologically impossible zero values with median imputation - simple, no extra library needed
# 3. StandardScaler for feature normalization - minimal overhead, improves convergence for logistic regression
# 4. StratifiedKFold (5-fold) ensures robust evaluation preserving class distribution
# 5. No deep learning, no large embeddings - entire pipeline runs in milliseconds on CPU
# 6. Minimal memory footprint: only numpy arrays and a single linear model in memory