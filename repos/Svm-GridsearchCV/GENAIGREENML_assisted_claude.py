# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
import numpy as np

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

dt = "bodyPerformance.csv"
try:
    df = pd.read_csv(dt)
    if df.shape[1] < 2:
        df = pd.read_csv(dt, sep=';', decimal=',')
except Exception:
    df = pd.read_csv(dt, sep=';', decimal=',')

target_col = None
for col in df.columns:
    if col.strip().lower() == 'blass':
        target_col = col
        break
if target_col is None:
    target_col = df.columns[-1]

for col in df.columns:
    if df[col].dtype == object and col != target_col:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))

X = df.drop([target_col], axis=1).values.astype(np.float32)
y = df[target_col].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_SEED
)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

svm = SVC()

param_grid = {
    'C': [1, 0.1, 0.01, 0.001],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['rbf', 'sigmoid']
}

grid_search = GridSearchCV(
    svm, param_grid, cv=5, n_jobs=-1, refit=True
)
grid_search.fit(X_train, y_train)

y_pred = grid_search.best_estimator_.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Removed all print statements, plots, and visualizations to eliminate unnecessary I/O overhead.
# 2. Used float32 instead of float64 for features to reduce memory footprint by ~50%.
# 3. Added StandardScaler preprocessing which helps SVM converge faster, reducing computation during grid search.
# 4. Used n_jobs=-1 in GridSearchCV to parallelize cross-validation across CPU cores, reducing wall-clock time and potentially energy.
# 5. Used refit=True (default) in GridSearchCV and called predict on best_estimator_ directly, avoiding redundant re-training of the best model.
# 6. Converted categorical columns (like gender if present) via LabelEncoder to ensure numeric input without manual intervention.
# 7. Used .values (numpy arrays) instead of DataFrames for training to reduce pandas overhead.
# 8. Robust CSV parsing with fallback to sep=';' and decimal=',' for input reliability.
# 9. Fixed random seed for reproducibility.
# 10. Dynamic target column detection from actual dataframe columns for robustness.