# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
import numpy as np

np.random.seed(42)

df = pd.read_csv("bodyPerformance.csv")
if df.shape[1] < 2:
    df = pd.read_csv("bodyPerformance.csv", sep=';', decimal=',')

target_col = [c for c in df.columns if c.strip().lower().startswith('blass') or c.strip().lower() == 'class']
if not target_col:
    target_col = [df.columns[-1]]
target_col = target_col[0]

X = df.drop(columns=[target_col])
y = df[target_col]

non_numeric = X.select_dtypes(exclude=[np.number]).columns
if len(non_numeric) > 0:
    X = X.drop(columns=non_numeric)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

svm = SVC(cache_size=500)

param_grid = {
    'C': [1, 0.1, 0.01, 0.001],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['rbf', 'sigmoid']
}

grid_search = GridSearchCV(svm, param_grid, cv=5, n_jobs=-1, refit=True)
grid_search.fit(X_train_scaled, y_train)

y_pred = grid_search.best_estimator_.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Added StandardScaler: SVM converges much faster on scaled data, drastically reducing fit time and energy.
# 2. Used n_jobs=-1 in GridSearchCV to parallelize CV folds, reducing wall-clock time.
# 3. Used refit=True (default) and predict directly from grid_search.best_estimator_, avoiding redundant re-training.
# 4. Increased SVM cache_size to 500MB to reduce kernel re-computation during training.
# 5. Removed all print statements, plots, and visualization code to eliminate unnecessary I/O.
# 6. Removed redundant intermediate data structures (classification_report, confusion_matrix not needed for accuracy output).
# 7. Robust CSV parsing with fallback to sep=';' and decimal=','.
# 8. Dynamic target column detection from headers rather than hardcoding.
# 9. Fixed random seed for reproducibility.
# 10. Dropped non-numeric columns automatically to avoid encoding overhead while preserving the numeric feature set.