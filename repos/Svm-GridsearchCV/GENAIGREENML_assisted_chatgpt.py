# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

RANDOM_SEED = 42
DATASET_HEADERS = [
    "age",
    "height_cm",
    "weight_kg",
    "body fat_%",
    "diastolic",
    "systolic",
    "gripForce",
    "sit and bend forward_cm",
    "sit-ups counts",
    "broad jump_cm",
    "Blass",
]


def _read_csv_with_fallback(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    expected = set(DATASET_HEADERS)

    def looks_wrong(d: pd.DataFrame) -> bool:
        if d.shape[1] == 1:
            return True
        cols = set(map(str, d.columns))
        if "Blass" not in cols and len(cols.intersection(expected)) < max(2, len(expected) // 2):
            return True
        return False

    if looks_wrong(df):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _select_target_column(df: pd.DataFrame) -> str:
    cols = list(df.columns)
    for c in cols:
        if str(c).strip() == "Blass":
            return c
    raise KeyError("Target column 'Blass' not found in dataset.")


def main() -> None:
    np.random.seed(RANDOM_SEED)

    dt = "bodyPerformance.csv"
    if not os.path.exists(dt):
        raise FileNotFoundError(f"Dataset file not found: {dt}")

    df = _read_csv_with_fallback(dt)

    target_col = _select_target_column(df)
    X = df.drop(columns=[target_col])
    y = df[target_col]

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=RANDOM_SEED,
        stratify=y if y.nunique() > 1 else None,
    )

    pipeline = Pipeline(
        steps=[
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
            ("svc", SVC()),
        ]
    )

    param_grid = {
        "svc__C": [1, 0.1, 0.01, 0.001],
        "svc__gamma": [1, 0.1, 0.01, 0.001],
        "svc__kernel": ["rbf", "sigmoid"],
    }

    grid_search = GridSearchCV(
        estimator=pipeline,
        param_grid=param_grid,
        cv=5,
        n_jobs=-1,
        refit=True,
    )
    grid_search.fit(X_train, y_train)

    y_pred = grid_search.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed all exploratory prints, reports, confusion matrix, and plotting to avoid unnecessary computation and I/O.
# - Used a Pipeline with StandardScaler+SVC to prevent repeated preprocessing work across CV folds and keep code modular.
# - Enabled parallel cross-validation via n_jobs=-1 to reduce wall-clock time (same search space and scoring intent).
# - Avoided retraining a second SVC: GridSearchCV(refit=True) directly provides the best refit model, reducing redundant training.
# - Implemented robust CSV parsing with a delimiter/decimal fallback to prevent expensive downstream failures and re-runs.
# - Added deterministic behavior via fixed RANDOM_SEED and NumPy seeding; used stratified split when feasible for stable evaluation.