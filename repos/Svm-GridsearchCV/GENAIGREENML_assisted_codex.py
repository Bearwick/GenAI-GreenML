# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
import random
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

DATASET_PATH = "bodyPerformance.csv"
DATASET_HEADERS = "age,height_cm,weight_kg,body fat_%,diastolic,systolic,gripForce,sit and bend forward_cm,sit-ups counts,broad jump_cm,Blass"

def parsing_suspect(df, expected_headers):
    if df.shape[1] <= 1:
        return True
    if expected_headers:
        matches = sum(1 for h in expected_headers if h in df.columns)
        if matches < max(1, len(expected_headers) // 3) and df.shape[1] < len(expected_headers):
            return True
    return False

def read_csv_with_fallback(path, expected_headers):
    df = pd.read_csv(path)
    if parsing_suspect(df, expected_headers):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df

def main():
    random.seed(42)
    np.random.seed(42)
    expected_headers = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]
    df = read_csv_with_fallback(DATASET_PATH, expected_headers)
    target_col = None
    for h in reversed(expected_headers):
        if h in df.columns:
            target_col = h
            break
    if target_col is None:
        target_col = df.columns[-1]
    y = df.pop(target_col)
    X = df
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    param_grid = {
        "C": [1, 0.1, 0.01, 0.001],
        "gamma": [1, 0.1, 0.01, 0.001],
        "kernel": ["rbf", "sigmoid"],
    }
    grid_search = GridSearchCV(SVC(random_state=42), param_grid, cv=5)
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Reused GridSearchCV's refitted best_estimator_ to avoid redundant training.
# - Used DataFrame.pop for in-place target extraction to minimize copies.
# - Removed non-essential reporting/plotting to cut computation and I/O.
# - Added robust CSV parsing fallback and fixed random seeds for stable runs.