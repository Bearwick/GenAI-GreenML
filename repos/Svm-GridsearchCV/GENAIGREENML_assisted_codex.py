# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

np.random.seed(42)

DATASET_HEADERS = "age,height_cm,weight_kg,body fat_%,diastolic,systolic,gripForce,sit and bend forward_cm,sit-ups counts,broad jump_cm,Blass"
EXPECTED_HEADERS = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]

def _normalize_columns(columns):
    return [c.strip() for c in columns]

def _needs_fallback(df, expected_headers):
    cols = _normalize_columns(df.columns)
    if len(cols) <= 1:
        return True
    if expected_headers:
        expected_lower = {c.lower() for c in expected_headers}
        cols_lower = {c.lower() for c in cols}
        if not expected_lower.issubset(cols_lower):
            return True
    return False

def load_dataset(path, expected_headers):
    df_default = pd.read_csv(path)
    df_default.columns = _normalize_columns(df_default.columns)
    if _needs_fallback(df_default, expected_headers):
        df_alt = pd.read_csv(path, sep=";", decimal=",")
        df_alt.columns = _normalize_columns(df_alt.columns)
        if not _needs_fallback(df_alt, expected_headers):
            return df_alt
        return df_alt if len(df_alt.columns) > len(df_default.columns) else df_default
    return df_default

def select_target_column(columns, expected_headers):
    if expected_headers:
        expected_target = expected_headers[-1]
        if expected_target in columns:
            return expected_target
        lower_map = {c.lower(): c for c in columns}
        match = lower_map.get(expected_target.lower())
        if match is not None:
            return match
    return columns[-1]

df = load_dataset("bodyPerformance.csv", EXPECTED_HEADERS)
target_col = select_target_column(list(df.columns), EXPECTED_HEADERS)

X = df.drop(columns=[target_col])
y = df[target_col]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

svm = SVC()

param_grid = {
    "C": [1, 0.1, 0.01, 0.001],
    "gamma": [1, 0.1, 0.01, 0.001],
    "kernel": ["rbf", "sigmoid"],
}

grid_search = GridSearchCV(svm, param_grid, cv=5)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# Reused GridSearchCV's refitted best_estimator_ to avoid redundant model training.
# Removed unused reporting and visualization steps to cut unnecessary computation and I/O.
# Added robust CSV parsing with column normalization and fallback handling for reliable ingestion.
# Set a fixed random seed and deterministic split for reproducible results.