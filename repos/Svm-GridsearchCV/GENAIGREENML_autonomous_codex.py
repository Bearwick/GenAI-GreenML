# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os, glob, re, warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier

warnings.filterwarnings("ignore")

default_path = "bodyPerformance.csv"
if not os.path.exists(default_path):
    csvs = glob.glob("*.csv") + glob.glob("*.CSV")
    if csvs:
        default_path = csvs[0]

def read_csv_fallback(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
        return df
    if df.shape[1] <= 1 or any(';' in str(c) for c in df.columns):
        try:
            df2 = pd.read_csv(path, sep=';', decimal=',')
            if df2.shape[1] > df.shape[1]:
                df = df2
        except Exception:
            pass
    return df

df = read_csv_fallback(default_path)

def normalize_colname(name):
    name = str(name)
    name = name.strip()
    name = re.sub(r'\s+', ' ', name)
    return name

df.columns = [normalize_colname(c) for c in df.columns]
df = df.loc[:, [c for c in df.columns if c and not c.lower().startswith('unnamed')]]
if df.columns.duplicated().any():
    df = df.loc[:, ~df.columns.duplicated()]

provided_headers = "age,height_cm,weight_kg,body fat_%,diastolic,systolic,gripForce,sit and bend forward_cm,sit-ups counts,broad jump_cm,Blass"
provided_list = [normalize_colname(h) for h in provided_headers.split(',') if h.strip() != '']
provided_target = provided_list[-1] if provided_list else None

def choose_target(df, provided_target=None):
    cols = list(df.columns)
    if not cols:
        return None
    if provided_target:
        for c in cols:
            if c.lower() == provided_target.lower():
                return c
    common = ['target', 'label', 'class', 'y', 'output', 'outcome']
    for c in cols:
        if c.lower() in common:
            return c
    num_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]
    for c in num_cols:
        if df[c].nunique(dropna=True) > 1:
            return c
    for c in cols:
        if df[c].nunique(dropna=True) > 1:
            return c
    return cols[0]

target_col = choose_target(df, provided_target)
if target_col is None:
    df['target'] = 0
    target_col = 'target'

y_raw = df[target_col]
mask = y_raw.notna()
df = df.loc[mask].reset_index(drop=True)
y_raw = df[target_col]
if y_raw.dtype == object:
    y_raw = y_raw.astype(str).str.strip()

y_for_task = y_raw
if not pd.api.types.is_numeric_dtype(y_for_task):
    y_str = y_for_task.astype(str).str.replace(',', '.')
    coerced = pd.to_numeric(y_str, errors='coerce')
    if coerced.notna().mean() > 0.9:
        y_for_task = coerced

n_unique = y_for_task.nunique(dropna=True)
if pd.api.types.is_bool_dtype(y_for_task):
    task = 'classification'
elif pd.api.types.is_numeric_dtype(y_for_task):
    if n_unique <= 20 or (n_unique / max(1, len(y_for_task))) < 0.05:
        task = 'classification'
    else:
        task = 'regression'
else:
    task = 'classification'

if task == 'classification':
    if pd.api.types.is_numeric_dtype(y_for_task):
        y = y_for_task
    else:
        y = y_raw.astype(str).str.strip()
else:
    y_series_tmp = pd.Series(y_for_task)
    y_str = y_series_tmp.astype(str).str.replace(',', '.')
    y = pd.to_numeric(y_str, errors='coerce')

y_series = pd.Series(y).reset_index(drop=True)
valid_mask = y_series.notna()
df = df.loc[valid_mask].reset_index(drop=True)
y_series = y_series.loc[valid_mask].reset_index(drop=True)

if target_col in df.columns:
    X = df.drop(columns=[target_col])
else:
    X = df.copy()

X = X.reset_index(drop=True)
X.replace([np.inf, -np.inf], np.nan, inplace=True)

numeric_cols = []
categorical_cols = []
for col in X.columns:
    series = X[col]
    if pd.api.types.is_numeric_dtype(series):
        numeric_cols.append(col)
    else:
        series_str = series.astype(str).str.replace(',', '.')
        coerced = pd.to_numeric(series_str, errors='coerce')
        non_na_ratio = coerced.notna().mean()
        if non_na_ratio > 0.7:
            X[col] = coerced
            numeric_cols.append(col)
        else:
            series_obj = series.astype(str).str.strip()
            series_obj = series_obj.where(series.notna(), np.nan)
            X[col] = series_obj
            categorical_cols.append(col)

def filter_cols(cols):
    filtered = []
    for c in cols:
        s = X[c]
        if s.notna().sum() == 0:
            continue
        if s.nunique(dropna=True) <= 1:
            continue
        filtered.append(c)
    return filtered

numeric_cols = filter_cols(numeric_cols)
categorical_cols = filter_cols(categorical_cols)

for col in numeric_cols:
    X[col] = pd.to_numeric(X[col], errors='coerce')

if not numeric_cols and not categorical_cols:
    X['_constant'] = 0
    numeric_cols = ['_constant']

assert len(df) > 0

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
])
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_cols),
        ('cat', categorical_transformer, categorical_cols)
    ],
    remainder='drop'
)

if task == 'classification':
    num_classes = y_series.nunique(dropna=True)
    if num_classes < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear', multi_class='ovr', random_state=42)
else:
    model = Ridge(alpha=1.0)

clf = Pipeline(steps=[('preprocessor', preprocessor),
                     ('model', model)])

n_samples = len(df)
if n_samples < 2:
    X_train, X_test = X.copy(), X.copy()
    y_train, y_test = y_series.copy(), y_series.copy()
else:
    test_size = 0.2 if n_samples >= 5 else 1
    stratify = None
    if task == 'classification' and y_series.nunique(dropna=True) > 1 and isinstance(test_size, float):
        if y_series.value_counts().min() >= 2:
            stratify = y_series
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_series, test_size=test_size, random_state=42, stratify=stratify
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_series, test_size=test_size, random_state=42, stratify=None
        )

assert len(X_train) > 0 and len(X_test) > 0

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if task == 'classification':
    try:
        accuracy = accuracy_score(y_test, y_pred)
    except Exception:
        accuracy = 0.0
else:
    try:
        r2 = r2_score(y_test, y_pred)
        if not np.isfinite(r2):
            r2 = 0.0
    except Exception:
        r2 = 0.0
    accuracy = max(0.0, min(1.0, 0.5 + 0.5 * r2))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) for CPU-efficient training and inference.
# - Applied a simple ColumnTransformer with basic imputation, scaling, and one-hot encoding for reproducible preprocessing.
# - Avoided expensive hyperparameter search and heavy ensembles to reduce computation and energy use.
# - Regression accuracy is derived from a clipped (r2+1)/2 mapping to keep the reported score in [0,1].