# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

def load_data(filepath):
    try:
        data = pd.read_csv(filepath)
        if data.shape[1] <= 1:
            raise ValueError
    except (ValueError, pd.errors.ParserError):
        data = pd.read_csv(filepath, sep=';', decimal=',')
    return data

df = load_data("bodyPerformance.csv")

target_column = 'Blass' if 'Blass' in df.columns else df.columns[-1]
X = df.drop([target_column], axis=1)
y = df[target_column]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

param_grid = {
    'C': [1, 0.1, 0.01, 0.001],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['rbf', 'sigmoid']
}

grid_search = GridSearchCV(
    SVC(cache_size=1000, random_state=42),
    param_grid,
    cv=5,
    n_jobs=-1,
    refit=True
)

grid_search.fit(X_train, y_train)

y_pred = grid_search.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Removed redundant model re-training by utilizing GridSearchCV's built-in refit=True attribute.
# 2. Implemented n_jobs=-1 in GridSearchCV to enable parallel processing, reducing total execution time.
# 3. Increased SVC cache_size to 1000MB to reduce redundant kernel matrix computations and disk I/O.
# 4. Streamlined data loading with a robust fallback mechanism to handle different CSV delimiters efficiently.
# 5. Eliminated multiple exploratory data analysis steps (head, dtypes, value_counts, isna) to reduce runtime and memory overhead.
# 6. Removed all visualization libraries and plotting calls to minimize computational footprint and dependency loading.
# 7. Optimized memory usage by removing intermediate variables and unnecessary data structures.
# 8. Set fixed random seeds (random_state) to ensure deterministic results and avoid redundant experimental runs.