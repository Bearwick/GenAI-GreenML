# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def load_data(file_path):
    try:
        df = pd.read_csv(file_path)
        if len(df.columns) <= 1:
            raise ValueError
    except:
        df = pd.read_csv(file_path, sep=';', decimal=',')
    
    # Normalize column names
    df.columns = [str(c).strip().replace('  ', ' ') for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

# Initialize dataset
dataset_path = "bodyPerformance.csv"
df = load_data(dataset_path)

if df.empty:
    import sys
    sys.exit(0)

# Identify target and features
target_candidate = 'Blass'
if target_candidate not in df.columns:
    # Fallback: choose the last column if 'Blass' is missing
    target_col = df.columns[-1]
else:
    target_col = target_candidate

y = df[target_col]
X = df.drop(columns=[target_col])

# Data Cleaning: Handle non-numeric entries in features and drop rows where target is NaN
df = df.dropna(subset=[target_col])
y = df[target_col]
X = df.drop(columns=[target_col])

# Separate numeric and categorical features
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()

# Ensure numeric columns are actually numeric
for col in numeric_features:
    X[col] = pd.to_numeric(X[col], errors='coerce')

# Preprocessing Pipeline
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Model selection: Logistic Regression is CPU-efficient and robust for baselines
clf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000, multi_class='auto', solver='lbfgs'))
])

# Robust check for classification compatibility
unique_classes = np.unique(y.dropna())
if len(unique_classes) < 2:
    # Trivial baseline if not enough classes
    accuracy = 1.0 if len(unique_classes) == 1 else 0.0
else:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Ensure train set is not empty after potential drops
    if len(X_train) > 0:
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        accuracy = 0.0

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Replaced SVM with GridSearchCV (O(n^2 to n^3) and heavy iterations) with Logistic Regression.
# 2. Logistic Regression (L-BFGS) is significantly more energy-efficient and faster on CPU.
# 3. Used scikit-learn Pipelines to prevent data leakage and ensure a clean, reproducible flow.
# 4. Implemented robust CSV parsing to handle different delimiters/decimals without crashing.
# 5. Added dynamic header normalization to handle whitespace or naming variations.
# 6. Used SimpleImputer with 'median' for numeric data to handle outliers efficiently without complex modeling.
# 7. Preferred StandardScaling over MinMax for better convergence in linear models.
# 8. Avoided deep learning or large ensembles (like Random Forests with 1000+ trees) to minimize CPU cycles and thermal output.
# 9. All computations are designed for single-node CPU execution with minimal memory footprint.
# 10. Fallback logic ensures the script returns a value even if the target column or data distribution is suboptimal.