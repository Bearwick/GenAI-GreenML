# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _read_csv_robust(path):
    # Try default read; if it yields suspiciously few columns, retry with European CSV conventions
    df1 = pd.read_csv(path)
    if df1.shape[1] <= 1:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        if df2.shape[1] > df1.shape[1]:
            return df2
    return df1


def _drop_unnamed(df):
    return df.loc[:, [c for c in df.columns if not re.match(r"^Unnamed:\s*\d+$", str(c))]]


def _pick_target(df):
    # Prefer a likely label column name if present; else choose a non-constant column (prefer low-cardinality integer/bool)
    cols = list(df.columns)
    lowered = {c: str(c).strip().lower() for c in cols}
    preferred = ["target", "label", "y", "class", "outcome", "result"]
    for p in preferred:
        for c in cols:
            if lowered[c] == p or lowered[c].endswith(" " + p) or lowered[c].startswith(p + " "):
                return c

    # Candidate: columns with few unique values (good for classification)
    best_c = None
    best_score = -1
    for c in cols:
        s = df[c]
        nunique = s.nunique(dropna=True)
        if nunique <= 1:
            continue
        # Score low-cardinality higher; numeric columns slightly preferred
        is_num = pd.api.types.is_numeric_dtype(s)
        score = 0
        if nunique <= 20:
            score += 3
        if is_num:
            score += 2
        # Avoid obvious ID-like high cardinality
        score += max(0, 5 - int(np.log10(max(nunique, 1)) + 1))
        if score > best_score:
            best_score = score
            best_c = c

    if best_c is not None:
        return best_c

    # Fallback: last column
    return cols[-1]


def _safe_numeric_coerce(df, numeric_cols):
    # Coerce numeric columns safely; avoid operating on object medians/means later
    for c in numeric_cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _is_classification_target(y_series):
    # Determine classification suitability with robust heuristics
    y = y_series.dropna()
    if y.empty:
        return False
    nunique = y.nunique(dropna=True)
    if nunique < 2:
        return False
    if pd.api.types.is_bool_dtype(y):
        return True
    if pd.api.types.is_integer_dtype(y):
        # If integer with small number of unique values, treat as classification
        return nunique <= 50
    if pd.api.types.is_numeric_dtype(y):
        # Numeric float could still be class labels if few unique values
        return nunique <= 20
    # object/category -> classification
    return True


def _regression_to_accuracy_proxy(y_true, y_pred):
    # Stable bounded proxy in [0,1]: 1 / (1 + normalized MAE), normalized by (IQR + eps)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    mae = np.mean(np.abs(yt - yp))
    q75, q25 = np.percentile(yt, [75, 25])
    iqr = float(q75 - q25)
    scale = iqr if np.isfinite(iqr) and iqr > 1e-12 else float(np.std(yt)) if float(np.std(yt)) > 1e-12 else 1.0
    nmae = mae / (scale + 1e-12)
    acc = 1.0 / (1.0 + nmae)
    acc = float(np.clip(acc, 0.0, 1.0))
    return acc


def main():
    file_path = "grafici.csv"
    if not os.path.exists(file_path):
        # Minimal fallback: try first CSV in cwd
        csvs = [f for f in os.listdir(".") if f.lower().endswith(".csv")]
        if csvs:
            file_path = csvs[0]

    df = _read_csv_robust(file_path)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Ensure non-empty
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _pick_target(df)
    y_raw = df[target_col]
    X_df = df.drop(columns=[target_col])

    # If no features left, create a constant feature
    if X_df.shape[1] == 0:
        X_df = pd.DataFrame({"const": np.ones(len(df), dtype=np.float32)})

    # Identify column types
    numeric_cols = [c for c in X_df.columns if pd.api.types.is_numeric_dtype(X_df[c])]
    cat_cols = [c for c in X_df.columns if c not in numeric_cols]

    # Coerce numerics safely and replace inf
    X_df = _safe_numeric_coerce(X_df, numeric_cols)
    if numeric_cols:
        X_df[numeric_cols] = X_df[numeric_cols].replace([np.inf, -np.inf], np.nan)

    # Decide task type
    classification = _is_classification_target(y_raw)

    # If classification, make y a clean 1d array; keep as string for robust class handling
    if classification:
        y = y_raw.astype("string")
        # Drop rows with missing y
        valid = y.notna()
        X_df = X_df.loc[valid].reset_index(drop=True)
        y = y.loc[valid].reset_index(drop=True)
        # Re-check
        if y.nunique(dropna=True) < 2:
            classification = False
    else:
        y = pd.to_numeric(y_raw, errors="coerce")
        valid = np.isfinite(y.to_numpy(dtype=float))
        X_df = X_df.loc[valid].reset_index(drop=True)
        y = y.loc[valid].reset_index(drop=True)

    assert len(X_df) > 1

    # Split
    strat = y if classification and y.nunique() >= 2 else None
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X_df, y, test_size=0.2, random_state=42, stratify=strat
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X_df, y, test_size=0.2, random_state=42
        )

    assert len(X_train) > 0 and len(X_test) > 0

    # Preprocessing
    num_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    cat_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    pre = ColumnTransformer(
        transformers=[
            ("num", num_pipe, numeric_cols),
            ("cat", cat_pipe, cat_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if classification:
        # Small linear model; 'saga' works with sparse, good CPU baseline
        clf = LogisticRegression(
            solver="saga",
            penalty="l2",
            C=1.0,
            max_iter=300,
            n_jobs=1,
            random_state=42,
        )
        model = Pipeline(steps=[("pre", pre), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Lightweight regression; stable on CPU
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("pre", pre), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _regression_to_accuracy_proxy(y_test.to_numpy(dtype=float), np.asarray(y_pred, dtype=float))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced RandomForest/CV/plots with a single train/test split and a small linear model (LogisticRegression/Ridge) to reduce CPU time and energy use.
# - Used ColumnTransformer + Pipeline for one-pass, reproducible preprocessing (impute + scale numeric; impute + one-hot categorical) and to avoid repeated transforms.
# - Implemented robust CSV parsing fallback (default, then sep=';' and decimal=',') and normalized headers; dropped 'Unnamed:*' columns to prevent wasted compute.
# - Defensive schema handling: target is inferred from common label names or low-cardinality columns; if unsuitable for classification, falls back to regression with a bounded [0,1] accuracy proxy.
# - Kept n_jobs=1 and modest max_iter to avoid oversubscription and unnecessary energy draw on shared CPU environments.