# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import glob
import pickle
import warnings
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols = []
    for c in df.columns:
        s = str(c)
        s = s.strip()
        s = re.sub(r"\s+", " ", s)
        cols.append(s)
    df.columns = cols
    df = df.loc[:, [c for c in df.columns if not str(c).startswith("Unnamed:")]]
    return df


def _read_csv_with_fallback(path: str) -> pd.DataFrame:
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d: pd.DataFrame) -> bool:
        if d is None or not isinstance(d, pd.DataFrame) or d.shape[0] == 0:
            return True
        if d.shape[1] <= 1:
            return True
        if d.shape[1] >= 2:
            col0 = d.columns[0]
            if isinstance(col0, str) and ((";" in col0) or ("," in col0 and d.shape[1] == 1)):
                return True
        return False

    if _looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            pass

    if df is None:
        df = pd.DataFrame()

    return df


def _find_data_file() -> str:
    candidates = []
    candidates += glob.glob("*.csv")
    candidates += glob.glob("*.tsv")
    candidates += glob.glob("*.txt")
    candidates += glob.glob("*.data")
    candidates += glob.glob("*.xlsx")
    candidates += glob.glob("*.parquet")

    candidates = [p for p in candidates if os.path.isfile(p)]
    candidates = [p for p in candidates if os.path.basename(p).lower() not in {"model.pkl"}]

    preferred = ["data.csv", "dataset.csv", "train.csv"]
    for p in preferred:
        if os.path.isfile(p):
            return p

    if candidates:
        candidates.sort(key=lambda p: os.path.getsize(p) if os.path.exists(p) else 0, reverse=True)
        return candidates[0]
    return ""


def _load_dataset() -> pd.DataFrame:
    path = _find_data_file()
    if not path:
        return pd.DataFrame()

    ext = os.path.splitext(path)[1].lower()
    if ext == ".xlsx":
        try:
            df = pd.read_excel(path)
        except Exception:
            df = pd.DataFrame()
    elif ext == ".parquet":
        try:
            df = pd.read_parquet(path)
        except Exception:
            df = pd.DataFrame()
    else:
        df = _read_csv_with_fallback(path)

    if isinstance(df, pd.DataFrame) and df.shape[1] > 0:
        df = _normalize_columns(df)
    return df


def _coerce_numeric_like_columns(df: pd.DataFrame) -> pd.DataFrame:
    # Coerce any object columns that look numeric to numeric, but keep categorical as object if not convertible.
    for c in df.columns:
        if df[c].dtype == "object":
            s = df[c].astype(str).str.strip()
            s = s.replace({"": np.nan, "nan": np.nan, "None": np.nan, "NA": np.nan, "N/A": np.nan})
            num = pd.to_numeric(s, errors="coerce")
            non_na = s.notna().sum()
            conv = num.notna().sum()
            if non_na > 0 and conv / non_na >= 0.85:
                df[c] = num
            else:
                df[c] = s
        elif pd.api.types.is_numeric_dtype(df[c]):
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _choose_target(df: pd.DataFrame):
    if df.shape[1] == 0:
        return None
    candidates = [c for c in df.columns if c.lower() not in {"target", "label", "y"}]
    if len(df.columns) > 1:
        candidates = list(df.columns)
    else:
        candidates = list(df.columns)

    best = None
    best_score = -1

    for c in candidates[::-1]:
        s = df[c]
        if s.isna().all():
            continue
        nun = s.nunique(dropna=True)
        if nun <= 1:
            continue

        # Prefer numeric target (regression) unless classification is obvious.
        if pd.api.types.is_numeric_dtype(s):
            score = 4 + min(nun, 50) / 50.0
        else:
            score = 3 + min(nun, 50) / 50.0

        if score > best_score:
            best_score = score
            best = c

    if best is None:
        for c in df.columns[::-1]:
            s = df[c]
            if s.isna().all():
                continue
            if s.nunique(dropna=True) > 1:
                best = c
                break
    return best


def _is_classification_target(y: pd.Series) -> bool:
    if y is None or len(y) == 0:
        return False
    if pd.api.types.is_numeric_dtype(y):
        nun = y.nunique(dropna=True)
        # If small number of unique numeric values, treat as classification.
        if nun <= 20:
            return True
        return False
    return True


def _safe_accuracy_proxy_regression(y_true, y_pred) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = np.sum((y_true - y_pred) ** 2)
    y_mean = np.mean(y_true)
    ss_tot = np.sum((y_true - y_mean) ** 2)
    if ss_tot <= 1e-12:
        return 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    # Bound to [0,1] for "ACCURACY" printing
    acc = float(np.clip(r2, 0.0, 1.0))
    return acc


def main():
    df = _load_dataset()

    if df is None or not isinstance(df, pd.DataFrame) or df.shape[0] == 0 or df.shape[1] == 0:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _normalize_columns(df)
    df = _coerce_numeric_like_columns(df)

    # Drop rows fully empty
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.dropna(axis=0, how="all")
    df = df.dropna(axis=1, how="all")

    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _choose_target(df)
    if target_col is None or target_col not in df.columns:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    y = df[target_col]
    X = df.drop(columns=[target_col])

    if X.shape[1] == 0:
        # Trivial baseline
        if _is_classification_target(y):
            y2 = y.dropna()
            if y2.nunique(dropna=True) < 2:
                accuracy = 0.0
            else:
                X_dummy = np.zeros((len(y2), 1))
                X_train, X_test, y_train, y_test = train_test_split(
                    X_dummy, y2, test_size=0.2, random_state=42, stratify=y2
                )
                clf = DummyClassifier(strategy="most_frequent")
                clf.fit(X_train, y_train)
                pred = clf.predict(X_test)
                accuracy = float(accuracy_score(y_test, pred))
        else:
            y2 = pd.to_numeric(y, errors="coerce").replace([np.inf, -np.inf], np.nan).dropna()
            if len(y2) < 2:
                accuracy = 0.0
            else:
                X_dummy = np.zeros((len(y2), 1))
                X_train, X_test, y_train, y_test = train_test_split(
                    X_dummy, y2, test_size=0.2, random_state=42
                )
                reg = DummyRegressor(strategy="mean")
                reg.fit(X_train, y_train)
                pred = reg.predict(X_test)
                accuracy = _safe_accuracy_proxy_regression(y_test, pred)
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Identify feature types after coercion
    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    # Ensure y is usable
    task_is_clf = _is_classification_target(y)
    if task_is_clf:
        y_clean = y.astype(str).replace({"nan": np.nan, "None": np.nan, "NA": np.nan, "N/A": np.nan})
    else:
        y_clean = pd.to_numeric(y, errors="coerce")

    # Combine X/y and drop rows with missing target
    mask = ~pd.isna(y_clean)
    X = X.loc[mask].copy()
    y_clean = y_clean.loc[mask].copy()

    X = X.replace([np.inf, -np.inf], np.nan)

    if len(y_clean) < 4:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    if task_is_clf:
        n_classes = y_clean.nunique(dropna=True)
        if n_classes < 2:
            accuracy = 0.0
            print(f"ACCURACY={accuracy:.6f}")
            return

        stratify = y_clean if n_classes <= max(2, int(0.5 * len(y_clean))) else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_clean, test_size=0.2, random_state=42, stratify=stratify
        )
        assert len(X_train) > 0 and len(X_test) > 0

        numeric_transformer = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ])
        categorical_transformer = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ])

        preprocessor = ColumnTransformer(
            transformers=[
                ("num", numeric_transformer, numeric_features),
                ("cat", categorical_transformer, categorical_features),
            ],
            remainder="drop",
            sparse_threshold=0.3,
        )

        # Lightweight baseline: Logistic Regression with saga supports sparse OHE efficiently on CPU.
        model = LogisticRegression(
            solver="saga",
            penalty="l2",
            C=1.0,
            max_iter=300,
            n_jobs=1,
            random_state=42,
        )

        clf = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ])

        try:
            clf.fit(X_train, y_train)
            pred = clf.predict(X_test)
            accuracy = float(accuracy_score(y_test, pred))
        except Exception:
            # Fallback to dummy classifier if something unexpected occurs
            dummy = Pipeline(steps=[
                ("preprocess", preprocessor),
                ("model", DummyClassifier(strategy="most_frequent")),
            ])
            dummy.fit(X_train, y_train)
            pred = dummy.predict(X_test)
            accuracy = float(accuracy_score(y_test, pred))
    else:
        y_num = pd.to_numeric(y_clean, errors="coerce").replace([np.inf, -np.inf], np.nan)
        m = ~pd.isna(y_num)
        X = X.loc[m].copy()
        y_num = y_num.loc[m].copy()

        if len(y_num) < 4:
            accuracy = 0.0
            print(f"ACCURACY={accuracy:.6f}")
            return

        X_train, X_test, y_train, y_test = train_test_split(
            X, y_num, test_size=0.2, random_state=42
        )
        assert len(X_train) > 0 and len(X_test) > 0

        numeric_transformer = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ])
        categorical_transformer = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ])

        preprocessor = ColumnTransformer(
            transformers=[
                ("num", numeric_transformer, numeric_features),
                ("cat", categorical_transformer, categorical_features),
            ],
            remainder="drop",
            sparse_threshold=0.3,
        )

        reg = Ridge(alpha=1.0, random_state=42)

        pipe = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", reg),
        ])

        try:
            pipe.fit(X_train, y_train)
            pred = pipe.predict(X_test)
            accuracy = _safe_accuracy_proxy_regression(y_test, pred)
        except Exception:
            dummy = Pipeline(steps=[
                ("preprocess", preprocessor),
                ("model", DummyRegressor(strategy="mean")),
            ])
            dummy.fit(X_train, y_train)
            pred = dummy.predict(X_test)
            accuracy = _safe_accuracy_proxy_regression(y_test, pred)

    # Save a lightweight artifact for reproducibility; ignore failures.
    try:
        artifact = {
            "target": target_col,
            "task": "classification" if task_is_clf else "regression",
            "model": clf if task_is_clf else pipe,
            "columns": list(df.columns),
        }
        with open("model.pkl", "wb") as f:
            pickle.dump(artifact, f, protocol=pickle.HIGHEST_PROTOCOL)
    except Exception:
        pass

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) for strong CPU baselines with low training energy.
# - ColumnTransformer + Pipeline ensures single-pass, reproducible preprocessing without redundant computations.
# - OneHotEncoder with sparse output reduces memory/CPU for categorical expansion; solver='saga' handles sparse efficiently.
# - Defensive schema handling: auto-detect target, coerce numeric-like text, robust CSV parsing fallback, and safe NaN/inf handling.
# - Regression fallback reports a bounded [0,1] proxy via clipped R^2 to keep a stable ACCURACY metric and avoid noisy scaling.