# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

def load_data(file_path):
    try:
        df = pd.read_csv(file_path)
        if df.shape[1] <= 1:
            raise ValueError
    except (ValueError, pd.errors.ParserError):
        df = pd.read_csv(file_path, sep=';', decimal=',')
    
    X = df.iloc[:, :-1].values.astype(np.float32)
    y = df.iloc[:, -1].values
    return X, y

def run_pipeline(file_path):
    X, y = load_data(file_path)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    model = RandomForestClassifier(
        n_estimators=100, 
        class_weight='balanced', 
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline('grafici.csv')

# Optimization Summary
# 1. Removed StandardScaler: RandomForest is a tree-based algorithm and is scale-invariant, making scaling redundant and computationally wasteful.
# 2. Removed redundant Cross-Validation: Streamlined the evaluation to a single train-test split to reduce total CPU cycles and energy consumption.
# 3. Optimized Data Types: Cast feature matrix to float32 to reduce memory footprint.
# 4. Parallelized Training: Enabled n_jobs=-1 in RandomForestClassifier to utilize all available CPU cores, reducing total execution runtime.
# 5. Robust Data Ingestion: Implemented a fallback mechanism for CSV parsing to handle different delimiters/decimals without manual intervention.
# 6. Eliminated Overhead: Removed all visualization code, unused imports (joblib, matplotlib, SVC), and file I/O operations (saving/loading models) to minimize disk activity and memory usage.
# 7. Optimized Data Splitting: Added 'stratify=y' to train_test_split to ensure stable and representative class distributions in training/testing sets, improving reproducibility.