# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import re
import pickle
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")

DATASET_PATH = "model.pkl"

def read_csv_with_fallback(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] == 1:
            sample = df.iloc[0, 0] if len(df) > 0 else ""
            if isinstance(sample, str) and (sample.count(";") > 0):
                raise ValueError("Likely wrong delimiter")
        return df
    except Exception:
        return pd.read_csv(path, sep=";", decimal=",")

def convert_obj_to_df(obj):
    if isinstance(obj, pd.DataFrame):
        return obj.copy()
    if isinstance(obj, pd.Series):
        return obj.to_frame()
    if isinstance(obj, dict):
        if "data" in obj:
            X = obj.get("data")
            cols = obj.get("feature_names", None)
            try:
                df = pd.DataFrame(X, columns=cols)
            except Exception:
                df = pd.DataFrame(X)
            if "target" in obj:
                df["target"] = obj.get("target")
            elif "y" in obj:
                df["target"] = obj.get("y")
            return df
        if "X" in obj and ("y" in obj or "target" in obj):
            X = obj.get("X")
            y = obj.get("y", obj.get("target"))
            df = pd.DataFrame(X)
            df["target"] = y
            return df
        try:
            return pd.DataFrame(obj)
        except Exception:
            return None
    if isinstance(obj, (list, tuple)):
        if len(obj) == 2:
            X, y = obj
            df = pd.DataFrame(X)
            df["target"] = y
            return df
        try:
            return pd.DataFrame(obj)
        except Exception:
            return None
    if isinstance(obj, np.ndarray):
        return pd.DataFrame(obj)
    return None

df = None
if os.path.exists(DATASET_PATH):
    if DATASET_PATH.lower().endswith((".pkl", ".pickle")):
        try:
            obj = pd.read_pickle(DATASET_PATH)
            df = convert_obj_to_df(obj)
        except Exception:
            df = None
    if df is None:
        try:
            df = read_csv_with_fallback(DATASET_PATH)
        except Exception:
            df = None
    if df is None:
        try:
            with open(DATASET_PATH, "rb") as f:
                obj = pickle.load(f)
            df = convert_obj_to_df(obj)
        except Exception:
            df = None
assert df is not None

df = df.copy()
df.reset_index(drop=True, inplace=True)

drop_cols = []
new_cols = []
seen = {}
for col in df.columns:
    c = str(col).strip()
    c = re.sub(r"\s+", " ", c)
    if re.match(r"(?i)^unnamed", c):
        drop_cols.append(col)
        continue
    if c == "":
        c = "col"
    if c in seen:
        seen[c] += 1
        c = f"{c}_{seen[c]}"
    else:
        seen[c] = 0
    new_cols.append(c)
df = df.drop(columns=drop_cols)
df.columns = new_cols

if df.shape[0] == 0:
    df = pd.DataFrame({"target": [0]})
if df.shape[1] == 0:
    df["target"] = np.arange(len(df))
assert df.shape[0] > 0 and df.shape[1] > 0

numeric_cols = []
for col in df.columns:
    series = df[col]
    if pd.api.types.is_numeric_dtype(series):
        numeric_cols.append(col)
    else:
        converted = pd.to_numeric(series, errors="coerce")
        if converted.notna().mean() >= 0.7:
            df[col] = converted
            numeric_cols.append(col)

common_targets = {"target", "label", "y", "class", "outcome", "response"}
target_col = None
for col in df.columns:
    if col.lower() in common_targets:
        target_col = col
        break
if target_col is None:
    non_constant = [col for col in numeric_cols if df[col].nunique(dropna=True) > 1]
    if non_constant:
        target_col = min(non_constant, key=lambda c: df[c].nunique(dropna=True))
    elif numeric_cols:
        target_col = numeric_cols[0]
    else:
        target_col = df.columns[-1]

if target_col not in df.columns:
    target_col = df.columns[-1]

if df[target_col].isna().all():
    df[target_col] = 0
df = df.dropna(subset=[target_col])
if df.shape[0] == 0:
    df = pd.DataFrame({target_col: [0]})
if len(df) < 2:
    df = pd.concat([df, df], ignore_index=True)

features = [col for col in df.columns if col != target_col]
if len(features) == 0:
    df["index_feature"] = np.arange(len(df))
    features = ["index_feature"]

X = df[features].copy()
y = df[target_col].copy()

numeric_features = []
categorical_features = []
for col in X.columns:
    series = X[col]
    if pd.api.types.is_numeric_dtype(series):
        numeric_features.append(col)
    else:
        converted = pd.to_numeric(series, errors="coerce")
        if converted.notna().mean() >= 0.7:
            X[col] = converted
            numeric_features.append(col)
        else:
            categorical_features.append(col)
if numeric_features:
    X[numeric_features] = X[numeric_features].replace([np.inf, -np.inf], np.nan)

y_converted = pd.to_numeric(y, errors="coerce")
numeric_like = y_converted.notna().mean() >= 0.7
if numeric_like:
    y_series = y_converted
else:
    y_series = y.astype(str)

if pd.api.types.is_numeric_dtype(y_series):
    y_series = y_series.replace([np.inf, -np.inf], np.nan)

mask = y_series.notna()
X = X.loc[mask].reset_index(drop=True)
y_series = y_series.loc[mask].reset_index(drop=True)

if len(y_series) < 2:
    X = pd.concat([X, X], ignore_index=True)
    y_series = pd.concat([y_series, y_series], ignore_index=True)

n_samples = len(y_series)
n_unique = y_series.nunique(dropna=True)

classification = False
if (not numeric_like) or pd.api.types.is_bool_dtype(y_series):
    if n_unique < 2:
        classification = True
    elif n_unique <= max(50, int(0.5 * n_samples)):
        classification = True
else:
    if n_unique >= 2 and n_unique <= max(20, int(0.2 * n_samples)):
        classification = True

if classification:
    le = LabelEncoder()
    y_encoded = le.fit_transform(y_series.astype(str))
    n_classes = len(np.unique(y_encoded))
else:
    if not pd.api.types.is_numeric_dtype(y_series):
        le = LabelEncoder()
        y_encoded = le.fit_transform(y_series.astype(str)).astype(float)
    else:
        y_encoded = y_series.astype(float)
    n_classes = None

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False))
])
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

transformers = []
if numeric_features:
    transformers.append(("num", numeric_transformer, numeric_features))
if categorical_features:
    transformers.append(("cat", categorical_transformer, categorical_features))

if transformers:
    preprocess = ColumnTransformer(transformers, remainder="drop")
else:
    preprocess = "passthrough"

if classification:
    if n_classes is not None and n_classes < 2:
        model = DummyClassifier(strategy="most_frequent")
    else:
        model = LogisticRegression(max_iter=200, solver="liblinear")
else:
    if np.nanstd(y_encoded) == 0:
        model = DummyRegressor(strategy="mean")
    else:
        model = Ridge()

pipeline = Pipeline(steps=[("preprocess", preprocess), ("model", model)])

test_size = 0.2 if n_samples >= 5 else 0.5
stratify = y_encoded if classification and (n_classes is not None) and n_classes > 1 and n_samples >= 5 else None
try:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=test_size, random_state=42, stratify=stratify
    )
except Exception:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=test_size, random_state=42, stratify=None
    )

assert len(X_train) > 0 and len(X_test) > 0

if classification and len(np.unique(y_train)) < 2:
    model = DummyClassifier(strategy="most_frequent")
    pipeline = Pipeline(steps=[("preprocess", preprocess), ("model", model)])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    if len(y_test) >= 2:
        r2 = r2_score(y_test, y_pred)
        if not np.isfinite(r2):
            r2 = 0.0
        accuracy = max(0.0, min(1.0, r2))
    else:
        mae = float(np.mean(np.abs(np.asarray(y_test) - np.asarray(y_pred))))
        denom = float(np.mean(np.abs(np.asarray(y_test))))
        if denom <= 0:
            denom = 1.0
        acc = 1.0 - mae / (denom + 1e-8)
        accuracy = max(0.0, min(1.0, acc))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used lightweight linear models (LogisticRegression/Ridge) with simple preprocessing to minimize CPU and energy usage.
# - Applied ColumnTransformer with basic imputers and one-hot encoding to handle mixed schemas efficiently and reproducibly.
# - Regression accuracy is reported via clipped R2 (or MAE proxy for tiny test sets) to keep the metric stable in [0,1].