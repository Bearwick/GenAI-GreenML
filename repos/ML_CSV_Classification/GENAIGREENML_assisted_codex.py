# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import random
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
import joblib

SEED = 42
DATASET_PATH = "model.pkl"
DATASET_HEADERS = None


def _read_csv_with_fallback(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] == 1 or any(isinstance(c, str) and ";" in c for c in df.columns):
            try:
                df_alt = pd.read_csv(path, sep=";", decimal=",")
                if df_alt.shape[1] > df.shape[1]:
                    df = df_alt
            except Exception:
                pass
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _apply_headers(df):
    headers = DATASET_HEADERS
    if isinstance(headers, str):
        headers = [h for h in (x.strip() for x in headers.split(",")) if h]
    if headers is not None and len(headers) == df.shape[1]:
        df.columns = list(headers)
    return df


def _extract_from_df(df):
    df = _apply_headers(df)
    data = df.to_numpy()
    if data.ndim != 2 or data.shape[1] < 2:
        raise ValueError("Dataset must have at least two columns")
    return data[:, :-1], data[:, -1]


def _extract_from_object(obj):
    if isinstance(obj, pd.DataFrame):
        return _extract_from_df(obj)
    if isinstance(obj, dict):
        for kx, ky in (
            ("X", "y"),
            ("data", "target"),
            ("features", "labels"),
            ("inputs", "labels"),
            ("inputs", "targets"),
            ("data", "label"),
        ):
            if kx in obj and ky in obj:
                return np.asarray(obj[kx]), np.asarray(obj[ky])
    if isinstance(obj, (list, tuple)) and len(obj) == 2:
        return np.asarray(obj[0]), np.asarray(obj[1])
    if isinstance(obj, np.ndarray) and obj.ndim == 2 and obj.shape[1] >= 2:
        return obj[:, :-1], obj[:, -1]
    raise ValueError("Unsupported data format")


def load_data(path):
    X = y = None
    if path.lower().endswith((".pkl", ".joblib")):
        try:
            obj = joblib.load(path)
            X, y = _extract_from_object(obj)
        except Exception:
            X = y = None
    if X is None:
        df = _read_csv_with_fallback(path)
        X, y = _extract_from_df(df)
    return np.asarray(X), np.asarray(y).ravel()


def preprocess_data(X):
    scaler = StandardScaler(copy=False)
    return scaler.fit_transform(X)


def train_model(X_train, y_train):
    model = RandomForestClassifier(
        n_estimators=100, class_weight="balanced", random_state=SEED
    )
    model.fit(X_train, y_train)
    return model


def cross_validate_model(X, y):
    if len(y) >= 5:
        model = RandomForestClassifier(
            n_estimators=100, class_weight="balanced", random_state=SEED
        )
        cross_val_score(model, X, y, cv=5)


def evaluate_model(model, X_test, y_test):
    return model.score(X_test, y_test)


def main():
    np.random.seed(SEED)
    random.seed(SEED)
    X, y = load_data(DATASET_PATH)
    X_scaled = preprocess_data(X)
    cross_validate_model(X_scaled, y)
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=SEED
    )
    model = train_model(X_train, y_train)
    accuracy = evaluate_model(model, X_test, y_test)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# Removed unused imports, plotting, model persistence, and extra evaluations to cut I/O and compute.
# Used in-place scaling and single array extraction to reduce memory allocations.
# Reused scaled data for cross-validation and splitting to avoid redundant preprocessing.
# Added robust data loading with CSV fallback and optional pickle handling to prevent parsing failures.
# Set fixed random seeds and consistent random_state for reproducibility.