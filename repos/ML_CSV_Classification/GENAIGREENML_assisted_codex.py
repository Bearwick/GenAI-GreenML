# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import json
import random
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

def get_candidate_paths():
    candidates = []
    for p in (globals().get("DATASET_PATH"), os.environ.get("DATASET_PATH"), "model.pkl", "grafici.csv"):
        if isinstance(p, str) and p:
            if p not in candidates:
                candidates.append(p)
    return candidates

def apply_dataset_headers(df):
    headers = None
    if "DATASET_HEADERS" in globals():
        headers = globals().get("DATASET_HEADERS")
    else:
        env = os.environ.get("DATASET_HEADERS")
        if env:
            try:
                headers = json.loads(env)
            except Exception:
                headers = [h.strip() for h in env.split(",") if h.strip()]
    if headers is not None:
        if isinstance(headers, str):
            headers = [h.strip() for h in headers.split(",") if h.strip()]
        if isinstance(headers, (list, tuple)) and len(headers) == df.shape[1]:
            df.columns = list(headers)
    return df

def read_csv_robust(path):
    def _read_default():
        return pd.read_csv(path)
    def _read_fallback():
        return pd.read_csv(path, sep=";", decimal=",")
    df = None
    try:
        df = _read_default()
        needs_fallback = False
        if df.shape[1] <= 1:
            needs_fallback = True
        else:
            feature_df = df.iloc[:, :-1]
            if feature_df.select_dtypes(exclude=[np.number]).shape[1] > 0:
                needs_fallback = True
        if needs_fallback:
            df_fb = _read_fallback()
            if isinstance(df_fb, pd.DataFrame) and df_fb.shape[1] > 1:
                df = df_fb
    except Exception:
        try:
            df = _read_fallback()
        except Exception:
            df = pd.read_pickle(path)
    return df

def load_data(paths):
    last_error = None
    for path in paths:
        try:
            df = read_csv_robust(path)
            if not isinstance(df, pd.DataFrame):
                continue
            df = apply_dataset_headers(df)
            if df.shape[1] < 2:
                continue
            X = df.iloc[:, :-1].to_numpy()
            y = df.iloc[:, -1].to_numpy()
            return X, y
        except Exception as exc:
            last_error = exc
            continue
    if last_error:
        raise last_error
    raise FileNotFoundError("Dataset not found")

def preprocess_data(X):
    return StandardScaler(copy=False).fit_transform(X)

def train_model(X_train, y_train):
    model = RandomForestClassifier(
        n_estimators=100,
        class_weight="balanced",
        random_state=RANDOM_SEED,
        n_jobs=1
    )
    model.fit(X_train, y_train)
    return model

def main():
    paths = get_candidate_paths()
    X, y = load_data(paths)
    X = preprocess_data(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_SEED
    )
    model = train_model(X_train, y_train)
    accuracy = model.score(X_test, y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed cross-validation, plotting, persistence, and extra diagnostics to cut redundant computation and I/O.
# - Streamlined evaluation via model.score and avoided storing predictions to reduce memory overhead.
# - Minimized imports and enforced deterministic seeds with single-threaded training for reproducibility and lower energy use.
# - Used in-place capable scaling and direct numpy extraction to limit data copies.
# - Implemented conditional CSV fallback parsing and candidate path handling to avoid unnecessary re-reads.