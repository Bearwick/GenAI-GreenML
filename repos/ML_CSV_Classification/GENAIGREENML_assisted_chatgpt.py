# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

RANDOM_SEED = 42
DATASET_PATH = "grafici.csv"


def _read_csv_robust(file_path: str) -> pd.DataFrame:
    df = pd.read_csv(file_path)
    if df.shape[1] < 2:
        df = pd.read_csv(file_path, sep=";", decimal=",")
    return df


def load_data(file_path: str):
    df = _read_csv_robust(file_path)

    X = df.iloc[:, :-1].to_numpy(dtype=np.float32, copy=False)
    y = df.iloc[:, -1].to_numpy(copy=False)

    if y.dtype.kind in {"O", "U", "S"}:
        y = pd.factorize(y)[0]
    if y.dtype != np.int64 and y.dtype != np.int32:
        y = y.astype(np.int64, copy=False)

    return X, y


def build_pipeline():
    return Pipeline(
        steps=[
            ("scaler", StandardScaler(copy=False)),
            (
                "model",
                RandomForestClassifier(
                    n_estimators=100,
                    class_weight="balanced",
                    random_state=RANDOM_SEED,
                    n_jobs=1,
                ),
            ),
        ]
    )


def main():
    np.random.seed(RANDOM_SEED)

    X, y = load_data(DATASET_PATH)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y
    )

    pipeline = build_pipeline()

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)
    _ = cross_val_score(pipeline, X, y, cv=cv, n_jobs=1)

    pipeline.fit(X_train, y_train)

    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed plotting, verbose evaluation prints, per-feature importance loop, and single-sample prediction to avoid extra compute and I/O while keeping the core train/evaluate task intact.
# - Used a single Pipeline (StandardScaler + RandomForest) to prevent redundant data movement and to keep preprocessing consistent across CV and final training.
# - Implemented robust CSV parsing with a fallback delimiter/decimal retry to reduce failure-induced reruns and ensure reliable ingestion.
# - Converted features to float32 and avoided unnecessary copies to reduce memory footprint and improve cache efficiency.
# - Used StratifiedKFold with fixed seed for reproducible CV splits and deterministic behavior.
# - Set n_jobs=1 to avoid high energy overhead from parallelism on small/medium datasets and to improve reproducibility across environments.