# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import r2_score

# Load dataset - handle European CSV format (semicolon-separated, comma decimals)
try:
    df = pd.read_csv("dataset.csv", sep=";", decimal=",", na_values=[-200, -200.0, "-200"])
except Exception:
    try:
        df = pd.read_csv("dataset.csv", na_values=[-200, -200.0, "-200"])
    except Exception:
        df = pd.read_csv("dataset.csv", sep=",", decimal=".", na_values=[-200, -200.0, "-200"])

# Drop unnamed/empty columns
df = df.dropna(axis=1, how="all")
df = df.loc[:, ~df.columns.str.contains("^Unnamed")]

# Drop rows that are entirely NaN
df = df.dropna(how="all")

# Parse datetime features from Date and Time columns if present
if "Date" in df.columns and "Time" in df.columns:
    try:
        datetime_col = pd.to_datetime(df["Date"] + " " + df["Time"], dayfirst=True, errors="coerce")
        df["hour"] = datetime_col.dt.hour
        df["dayofweek"] = datetime_col.dt.dayofweek
        df["month"] = datetime_col.dt.month
    except Exception:
        pass
    df = df.drop(columns=["Date", "Time"], errors="ignore")

# Convert all columns to numeric
for col in df.columns:
    df[col] = pd.to_numeric(df[col], errors="coerce")

# Replace -200 sentinel values with NaN (common in this UCI Air Quality dataset)
df = df.replace(-200, np.nan)
df = df.replace(-200.0, np.nan)

# Target: CO(GT) - ground truth CO concentration
# This is a regression task predicting CO concentration from sensor readings
target_col = "CO(GT)"

if target_col not in df.columns:
    # Fallback: use first column as target
    target_col = df.columns[0]

# Drop rows where target is missing
df = df.dropna(subset=[target_col])

# Separate features and target
y = df[target_col].values
X = df.drop(columns=[target_col])

# Drop columns with >50% missing values for efficiency
threshold = 0.5
missing_frac = X.isnull().mean()
cols_to_keep = missing_frac[missing_frac < threshold].index.tolist()
X = X[cols_to_keep]

# Impute remaining missing values with median (lightweight approach)
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="median")
X_imputed = imputer.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42
)

# Scale features - important for consistent performance
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Use GradientBoostingRegressor - good balance of accuracy and efficiency for tabular data
# Moderate n_estimators and max_depth to keep training fast on CPU
model = GradientBoostingRegressor(
    n_estimators=200,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42
)

model.fit(X_train_scaled, y_train)

# Predict and evaluate
y_pred = model.predict(X_test_scaled)

# Use R^2 score as accuracy metric for