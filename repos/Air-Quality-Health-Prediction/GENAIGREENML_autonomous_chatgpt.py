# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATA_PATH_CANDIDATES = [
    "data/AirQualityUCI.csv",
    "AirQualityUCI.csv",
    "./data.csv",
    "./dataset.csv",
]


def _find_data_path():
    for p in DATA_PATH_CANDIDATES:
        if os.path.exists(p) and os.path.isfile(p):
            return p
    for fn in os.listdir("."):
        if fn.lower().endswith(".csv") and os.path.isfile(fn):
            return fn
    for root, _, files in os.walk("."):
        for fn in files:
            if fn.lower().endswith(".csv"):
                return os.path.join(root, fn)
    return None


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = "" if c is None else str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed_columns(df):
    drop_cols = []
    for c in df.columns:
        if c is None:
            continue
        cs = str(c)
        if cs.strip() == "":
            drop_cols.append(c)
        elif cs.startswith("Unnamed:"):
            drop_cols.append(c)
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _robust_read_csv(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(d):
        if d is None or d.empty:
            return True
        if d.shape[1] <= 1:
            return True
        col0 = str(d.columns[0]) if len(d.columns) else ""
        if ";" in col0:
            return True
        return False

    if looks_wrong(df):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            df = df2
        except Exception:
            pass
    if df is None:
        raise RuntimeError("Could not read CSV with robust fallbacks.")
    return df


def _coerce_numeric_like(df):
    df2 = df.copy()
    for c in df2.columns:
        if pd.api.types.is_numeric_dtype(df2[c]):
            continue
        if pd.api.types.is_datetime64_any_dtype(df2[c]):
            continue
        if df2[c].dtype == object:
            s = df2[c].astype(str).str.strip()
            s = s.replace({"": np.nan, "nan": np.nan, "NaN": np.nan, "None": np.nan, "-200": np.nan, "-200.0": np.nan})
            parsed = pd.to_numeric(s, errors="coerce")
            non_na_rate = float(parsed.notna().mean()) if len(parsed) else 0.0
            if non_na_rate >= 0.6:
                df2[c] = parsed
    return df2


def _select_target(df):
    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    numeric_cols = [c for c in numeric_cols if df[c].nunique(dropna=True) > 1]
    preferred = ["CO(GT)", "NOx(GT)", "NO2(GT)", "C6H6(GT)", "T", "RH", "AH"]
    for p in preferred:
        if p in df.columns and p in numeric_cols:
            return p
    if numeric_cols:
        return numeric_cols[0]
    for c in df.columns:
        if df[c].nunique(dropna=True) > 1:
            return c
    return df.columns[0] if len(df.columns) else None


def _choose_task_and_prepare_target(y):
    y_series = pd.Series(y).copy()
    if pd.api.types.is_numeric_dtype(y_series):
        y_num = pd.to_numeric(y_series, errors="coerce")
        y_num = y_num.replace([np.inf, -np.inf], np.nan)
        if y_num.notna().sum() < 3:
            return "trivial", None, None
        median = float(y_num.median(skipna=True))
        y_bin = (y_num >= median).astype(int)
        if y_bin.nunique(dropna=True) >= 2:
            return "classification", y_bin, None
        return "regression", y_num, None
    else:
        y_cat = y_series.astype("string")
        y_cat = y_cat.replace({"": pd.NA, "nan": pd.NA, "NaN": pd.NA, "None": pd.NA})
        if y_cat.dropna().nunique() < 2:
            return "trivial", None, None
        top2 = y_cat.value_counts(dropna=True).index[:2].tolist()
        y_bin = y_cat.isin(top2).astype(int)
        if y_bin.nunique(dropna=True) >= 2:
            return "classification", y_bin, None
        return "trivial", None, None


def _build_preprocessor(X):
    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )
    return preprocessor


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() < 2:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    ss_tot = float(np.sum((y_true - float(np.mean(y_true))) ** 2))
    r2 = 1.0 - (ss_res / ss_tot) if ss_tot > 0 else 0.0
    r2 = max(-1.0, min(1.0, r2))
    return float((r2 + 1.0) / 2.0)


def main():
    path = _find_data_path()
    if path is None:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _robust_read_csv(path)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed_columns(df)

    if df.shape[0] == 0 or df.shape[1] == 0:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _coerce_numeric_like(df)

    target_col = _select_target(df)
    if target_col is None or target_col not in df.columns:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    feature_cols = [c for c in df.columns if c != target_col]
    if not feature_cols:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    X = df[feature_cols].copy()
    y_raw = df[target_col].copy()

    task, y, _ = _choose_task_and_prepare_target(y_raw)

    if task == "trivial":
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    if task == "classification":
        full = pd.concat([X, y.rename("__y__")], axis=1)
        full = full.replace([np.inf, -np.inf], np.nan)
        full = full.dropna(subset=["__y__"])
        assert full.shape[0] > 0

        X = full.drop(columns=["__y__"])
        y = full["__y__"].astype(int)

        stratify = y if y.nunique() >= 2 and y.value_counts().min() >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify
        )
        assert len(X_train) > 0 and len(X_test) > 0

        preprocessor = _build_preprocessor(X_train)

        clf = LogisticRegression(
            solver="lbfgs",
            max_iter=300,
            n_jobs=1,
        )

        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))

    else:
        y_num = pd.to_numeric(pd.Series(y_raw), errors="coerce").replace([np.inf, -np.inf], np.nan)
        full = pd.concat([X, y_num.rename("__y__")], axis=1)
        full = full.dropna(subset=["__y__"])
        assert full.shape[0] > 0

        X = full.drop(columns=["__y__"])
        y_num = full["__y__"].astype(float)

        X_train, X_test, y_train, y_test = train_test_split(
            X, y_num, test_size=0.2, random_state=42
        )
        assert len(X_train) > 0 and len(X_test) > 0

        preprocessor = _build_preprocessor(X_train)
        reg = Ridge(alpha=1.0, random_state=42)

        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_score(y_test.values, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) for strong CPU baselines with low energy use vs. ensembles/deep nets.
# - ColumnTransformer+Pipeline ensures single-pass, reproducible preprocessing; avoids manual repeated transformations.
# - Robust CSV loading fallback (default -> sep=';' & decimal=',') to handle common AirQualityUCI formatting without retries loops.
# - Coerces numeric-like object columns to numeric only when parse success rate is high, reducing expensive/incorrect conversions.
# - Handles missing/inf safely via SimpleImputer + dropping missing targets; avoids computing statistics on object dtypes.
# - Classification target is derived via median-threshold binarization when numeric, enabling accuracy scoring without heavy feature engineering.
# - Regression fallback reports bounded "accuracy" = (R2 clipped to [-1,1] + 1)/2 in [0,1] for stable printing.