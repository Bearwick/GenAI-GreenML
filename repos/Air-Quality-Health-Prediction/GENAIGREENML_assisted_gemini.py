# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def run_pipeline():
    data_path = 'data/AirQualityUCI.csv'
    
    try:
        df = pd.read_csv(data_path, sep=';', decimal=',', low_memory=False)
        if df.columns[0] != 'Date':
            raise ValueError
    except:
        df = pd.read_csv(data_path, sep=',', decimal='.', low_memory=False)

    valid_headers = ['Date', 'Time', 'CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)', 
                     'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 
                     'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH']
    df = df[df.columns.intersection(valid_headers)].dropna(subset=['Date', 'Time'])

    features = ['CO(GT)', 'NOx(GT)', 'NO2(GT)', 'C6H6(GT)', 'T', 'RH']
    
    for col in features:
        if df[col].dtype == object:
            df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '.'), errors='coerce')
    
    df = df.replace(-200, np.nan).dropna(subset=features)
    
    df['Datetime'] = pd.to_datetime(
        df['Date'].astype(str) + ' ' + df['Time'].astype(str).str.replace('.', ':', regex=False),
        dayfirst=True, errors='coerce'
    )
    df = df.dropna(subset=['Datetime']).set_index('Datetime')
    
    df_daily = df[features].resample('D').mean().dropna()

    np.random.seed(42)
    df_daily['Hospital_Visits'] = (
        df_daily['CO(GT)'] * 2.5 + 
        df_daily['NOx(GT)'] * 0.05 + 
        df_daily['NO2(GT)'] * 0.1 + 
        df_daily['C6H6(GT)'] * 1.2 +
        np.random.normal(5, 2, len(df_daily))
    ).clip(lower=0)
    
    median_visits = df_daily['Hospital_Visits'].median()
    df_daily['Risk_Label'] = (df_daily['Hospital_Visits'] > median_visits).astype(int)

    X = df_daily[features]
    y_reg = df_daily['Hospital_Visits']
    y_clf = df_daily['Risk_Label']

    X_train, X_test, y_train_reg, y_test_reg = train_test_split(X, y_reg, test_size=0.2, random_state=42)
    _, _, y_train_clf, y_test_clf = train_test_split(X, y_clf, test_size=0.2, random_state=42)

    rf_params = {'n_estimators': 50, 'max_depth': 8, 'random_state': 42, 'n_jobs': -1}
    
    reg_model = RandomForestRegressor(**rf_params)
    reg_model.fit(X_train, y_train_reg)

    clf_model = RandomForestClassifier(**rf_params)
    clf_model.fit(X_train, y_train_clf)

    y_pred_clf = clf_model.predict(X_test)
    accuracy = accuracy_score(y_test_clf, y_pred_clf)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary
# 1. Implemented robust single-pass CSV loading with conditional separator and decimal detection.
# 2. Filtered necessary columns immediately after loading to reduce memory footprint.
# 3. Utilized vectorized pandas operations for data cleaning and numeric conversion instead of iterative loops.
# 4. Replaced the -200 placeholder values and handled missing data efficiently before resampling.
# 5. Reduced model complexity by capping max_depth and reducing n_estimators in Random Forest to save energy.
# 6. Enabled parallel processing (n_jobs=-1) to minimize total runtime and idle CPU energy consumption.
# 7. Removed all visualization libraries and plotting overhead to reduce computational and memory costs.
# 8. Optimized the resampling process by selecting only relevant feature columns before the aggregation step.