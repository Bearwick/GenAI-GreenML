# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import numpy as np
import pandas as pd

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

SEED = 42


def _read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    looks_wrong = df.shape[1] <= 2 or all((";" in str(c)) for c in df.columns)
    if looks_wrong:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def load_and_preprocess(path: str) -> pd.DataFrame:
    df = _read_csv_robust(path)

    df = df.loc[:, ~df.columns.astype(str).str.match(r"^Unnamed")].copy()
    df.columns = [str(c).strip() for c in df.columns]

    cols = set(df.columns)
    date_col = "Date" if "Date" in cols else None
    time_col = "Time" if "Time" in cols else None

    if date_col and time_col:
        dt = pd.to_datetime(
            df[date_col].astype(str).str.strip() + " " + df[time_col].astype(str).str.strip(),
            dayfirst=True,
            errors="coerce",
        )
        df = df.drop(columns=[date_col, time_col], errors="ignore")
        df.insert(0, "Datetime", dt)
        df = df.dropna(subset=["Datetime"])
        df = df.set_index("Datetime").sort_index()
        df = df[~df.index.duplicated(keep="first")]

    candidate_features = ["CO(GT)", "NOx(GT)", "NO2(GT)", "C6H6(GT)", "T", "RH"]
    features = [c for c in candidate_features if c in df.columns]

    df = df.replace(-200, np.nan)

    if features:
        df[features] = df[features].apply(pd.to_numeric, errors="coerce")

    if isinstance(df.index, pd.DatetimeIndex):
        daily = df[features].resample("D").mean()
    else:
        daily = df[features].copy()

    daily = daily.dropna()

    if len(features) == 0 or daily.empty:
        return pd.DataFrame()

    daily["Hospital_Visits"] = daily[features].sum(axis=1)
    median_visits = float(daily["Hospital_Visits"].median())
    daily["Risk_Label"] = (daily["Hospital_Visits"] > median_visits).astype(np.int8)

    return daily


def train_models(data: pd.DataFrame, features: list[str]):
    X = data[features]
    y_reg = data["Hospital_Visits"]
    y_clf = data["Risk_Label"]

    X_train, X_test, y_reg_train, y_reg_test, y_clf_train, y_clf_test = train_test_split(
        X, y_reg, y_clf, test_size=0.2, random_state=SEED, shuffle=True, stratify=y_clf if y_clf.nunique() > 1 else None
    )

    reg_model = RandomForestRegressor(
        n_estimators=100,
        random_state=SEED,
        n_jobs=-1,
    )
    reg_model.fit(X_train, y_reg_train)

    clf_model = RandomForestClassifier(
        n_estimators=100,
        random_state=SEED,
        n_jobs=-1,
    )
    clf_model.fit(X_train, y_clf_train)

    y_clf_pred = clf_model.predict(X_test)
    return clf_model, X_test, y_clf_test, y_clf_pred


def main():
    np.random.seed(SEED)

    data = load_and_preprocess("data/AirQualityUCI.csv")
    dataset_headers = [
        "Date",
        "Time",
        "CO(GT)",
        "PT08.S1(CO)",
        "NMHC(GT)",
        "C6H6(GT)",
        "PT08.S2(NMHC)",
        "NOx(GT)",
        "PT08.S3(NOx)",
        "NO2(GT)",
        "PT08.S4(NO2)",
        "PT08.S5(O3)",
        "T",
        "RH",
        "AH",
    ]
    preferred_features = ["CO(GT)", "NOx(GT)", "NO2(GT)", "C6H6(GT)", "T", "RH"]
    available_features = [c for c in preferred_features if c in data.columns]
    if not available_features:
        fallback = [c for c in dataset_headers if c in data.columns and c not in ("Hospital_Visits", "Risk_Label")]
        available_features = fallback[:6] if fallback else []

    if data.empty or not available_features or "Risk_Label" not in data.columns:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    clf_model, X_test, y_clf_test, y_clf_pred = train_models(data, available_features)
    accuracy = accuracy_score(y_clf_test, y_clf_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed all plotting/visualization and intermediate reporting to avoid unnecessary computation and I/O.
# - Implemented robust CSV parsing with a fallback separator/decimal strategy to prevent costly downstream errors/retries.
# - Reduced data movement by selecting and operating on only required feature columns early in preprocessing.
# - Used vectorized pandas operations (numeric conversion, replacement, daily resampling, target creation) to avoid Python loops.
# - Dropped unused columns and invalid rows early to reduce memory footprint and speed up model fitting.
# - Ensured reproducibility with a fixed global seed and fixed random_state for train/test split and both models.
# - Enabled parallelism via n_jobs=-1 for RandomForest training to reduce wall-clock time while preserving model behavior.