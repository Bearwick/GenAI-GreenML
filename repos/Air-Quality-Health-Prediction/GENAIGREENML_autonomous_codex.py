# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import os
import glob
import warnings
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier

warnings.filterwarnings("ignore")

def find_dataset():
    candidates = [
        "data/AirQualityUCI.csv",
        "AirQualityUCI.csv",
        "data.csv",
        "dataset.csv",
        "train.csv"
    ]
    for p in candidates:
        if os.path.isfile(p):
            return p
    files = glob.glob("*.csv") + glob.glob("data/*.csv") + glob.glob("dataset/*.csv")
    if files:
        return sorted(files)[0]
    return None

def make_unique(cols):
    counts = {}
    new_cols = []
    for c in cols:
        if c in counts:
            counts[c] += 1
            new_cols.append(f"{c}_{counts[c]}")
        else:
            counts[c] = 0
            new_cols.append(c)
    return new_cols

def normalize_columns(df):
    cols = []
    for c in df.columns:
        c_str = "" if c is None else str(c)
        c_str = c_str.strip()
        c_str = " ".join(c_str.split())
        cols.append(c_str)
    cols = make_unique(cols)
    df.columns = cols
    drop_cols = [c for c in df.columns if c == "" or c.lower().startswith("unnamed")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    df = normalize_columns(df)
    if df.shape[1] == 1:
        try:
            df2 = pd.read_csv(path, sep=';', decimal=',')
            df2 = normalize_columns(df2)
            if df2.shape[1] > 1:
                df = df2
        except Exception:
            pass
    return df

def coerce_numeric(df):
    for col in df.columns:
        if df[col].dtype == object:
            ser = df[col].astype(str).str.replace(',', '.', regex=False)
            converted = pd.to_numeric(ser, errors='coerce')
            if converted.notna().sum() >= max(1, int(0.5 * len(df))):
                df[col] = converted
    for col in df.columns:
        if np.issubdtype(df[col].dtype, np.datetime64):
            df[col] = df[col].astype(str)
    return df

def pick_target(df):
    candidates = ['target', 'label', 'class', 'y', 'risk_label', 'risk', 'outcome', 'result']
    lower_map = {c.lower(): c for c in df.columns}
    for cand in candidates:
        if cand in lower_map:
            return lower_map[cand]
    n_rows = len(df)
    nunique = {c: df[c].nunique(dropna=True) for c in df.columns}
    low_unique_threshold = min(20, max(2, int(0.1 * n_rows) + 1))
    cat_candidates = [c for c in df.columns if df[c].dtype == object and 1 < nunique[c] <= low_unique_threshold]
    if cat_candidates:
        cat_candidates.sort(key=lambda c: nunique[c])
        return cat_candidates[0]
    num_candidates = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and 1 < nunique[c] <= low_unique_threshold]
    if num_candidates:
        num_candidates.sort(key=lambda c: nunique[c])
        return num_candidates[0]
    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    if num_cols:
        variances = {c: df[c].var(skipna=True) for c in num_cols}
        non_const = [c for c in num_cols if variances.get(c, 0) is not None and not np.isnan(variances[c]) and variances[c] > 0]
        if non_const:
            return max(non_const, key=lambda c: variances[c])
        return num_cols[-1]
    return df.columns[-1]

def safe_train_test_split(X, y, test_size=0.2, classification=False):
    n = len(X)
    if n < 2:
        return X, X, y, y
    if isinstance(test_size, float):
        n_test = max(1, int(round(n * test_size)))
    else:
        n_test = int(test_size)
    if n_test >= n:
        n_test = max(1, n - 1)
    stratify = None
    if classification:
        value_counts = y.value_counts()
        if len(value_counts) >= 2 and value_counts.min() >= 2:
            stratify = y
            n_classes = len(value_counts)
            if n_test < n_classes:
                n_test = n_classes
                if n_test >= n:
                    n_test = max(1, n - 1)
    return train_test_split(X, y, test_size=n_test, random_state=42, stratify=stratify)

def main():
    path = find_dataset()
    if path is None:
        raise FileNotFoundError("No CSV file found.")
    df = read_csv_robust(path)
    df = df.dropna(axis=1, how='all')
    df = df.dropna(axis=0, how='all')
    assert df.shape[0] > 0 and df.shape[1] > 0
    df = coerce_numeric(df)
    df.replace([np.inf, -np.inf], np.nan, inplace=True)

    target_col = pick_target(df)
    df = df.dropna(subset=[target_col])
    assert len(df) > 0

    features = [c for c in df.columns if c != target_col]
    features = [c for c in features if df[c].notna().any()]
    features = [c for c in features if df[c].dropna().nunique() > 1]

    filtered = []
    for c in features:
        if df[c].dtype == object:
            nunique = df[c].nunique(dropna=True)
            if nunique > max(50, int(0.5 * len(df))):
                continue
        filtered.append(c)
    if not filtered:
        df["__index__"] = np.arange(len(df))
        filtered = ["__index__"]
    features = filtered

    X = df[features]
    y = df[target_col]

    n_unique = y.nunique(dropna=True)
    task = 'regression'
    if (y.dtype == object) or (n_unique <= min(20, max(2, int(0.05 * len(df))))):
        task = 'classification'

    accuracy = 0.0

    if task == 'classification':
        X_train, X_test, y_train, y_test = safe_train_test_split(X, y, test_size=0.2, classification=True)
        assert len(X_train) > 0 and len(X_test) > 0
        if y_train.nunique(dropna=True) < 2:
            model = DummyClassifier(strategy='most_frequent')
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
        else:
            numeric_features = [c for c in features if pd.api.types.is_numeric_dtype(df[c])]
            categorical_features = [c for c in features if c not in numeric_features]
            transformers = []
            if numeric_features:
                numeric_transformer = Pipeline(steps=[
                    ('imputer', SimpleImputer(strategy='median')),
                    ('scaler', StandardScaler())
                ])
                transformers.append(('num', numeric_transformer, numeric_features))
            if categorical_features:
                categorical_transformer = Pipeline(steps=[
                    ('imputer', SimpleImputer(strategy='most_frequent')),
                    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))
                ])
                transformers.append(('cat', categorical_transformer, categorical_features))
            preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')
            if y_train.nunique() == 2:
                clf = LogisticRegression(max_iter=200, solver='liblinear')
            else:
                clf = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='auto')
            model = Pipeline(steps=[('preprocess', preprocessor), ('model', clf)])
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
    else:
        X_train, X_test, y_train, y_test = safe_train_test_split(X, y, test_size=0.2, classification=False)
        assert len(X_train) > 0 and len(X_test) > 0
        numeric_features = [c for c in features if pd.api.types.is_numeric_dtype(df[c])]
        categorical_features = [c for c in features if c not in numeric_features]
        transformers = []
        if numeric_features:
            numeric_transformer = Pipeline(steps=[
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler())
            ])
            transformers.append(('num', numeric_transformer, numeric_features))
        if categorical_features:
            categorical_transformer = Pipeline(steps=[
                ('imputer', SimpleImputer(strategy='most_frequent')),
                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))
            ])
            transformers.append(('cat', categorical_transformer, categorical_features))
        preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')
        reg = LinearRegression()
        model = Pipeline(steps=[('preprocess', preprocessor), ('model', reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        if not np.isfinite(r2):
            r2 = 0.0
        accuracy = (r2 + 1.0) / 2.0
        if accuracy < 0.0:
            accuracy = 0.0
        if accuracy > 1.0:
            accuracy = 1.0

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Used lightweight linear/logistic models with simple preprocessing for CPU efficiency.
# - Applied imputation, scaling, and one-hot encoding through a reproducible pipeline.
# - Dropped very high-cardinality categorical features to limit feature explosion.
# - Converted regression R2 to a bounded [0,1] accuracy proxy via (R2+1)/2.