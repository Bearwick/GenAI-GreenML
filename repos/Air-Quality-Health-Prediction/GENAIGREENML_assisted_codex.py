# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
import random
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import accuracy_score

DATASET_PATH = "data/AirQualityUCI.csv"
DATASET_HEADERS = "Date,Time,CO(GT),PT08.S1(CO),NMHC(GT),C6H6(GT),PT08.S2(NMHC),NOx(GT),PT08.S3(NOx),NO2(GT),PT08.S4(NO2),PT08.S5(O3),T,RH,AH,,"
SEED = 42

def robust_read_csv(path, expected_headers):
    expected_set = {h for h in expected_headers if h}
    try:
        sample = pd.read_csv(path, nrows=5)
    except Exception:
        return pd.read_csv(path, sep=";", decimal=",")
    sample_cols = {str(c).strip() for c in sample.columns}
    if len(sample_cols.intersection(expected_set)) < 3:
        return pd.read_csv(path, sep=";", decimal=",")
    return pd.read_csv(path)

def resolve_columns(df_columns, names):
    mapping = {str(c).strip().lower(): c for c in df_columns}
    resolved = []
    for name in names:
        key = name.strip().lower()
        if key in mapping:
            resolved.append(mapping[key])
    return resolved

def preprocess(df, feature_names):
    df = df.loc[:, ~df.columns.astype(str).str.contains("^Unnamed")]
    col_map = {str(c).strip().lower(): c for c in df.columns}
    date_col = col_map.get("date")
    time_col = col_map.get("time")
    if date_col is None or time_col is None:
        raise ValueError("Required Date/Time columns not found")
    feature_cols = resolve_columns(df.columns, feature_names)
    if len(feature_cols) != len(feature_names):
        raise ValueError("Required feature columns not found")
    df = df[[date_col, time_col] + feature_cols].copy()
    numeric_data = df[feature_cols].apply(pd.to_numeric, errors="coerce")
    numeric_data = numeric_data.mask(numeric_data == -200)
    df[feature_cols] = numeric_data
    dt_str = df[date_col].astype(str).str.strip() + " " + df[time_col].astype(str).str.strip()
    dt = pd.to_datetime(dt_str, format="%d/%m/%Y %H.%M.%S", errors="coerce")
    if dt.isna().all():
        dt = pd.to_datetime(dt_str, errors="coerce", dayfirst=True)
    valid = dt.notna()
    df = df.loc[valid].drop(columns=[date_col, time_col])
    df.index = dt[valid]
    if not df.index.is_monotonic_increasing:
        df = df.sort_index()
    df_daily = df.resample("D").mean().dropna()
    features_data = df_daily[feature_cols].to_numpy(dtype=float, copy=False)
    hospital = features_data.sum(axis=1)
    median = np.median(hospital)
    risk = (hospital > median).astype(int)
    return features_data, hospital, risk

def main():
    random.seed(SEED)
    np.random.seed(SEED)
    expected_headers = [h for h in DATASET_HEADERS.split(",") if h]
    df = robust_read_csv(DATASET_PATH, expected_headers)
    desired_order = ["CO(GT)", "NOx(GT)", "NO2(GT)", "C6H6(GT)", "T", "RH"]
    feature_names = [h for h in desired_order if h in expected_headers]
    X, y_reg, y_clf = preprocess(df, feature_names)
    X_train, X_test, y_reg_train, _, y_clf_train, y_clf_test = train_test_split(
        X, y_reg, y_clf, test_size=0.2, random_state=SEED, shuffle=True
    )
    reg_model = RandomForestRegressor(random_state=SEED, n_estimators=100)
    clf_model = RandomForestClassifier(random_state=SEED, n_estimators=100)
    reg_model.fit(X_train, y_reg_train)
    clf_model.fit(X_train, y_clf_train)
    y_pred_clf = clf_model.predict(X_test)
    accuracy = accuracy_score(y_clf_test, y_pred_clf)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# Used a small sample read to detect delimiter and avoid full incorrect parsing.
# Dropped unused and unnamed columns early to reduce memory and processing.
# Applied vectorized numeric conversion and masking to minimize per-column overhead.
# Computed targets directly from NumPy arrays without extra DataFrame columns.
# Fixed random seeds for deterministic, reproducible model training.