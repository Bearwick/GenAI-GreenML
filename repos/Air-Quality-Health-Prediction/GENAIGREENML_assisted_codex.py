# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
import random
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import r2_score, mean_squared_error, classification_report, accuracy_score

DATASET_HEADERS = "Date,Time,CO(GT),PT08.S1(CO),NMHC(GT),C6H6(GT),PT08.S2(NMHC),NOx(GT),PT08.S3(NOx),NO2(GT),PT08.S4(NO2),PT08.S5(O3),T,RH,AH,,"
EXPECTED_HEADERS = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]
RANDOM_STATE = 42

def _looks_misparsed(df):
    if df is None or df.empty:
        return True
    if df.shape[1] <= 1:
        return True
    if any(isinstance(c, str) and ";" in c for c in df.columns):
        return True
    overlap = len(set(EXPECTED_HEADERS).intersection(map(str, df.columns)))
    if overlap < max(1, len(EXPECTED_HEADERS) // 3):
        return True
    return False

def _read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
        return df
    if _looks_misparsed(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            pass
    return df

def load_and_preprocess(path):
    df = _read_csv_robust(path)
    if df is None or df.empty:
        raise ValueError("Dataset could not be loaded.")
    if all(isinstance(c, (int, np.integer)) for c in df.columns):
        if len(df.columns) >= len(EXPECTED_HEADERS):
            extras = [f"Extra_{i}" for i in range(len(df.columns) - len(EXPECTED_HEADERS))]
            df.columns = EXPECTED_HEADERS + extras
        else:
            df.columns = EXPECTED_HEADERS[: len(df.columns)]
    df.columns = df.columns.astype(str).str.strip()
    mask = ~df.columns.str.match(r"^Unnamed") & (df.columns.str.len() > 0)
    df = df.loc[:, mask]
    df.dropna(axis=1, how="all", inplace=True)
    for col in df.columns:
        if col in ("Date", "Time"):
            continue
        if df[col].dtype == object:
            df[col] = pd.to_numeric(df[col].str.replace(",", ".", regex=False), errors="coerce")
        elif not pd.api.types.is_numeric_dtype(df[col]):
            df[col] = pd.to_numeric(df[col], errors="coerce")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        df[numeric_cols] = df[numeric_cols].replace(-200, np.nan)
    return df

def _select_features(df, features):
    available = [f for f in features if f in df.columns]
    if not available:
        numeric_cols = list(df.select_dtypes(include=[np.number]).columns)
        available = numeric_cols[: min(len(numeric_cols), 6)]
    return list(dict.fromkeys(available))

def _select_target(df, features):
    for col in EXPECTED_HEADERS:
        if col in df.columns and col not in features and pd.api.types.is_numeric_dtype(df[col]):
            return col
    for col in EXPECTED_HEADERS:
        if col in df.columns and col not in features:
            if df[col].dtype == object:
                df[col] = pd.to_numeric(df[col].str.replace(",", ".", regex=False), errors="coerce")
                if pd.api.types.is_numeric_dtype(df[col]):
                    return col
    numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c not in features]
    return numeric_cols[0] if numeric_cols else None

def train_models(data, features):
    features = _select_features(data, features)
    target_col = _select_target(data, features)
    if target_col is None:
        raise ValueError("No suitable target column available.")
    cols_needed = features + [target_col]
    df = data.loc[:, cols_needed].dropna()
    if df.empty:
        df = data.loc[:, cols_needed].copy()
        for col in df.columns:
            if pd.api.types.is_numeric_dtype(df[col]):
                if df[col].isna().all():
                    df[col].fillna(0, inplace=True)
                else:
                    df[col].fillna(df[col].median(), inplace=True)
            else:
                df[col].fillna(method="ffill", inplace=True)
                df[col].fillna(method="bfill", inplace=True)
        df.dropna(inplace=True)
    X = df[features].to_numpy(dtype=np.float64, copy=False)
    y_reg = df[target_col].to_numpy(dtype=np.float64, copy=False).ravel()
    median_val = np.nanmedian(y_reg)
    if np.isnan(median_val):
        median_val = np.nanmean(y_reg)
    if np.isnan(median_val):
        median_val = 0.0
    y_clf = (y_reg > median_val).astype(int)
    if len(np.unique(y_clf)) < 2:
        mean_val = np.nanmean(y_reg)
        if np.isnan(mean_val):
            mean_val = 0.0
        y_clf = (y_reg >= mean_val).astype(int)
        if len(np.unique(y_clf)) < 2:
            y_clf = (np.arange(len(y_reg)) % 2).astype(int)
    stratify = y_clf if len(np.unique(y_clf)) > 1 else None
    X_train, X_test, y_reg_train, y_reg_test, y_clf_train, y_clf_test = train_test_split(
        X, y_reg, y_clf, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
    )
    reg_model = LinearRegression()
    reg_model.fit(X_train, y_reg_train)
    y_pred_reg = reg_model.predict(X_test)
    clf_model = LogisticRegression(max_iter=200, solver="liblinear", random_state=RANDOM_STATE)
    clf_model.fit(X_train, y_clf_train)
    y_clf_pred = clf_model.predict(X_test)
    reg_results = {
        "R2": r2_score(y_reg_test, y_pred_reg),
        "RMSE": mean_squared_error(y_reg_test, y_pred_reg, squared=False),
        "Target": target_col,
    }
    clf_results = classification_report(y_clf_test, y_clf_pred, output_dict=True, zero_division=0)
    return reg_model, clf_model, reg_results, clf_results, X_test, y_reg_test, y_pred_reg, y_clf_test, y_clf_pred

def main():
    np.random.seed(RANDOM_STATE)
    random.seed(RANDOM_STATE)
    data = load_and_preprocess("data/AirQualityUCI.csv")
    features = ["CO(GT)", "NOx(GT)", "NO2(GT)", "C6H6(GT)", "T", "RH"]
    _, _, _, _, _, _, _, y_clf_test, y_clf_pred = train_models(data, features)
    accuracy = accuracy_score(y_clf_test, y_clf_pred)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Consolidated preprocessing steps and reused the same train/test split for both tasks to avoid redundant computation.
# - Removed all plotting and logging to eliminate unnecessary overhead and data movement.
# - Performed in-place cleaning and vectorized numeric conversion to reduce memory footprint.
# - Selected features and target dynamically from the provided schema to prevent extra passes over data.
# - Used lightweight linear and logistic models suitable for the task to minimize training cost.