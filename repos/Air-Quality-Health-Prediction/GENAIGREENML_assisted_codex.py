# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import r2_score, mean_squared_error, accuracy_score

DATASET_PATH = "data/AirQualityUCI.csv"
DATASET_HEADERS = "Date,Time,CO(GT),PT08.S1(CO),NMHC(GT),C6H6(GT),PT08.S2(NMHC),NOx(GT),PT08.S3(NOx),NO2(GT),PT08.S4(NO2),PT08.S5(O3),T,RH,AH,,"
RANDOM_STATE = 42

EXPECTED_HEADERS = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]
DATE_COL = next((h for h in EXPECTED_HEADERS if h.lower() == "date"), None)
TIME_COL = next((h for h in EXPECTED_HEADERS if h.lower() == "time"), None)
TARGET_FEATURES = ["CO(GT)", "NOx(GT)", "NO2(GT)", "C6H6(GT)", "T", "RH"]
FEATURE_CANDIDATES = [h for h in EXPECTED_HEADERS if h in TARGET_FEATURES]


def parsing_looks_wrong(df):
    if df.shape[1] <= 2:
        return True
    cols = [str(c).strip() for c in df.columns]
    if DATE_COL and DATE_COL not in cols:
        return True
    if TIME_COL and TIME_COL not in cols:
        return True
    if any(";" in c for c in cols):
        return True
    expected_count = len(EXPECTED_HEADERS)
    if expected_count and abs(df.shape[1] - expected_count) > 5:
        return True
    return False


def read_csv_with_fallback(path):
    df = pd.read_csv(path)
    if parsing_looks_wrong(df):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def clean_dataframe(df):
    df.columns = [str(c).strip() for c in df.columns]
    df = df.dropna(axis=1, how="all")
    df = df.loc[:, ~df.columns.str.contains("^Unnamed")]
    df = df.loc[:, df.columns != ""]
    return df


def convert_numeric(df):
    for col in df.columns:
        if df[col].dtype == object:
            df[col] = pd.to_numeric(
                df[col].astype(str).str.replace(",", ".", regex=False), errors="coerce"
            )
    return df


def preprocess_data(df, feature_names):
    df = clean_dataframe(df)
    if DATE_COL and TIME_COL and DATE_COL in df.columns and TIME_COL in df.columns:
        dt = pd.to_datetime(
            df[DATE_COL].astype(str).str.strip() + " " + df[TIME_COL].astype(str).str.strip(),
            dayfirst=True,
            errors="coerce",
        )
        df = df.drop(columns=[DATE_COL, TIME_COL])
        df.index = dt
        df = df.loc[~df.index.isna()]
    feature_cols = [f for f in feature_names if f in df.columns]
    if feature_cols:
        df = df[feature_cols]
    df = convert_numeric(df)
    df.replace(-200, np.nan, inplace=True)
    df = df.dropna(how="all")
    if not feature_cols:
        feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        df = df[feature_cols]
    df = df.dropna()
    if isinstance(df.index, pd.DatetimeIndex):
        df = df.resample("D").mean().dropna()
    return df, feature_cols


def create_targets(X):
    rng = np.random.default_rng(RANDOM_STATE)
    visits = X.sum(axis=1)
    visits = visits + rng.normal(0, 1, size=visits.shape[0])
    median_val = np.median(visits)
    risk_label = (visits > median_val).astype(int)
    return visits, risk_label


def train_and_evaluate(X, y_reg, y_clf):
    X_train, X_test, y_reg_train, y_reg_test, y_clf_train, y_clf_test = train_test_split(
        X, y_reg, y_clf, test_size=0.2, random_state=RANDOM_STATE
    )
    reg = RandomForestRegressor(random_state=RANDOM_STATE, n_estimators=100, n_jobs=1)
    clf = RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=100, n_jobs=1)
    reg.fit(X_train, y_reg_train)
    clf.fit(X_train, y_clf_train)
    y_reg_pred = reg.predict(X_test)
    y_clf_pred = clf.predict(X_test)
    _ = r2_score(y_reg_test, y_reg_pred)
    _ = mean_squared_error(y_reg_test, y_reg_pred, squared=False)
    accuracy = accuracy_score(y_clf_test, y_clf_pred)
    return accuracy


def main():
    df = read_csv_with_fallback(DATASET_PATH)
    df, _ = preprocess_data(df, FEATURE_CANDIDATES)
    X = df.to_numpy()
    y_reg, y_clf = create_targets(X)
    accuracy = train_and_evaluate(X, y_reg, y_clf)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()
# Optimization Summary
# - Dropped empty/unused columns early and restricted processing to required features to cut memory usage.
# - Converted only object-typed columns to numeric with vectorized replacement to avoid redundant work.
# - Resampled once at daily granularity and reused a single split for both models to reduce data movement.
# - Used NumPy arrays for target creation and model training to minimize pandas overhead.
# - Ensured deterministic results with fixed seeds and removed all plotting/logging to lower runtime.