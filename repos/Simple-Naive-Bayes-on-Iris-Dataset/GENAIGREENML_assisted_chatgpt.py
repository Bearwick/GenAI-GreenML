# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder


DATASET_HEADERS = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]
RANDOM_SEED = 42


def read_csv_robust(path: str, expected_headers: list[str]) -> pd.DataFrame:
    df = pd.read_csv(path)
    if not _looks_like_expected_schema(df, expected_headers):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _looks_like_expected_schema(df: pd.DataFrame, expected_headers: list[str]) -> bool:
    cols = [str(c).strip() for c in df.columns]
    if len(cols) < 2:
        return False
    if any(c.startswith("Unnamed") for c in cols):
        return False
    expected_set = {c.strip() for c in expected_headers}
    return len(expected_set.intersection(cols)) >= max(2, len(expected_set) // 2)


def _select_target_column(df: pd.DataFrame, expected_headers: list[str]) -> str:
    expected = {c: c for c in expected_headers}
    for c in df.columns:
        if str(c).strip() in expected and str(c).strip().lower() == "species":
            return c
    for c in df.columns:
        if str(c).strip().lower() == "species":
            return c
    return df.columns[-1]


def prepare_features_and_target(df: pd.DataFrame, expected_headers: list[str]) -> tuple[np.ndarray, np.ndarray]:
    target_col = _select_target_column(df, expected_headers)

    y_raw = df[target_col]
    le = LabelEncoder()
    y = le.fit_transform(y_raw).astype(np.int64, copy=False)

    feature_df = df.drop(columns=[target_col])

    id_like = [c for c in feature_df.columns if str(c).strip().lower() in ("id",)]
    if id_like:
        feature_df = feature_df.drop(columns=id_like)

    feature_df = feature_df.select_dtypes(include=[np.number])
    X = feature_df.to_numpy(dtype=np.float32, copy=False)

    return X, y


def compute_accuracy_for_model(model, X_train, X_test, y_train, y_test) -> float:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return float(accuracy_score(y_test, y_pred))


def main() -> None:
    df = read_csv_robust("iris.csv", DATASET_HEADERS)
    X, y = prepare_features_and_target(df, DATASET_HEADERS)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_SEED
    )

    models = (GaussianNB(), MultinomialNB(), BernoulliNB())
    accuracies = [compute_accuracy_for_model(m, X_train, X_test, y_train, y_test) for m in models]

    accuracy = accuracies[0]
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed plotting/visualization dependencies and code paths to cut runtime, memory use, and import overhead.
# - Eliminated redundant metric computations (confusion matrix/precision/recall/error rate) since only final accuracy is required.
# - Converted feature matrix to a contiguous NumPy array (float32) to reduce memory footprint and speed up model fitting/prediction.
# - Dropped non-feature identifier columns (e.g., Id) and restricted features to numeric types to avoid unnecessary data movement/casting.
# - Implemented robust CSV parsing with a fallback delimiter/decimal configuration to prevent re-runs/manual fixes.
# - Kept reproducibility via a fixed random seed in train/test split while avoiding additional nondeterministic operations.
# - Reduced intermediate structures and function overhead by using a lightweight evaluation function and list comprehension for model runs.