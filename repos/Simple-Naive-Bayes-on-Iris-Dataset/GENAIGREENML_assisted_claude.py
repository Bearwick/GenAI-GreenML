# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder

try:
    iris = pd.read_csv("iris.csv")
    if iris.shape[1] < 3:
        iris = pd.read_csv("iris.csv", sep=';', decimal=',')
except Exception:
    iris = pd.read_csv("iris.csv", sep=';', decimal=',')

if 'Id' in iris.columns:
    iris = iris.drop(columns=['Id'])

label_encoder = LabelEncoder()
iris["Species"] = label_encoder.fit_transform(iris["Species"])

X = iris.iloc[:, :-1].values
y = iris.iloc[:, -1].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Removed unused imports (matplotlib, seaborn, MultinomialNB, BernoulliNB, confusion_matrix, precision_score, recall_score).
# 2. Dropped the 'Id' column early to avoid carrying unnecessary data through the pipeline.
# 3. Converted features to numpy arrays (.values) to avoid pandas overhead during model fitting.
# 4. Kept only GaussianNB as the primary model (best accuracy per README: 1.00); removed Multinomial and Bernoulli variants that were secondary evaluations.
# 5. Removed all plotting, printing, and visualization code.
# 6. Removed the evaluate_model and plot_confusion_matrix helper functions to eliminate unused computation.
# 7. Added robust CSV parsing fallback for separator/decimal variations.
# 8. Fixed random_state=42 for reproducibility (same as original).
# 9. No artifacts saved to disk.