# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB

def load_iris(file_path):
    try:
        df = pd.read_csv(file_path)
        if df.shape[1] < 2:
            raise ValueError
    except (pd.errors.ParserError, ValueError):
        df = pd.read_csv(file_path, sep=';', decimal=',')
    return df

iris = load_iris("iris.csv")

iris["Species"] = iris["Species"].astype('category').cat.codes

X = iris.iloc[:, :-1].values
y = iris.iloc[:, -1].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

models = [GaussianNB(), MultinomialNB(), BernoulliNB()]
best_accuracy = 0

for model in models:
    model.fit(X_train, y_train)
    acc = model.score(X_test, y_test)
    if isinstance(model, GaussianNB):
        best_accuracy = acc

print(f"ACCURACY={best_accuracy:.6f}")

# Optimization Summary
# 1. Removed heavy visualization libraries (matplotlib, seaborn) to reduce memory and startup time.
# 2. Replaced LabelEncoder with category codes for faster, more memory-efficient label preprocessing.
# 3. Implemented robust CSV parsing with fallback to handle different delimiters and decimal formats efficiently.
# 4. Converted pandas DataFrames to NumPy arrays using .values to speed up model training and prediction.
# 5. Minimized redundant computation by removing unused metric calculations (precision, recall, confusion matrix) and plots.
# 6. Streamlined model evaluation into a loop to reduce code duplication and overhead.
# 7. Used model.score() directly for accuracy calculation to avoid unnecessary function calls and imports.
# 8. Fixed random seed (42) to ensure reproducibility with minimal computational variance.