# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score

def run_pipeline():
    path = "iris.csv"
    try:
        df = pd.read_csv(path)
        if df.shape[1] < 2:
            raise ValueError
    except (pd.errors.ParserError, ValueError):
        df = pd.read_csv(path, sep=';', decimal=',')

    X = df.iloc[:, :-1].values
    y = pd.factorize(df.iloc[:, -1], sort=True)[0]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    gnb = GaussianNB()
    gnb.fit(X_train, y_train)
    gnb_acc = accuracy_score(y_test, gnb.predict(X_test))

    mnb = MultinomialNB()
    mnb.fit(X_train, y_train)
    mnb.predict(X_test)

    bnb = BernoulliNB()
    bnb.fit(X_train, y_train)
    bnb.predict(X_test)

    print(f"ACCURACY={gnb_acc:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary
# 1. Removed heavy visualization dependencies (matplotlib, seaborn) to minimize memory footprint and import time.
# 2. Replaced LabelEncoder with pd.factorize(sort=True) for faster and more memory-efficient target encoding.
# 3. Utilized .values to pass NumPy arrays directly to scikit-learn, reducing dataframe overhead during model training.
# 4. Eliminated redundant metric calculations (Precision, Recall, Error Rate) and Confusion Matrix generation as they were not required for the final output.
# 5. Removed all plotting logic and external file saving to reduce I/O overhead and energy consumption.
# 6. Optimized CSV loading with a robust fallback mechanism and minimal column processing.
# 7. Ensured reproducibility by maintaining the fixed random seed (42).
# 8. Streamlined model execution flow to avoid redundant function calls and intermediate data structures.
# 9. Reduced total runtime by skipping unnecessary prediction logging and iterative printing.