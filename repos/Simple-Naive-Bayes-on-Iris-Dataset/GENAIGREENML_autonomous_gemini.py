# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, r2_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# Robust CSV loading
def load_data(path):
    try:
        df = pd.read_csv(path)
        if len(df.columns) <= 1:
            raise ValueError
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    
    # Normalize column names
    df.columns = [" ".join(str(c).strip().split()) for c in df.columns]
    df = df.drop(columns=[c for c in df.columns if 'Unnamed' in c], errors='ignore')
    return df

# Initialize data
DATA_PATH = 'iris.csv'
df = load_data(DATA_PATH)

# Identify target and features
potential_targets = ['Species', 'target', 'class', 'label', 'type']
target_col = None

for col in df.columns:
    if col.lower() in [t.lower() for t in potential_targets]:
        target_col = col
        break

if target_col is None:
    target_col = df.columns[-1]

# Feature selection: drop ID columns and target
id_keywords = ['id', 'index', 'row']
features = [c for c in df.columns if c != target_col and not any(k in c.lower() for k in id_keywords)]

# If no numeric features found, try to find any numeric column
X_raw = df[features]
y_raw = df[target_col]

# Pre-processing target: Handle non-numeric or continuous
is_classification = True
if y_raw.dtype == 'object' or len(y_raw.unique()) < len(y_raw) * 0.05:
    le = LabelEncoder()
    y = le.fit_transform(y_raw.astype(str))
    is_classification = True
else:
    # Regression fallback if target looks continuous
    y = pd.to_numeric(y_raw, errors='coerce').fillna(0).values
    is_classification = False

# Filter features: only numeric for this lightweight pipeline
numeric_features = []
for col in features:
    try:
        df[col] = pd.to_numeric(df[col], errors='coerce')
        if not df[col].isna().all():
            numeric_features.append(col)
    except:
        continue

# Defensive check
if not numeric_features:
    numeric_features = features

X = df[numeric_features].copy()
# Handle potential Inf/NaN in features
X = X.replace([np.inf, -np.inf], np.nan)

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Verify data exists
if X_train.empty:
    accuracy = 0.0
else:
    if is_classification:
        # Energy-efficient Gaussian Naive Bayes
        pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler()),
            ('model', GaussianNB())
        ])
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        # Fallback to simple Linear Regression for continuous target
        from sklearn.linear_model import LinearRegression
        pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler()),
            ('model', LinearRegression())
        ])
        pipeline.fit(X_train, y_train)
        score = pipeline.score(X_test, y_test)
        # Map R^2 to a 0-1 range for the accuracy proxy
        accuracy = max(0, min(1, score))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary:
# 1. Model Choice: Utilized Gaussian Naive Bayes (O(n_features * n_samples)), which is highly CPU-efficient and has low memory overhead.
# 2. Resource Usage: Avoided computationally expensive ensembles (Random Forest/XGBoost) or deep learning, making the solution suitable for green environments.
# 3. Robustness: Implemented a robust CSV parser and dynamic schema inference to ensure end-to-end execution without manual intervention.
# 4. Preprocessing: Used a simple Pipeline with median imputation and scaling to minimize the training CO2 footprint while maintaining stability.
# 5. Fallback Logic: Included a regression path with an R2-to-accuracy proxy to handle unexpected target data types safely.
# 6. Efficiency: Only processed numeric features and dropped identifier columns (like 'Id') to reduce redundant matrix computations.