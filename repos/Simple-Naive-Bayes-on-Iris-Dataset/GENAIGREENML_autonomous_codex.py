# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, r2_score

DATASET_PATH = "iris.csv"
DATASET_HEADERS = "Id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species"
expected_headers = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]

def read_csv_robust(path, expected_headers):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is None or df.empty:
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df = pd.DataFrame()
    else:
        if df.shape[1] == 1 or any(df.columns.astype(str).str.contains(";")):
            try:
                df2 = pd.read_csv(path, sep=";", decimal=",")
                if df2.shape[1] > df.shape[1]:
                    df = df2
            except Exception:
                pass
    if df.shape[1] == 1 and len(expected_headers) > 1:
        try:
            df2 = pd.read_csv(path, header=None, names=expected_headers)
            if df2.shape[1] == len(expected_headers):
                df = df2
        except Exception:
            pass
    return df

def normalize_columns(df):
    df.columns = [re.sub(r"\s+", " ", str(c).strip()) for c in df.columns]
    return df

def choose_target(df, expected_headers):
    target = None
    candidates = []
    if expected_headers:
        candidates.append(expected_headers[-1])
    candidates.extend(["target", "label", "class", "species", "y"])
    cols_lower = {str(c).lower(): c for c in df.columns}
    for cand in candidates:
        if cand is None:
            continue
        key = str(cand).lower()
        if key in cols_lower:
            target = cols_lower[key]
            break
    if target is None and len(df.columns) > 0:
        target = df.columns[-1]
    def non_constant(col):
        return df[col].dropna().nunique() > 1
    if target is not None and not non_constant(target):
        numeric_candidates = []
        for col in df.columns:
            if col == target:
                continue
            s = pd.to_numeric(df[col], errors="coerce")
            if s.notna().mean() > 0.5 and s.dropna().nunique() > 1:
                numeric_candidates.append(col)
        if numeric_candidates:
            target = numeric_candidates[-1]
        else:
            for col in df.columns:
                if non_constant(col):
                    target = col
                    break
    if target is None and len(df.columns) > 0:
        target = df.columns[-1]
    return target

df = read_csv_robust(DATASET_PATH, expected_headers)
if df is None:
    df = pd.DataFrame()

df = normalize_columns(df)

if expected_headers and len(expected_headers) == df.shape[1]:
    if all([str(c).isdigit() for c in df.columns]):
        df.columns = expected_headers

df = df.loc[:, ~df.columns.astype(str).str.match(r"^Unnamed", case=False)]
df = normalize_columns(df)

assert df is not None and not df.empty

target = choose_target(df, expected_headers)
if target not in df.columns:
    target = df.columns[-1]

y_raw = df[target]
mask = ~y_raw.isna()
dtype_str = str(y_raw.dtype)
is_obj_dtype = y_raw.dtype == object or dtype_str.startswith("category") or dtype_str == "string"
if is_obj_dtype:
    mask &= y_raw.astype(str).str.strip().ne("")
df = df.loc[mask].copy()
y_raw = df[target]

assert len(df) > 0

y_num = pd.to_numeric(y_raw, errors="coerce")
num_ratio = y_num.notna().mean()
if is_obj_dtype and num_ratio < 0.9:
    is_classification = True
else:
    unique_count = y_num.dropna().nunique()
    if unique_count <= max(20, int(len(y_num) * 0.05)):
        is_classification = True
    else:
        is_classification = False
if is_classification and y_raw.nunique(dropna=True) < 2:
    is_classification = False

if is_classification:
    y = y_raw.astype(str).str.strip()
else:
    y_num = pd.to_numeric(y_raw, errors="coerce")
    mask = y_num.notna()
    df = df.loc[mask].copy()
    y = y_num.loc[mask].astype(float)

assert len(df) > 0

orig_feature_cols = [c for c in df.columns if c != target]

def is_id_like(col):
    name = str(col).lower()
    if "id" in name:
        series = df[col]
        if series.nunique(dropna=True) >= 0.9 * len(series):
            return True
    return False

feature_cols = [c for c in orig_feature_cols if not is_id_like(c)]
if not feature_cols:
    feature_cols = orig_feature_cols

X = df[feature_cols].copy()

all_nan_cols = [col for col in X.columns if X[col].isna().all()]
if all_nan_cols:
    X = X.drop(columns=all_nan_cols)
    feature_cols = [c for c in feature_cols if c not in all_nan_cols]

numeric_cols = []
categorical_cols = []
for col in feature_cols:
    conv = pd.to_numeric(X[col], errors="coerce")
    non_nan_ratio = conv.notna().mean()
    if non_nan_ratio >= 0.8:
        X[col] = conv
        numeric_cols.append(col)
    else:
        categorical_cols.append(col)

numeric_cols = [col for col in numeric_cols if X[col].notna().any()]
categorical_cols = [col for col in categorical_cols if X[col].notna().any()]

if not numeric_cols and not categorical_cols:
    X = pd.DataFrame({"dummy": np.ones(len(df))})
    numeric_cols = ["dummy"]
    categorical_cols = []

if numeric_cols:
    X[numeric_cols] = X[numeric_cols].replace([np.inf, -np.inf], np.nan)

if is_classification:
    if len(categorical_cols) == 0:
        model = GaussianNB()
        use_scaler = False
    else:
        model = LogisticRegression(max_iter=200, solver="liblinear", multi_class="ovr")
        use_scaler = True
else:
    model = Ridge(alpha=1.0)
    use_scaler = True

transformers = []
if numeric_cols:
    num_steps = [("imputer", SimpleImputer(strategy="median"))]
    if use_scaler:
        num_steps.append(("scaler", StandardScaler()))
    numeric_transformer = Pipeline(steps=num_steps)
    transformers.append(("num", numeric_transformer, numeric_cols))
if categorical_cols:
    cat_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=False))
    ])
    transformers.append(("cat", cat_transformer, categorical_cols))

preprocess = ColumnTransformer(transformers=transformers, remainder="drop")
clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])

assert len(X) == len(y) and len(X) > 0

n_samples = len(X)
if n_samples < 2:
    X_train = X.copy()
    X_test = X.copy()
    y_train = y.copy()
    y_test = y.copy()
else:
    test_size = 0.2
    if n_samples < 5:
        test_size = 0.5
    if is_classification:
        n_classes = pd.Series(y).nunique(dropna=True)
        stratify = y if n_classes > 1 else None
    else:
        stratify = None
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=stratify
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=None
        )

assert len(X_train) > 0 and len(X_test) > 0

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if is_classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    if np.isnan(r2):
        accuracy = 0.0
    else:
        accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Selected lightweight models (GaussianNB/LogisticRegression/Ridge) based on feature types to keep CPU usage low.
# - Used a simple, reproducible preprocessing pipeline with imputers and optional scaling/one-hot encoding.
# - For regression fallback, mapped R2 to a bounded [0,1] accuracy proxy via (r2+1)/2 to keep metrics stable.