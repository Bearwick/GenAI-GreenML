# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.base import BaseEstimator, TransformerMixin


RANDOM_STATE = 42
DATASET_PATH = "iris.csv"


class DataFrameToNumpy(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        # Ensures compatibility with estimators that prefer dense numpy arrays (e.g., GaussianNB)
        if hasattr(X, "toarray"):
            X = X.toarray()
        return np.asarray(X)


def _read_csv_robust(path):
    # Try default read
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d):
        if d is None or not isinstance(d, pd.DataFrame) or d.shape[0] == 0:
            return True
        # If only one column, likely wrong separator; or if many columns unnamed
        if d.shape[1] == 1:
            return True
        if sum(str(c).startswith("Unnamed:") for c in d.columns) >= max(1, d.shape[1] // 2):
            return True
        return False

    if _looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            # Final fallback: try python engine with automatic separator sniffing
            df = pd.read_csv(path, sep=None, engine="python")
    return df


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c = str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        normed.append(c)
    return normed


def _drop_unnamed(df):
    keep_cols = [c for c in df.columns if not str(c).strip().startswith("Unnamed:")]
    return df[keep_cols]


def _coerce_numeric_inplace(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")


def _choose_target(df, dataset_headers=None):
    cols = list(df.columns)
    lower_map = {c.lower(): c for c in cols}

    # Prefer provided/known target names if present (schema-robust)
    candidates = []
    if dataset_headers:
        for name in dataset_headers:
            if name.lower() in lower_map:
                candidates.append(lower_map[name.lower()])

    # Common target-like names
    for name in ["target", "label", "class", "species", "y"]:
        if name in lower_map:
            candidates.append(lower_map[name])

    # Pick first viable: non-null, non-constant
    for c in candidates:
        s = df[c]
        if s.notna().sum() > 0 and s.nunique(dropna=True) > 1:
            return c

    # Fallback: choose a non-constant object column if exists
    obj_cols = [c for c in cols if df[c].dtype == "object"]
    for c in obj_cols:
        s = df[c]
        if s.notna().sum() > 0 and s.nunique(dropna=True) > 1:
            return c

    # Fallback: choose a non-constant numeric column if exists
    # Coerce numerics to check uniqueness safely
    for c in cols:
        tmp = pd.to_numeric(df[c], errors="coerce")
        if tmp.notna().sum() > 0 and tmp.nunique(dropna=True) > 1:
            return c

    # Last resort: first column
    return cols[0]


def _bounded_regression_score(y_true, y_pred):
    # Stable [0,1] proxy: 1 / (1 + MAE / (IQR + eps))
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = np.nanmean(np.abs(y_true - y_pred))
    q75, q25 = np.nanpercentile(y_true, [75, 25])
    iqr = float(q75 - q25)
    eps = 1e-12
    denom = iqr + eps
    score = 1.0 / (1.0 + (mae / denom))
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    dataset_headers = ["Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"]

    if not os.path.exists(DATASET_PATH):
        # Minimal hard fallback: empty -> accuracy 0
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Defensive: ensure non-empty
    df = df.copy()
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _choose_target(df, dataset_headers=dataset_headers)

    # Remove obvious ID-like columns from features if present
    cols_lower = {c.lower(): c for c in df.columns}
    id_like = set()
    for name in ["id", "index"]:
        if name in cols_lower:
            id_like.add(cols_lower[name])
    # Also drop columns that are unique for nearly all rows (high-cardinality identifiers)
    for c in df.columns:
        if c == target_col:
            continue
        nun = df[c].nunique(dropna=True)
        if df.shape[0] > 0 and nun >= int(0.98 * df.shape[0]):
            id_like.add(c)

    feature_cols = [c for c in df.columns if c != target_col and c not in id_like]
    if len(feature_cols) == 0:
        # Fallback: use all except target
        feature_cols = [c for c in df.columns if c != target_col]

    X = df[feature_cols].copy()
    y = df[target_col].copy()

    # Replace inf with nan for safety
    X = X.replace([np.inf, -np.inf], np.nan)
    if isinstance(y, pd.Series):
        y = y.replace([np.inf, -np.inf], np.nan)

    # Determine task type: classification if target is object or few discrete values
    y_non_null = y.dropna()
    is_classification = False
    if y.dtype == "object":
        is_classification = True
    else:
        y_num = pd.to_numeric(y, errors="coerce")
        unique_vals = y_num.dropna().unique()
        if len(unique_vals) > 0 and len(unique_vals) <= 20:
            # Heuristic: small number of unique numeric values => likely classification labels
            is_classification = True

    # If classification but only one class, fallback to regression score or trivial accuracy
    if is_classification:
        y_cls = y.astype("object")
        # Drop rows where target missing
        valid_mask = y_cls.notna()
        X = X.loc[valid_mask].copy()
        y_cls = y_cls.loc[valid_mask].copy()
        assert X.shape[0] > 1

        n_classes = y_cls.nunique(dropna=True)
        if n_classes < 2:
            # Trivial baseline: always predict the only class
            accuracy = 1.0
            print(f"ACCURACY={accuracy:.6f}")
            return

        # Split with stratify when possible
        stratify = y_cls if n_classes > 1 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_cls, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
        )
        assert X_train.shape[0] > 0 and X_test.shape[0] > 0

        # Identify numeric/categorical columns robustly (attempt numeric coercion)
        numeric_cols = []
        categorical_cols = []
        for c in X_train.columns:
            s = X_train[c]
            if pd.api.types.is_numeric_dtype(s):
                numeric_cols.append(c)
            else:
                s_num = pd.to_numeric(s, errors="coerce")
                if s_num.notna().mean() >= 0.8:
                    numeric_cols.append(c)
                    _coerce_numeric_inplace(X_train, [c])
                    _coerce_numeric_inplace(X_test, [c])
                else:
                    categorical_cols.append(c)

        # Preprocess
        numeric_transformer = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="median")),
            ]
        )
        categorical_transformer = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False)),
            ]
        )

        preprocessor = ColumnTransformer(
            transformers=[
                ("num", numeric_transformer, numeric_cols),
                ("cat", categorical_transformer, categorical_cols),
            ],
            remainder="drop",
            sparse_threshold=0.0,
        )

        # Prefer GaussianNB for classic Iris-like numeric features; use LogisticRegression if one-hot exists
        use_nb = (len(categorical_cols) == 0)
        if use_nb:
            clf = Pipeline(
                steps=[
                    ("preprocess", preprocessor),
                    ("to_numpy", DataFrameToNumpy()),
                    ("model", GaussianNB()),
                ]
            )
        else:
            clf = Pipeline(
                steps=[
                    ("preprocess", preprocessor),
                    ("scale", StandardScaler(with_mean=True, with_std=True)),
                    ("model", LogisticRegression(max_iter=200, solver="lbfgs", n_jobs=1)),
                ]
            )

        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Regression fallback path
    y_num = pd.to_numeric(y, errors="coerce")
    valid_mask = y_num.notna()
    X = X.loc[valid_mask].copy()
    y_num = y_num.loc[valid_mask].copy()
    assert X.shape[0] > 1

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_num, test_size=0.2, random_state=RANDOM_STATE
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    numeric_cols = []
    categorical_cols = []
    for c in X_train.columns:
        s = X_train[c]
        if pd.api.types.is_numeric_dtype(s):
            numeric_cols.append(c)
        else:
            s_num = pd.to_numeric(s, errors="coerce")
            if s_num.notna().mean() >= 0.8:
                numeric_cols.append(c)
                _coerce_numeric_inplace(X_train, [c])
                _coerce_numeric_inplace(X_test, [c])
            else:
                categorical_cols.append(c)

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scale", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.0,
    )

    reg = Pipeline(
        steps=[
            ("preprocess", preprocessor),
            ("to_numpy", DataFrameToNumpy()),
            ("model", Ridge(alpha=1.0, random_state=RANDOM_STATE)),
        ]
    )

    reg.fit(X_train, y_train)
    y_pred = reg.predict(X_test)
    accuracy = _bounded_regression_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses simple, CPU-friendly models: GaussianNB for numeric classification; LogisticRegression only when categoricals require one-hot; Ridge for regression fallback.
# - Employs sklearn Pipeline + ColumnTransformer to avoid redundant preprocessing and ensure reproducibility.
# - Robust CSV parsing with fallback separator/decimal to prevent expensive manual fixes and reruns.
# - Minimal feature engineering: median/mode imputation and optional standardization for linear models.
# - Defensive schema handling: normalized headers, drops Unnamed columns, avoids ID-like columns, and selects a valid target even if headers differ.
# - Regression fallback reports a bounded [0,1] "accuracy" proxy: 1/(1+MAE/IQR) for stability across scales.