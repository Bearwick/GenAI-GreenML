# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import pickle
import random
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

SEED = 42
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)

DATASET_PATH = "dict.pickle"

def read_csv_with_fallback(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is None or df.shape[1] <= 1:
        try:
            df_alt = pd.read_csv(path, sep=";", decimal=",")
            if df is None or df_alt.shape[1] >= df.shape[1]:
                df = df_alt
        except Exception:
            pass
    return df

def get_header_list(df):
    headers = globals().get("DATASET_HEADERS")
    if headers is None:
        return list(df.columns)
    try:
        headers_list = list(headers)
    except Exception:
        return list(df.columns)
    if len(headers_list) != df.shape[1]:
        return list(df.columns)
    return headers_list

def detect_label_column(df):
    headers = get_header_list(df)
    candidates = {"label", "labels", "target", "class", "y"}
    for idx, name in enumerate(headers):
        if str(name).strip().lower() in candidates:
            return df.columns[idx]
    return df.columns[-1]

def extract_xy_from_dataframe(df):
    label_col = detect_label_column(df)
    y_series = df[label_col]
    X_df = df.drop(columns=[label_col])
    X = X_df.to_numpy(copy=False)
    y = y_series.to_numpy(copy=False)
    if not np.issubdtype(X.dtype, np.number):
        X = X_df.apply(pd.to_numeric, errors="coerce").to_numpy(dtype=float)
    else:
        X = X.astype(float, copy=False)
    if not np.issubdtype(y.dtype, np.number):
        y = pd.to_numeric(y_series, errors="coerce").to_numpy(dtype=float)
    else:
        y = y.astype(float, copy=False)
    return X, y

def extract_xy_from_dict(d):
    key_pairs = [
        ("data", "target"),
        ("X", "y"),
        ("x", "y"),
        ("features", "labels"),
        ("feature", "label"),
        ("inputs", "output"),
        ("inputs", "outputs"),
        ("input", "output"),
    ]
    for x_key, y_key in key_pairs:
        if x_key in d and y_key in d:
            return d[x_key], d[y_key]
    train_pairs = [
        ("X_train", "y_train"),
        ("x_train", "y_train"),
        ("train_X", "train_y"),
        ("train_data", "train_labels"),
        ("features_train", "labels_train"),
    ]
    for x_key, y_key in train_pairs:
        if x_key in d and y_key in d:
            return d[x_key], d[y_key]
    for value in d.values():
        arr = np.asarray(value)
        if arr.ndim == 2 and arr.shape[1] >= 2:
            return arr[:, :-1], arr[:, -1]
    return None, None

def extract_xy(obj):
    if isinstance(obj, pd.DataFrame):
        return extract_xy_from_dataframe(obj)
    if isinstance(obj, dict):
        return extract_xy_from_dict(obj)
    if isinstance(obj, (list, tuple)):
        if len(obj) == 2:
            return obj[0], obj[1]
        arr = np.asarray(obj)
        if arr.ndim == 2 and arr.shape[1] >= 2:
            return arr[:, :-1], arr[:, -1]
    if hasattr(obj, "data") and hasattr(obj, "target"):
        return obj.data, obj.target
    if isinstance(obj, np.ndarray):
        if obj.ndim == 2 and obj.shape[1] >= 2:
            return obj[:, :-1], obj[:, -1]
    return None, None

def extract_xy_from_model(model):
    for x_attr, y_attr in (("_fit_X", "_y"), ("X_", "y_"), ("data", "target")):
        if hasattr(model, x_attr) and hasattr(model, y_attr):
            return getattr(model, x_attr), getattr(model, y_attr)
    return None, None

def load_dataset(path):
    obj = None
    if os.path.exists(path):
        try:
            with open(path, "rb") as f:
                obj = pickle.load(f)
        except Exception:
            obj = None
    X = y = None
    if obj is not None:
        X, y = extract_xy(obj)
        if (X is None or y is None) and callable(getattr(obj, "predict", None)):
            X, y = extract_xy_from_model(obj)
    if (X is None or y is None) and os.path.exists(path):
        df = read_csv_with_fallback(path)
        if df is not None:
            X, y = extract_xy_from_dataframe(df)
    return X, y

def to_float_array(arr):
    a = np.asarray(arr)
    if a.ndim == 0:
        a = a.reshape(1)
    try:
        return a.astype(float, copy=False)
    except Exception:
        if a.ndim == 1:
            return pd.to_numeric(a, errors="coerce").to_numpy(dtype=float)
        return pd.DataFrame(a).apply(pd.to_numeric, errors="coerce").to_numpy(dtype=float)

def normalize_labels(y):
    y = np.asarray(y, dtype=float).reshape(-1)
    return np.where(y > 0, 1, np.where(y < 0, -1, 0))

def prepare_xy(X, y):
    X = to_float_array(X)
    if X.ndim == 1:
        X = X.reshape(-1, 1)
    y = to_float_array(y).reshape(-1)
    if X.shape[0] != y.shape[0]:
        min_len = min(X.shape[0], y.shape[0])
        X = X[:min_len]
        y = y[:min_len]
    X = np.nan_to_num(X, copy=False)
    y = normalize_labels(y)
    return X, y

def evaluate_model(X, y, seed=SEED):
    n_samples = X.shape[0]
    if n_samples == 0:
        return 0.0
    if n_samples < 2:
        return 1.0
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed
    )
    if y_train.size == 0 or y_test.size == 0:
        return 1.0
    n_neighbors = min(4, y_train.size)
    model = KNeighborsClassifier(n_neighbors=n_neighbors)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return float(accuracy_score(y_test, y_pred))

def main():
    X, y = load_dataset(DATASET_PATH)
    if X is None or y is None:
        accuracy = 0.0
    else:
        X, y = prepare_xy(X, y)
        accuracy = evaluate_model(X, y, SEED)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Replaced manual row parsing with vectorized pandas loading and a delimiter fallback to cut Python-level loops.
# - Centralized dataset extraction for multiple formats to avoid redundant conversions and data movement.
# - Used NumPy-based numeric coercion and label normalization to minimize intermediate structures.
# - Added fixed random seeds and deterministic splitting for reproducible evaluation.
# - Removed unused training/prediction side effects and avoided artifact saving to reduce I/O.