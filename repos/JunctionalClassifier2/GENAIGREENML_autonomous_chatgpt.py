# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import pickle
import warnings
from typing import Tuple, Optional, List

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")


DATASET_PATH = "dict.pickle"
RANDOM_STATE = 42


def _normalize_columns(cols: List[str]) -> List[str]:
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed_columns(df: pd.DataFrame) -> pd.DataFrame:
    drop_cols = [c for c in df.columns if isinstance(c, str) and c.strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _safe_read_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    df.columns = _normalize_columns(list(df.columns))
    df = _drop_unnamed_columns(df)

    # Heuristic: if only 1 column and it contains separators, parsing likely wrong; retry with European format.
    if df.shape[1] <= 1:
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            df2.columns = _normalize_columns(list(df2.columns))
            df2 = _drop_unnamed_columns(df2)
            if df2.shape[1] > df.shape[1]:
                df = df2
        except Exception:
            pass
    return df


def _to_dataframe(obj) -> pd.DataFrame:
    if isinstance(obj, pd.DataFrame):
        df = obj.copy()
        df.columns = _normalize_columns(list(df.columns))
        return _drop_unnamed_columns(df)

    if isinstance(obj, dict):
        # Prefer dict of columns -> lists/arrays; otherwise attempt to find nested DataFrame-like content.
        try:
            df = pd.DataFrame(obj)
            df.columns = _normalize_columns(list(df.columns))
            return _drop_unnamed_columns(df)
        except Exception:
            # Search for first DataFrame-like value
            for v in obj.values():
                if isinstance(v, pd.DataFrame):
                    df = v.copy()
                    df.columns = _normalize_columns(list(df.columns))
                    return _drop_unnamed_columns(df)
            # As a last resort, try to represent dict as rows
            df = pd.json_normalize(obj)
            df.columns = _normalize_columns(list(df.columns))
            return _drop_unnamed_columns(df)

    if isinstance(obj, (list, tuple, np.ndarray)):
        # If list of dicts/records
        try:
            df = pd.DataFrame(obj)
            df.columns = _normalize_columns(list(df.columns))
            return _drop_unnamed_columns(df)
        except Exception:
            pass

    # Final fallback: attempt string path
    if isinstance(obj, str) and os.path.exists(obj) and obj.lower().endswith(".csv"):
        return _safe_read_csv(obj)

    raise ValueError("Unsupported dataset format for conversion to DataFrame.")


def _load_dataset(path: str) -> pd.DataFrame:
    if not os.path.exists(path):
        # If dict.pickle missing but a CSV is present, try it.
        for fname in os.listdir("."):
            if fname.lower().endswith(".csv"):
                df = _safe_read_csv(fname)
                if df.shape[0] > 0 and df.shape[1] > 0:
                    return df
        raise FileNotFoundError(f"Dataset not found: {path}")

    if path.lower().endswith(".csv"):
        return _safe_read_csv(path)

    with open(path, "rb") as f:
        obj = pickle.load(f)

    # If pickle contains a path to a CSV, follow it.
    if isinstance(obj, str) and os.path.exists(obj) and obj.lower().endswith(".csv"):
        return _safe_read_csv(obj)

    return _to_dataframe(obj)


def _coerce_numeric_inplace(df: pd.DataFrame) -> pd.DataFrame:
    # Try to convert object columns that look numeric to numeric.
    for c in df.columns:
        if df[c].dtype == "object":
            s = df[c]
            # Only attempt conversion if it doesn't look like free text (high unique ratio).
            nunique = s.nunique(dropna=True)
            if len(s) == 0:
                continue
            if nunique <= max(50, int(0.5 * len(s))):
                df[c] = pd.to_numeric(s, errors="coerce")
    return df


def _choose_target(df: pd.DataFrame) -> Tuple[str, str]:
    # Prefer common label names if present.
    lowered = {c.lower(): c for c in df.columns}
    for candidate in ["label", "class", "target", "y", "output", "junction", "junctional", "state"]:
        if candidate in lowered:
            col = lowered[candidate]
            # decide type
            if pd.api.types.is_numeric_dtype(df[col]):
                return col, "auto"
            else:
                return col, "classification"

    # Otherwise prefer a non-constant column with low cardinality for classification.
    best_cls = None
    best_cls_card = None
    for c in df.columns:
        if df[c].nunique(dropna=True) < 2:
            continue
        if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_bool_dtype(df[c]):
            card = df[c].nunique(dropna=True)
            if best_cls is None or card < best_cls_card:
                best_cls = c
                best_cls_card = card
    if best_cls is not None and best_cls_card is not None and best_cls_card <= 20:
        return best_cls, "classification"

    # Next: numeric column that looks like class labels (few unique integers)
    best_num_cls = None
    best_num_cls_card = None
    for c in df.columns:
        if not pd.api.types.is_numeric_dtype(df[c]):
            continue
        s = df[c]
        if s.nunique(dropna=True) < 2:
            continue
        card = s.nunique(dropna=True)
        if card <= 20:
            best_num_cls = c
            best_num_cls_card = card
            break
    if best_num_cls is not None:
        return best_num_cls, "classification"

    # Fallback: choose a numeric target for regression (prefer last numeric col with variance).
    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    for c in reversed(numeric_cols):
        if df[c].nunique(dropna=True) >= 2:
            return c, "regression"

    # Final fallback: use first column as target (will force trivial baseline)
    return df.columns[-1], "classification"


def _build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:
    num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    cat_cols = [c for c in X.columns if c not in num_cols]

    numeric_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    pre = ColumnTransformer(
        transformers=[
            ("num", numeric_pipe, num_cols),
            ("cat", categorical_pipe, cat_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )
    return pre


def _bounded_regression_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    denom = np.sum((y_true - np.mean(y_true)) ** 2)
    if not np.isfinite(denom) or denom <= 0:
        # No variance: perfect if predictions match else 0
        err = np.mean(np.abs(y_true - y_pred))
        return 1.0 if err == 0 else 0.0
    ss_res = np.sum((y_true - y_pred) ** 2)
    r2 = 1.0 - (ss_res / denom)
    if not np.isfinite(r2):
        r2 = -1.0
    # Map to [0,1], clipping to keep stability.
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    df = _load_dataset(DATASET_PATH)
    assert isinstance(df, pd.DataFrame)
    df.columns = _normalize_columns(list(df.columns))
    df = _drop_unnamed_columns(df)
    df = df.copy()

    # Basic cleanup
    df.replace([np.inf, -np.inf], np.nan, inplace=True)

    # Coerce likely numeric columns safely
    df = _coerce_numeric_inplace(df)

    # Drop fully empty columns/rows
    df.dropna(axis=1, how="all", inplace=True)
    df.dropna(axis=0, how="all", inplace=True)
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col, preferred_task = _choose_target(df)

    # If target missing due to weird normalization, pick last column
    if target_col not in df.columns:
        target_col = df.columns[-1]

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # If no features, create a constant feature to keep pipeline alive.
    if X.shape[1] == 0:
        X = pd.DataFrame({"__bias__": np.ones(len(df), dtype=float)})

    # Determine task
    task = preferred_task
    if task == "auto":
        if pd.api.types.is_numeric_dtype(y_raw):
            nunique = y_raw.nunique(dropna=True)
            task = "classification" if nunique <= 20 else "regression"
        else:
            task = "classification"

    # Prepare y
    if task == "classification":
        y = y_raw.copy()
        if pd.api.types.is_numeric_dtype(y):
            # Keep as is; but remove NaNs
            pass
        else:
            y = y.astype(str)
            y = y.where(~y_raw.isna(), other=np.nan)
        mask = ~pd.isna(y)
        X = X.loc[mask].reset_index(drop=True)
        y = pd.Series(y.loc[mask]).reset_index(drop=True)
    else:
        y = pd.to_numeric(y_raw, errors="coerce")
        mask = np.isfinite(y.to_numpy(dtype=float, copy=False))
        X = X.loc[mask].reset_index(drop=True)
        y = pd.Series(y.loc[mask]).reset_index(drop=True)

    assert len(X) > 0 and len(y) > 0

    # If too few samples, fallback to trivial baseline score
    if len(y) < 4:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Train/test split (stratify only if feasible)
    stratify = None
    if task == "classification":
        n_classes = y.nunique(dropna=True)
        if n_classes >= 2:
            vc = y.value_counts(dropna=True)
            if (vc.min() >= 2) and (len(y) >= 10):
                stratify = y

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=RANDOM_STATE,
        stratify=stratify,
    )
    assert len(X_train) > 0 and len(X_test) > 0

    preprocessor = _build_preprocessor(X_train)

    if task == "classification":
        # If <2 classes in training, use DummyClassifier and still evaluate
        if pd.Series(y_train).nunique(dropna=True) < 2:
            model = DummyClassifier(strategy="most_frequent")
        else:
            model = LogisticRegression(
                solver="liblinear",
                max_iter=200,
                random_state=RANDOM_STATE,
            )
        clf = Pipeline(steps=[("pre", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Simple linear regressor; if degenerate, fallback to DummyRegressor
        if np.nanstd(np.asarray(y_train, dtype=float)) <= 0:
            reg_model = DummyRegressor(strategy="mean")
        else:
            reg_model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        reg = Pipeline(steps=[("pre", preprocessor), ("model", reg_model)])
        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        accuracy = _bounded_regression_score(y_test.to_numpy(dtype=float), np.asarray(y_pred, dtype=float))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight CPU-friendly models (LogisticRegression(liblinear) / Ridge) instead of ensembles/deep nets to reduce compute/energy.
# - Robust schema handling: normalizes headers, drops 'Unnamed:' columns, auto-selects a usable target, and falls back to trivial baselines if needed.
# - ColumnTransformer + Pipeline ensures single-pass preprocessing and reproducibility; OneHotEncoder(handle_unknown='ignore') avoids costly manual feature alignment.
# - Defensive numeric coercion (errors='coerce') and NaN/inf handling prevents failures and avoids heavy data cleaning iterations.
# - For regression fallback, reports a bounded proxy accuracy in [0,1] via clipped (R2+1)/2 to keep a stable "ACCURACY" metric.