# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline

# Load a standard small-scale classification dataset
data = load_breast_cancer()
X, y = data.data, data.target

# Split data with a fixed seed for reproducibility without excessive shuffling
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Use a pipeline to streamline preprocessing and model execution
# StandardScaler is essential for linear model convergence speed
# LogisticRegression is chosen for its extremely low computational footprint and high efficiency on CPU
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression(
        penalty='l2', 
        max_iter=1000, 
        solver='lbfgs', 
        tol=1e-4, 
        class_weight='balanced'
    ))
])

# Fit model using lbfgs solver which is memory-efficient and fast for small datasets
pipeline.fit(X_train, y_train)

# Generate predictions
predictions = pipeline.predict(X_test)

# Calculate performance metric
accuracy = accuracy_score(y_test, predictions)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Model Selection: Logistic Regression was selected over Deep Learning or Ensemble methods to minimize CO2 footprint and CPU cycles.
# 2. Convergence Efficiency: Feature scaling via StandardScaler ensures the solver reaches the global optimum with fewer iterations.
# 3. Memory Management: The use of Scikit-Learn Pipelines prevents data leakage and ensures efficient memory allocation during the transformation-estimation sequence.
# 4. Hardware Independence: The solution relies on vectorized NumPy operations, optimized for standard CPUs without requiring GPU/TPU acceleration.
# 5. Algorithmic Complexity: The LBFGS solver provides an O(n) memory complexity, making it ideal for energy-constrained environments.