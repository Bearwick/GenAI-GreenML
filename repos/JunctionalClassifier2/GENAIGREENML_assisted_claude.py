# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import csv
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics

def load_csv(filename):
    features = []
    labels = []
    with open(filename, newline='') as csvfile:
        reader = csv.reader(csvfile)
        next(reader)
        for row in reader:
            cols = [c for c in row if c != '']
            vals = list(map(float, cols))
            label_val = vals.pop()
            if label_val > 0:
                label_val = 1.0
            elif label_val < 0:
                label_val = -1.0
            labels.append(label_val)
            features.append(vals)
    return np.array(features, dtype=np.float64), np.array(labels, dtype=np.float64)

def train_and_evaluate():
    X, y = load_csv('14k.csv')
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
    classifier = KNeighborsClassifier(n_neighbors=4, algorithm='ball_tree')
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
    accuracy = metrics.accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")

train_and_evaluate()

# OPTIMIZATION SUMMARY
# 1. Removed unused imports (svm, pickle, copy, unittest) to reduce load time and memory.
# 2. Removed global mutable state (features, labels, pFeatures) in favor of local returns.
# 3. Removed model saving/loading (pickle), predict(), takeInput(), and unittest class as not needed.
# 4. Simplified CSV parsing: used csv.reader directly without manual splitting and redundant checks.
# 5. Converted features/labels to numpy arrays upfront to avoid repeated list-to-array conversions.
# 6. Used algorithm='ball_tree' for KNN to speed up neighbor queries on medium-sized datasets.
# 7. Removed all prints, plots, and interactive inputs per requirements.
# 8. Single function call avoids unnecessary global variable manipulation and function overhead.