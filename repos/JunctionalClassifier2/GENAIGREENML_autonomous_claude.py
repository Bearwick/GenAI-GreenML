# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer

# Load dataset
df = pd.read_csv("./dataset.csv")

# Identify target column (assume last column is target)
target_col = df.columns[-1]

# Separate features and target
X = df.drop(columns=[target_col])
y = df[target_col]

# Encode target if categorical
if y.dtype == 'object' or y.dtype.name == 'category':
    le_target = LabelEncoder()
    y = le_target.fit_transform(y)

# Identify numeric and categorical columns
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()

# Impute missing values for numeric columns with median (robust to outliers)
if numeric_cols:
    num_imputer = SimpleImputer(strategy='median')
    X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])

# Encode categorical columns with LabelEncoder and impute missing with mode
label_encoders = {}
for col in categorical_cols:
    X[col] = X[col].fillna(X[col].mode()[0] if not X[col].mode().empty else 'missing')
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    label_encoders[col] = le

# Scale numeric features for consistency
scaler = StandardScaler()
X[numeric_cols] = scaler.fit_transform(X[numeric_cols])

# Drop columns with zero variance
variances = X.var()
zero_var_cols = variances[variances == 0].index.tolist()
X = X.drop(columns=zero_var_cols)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) > 1 else None
)

# Use RandomForest - efficient, no GPU needed, handles mixed features well
# Limit n_estimators and depth for energy efficiency
n_samples = X_train.shape[0]
n_features = X_train.shape[1]

# Adaptive model complexity based on dataset size
if n_samples < 500:
    n_estimators = 100
    max_depth = 10
elif n_samples < 5000:
    n_estimators = 150
    max_depth = 15
else:
    n_estimators = 200
    max_depth = 20

model = RandomForestClassifier(
    n_estimators=n_estimators,
    max_depth=max_depth,
    min_samples_split=5,
    min_samples_leaf=2,
    max_features='sqrt',
    random_state=42,
    n_jobs=-1
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Used RandomForestClassifier - lightweight, CPU-efficient, no deep learning overhead
# 2. Adaptive model complexity: n_estimators and max_depth scale with dataset size
# 3. StandardScaler + median imputation: robust preprocessing with minimal compute
# 4. LabelEncoder for categoricals: memory-efficient vs one-hot for high cardinality
# 5. Zero-variance column removal: reduces unnecessary computation
# 6. n_jobs=-1 for parallel tree fitting: maximizes CPU utilization efficiently
# 7. min_samples_split=5, min_samples_leaf=2: prevents overfitting, reduces tree depth