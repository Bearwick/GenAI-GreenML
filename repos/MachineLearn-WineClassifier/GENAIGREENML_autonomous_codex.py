# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")
import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier

def read_csv_with_fallback(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    needs_fallback = False
    if df is None:
        needs_fallback = True
    else:
        if df.shape[1] <= 1:
            needs_fallback = True
        else:
            if df.shape[1] == 1:
                col = df.columns[0]
                if isinstance(col, str) and ';' in col:
                    needs_fallback = True
    if needs_fallback:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt is not None and (df is None or df_alt.shape[1] >= df.shape[1]):
                df = df_alt
        except Exception:
            if df is None:
                df = pd.DataFrame()
    if df is None:
        df = pd.DataFrame()
    return df

def normalize_columns(df):
    cols = []
    for c in df.columns:
        if not isinstance(c, str):
            c = str(c)
        c = re.sub(r'\s+', ' ', c.strip())
        cols.append(c)
    df.columns = cols
    drop_cols = [c for c in df.columns if c.lower().startswith('unnamed')]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df

def make_onehot():
    try:
        return OneHotEncoder(handle_unknown='ignore', sparse=False)
    except TypeError:
        return OneHotEncoder(handle_unknown='ignore', sparse_output=False)

path = "data/wine_dataset.csv"
df = read_csv_with_fallback(path)
df = normalize_columns(df)
df.replace([np.inf, -np.inf], np.nan, inplace=True)
assert not df.empty

numeric_cols = []
n_rows = len(df)
for col in df.columns:
    series = pd.to_numeric(df[col], errors='coerce')
    non_na = series.notna().sum()
    if non_na > 0:
        if pd.api.types.is_numeric_dtype(df[col]) or (n_rows > 0 and non_na / n_rows >= 0.6):
            df[col] = series
            numeric_cols.append(col)

target = None
lower_map = {c.lower(): c for c in df.columns}
for name in ['quality', 'target', 'label', 'y']:
    if name in lower_map:
        target = lower_map[name]
        break
if target is None:
    candidate_numeric = [c for c in numeric_cols if df[c].nunique(dropna=True) > 1]
    if candidate_numeric:
        target = candidate_numeric[-1]
    elif numeric_cols:
        target = numeric_cols[-1]
    elif len(df.columns) > 0:
        target = df.columns[-1]
    else:
        df['_target'] = 0
        target = '_target'
        numeric_cols.append('_target')

features = [c for c in df.columns if c != target]

y = df[target]
if target in numeric_cols:
    y = pd.to_numeric(y, errors='coerce')
if y.isna().all():
    df['_target'] = 0
    target = '_target'
    y = df[target]
    if '_target' not in numeric_cols:
        numeric_cols.append('_target')
    features = [c for c in df.columns if c != target]

if not features:
    df['_dummy'] = 0
    features = ['_dummy']
    if '_dummy' not in numeric_cols:
        numeric_cols.append('_dummy')

all_nan_cols = [c for c in features if df[c].isna().all()]
if all_nan_cols:
    df = df.drop(columns=all_nan_cols)
    features = [c for c in features if c not in all_nan_cols]
    numeric_cols = [c for c in numeric_cols if c not in all_nan_cols]

if not features:
    df['_dummy'] = 0
    features = ['_dummy']
    if '_dummy' not in numeric_cols:
        numeric_cols.append('_dummy')

y = df[target]
if target in numeric_cols:
    y = pd.to_numeric(y, errors='coerce')
mask = ~y.isna()
df = df.loc[mask].copy()
y = y.loc[mask]

assert len(df) > 0

all_nan_cols = [c for c in features if df[c].isna().all()]
if all_nan_cols:
    df = df.drop(columns=all_nan_cols)
    features = [c for c in features if c not in all_nan_cols]
    numeric_cols = [c for c in numeric_cols if c not in all_nan_cols]

if not features:
    df['_dummy'] = 0
    features = ['_dummy']
    if '_dummy' not in numeric_cols:
        numeric_cols.append('_dummy')

X = df[features]

if pd.api.types.is_numeric_dtype(y):
    unique_vals = y.nunique(dropna=True)
    y_non = y.dropna()
    int_like = False
    if len(y_non) > 0:
        int_like = np.all(np.isclose(y_non, np.round(y_non)))
    if int_like and unique_vals <= 20:
        task = 'classification'
    else:
        task = 'regression'
else:
    task = 'classification'

if task == 'classification':
    if y.nunique(dropna=True) < 2:
        task = 'trivial_classification'

n_samples = len(X)
assert n_samples >= 2

test_size = 0.2 if n_samples >= 5 else 0.5
stratify = None
if task in ['classification', 'trivial_classification']:
    n_classes = y.nunique(dropna=True)
    if n_classes >= 2:
        counts = y.value_counts()
        if counts.min() >= 2 and n_samples >= n_classes * 2:
            stratify = y

try:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=stratify)
except Exception:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=None)

assert len(X_train) > 0 and len(X_test) > 0

if task == 'classification' and y_train.nunique(dropna=True) < 2:
    task = 'trivial_classification'

numeric_features = [c for c in features if c in numeric_cols and c in df.columns]
categorical_features = [c for c in features if c not in numeric_features and c in df.columns]

transformers = []
if numeric_features:
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler(with_mean=False))
    ])
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', make_onehot())
    ])
    transformers.append(('cat', categorical_transformer, categorical_features))

if not transformers:
    df['_dummy'] = 0
    features = ['_dummy']
    numeric_features = ['_dummy']
    categorical_features = []
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler(with_mean=False))
    ])
    transformers = [('num', numeric_transformer, numeric_features)]
    X = df[features]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=None)

preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')

if task == 'classification':
    n_classes = y.nunique(dropna=True)
    solver = 'liblinear' if n_classes <= 2 else 'lbfgs'
    model = LogisticRegression(max_iter=200, solver=solver, n_jobs=1, random_state=42)
elif task == 'regression':
    model = Ridge(alpha=1.0)
else:
    model = DummyClassifier(strategy='most_frequent')

pipeline = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if task in ['classification', 'trivial_classification']:
    accuracy = accuracy_score(y_test, y_pred)
else:
    accuracy = None
    try:
        if len(y_test) >= 2:
            r2 = r2_score(y_test, y_pred)
            if np.isnan(r2):
                raise ValueError
            accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))
        else:
            raise ValueError
    except Exception:
        y_true_arr = np.array(y_test)
        y_pred_arr = np.array(y_pred)
        mae = np.mean(np.abs(y_true_arr - y_pred_arr))
        denom = np.mean(np.abs(y_true_arr)) + 1e-8
        score = 1.0 - mae / denom
        accuracy = max(0.0, min(1.0, score))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Chose lightweight linear models and a DummyClassifier fallback to keep CPU usage low.
# - Used a minimal ColumnTransformer pipeline with simple imputers, scaling, and one-hot encoding for reproducibility.
# - Regression accuracy is a bounded R2-based score with an MAE fallback to remain stable in [0,1].