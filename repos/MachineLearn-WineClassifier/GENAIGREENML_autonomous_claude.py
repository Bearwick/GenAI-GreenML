# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("dataset.csv")

# Encode 'style' column if present (categorical feature)
le_style = LabelEncoder()
df["style"] = le_style.fit_transform(df["style"])

# Separate features and target
X = df.drop("quality", axis=1)
y = df["quality"]

# Handle missing values with median imputation (lightweight)
X = X.fillna(X.median())

# Train/test split with stratification for balanced evaluation
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# StandardScaler for numerical stability (lightweight preprocessing)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# RandomForest: good balance of accuracy and efficiency for tabular data
# Using moderate n_estimators to keep training fast on CPU
model = RandomForestClassifier(
    n_estimators=200,
    max_depth=20,
    min_samples_split=5,
    min_samples_leaf=2,
    n_jobs=-1,
    random_state=42,
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. RandomForestClassifier chosen: excellent for small-to-medium tabular datasets,
#    no GPU required, fast training on CPU with n_jobs=-1 parallelism.
# 2. 200 estimators with max_depth=20 provides strong performance without
#    excessive computation; diminishing returns beyond this for most tabular tasks.
# 3. StandardScaler used for consistent feature scaling (low overhead).
# 4. LabelEncoder for the categorical 'style' column avoids one-hot expansion,
#    which is acceptable for tree-based models that handle ordinal encoding well.
# 5. Median imputation is robust to outliers and computationally trivial.
# 6. Stratified split ensures representative class distribution in test set.
# 7. No deep learning, no large embeddings â€” purely classical ML for energy efficiency.