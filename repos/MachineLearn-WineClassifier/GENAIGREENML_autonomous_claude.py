# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# --- Robust CSV loading ---
try:
    df = pd.read_csv("data/wine_dataset.csv")
    if df.shape[1] < 3:
        df = pd.read_csv("data/wine_dataset.csv", sep=";", decimal=",")
except Exception:
    df = pd.read_csv("data/wine_dataset.csv", sep=";", decimal=",")

# --- Column name normalization ---
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
df = df.loc[:, ~df.columns.str.startswith('Unnamed')]

# --- Schema detection ---
expected_numeric = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',
                    'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density',
                    'pH', 'sulphates', 'alcohol']
expected_cat = ['style']
expected_target = 'quality'

available_cols = list(df.columns)

# Determine target
if expected_target in available_cols:
    target_col = expected_target
else:
    numeric_candidates = df.select_dtypes(include=[np.number]).columns.tolist()
    non_const = [c for c in numeric_candidates if df[c].nunique() > 1]
    target_col = non_const[-1] if non_const else numeric_candidates[0]

# Determine features
feature_cols = [c for c in available_cols if c != target_col]

# Coerce numeric columns
for c in feature_cols:
    if df[c].dtype == object:
        try:
            converted = pd.to_numeric(df[c], errors='coerce')
            if converted.notna().sum() > 0.5 * len(df):
                df[c] = converted
        except Exception:
            pass

# Coerce target
df[target_col] = pd.to_numeric(df[target_col], errors='coerce')

# Drop rows where target is NaN
df = df.dropna(subset=[target_col])

# Replace inf with NaN, then handle
df = df.replace([np.inf, -np.inf], np.nan)

# Separate numeric and categorical feature columns
num_features = [c for c in feature_cols if c in df.columns and df[c].dtype in [np.float64, np.int64, np.float32, np.int32]]
cat_features = [c for c in feature_cols if c in df.columns and df[c].dtype == object]

# Fill NaN in numeric features with median
for c in num_features:
    median_val = df[c].median()
    df[c] = df[c].fillna(median_val)

# Fill NaN in categorical features with mode
for c in cat_features:
    mode_val = df[c].mode()
    if len(mode_val) > 0:
        df[c] = df[c].fillna(mode_val.iloc[0])
    else:
        df[c] = df[c].fillna('missing')

assert len(df) > 0, "Dataset is empty after preprocessing"

# --- Determine task type ---
target = df[target_col].values
n_classes = len(np.unique(target[~np.isnan(target)])) if np.issubdtype(target.dtype, np.number) else len(set(target))
is_classification = n_classes <= 30 and n_classes >= 2

if not is_classification or n_classes < 2:
    # Fallback: treat as regression, report bounded R^2 as accuracy proxy
    from sklearn.linear_model import Ridge
    X