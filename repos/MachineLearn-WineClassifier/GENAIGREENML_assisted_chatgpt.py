# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import train_test_split


SEED = 42
DATASET_HEADERS = [
    "fixed_acidity",
    "volatile_acidity",
    "citric_acid",
    "residual_sugar",
    "chlorides",
    "free_sulfur_dioxide",
    "total_sulfur_dioxide",
    "density",
    "pH",
    "sulphates",
    "alcohol",
    "quality",
    "style",
]


def read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] < 2:
        df = pd.read_csv(path, sep=";", decimal=",")
    else:
        if len(df.columns) == 1:
            df = pd.read_csv(path, sep=";", decimal=",")
        else:
            col0 = str(df.columns[0]).lower()
            if (";" in col0) or (col0.count(",") >= 5):
                df = pd.read_csv(path, sep=";", decimal=",")
    return df


def resolve_style_col(df: pd.DataFrame, expected_headers: list[str]) -> str:
    if "style" in df.columns:
        return "style"
    lower_map = {c.lower(): c for c in df.columns}
    if "style" in lower_map:
        return lower_map["style"]
    expected_lower = [h.lower() for h in expected_headers]
    candidates = [c for c in df.columns if c.lower() in expected_lower and c.lower() == "style"]
    if candidates:
        return candidates[0]
    raise KeyError("Style column not found in dataset.")


def prepare_data(df: pd.DataFrame, expected_headers: list[str]) -> tuple[pd.DataFrame, pd.Series]:
    style_col = resolve_style_col(df, expected_headers)

    style = (
        df[style_col]
        .astype("string", copy=False)
        .str.strip()
        .str.lower()
    )
    keep = style.isin(("red", "white"))
    df = df.loc[keep].copy()
    style = style.loc[keep].map({"red": 0, "white": 1}).astype("int64")

    X = df.drop(columns=[style_col])
    numeric_cols = X.columns
    X[numeric_cols] = X[numeric_cols].apply(pd.to_numeric, errors="coerce")

    mask = X.notna().all(axis=1)
    X = X.loc[mask]
    y = style.loc[mask]

    return X, y


def train_and_evaluate(X: pd.DataFrame, y: pd.Series) -> float:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=SEED, stratify=y
    )

    model = ExtraTreesClassifier(
        n_estimators=100,
        random_state=SEED,
        n_jobs=-1,
    )
    model.fit(X_train, y_train)
    accuracy = float(model.score(X_test, y_test))

    _ = model.predict(X_test.iloc[300:600])

    return accuracy


def main() -> None:
    np.random.seed(SEED)

    path = "data/wine_dataset.csv"
    df = read_csv_robust(path)
    X, y = prepare_data(df, DATASET_HEADERS)
    accuracy = train_and_evaluate(X, y)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Added robust CSV parsing with a lightweight fallback to handle delimiter/decimal issues without extra passes.
# - Avoided redundant intermediate DataFrame copies by filtering once and reusing the cleaned style Series for y.
# - Limited numeric conversion to feature columns only and performed a single NaN mask computation for both X and y.
# - Enabled parallel training via n_jobs=-1 for ExtraTreesClassifier to reduce wall-clock runtime on multicore systems.
# - Ensured reproducibility by fixing a single seed for train/test split and model, and setting NumPy's seed.
# - Removed all non-required prints and kept only the final accuracy output in the mandated format.