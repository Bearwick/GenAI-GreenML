# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import train_test_split
import pandas as pd


def load_and_prepare_data(path: str):
    df = pd.read_csv(path, usecols=lambda c: c != "style" or c == "style")

    style = df["style"].astype("string").str.strip().str.lower()
    valid = style.isin(("red", "white"))
    df = df.loc[valid].copy()

    df["style"] = style.loc[valid].map({"red": 0, "white": 1}).astype("int8")

    y = df["style"]
    X = df.drop(columns=["style"])

    X = X.apply(pd.to_numeric, errors="coerce")
    ok = X.notna().all(axis=1)
    return X.loc[ok], y.loc[ok]


def train_and_evaluate(X, y, seed: int = 42):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=seed, stratify=y
    )

    model = ExtraTreesClassifier(
        n_estimators=100,
        random_state=seed,
        n_jobs=-1,
        bootstrap=False,
    )
    model.fit(X_train, y_train)

    accuracy = model.score(X_test, y_test)
    print(f"ACCURACY={accuracy:.6f}")
    return model, accuracy


def main():
    X, y = load_and_prepare_data("data/wine_dataset.csv")
    train_and_evaluate(X, y, seed=42)


if __name__ == "__main__":
    main()

# OPTIMIZATION SUMMARY
# - Kept preprocessing minimal and vectorized (string cleanup + isin + map) to avoid Python-level loops.
# - Converted label to int8 to reduce memory footprint without changing semantics.
# - Filtered rows once using boolean masks to minimize redundant DataFrame copies and data movement.
# - Dropped unused prediction/printing of slices to avoid extra inference work and I/O overhead.
# - Enabled multi-core training via n_jobs=-1 to reduce wall-clock time (and total energy for fixed work).
# - Encapsulated steps into functions for modularity and reproducibility while keeping identical task/behavior.