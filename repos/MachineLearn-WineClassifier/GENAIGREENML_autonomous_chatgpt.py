# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import warnings

import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.exceptions import ConvergenceWarning
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler


DATASET_PATH = "data/wine_dataset.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    # Attempt default CSV parsing; fallback to ';' separator and ',' decimal if needed.
    df1 = pd.read_csv(path)
    df1.columns = _normalize_columns(df1.columns)

    def looks_wrong(df):
        if df is None or df.empty:
            return True
        # If only 1 column, likely separator issue
        if df.shape[1] <= 1:
            return True
        # If most column names contain ';' it's likely parsed as one column header
        if any(";" in c for c in df.columns):
            return True
        return False

    if looks_wrong(df1):
        df2 = pd.read_csv(path, sep=";", decimal=",")
        df2.columns = _normalize_columns(df2.columns)
        if not looks_wrong(df2):
            return df2
    return df1


def _drop_unnamed(df):
    cols = [c for c in df.columns if not re.match(r"^Unnamed:\s*\d+$", c)]
    return df[cols]


def _safe_to_numeric(series):
    return pd.to_numeric(series, errors="coerce")


def _infer_target_and_task(df, expected_headers=None):
    cols = list(df.columns)

    # Prefer known "quality" if present; else choose best candidate.
    preferred_targets = []
    if expected_headers:
        for t in ["quality", "target", "label", "y"]:
            if t in expected_headers:
                preferred_targets.append(t)
    preferred_targets += ["quality", "target", "label", "y"]

    for t in preferred_targets:
        if t in cols:
            return t

    # Otherwise: choose a non-constant numeric-like column with highest cardinality.
    numeric_candidates = []
    for c in cols:
        s = _safe_to_numeric(df[c])
        non_na = s.dropna()
        if non_na.empty:
            continue
        nunique = non_na.nunique()
        if nunique >= 2:
            numeric_candidates.append((nunique, non_na.shape[0], c))
    if numeric_candidates:
        numeric_candidates.sort(reverse=True)
        return numeric_candidates[0][2]

    # Fallback: choose any non-constant column (object OK).
    for c in cols:
        nunique = df[c].dropna().nunique()
        if nunique >= 2:
            return c

    # If everything constant, return last column (will trigger trivial baseline path)
    return cols[-1] if cols else None


def _is_classification_target(y):
    # Classification if object/bool or small integer-like cardinality.
    if y is None:
        return False
    if pd.api.types.is_bool_dtype(y) or pd.api.types.is_object_dtype(y) or pd.api.types.is_categorical_dtype(y):
        return True

    # Try numeric
    yn = _safe_to_numeric(y)
    non_na = yn.dropna()
    if non_na.empty:
        # If all missing, treat as regression but will degrade to trivial.
        return False

    # If values look like integers and unique classes are limited.
    vals = non_na.values
    if np.all(np.isfinite(vals)):
        # Check near-integers
        if np.all(np.isclose(vals, np.round(vals))):
            nunique = pd.Series(np.round(vals).astype(int)).nunique()
            if nunique <= 20:
                return True
    return False


def _make_preprocessor(X):
    # Identify column types robustly.
    # Coerce likely-numeric object columns to numeric only for selecting numeric cols (cheap heuristic).
    numeric_cols = []
    categorical_cols = []

    for c in X.columns:
        s = X[c]
        if pd.api.types.is_numeric_dtype(s):
            numeric_cols.append(c)
        else:
            sn = _safe_to_numeric(s)
            # If mostly numeric, treat as numeric.
            ratio_numeric = sn.notna().mean() if len(sn) else 0.0
            if ratio_numeric >= 0.85:
                numeric_cols.append(c)
            else:
                categorical_cols.append(c)

    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )
    return preprocessor, numeric_cols, categorical_cols


def _bounded_regression_score(y_true, y_pred):
    # Convert regression performance to [0,1] stable "accuracy proxy":
    # score = 1 / (1 + normalized_mae), where normalized_mae = MAE / (MAD + eps)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = np.mean(np.abs(y_true - y_pred)) if y_true.size else 0.0
    med = np.median(y_true) if y_true.size else 0.0
    mad = np.median(np.abs(y_true - med)) if y_true.size else 0.0
    denom = mad + 1e-9
    nmae = mae / denom if denom > 0 else (mae / (np.std(y_true) + 1e-9) if y_true.size else 0.0)
    score = 1.0 / (1.0 + float(nmae))
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    warnings.filterwarnings("ignore", category=ConvergenceWarning)

    if not os.path.exists(DATASET_PATH):
        # Minimal stdout: still must print ACCURACY line.
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _read_csv_robust(DATASET_PATH)
    df = _drop_unnamed(df)

    # Normalize headers
    df.columns = _normalize_columns(df.columns)

    # Drop fully empty columns
    df = df.dropna(axis=1, how="all")

    assert df is not None and df.shape[0] > 0 and df.shape[1] > 0

    expected_headers = _normalize_columns([
        "fixed_acidity", "volatile_acidity", "citric_acid", "residual_sugar", "chlorides",
        "free_sulfur_dioxide", "total_sulfur_dioxide", "density", "pH", "sulphates",
        "alcohol", "quality", "style"
    ])

    target_col = _infer_target_and_task(df, expected_headers=expected_headers)
    if target_col is None or target_col not in df.columns:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    y_raw = df[target_col]
    X = df.drop(columns=[target_col])

    # If no features remain, create a trivial feature to keep pipeline valid.
    if X.shape[1] == 0:
        X = pd.DataFrame({"__bias__": np.ones(len(df), dtype=float)})

    # Determine task type
    is_clf = _is_classification_target(y_raw)

    # Prepare y
    if is_clf:
        # For numeric-like classification targets, keep as int labels if possible
        if pd.api.types.is_numeric_dtype(y_raw):
            y_num = _safe_to_numeric(y_raw)
            y = y_num.round().astype("Int64").astype(object).where(y_num.notna(), other=np.nan)
        else:
            y = y_raw.astype(object)
    else:
        y = _safe_to_numeric(y_raw)

    # Combine for cleaning rows with missing target
    data = X.copy()
    data["__target__"] = y
    data = data.replace([np.inf, -np.inf], np.nan)
    data = data.dropna(subset=["__target__"])
    assert data.shape[0] > 1

    y_clean = data["__target__"]
    X_clean = data.drop(columns=["__target__"])

    # Re-check classes for classification
    if is_clf:
        # Drop rare missing labels already; now evaluate class count.
        y_series = pd.Series(y_clean, dtype="object")
        y_series = y_series.astype(str)
        class_counts = y_series.value_counts(dropna=True)
        if class_counts.shape[0] < 2:
            is_clf = False
            y_clean = _safe_to_numeric(y_raw).loc[data.index]
        else:
            y_clean = y_series

    # Train/test split
    if is_clf:
        # Try stratify; if fails, fallback without.
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X_clean, y_clean, test_size=0.2, random_state=RANDOM_STATE, stratify=y_clean
            )
        except Exception:
            X_train, X_test, y_train, y_test = train_test_split(
                X_clean, y_clean, test_size=0.2, random_state=RANDOM_STATE
            )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X_clean, y_clean, test_size=0.2, random_state=RANDOM_STATE
        )

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    preprocessor, num_cols, cat_cols = _make_preprocessor(X_train)

    if is_clf:
        # Lightweight linear classifier for small datasets; saga handles sparse one-hot efficiently.
        clf = LogisticRegression(
            max_iter=300,
            solver="saga",
            n_jobs=1,
            class_weight=None
        )
        model = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", clf),
        ])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        model = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", reg),
        ])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _bounded_regression_score(np.asarray(y_test, dtype=float), np.asarray(y_pred, dtype=float))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses robust CSV parsing with a low-cost fallback (sep=';' and decimal=',') to avoid manual fixes.
# - Normalizes column names and drops 'Unnamed:' columns to reduce schema brittleness and wasted processing.
# - Infers target defensively; prefers 'quality' but falls back to any non-constant numeric/object column to keep end-to-end execution.
# - Employs a ColumnTransformer + Pipeline for reproducible preprocessing; minimizes repeated passes over data.
# - Uses lightweight models: LogisticRegression (saga) for classification with sparse one-hot, and Ridge for regression; both CPU-friendly.
# - Avoids heavy ensembles/deep learning; keeps feature engineering minimal (median impute + standardize; one-hot for categoricals).
# - Handles NaN/inf safely and asserts non-empty splits to prevent runtime failures.
# - If regression fallback is used, reports a bounded [0,1] "accuracy proxy" = 1/(1+normalized MAE) for stable single-metric output.