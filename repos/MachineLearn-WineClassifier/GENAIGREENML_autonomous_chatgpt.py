# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score


DATASET_HEADERS = [
    "fixed_acidity",
    "volatile_acidity",
    "citric_acid",
    "residual_sugar",
    "chlorides",
    "free_sulfur_dioxide",
    "total_sulfur_dioxide",
    "density",
    "pH",
    "sulphates",
    "alcohol",
    "quality",
    "style",
]


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = []
    for c in df.columns:
        if isinstance(c, str) and c.strip().lower().startswith("unnamed"):
            drop_cols.append(c)
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _robust_read_csv(path):
    df = None
    # Try default
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d):
        if d is None or d.empty:
            return True
        # If header parsing failed, often results in 1 column with separators inside
        if d.shape[1] == 1:
            sample = ""
            try:
                sample = str(d.columns[0]) + " " + str(d.iloc[0, 0])
            except Exception:
                sample = str(d.columns[0])
            if (";" in sample) or ("," in sample):
                return True
        return False

    if _looks_wrong(df):
        # Retry with semicolon separator and comma decimal
        df2 = None
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df2 = None
        if df2 is not None and not df2.empty and df2.shape[1] >= 2:
            df = df2

    if df is None:
        raise RuntimeError("Failed to read dataset.")
    return df


def _pick_target(df):
    cols_lower = {c.lower(): c for c in df.columns if isinstance(c, str)}
    # Prefer known target 'style' if present
    if "style" in cols_lower:
        return cols_lower["style"]

    # Else prefer a reasonable classification-like column name if present
    for name in ["target", "label", "class", "y"]:
        if name in cols_lower:
            return cols_lower[name]

    # Else choose a non-constant column, preferring object/category then numeric
    nunique = df.nunique(dropna=True)
    candidates = [c for c in df.columns if nunique.get(c, 0) >= 2]
    if not candidates:
        return None

    obj_cands = [c for c in candidates if df[c].dtype == "object"]
    if obj_cands:
        # Choose the one with fewest unique values (likely classification)
        obj_cands.sort(key=lambda c: nunique.get(c, np.inf))
        return obj_cands[0]

    # Numeric candidates: choose with few unique values if any, else last column
    num_cands = [c for c in candidates if pd.api.types.is_numeric_dtype(df[c])]
    if num_cands:
        num_cands.sort(key=lambda c: nunique.get(c, np.inf))
        return num_cands[0]

    return candidates[-1]


def _coerce_numeric_safe(s):
    return pd.to_numeric(s, errors="coerce")


def _bounded_regression_accuracy(y_true, y_pred):
    # Map R^2 to [0,1] for a stable "accuracy proxy"
    r2 = r2_score(y_true, y_pred)
    if not np.isfinite(r2):
        r2 = -1.0
    acc = 0.5 * (r2 + 1.0)
    if acc < 0.0:
        acc = 0.0
    if acc > 1.0:
        acc = 1.0
    return float(acc)


path = os.path.join("data", "wine_dataset.csv")
df = _robust_read_csv(path)

df.columns = _normalize_columns(df.columns)
df = _drop_unnamed(df)

# Light cleanup: remove fully empty rows/cols
df = df.dropna(axis=0, how="all").dropna(axis=1, how="all")

assert df.shape[0] > 0 and df.shape[1] > 0

# If dataset headers are provided, attempt to align but never hard-fail
# Keep available subset intersection; else proceed with current columns
headers_norm = _normalize_columns(DATASET_HEADERS)
existing = set(df.columns)
intersect = [c for c in headers_norm if c in existing]
if len(intersect) >= 2:
    df = df[intersect].copy()
else:
    df = df.copy()

target_col = _pick_target(df)
if target_col is None or target_col not in df.columns:
    # Fall back to last column as target
    target_col = df.columns[-1]

y_raw = df[target_col].copy()
X = df.drop(columns=[target_col], errors="ignore").copy()

# Ensure there is at least one feature; if not, create a constant feature
if X.shape[1] == 0:
    X = pd.DataFrame({"__const__": np.ones(len(df), dtype=float)})

# Decide task type
is_classification = False
y = y_raw

# Special handling for "style" if present: normalize red/white mapping when possible
if isinstance(target_col, str) and target_col.strip().lower() == "style":
    y = y.astype(str).str.strip().str.lower()
    y = y.where(y.isin(["red", "white"]), other=np.nan)
    keep = y.notna()
    X = X.loc[keep].copy()
    y = y.loc[keep].map({"red": 0, "white": 1}).astype("int64")
    is_classification = True
else:
    # Generic inference: if object -> classification; if numeric with few uniques -> classification
    if y.dtype == "object":
        is_classification = True
        y = y.astype(str).str.strip()
        # Avoid degenerate empty strings
        y = y.replace({"": np.nan})
    else:
        y_num = _coerce_numeric_safe(y)
        # If many NaNs after coercion, treat as classification (string-like)
        nan_ratio = float(y_num.isna().mean())
        if nan_ratio < 0.2:
            # numeric target
            nunique = int(y_num.nunique(dropna=True))
            is_classification = (nunique >= 2 and nunique <= 20)
            y = y_num
        else:
            is_classification = True
            y = y.astype(str).str.strip()
            y = y.replace({"": np.nan})

# Drop rows with missing target
mask_y = pd.notna(y)
X = X.loc[mask_y].copy()
y = y.loc[mask_y].copy()

assert len(X) > 1

# Identify column types based on actual data
# Coerce numeric-like object columns for numeric pipeline only if most values parse as numeric
numeric_features = []
categorical_features = []
for c in X.columns:
    col = X[c]
    if pd.api.types.is_numeric_dtype(col):
        numeric_features.append(c)
    else:
        coerced = _coerce_numeric_safe(col)
        ok_ratio = float(coerced.notna().mean())
        if ok_ratio >= 0.9:
            X[c] = coerced
            numeric_features.append(c)
        else:
            categorical_features.append(c)

# Build preprocessing
numeric_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ]
)

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop",
)

# Ensure at least one transformer has columns; if none, create constant numeric feature
if len(numeric_features) == 0 and len(categorical_features) == 0:
    X["__const__"] = 1.0
    numeric_features = ["__const__"]
    preprocess = ColumnTransformer(
        transformers=[("num", numeric_transformer, numeric_features)],
        remainder="drop",
    )

# Train/test split
random_state = 42
test_size = 0.3

if is_classification:
    # Encode y to integer classes
    # If y is already integer but could be non-contiguous, factorize for safety
    if not pd.api.types.is_integer_dtype(y):
        y_enc, _ = pd.factorize(y, sort=True)
        y_enc = pd.Series(y_enc, index=y.index)
    else:
        y_enc, _ = pd.factorize(y, sort=True)
        y_enc = pd.Series(y_enc, index=y.index)

    # If <2 classes, fall back to regression-like proxy using constant predictor
    n_classes = int(pd.Series(y_enc).nunique(dropna=True))
    if n_classes < 2:
        # Trivial accuracy: always predict the only class
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_enc, test_size=test_size, random_state=random_state
        )
        assert len(X_train) > 0 and len(X_test) > 0
        y_pred = np.full(shape=len(y_test), fill_value=int(y_train.mode().iloc[0]))
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        stratify = y_enc if n_classes >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_enc, test_size=test_size, random_state=random_state, stratify=stratify
        )
        assert len(X_train) > 0 and len(X_test) > 0

        # Lightweight linear model; 'saga' handles sparse one-hot efficiently
        clf = LogisticRegression(
            max_iter=300,
            solver="saga",
            n_jobs=1,
            random_state=random_state,
        )

        model = Pipeline(steps=[("preprocess", preprocess), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Regression path with bounded accuracy proxy
    y_reg = _coerce_numeric_safe(y)
    mask = y_reg.notna() & np.isfinite(y_reg.to_numpy(dtype=float, na_value=np.nan))
    X = X.loc[mask].copy()
    y_reg = y_reg.loc[mask].astype(float)
    assert len(X) > 1

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_reg, test_size=test_size, random_state=random_state
    )
    assert len(X_train) > 0 and len(X_test) > 0

    reg = Ridge(alpha=1.0, random_state=random_state)
    model = Pipeline(steps=[("preprocess", preprocess), ("model", reg)])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = _bounded_regression_accuracy(y_test, y_pred)

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Replaced heavy tree ensembles with LogisticRegression (saga) as a CPU-friendly, energy-efficient linear baseline.
# - Used ColumnTransformer + Pipeline to ensure one-pass, reproducible preprocessing and avoid redundant dataframe copies.
# - Median/most_frequent imputers are lightweight and robust; StandardScaler stabilizes linear optimization.
# - OneHotEncoder with sparse_output reduces memory/compute for categoricals; solver='saga' works efficiently with sparse matrices.
# - Implemented robust CSV parsing fallback (default then sep=';' & decimal=',') to handle locale/format inconsistencies.
# - Defensive schema handling: normalized column names, dropped 'Unnamed' columns, inferred/validated target, and safe fallbacks.
# - Regression fallback uses a bounded accuracy proxy: ACC = clip((R^2 + 1)/2, 0, 1) for stable reporting in [0,1].