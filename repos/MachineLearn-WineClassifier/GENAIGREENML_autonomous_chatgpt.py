# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "data/wine_dataset.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c = str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        out.append(c)
    return out


def _drop_unnamed_columns(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _try_read_csv(path):
    df = None
    # Try default parsing
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(dfx):
        if dfx is None or dfx.empty:
            return True
        if dfx.shape[1] <= 1:
            return True
        # Heuristic: if many columns are unnamed or single giant column, likely wrong delimiter
        unnamed_ratio = sum(str(c).strip().lower().startswith("unnamed:") for c in dfx.columns) / max(1, len(dfx.columns))
        if unnamed_ratio > 0.5:
            return True
        return False

    if looks_wrong(df):
        # Retry with semicolon separator and comma decimal (common in EU exports)
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            df = None

    if df is None:
        raise RuntimeError("Failed to read dataset.")
    return df


def _coerce_numeric_columns(df, numeric_cols):
    for c in numeric_cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _pick_target(df, dataset_headers=None):
    cols = list(df.columns)

    # Prefer quality if present
    for preferred in ["quality", "Quality", "target", "Target", "label", "Label", "y"]:
        if preferred in cols:
            return preferred

    # If headers provided, pick from intersection preference
    if dataset_headers:
        for h in dataset_headers:
            if h in cols and h.lower() in ["quality", "target", "label", "y"]:
                return h

    # Choose a numeric non-constant column as target
    numeric_candidates = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() >= max(10, int(0.2 * len(df))):
            nun = s.nunique(dropna=True)
            if nun and nun > 1:
                numeric_candidates.append((c, nun))
    if numeric_candidates:
        numeric_candidates.sort(key=lambda x: x[1], reverse=True)
        return numeric_candidates[0][0]

    # Fall back to any non-constant column
    for c in cols:
        nun = df[c].nunique(dropna=True)
        if nun and nun > 1:
            return c

    # Last resort: first column
    return cols[0] if cols else None


def _bounded_regression_score(y_true, y_pred):
    # Stable [0,1] proxy: 1 / (1 + normalized MAE)
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    mae = np.mean(np.abs(y_true - y_pred))
    scale = np.std(y_true)
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = np.mean(np.abs(y_true))
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = 1.0
    score = 1.0 / (1.0 + (mae / scale))
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    df = _try_read_csv(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed_columns(df)

    assert df.shape[0] > 0 and df.shape[1] > 0, "Dataset is empty after loading/cleanup."

    dataset_headers = [
        "fixed_acidity", "volatile_acidity", "citric_acid", "residual_sugar", "chlorides",
        "free_sulfur_dioxide", "total_sulfur_dioxide", "density", "pH", "sulphates",
        "alcohol", "quality", "style"
    ]

    target_col = _pick_target(df, dataset_headers=dataset_headers)
    if target_col is None or target_col not in df.columns:
        # Defensive fallback: create a dummy target to keep pipeline runnable
        df["_target_fallback_"] = 0
        target_col = "_target_fallback_"

    y_raw = df[target_col]
    X = df.drop(columns=[target_col])

    # Identify column types conservatively
    numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    # Attempt numeric coercion for object columns that look numeric
    object_cols = [c for c in X.columns if c not in numeric_cols]

    # Coerce numeric-like objects
    for c in object_cols:
        coerced = pd.to_numeric(X[c], errors="coerce")
        # If coercion yields many valid numbers, treat as numeric
        if coerced.notna().mean() >= 0.8 and coerced.nunique(dropna=True) > 1:
            X[c] = coerced
            numeric_cols.append(c)

    numeric_cols = list(dict.fromkeys([c for c in numeric_cols if c in X.columns]))
    categorical_cols = [c for c in X.columns if c not in numeric_cols]

    # Coerce numeric features to float safely
    X = _coerce_numeric_columns(X, numeric_cols)

    # Decide task type using target
    y_num = pd.to_numeric(y_raw, errors="coerce")
    classification = False

    # Classification if target is non-numeric OR has small number of unique discrete values
    if y_num.isna().mean() > 0.5:
        classification = True
        y = y_raw.astype(str).fillna("NA")
    else:
        # Use numeric target, decide based on unique count and integer-likeness
        y_num_valid = y_num.dropna()
        nun = y_num_valid.nunique(dropna=True)
        is_intlike = np.all(np.isclose(y_num_valid.values, np.round(y_num_valid.values))) if len(y_num_valid) else False
        if nun >= 2 and nun <= 20 and is_intlike:
            classification = True
            y = y_num.round().astype("Int64").astype(str).fillna("NA")
        else:
            classification = False
            y = y_num

    # If target has too few classes for classification, fallback to regression
    if classification:
        class_count = pd.Series(y).nunique(dropna=True)
        if class_count < 2:
            classification = False
            y = y_num

    # Drop rows with missing target (for regression) or missing label (rare)
    if not classification:
        mask = y.notna() & np.isfinite(y.values)
        X = X.loc[mask].copy()
        y = y.loc[mask].copy()
    else:
        mask = pd.Series(y).notna()
        X = X.loc[mask].copy()
        y = pd.Series(y).loc[mask].copy()

    assert len(X) > 5, "Not enough samples after cleaning."

    # Preprocessing
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Split
    stratify = y if classification else None
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE
        )

    assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

    # Model (lightweight baseline)
    if classification:
        # Use efficient solver; cap iterations for CPU-friendliness
        model = LogisticRegression(
            max_iter=200,
            solver="lbfgs",
            n_jobs=1,
            multi_class="auto",
        )
        clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        reg = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        accuracy = _bounded_regression_score(y_test.values, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Chose lightweight linear models (LogisticRegression/Ridge) for strong CPU efficiency and fast convergence on tabular data.
# - Used ColumnTransformer + Pipeline to ensure reproducible preprocessing and avoid repeated transforms (saves CPU/energy).
# - Minimal feature engineering: median/mode imputation, standardization for numeric stability, sparse one-hot for categoricals.
# - Robust schema handling: normalize headers, drop Unnamed columns, infer target defensively, coerce numeric with errors='coerce'.
# - CSV parsing fallback (sep=';' and decimal=',') to reduce failures without manual edits.
# - Regression fallback uses bounded MAE-based score in [0,1] (1/(1+MAE/scale)) to keep ACCURACY stable and comparable.