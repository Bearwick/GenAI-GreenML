# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def _load_dataset() -> pd.DataFrame:
    # Keep I/O minimal and robust: try common filenames/locations without interactive inputs.
    candidates = [
        "data.csv",
        "dataset.csv",
        "train.csv",
        "wine.csv",
        "wines.csv",
        "wine_quality.csv",
        "winequality.csv",
        os.path.join("data", "data.csv"),
        os.path.join("data", "dataset.csv"),
        os.path.join("data", "train.csv"),
        os.path.join("input", "data.csv"),
        os.path.join("input", "dataset.csv"),
        os.path.join("input", "train.csv"),
    ]
    for path in candidates:
        if os.path.exists(path):
            return pd.read_csv(path)

    csvs = [f for f in os.listdir(".") if f.lower().endswith(".csv")]
    if len(csvs) == 1:
        return pd.read_csv(csvs[0])

    # If multiple CSVs exist, prefer one that has required headers.
    required = {
        "fixed_acidity",
        "volatile_acidity",
        "citric_acid",
        "residual_sugar",
        "chlorides",
        "free_sulfur_dioxide",
        "total_sulfur_dioxide",
        "density",
        "pH",
        "sulphates",
        "alcohol",
        "quality",
        "style",
    }
    for f in csvs:
        try:
            df = pd.read_csv(f, nrows=5)
            if required.issubset(set(df.columns)):
                return pd.read_csv(f)
        except Exception:
            continue

    raise FileNotFoundError("No suitable CSV dataset found in the working directory.")


def _coerce_numeric(df: pd.DataFrame, cols) -> pd.DataFrame:
    for c in cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def main():
    df = _load_dataset()

    target_col = "style"
    feature_cols = [c for c in df.columns if c != target_col]

    X = df[feature_cols].copy()
    y = df[target_col].copy()

    numeric_cols = [
        "fixed_acidity",
        "volatile_acidity",
        "citric_acid",
        "residual_sugar",
        "chlorides",
        "free_sulfur_dioxide",
        "total_sulfur_dioxide",
        "density",
        "pH",
        "sulphates",
        "alcohol",
        "quality",
    ]
    X = _coerce_numeric(X, numeric_cols)

    categorical_cols = [c for c in X.columns if c not in numeric_cols]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, [c for c in numeric_cols if c in X.columns]),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y if y.nunique() > 1 else None,
    )

    clf = LogisticRegression(
        solver="saga",
        max_iter=300,
        tol=1e-3,
        n_jobs=1,
        C=1.0,
        penalty="l2",
        random_state=42,
    )

    model = Pipeline(steps=[("preprocess", preprocessor), ("clf", clf)])
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()


# OPTIMIZATION SUMMARY
# - Uses a linear LogisticRegression model (small parameter count) instead of deep learning to reduce CPU/GPU energy use.
# - Preprocessing is a single sklearn Pipeline/ColumnTransformer to avoid duplicated work and ensure reproducibility.
# - Median/mode imputations are O(n) and lightweight; scaling helps convergence so fewer solver iterations are needed.
# - OneHotEncoder is sparse to reduce memory and compute for categorical features.
# - 'saga' solver supports sparse input efficiently; capped iterations + relaxed tolerance reduces unnecessary training compute.
# - Single-thread execution (n_jobs=1) avoids extra power draw from oversubscription on shared CPU environments.