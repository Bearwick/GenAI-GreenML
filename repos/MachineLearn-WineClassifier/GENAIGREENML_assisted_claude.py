# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier

path = "data/wine_dataset.csv"
try:
    df = pd.read_csv(path)
    if df.shape[1] < 2:
        df = pd.read_csv(path, sep=';', decimal=',')
except Exception:
    df = pd.read_csv(path, sep=';', decimal=',')

df["style"] = df["style"].astype(str).str.strip().str.lower()
df = df[df["style"].isin(["red", "white"])].copy()
df["style"] = df["style"].map({"red": 0, "white": 1}).astype(np.int8)

feature_cols = [c for c in df.columns if c != "style"]
df[feature_cols] = df[feature_cols].apply(pd.to_numeric, errors="coerce")
df.dropna(subset=feature_cols, inplace=True)

X = df[feature_cols].values
y = df["style"].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

modelo = ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)
modelo.fit(X_train, y_train)

accuracy = modelo.score(X_test, y_test)
print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Converted DataFrames to NumPy arrays (.values) before train/test split and fitting to reduce memory and avoid pandas overhead during model training.
# 2. Used np.int8 for binary target instead of int64, reducing memory by 8x for that column.
# 3. Used dropna(inplace=True) to avoid creating an extra copy of the dataframe.
# 4. Derived feature columns dynamically from df.columns to avoid hardcoding.
# 5. Added n_jobs=-1 to ExtraTreesClassifier to leverage all CPU cores, reducing wall-clock time and overall energy for the same computation.
# 6. Removed redundant mask-based filtering; used dropna directly on the dataframe.
# 7. Removed plots, prints, and unnecessary predict call on a slice that was not used for evaluation.
# 8. Added robust CSV parsing fallback (sep=';', decimal=',').
# 9. Fixed random_state=42 for reproducibility.