# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier

path = "data/wine_dataset.csv"

try:
    df = pd.read_csv(path)
    if df.shape[1] < 2:
        df = pd.read_csv(path, sep=';', decimal=',')
except Exception:
    df = pd.read_csv(path, sep=';', decimal=',')

style_col = "style"
df[style_col] = df[style_col].astype(str).str.strip().str.lower()
df = df[df[style_col].isin(["red", "white"])].copy()
df[style_col] = df[style_col].map({"red": 0, "white": 1}).astype("int8")

X = df.drop(columns=[style_col])
y = df[style_col]

X = X.apply(pd.to_numeric, errors="coerce")
valid = X.notna().all(axis=1)
X = X.loc[valid]
y = y.loc[valid]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

model = ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)
model.fit(X_train, y_train)

accuracy = model.score(X_test, y_test)

_ = model.predict(X_test.iloc[300:600])

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Used int8 instead of int64 for binary label column to reduce memory footprint.
# 2. Added n_jobs=-1 to ExtraTreesClassifier to parallelize tree fitting, reducing wall-clock time and overall energy use.
# 3. Removed all print statements except the required accuracy output.
# 4. Removed redundant variable aliases (arquivo -> df, modelo -> model) for clarity.
# 5. Added robust CSV parsing fallback (sep=';', decimal=',') for input reliability.
# 6. Removed plots, visualizations, and artifact saving.
# 7. Fixed random_state=42 for reproducibility.
# 8. Kept prediction call to preserve original behavior but discarded output since it is not needed.