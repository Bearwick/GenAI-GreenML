# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier

path = "data/wine_dataset.csv"
df = pd.read_csv(path)

df["style"] = df["style"].astype(str).str.strip().str.lower()
df = df[df["style"].isin(["red", "white"])].copy()
df["style"] = df["style"].map({"red": 0, "white": 1}).astype(np.int8)

y = df["style"]
X = df.drop(columns=["style"])

X = X.apply(pd.to_numeric, errors="coerce")
mask = X.notna().all(axis=1)
X = X.loc[mask].values.astype(np.float32)
y = y.loc[mask].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

model = ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)
model.fit(X_train, y_train)

accuracy = model.score(X_test, y_test)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Converted features to float32 and labels to int8 to reduce memory footprint.
# 2. Converted X to numpy array early to avoid pandas overhead during fitting.
# 3. Added n_jobs=-1 to ExtraTreesClassifier to parallelize tree building, reducing wall-clock time and energy.
# 4. Removed redundant prediction on a subset (previsoes) that was not used for evaluation.
# 5. Removed plots, prints, and model saving as required.
# 6. Kept the same model, dataset, random_state, and split to preserve original behavior.