# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier

df = pd.read_csv("data/wine_dataset.csv")

df['style'] = df['style'].str.strip().str.lower()
df = df[df['style'].isin(['red', 'white'])].copy()
df['style'] = df['style'].map({'red': 0, 'white': 1}).astype('int8')

feature_cols = df.columns.drop('style')
df[feature_cols] = df[feature_cols].apply(pd.to_numeric, errors='coerce')
df.dropna(subset=feature_cols, inplace=True)

X = df[feature_cols].values.astype('float32')
y = df['style'].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

model = ExtraTreesClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

accuracy = model.score(X_test, y_test)
print(f"ACCURACY={accuracy:.6f}")

_ = model.predict(X_test[300:600])

# OPTIMIZATION SUMMARY
# 1. Reduced memory usage by downcasting numerical features to float32 and the target variable to int8.
# 2. Optimized data cleaning by replacing sequential string manipulations with an efficient map-based dictionary approach.
# 3. Minimized data movement and redundant copies by performing preprocessing and NaN filtering on the original dataframe before conversion to numpy arrays.
# 4. Improved computational efficiency by utilizing vectorized pandas operations for numerical conversion instead of iterative methods.
# 5. Preserved model reproducibility and behavior by maintaining original random seeds and algorithm parameters while streamlining the underlying data pipeline.