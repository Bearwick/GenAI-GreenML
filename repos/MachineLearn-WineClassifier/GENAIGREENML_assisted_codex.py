# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier
import pandas as pd

COLUMNS = [
    "fixed_acidity",
    "volatile_acidity",
    "citric_acid",
    "residual_sugar",
    "chlorides",
    "free_sulfur_dioxide",
    "total_sulfur_dioxide",
    "density",
    "pH",
    "sulphates",
    "alcohol",
    "quality",
    "style",
]

STYLE_MAP = {"red": 0, "white": 1}


def load_and_prepare(path):
    data = pd.read_csv(path, usecols=COLUMNS)
    style = data.pop("style").astype(str).str.strip().str.lower().map(STYLE_MAP)
    mask = style.notna()
    if not mask.all():
        data = data.loc[mask]
        style = style.loc[mask]
    style = style.astype("int64")
    if not all(pd.api.types.is_numeric_dtype(t) for t in data.dtypes):
        data = data.apply(pd.to_numeric, errors="coerce")
    mask = data.notna().all(axis=1)
    if not mask.all():
        data = data.loc[mask]
        style = style.loc[mask]
    return data, style


def train_and_evaluate(features, target):
    X_train, X_test, y_train, y_test = train_test_split(
        features, target, test_size=0.3, random_state=42, stratify=target
    )
    model = ExtraTreesClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    return model.score(X_test, y_test)


path = "data/wine_dataset.csv"
X, y = load_and_prepare(path)
accuracy = train_and_evaluate(X, y)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# - Loaded only required columns to reduce I/O and memory usage.
# - Removed the label column with pop and merged cleaning/mapping to minimize data copies.
# - Skipped numeric conversion when dtypes are already numeric to avoid redundant work.
# - Applied conditional filtering to avoid unnecessary slicing when no invalid rows exist.
# - Eliminated unused prediction generation to reduce computation while keeping deterministic settings.