# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def load_data(path):
    # Robust loading: try default, fallback to semicolon
    try:
        df = pd.read_csv(path)
        if len(df.columns) <= 1:
            raise ValueError
    except:
        df = pd.read_csv(path, sep=';', decimal=',')
    
    # Normalize column names: strip whitespace, collapse internal spaces, drop Unnamed
    df.columns = [" ".join(str(c).split()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def solve():
    dataset_path = 'data/wine_dataset.csv'
    try:
        df = load_data(dataset_path)
    except Exception:
        # Trivial fallback for missing file/read error during CI/CD
        print(f"ACCURACY={0.000000:.6f}")
        return

    if df.empty:
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Identify Target
    # Based on wine dataset context: 'style' is preferred (binary), then 'quality'
    possible_targets = ['style', 'quality']
    target_col = None
    for pt in possible_targets:
        if pt in df.columns:
            target_col = pt
            break
    
    if target_col is None:
        target_col = df.columns[-1]

    # Pre-process numeric columns (coerce strings to NaN)
    for col in df.columns:
        if col != target_col:
            if df[col].dtype == 'object':
                # Try to convert to numeric if it looks like numbers
                converted = pd.to_numeric(df[col], errors='coerce')
                if not converted.isna().all():
                    df[col] = converted

    # Drop rows where target is NaN
    df = df.dropna(subset=[target_col])
    
    # Check classes
    unique_targets = df[target_col].unique()
    if len(unique_targets) < 2:
        # Not enough classes for classification, return trivial accuracy
        print(f"ACCURACY={1.000000:.6f}")
        return

    # Prepare features and target
    X = df.drop(columns=[target_col])
    y = df[target_col]

    # Handle y encoding if it's categorical
    is_regression = False
    if np.issubdtype(y.dtype, np.number) and len(unique_targets) > 20:
        is_regression = True
    else:
        le = LabelEncoder()
        y = le.fit_transform(y.astype(str))

    # Identify feature types
    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_features = X.select_dtypes(include=['object', 'bool', 'category']).columns.tolist()

    # Pipelines
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ],
        remainder='drop'
    )

    # Energy-efficient model choice
    if is_regression:
        from sklearn.linear_model import Ridge
        model = Ridge()
    else:
        model = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto')

    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])

    # Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    if len(X_train) == 0 or len(X_test) == 0:
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Train
    pipeline.fit(X_train, y_train)

    # Evaluate
    if is_regression:
        # Map R^2 to a [0, 1] "accuracy" proxy
        score = pipeline.score(X_test, y_test)
        accuracy = max(0, min(1, score))
    else:
        y_pred = pipeline.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# Optimization Summary:
# 1. Model Choice: Used LogisticRegression (classification) or Ridge (regression). 
#    These are linear models with low computational complexity (O(n_features * n_samples)), 
#    making them highly energy-efficient and CPU-friendly compared to ensembles or deep learning.
# 2. Preprocessing: Utilized Scikit-learn Pipelines for efficiency and to prevent data leakage. 
#    StandardScaler ensures faster convergence for linear solvers.
# 3. Robustness: Implemented a multi-stage CSV loader and schema normalization to handle 
#    potential formatting inconsistencies without manual intervention.
# 4. Data Handling: Numeric coercion and median imputation ensure the model runs even 
#    with dirty data, while specific dtype selection avoids expensive operations on strings.
# 5. Hardware: The entire pipeline is designed for single-core CPU execution with minimal memory footprint.
# 6. Accuracy Proxy: In the rare case of a regression fallback, R^2 is clipped to [0,1] to meet 
#    the consistent output requirement while maintaining a metric for model quality.