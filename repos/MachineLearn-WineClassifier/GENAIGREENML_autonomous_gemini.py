# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

df = pd.read_csv('data.csv')
df.dropna(inplace=True)

if 'style' in df.columns:
    le = LabelEncoder()
    df['style'] = le.fit_transform(df['style'].astype(str))

X = df.drop(columns=['quality'])
y = df['quality']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = RandomForestClassifier(
    n_estimators=25,
    max_depth=10,
    random_state=42,
    n_jobs=1
)
model.fit(X_train, y_train)

accuracy = accuracy_score(y_test, model.predict(X_test))
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Energy Efficiency: Utilized a small Random Forest with only 25 estimators to minimize the number of CPU operations and reduce power consumption during training and inference.
# 2. Computational Lightweighting: Limited max_depth to 10 to prevent the model from growing excessively large, which saves memory and speeds up decision-making processes.
# 3. CPU Efficiency: Set n_jobs=1 to avoid the energy overhead associated with parallel process management and context switching, which is often inefficient for small datasets.
# 4. Resource-Friendly Preprocessing: Used Label Encoding for the 'style' feature instead of One-Hot Encoding to keep the feature space small and the memory footprint low.
# 5. Stability: Included StandardScaler to normalize feature scales, ensuring the model's decision boundaries are calculated efficiently without unnecessary numerical instability.
# 6. Green Design: Avoided deep learning components or heavy gradient boosting frameworks, opting for a traditional ensemble method that provides a strong accuracy-to-energy ratio on CPU hardware.