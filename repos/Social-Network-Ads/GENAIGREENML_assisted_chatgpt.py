# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


RANDOM_SEED = 42
DATASET_PATH = "Social_Network_Ads.csv"
DATASET_HEADERS = ["User ID", "Gender", "Age", "EstimatedSalary", "Purchased"]


def read_csv_robust(path: str, expected_headers: list[str]) -> pd.DataFrame:
    df = pd.read_csv(path)
    if not _looks_like_correct_parse(df, expected_headers):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _looks_like_correct_parse(df: pd.DataFrame, expected_headers: list[str]) -> bool:
    cols = [str(c).strip() for c in df.columns]
    if len(cols) <= 1:
        return False
    expected = [str(h).strip() for h in expected_headers]
    expected_set = set(expected)
    cols_set = set(cols)
    if expected_set.issubset(cols_set):
        return True
    if len(cols) == 1 and ("," in cols[0] or ";" in cols[0]):
        return False
    return False


def resolve_columns(df: pd.DataFrame, expected_headers: list[str]) -> dict[str, str]:
    normalized_map = {str(c).strip().lower(): c for c in df.columns}
    mapping = {}
    for h in expected_headers:
        key = str(h).strip().lower()
        if key in normalized_map:
            mapping[h] = normalized_map[key]
    return mapping


def compute_metrics_from_cm(cm: np.ndarray) -> dict[str, int]:
    if cm.shape == (2, 2):
        tn, fp, fn, tp = cm.ravel()
        return {"TP": int(tp), "FP": int(fp), "TN": int(tn), "FN": int(fn)}
    return {"TP": 0, "FP": 0, "TN": 0, "FN": 0}


def main() -> None:
    np.random.seed(RANDOM_SEED)

    df = read_csv_robust(DATASET_PATH, DATASET_HEADERS)
    colmap = resolve_columns(df, DATASET_HEADERS)

    age_col = colmap.get("Age")
    sal_col = colmap.get("EstimatedSalary")
    y_col = colmap.get("Purchased")

    if age_col is None or sal_col is None or y_col is None:
        raise ValueError(f"Required columns not found. Available columns: {list(df.columns)}")

    X = df[[age_col, sal_col]].to_numpy(dtype=np.float64, copy=False)
    y = df[y_col].to_numpy(copy=False)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=RANDOM_SEED
    )

    scaler = StandardScaler(copy=False)
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = LogisticRegression(solver="lbfgs", random_state=RANDOM_SEED)
    model.fit(X_train_scaled, y_train)

    y_pred = model.predict(X_test_scaled)

    cm = confusion_matrix(y_test, y_pred)
    _ = compute_metrics_from_cm(cm)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused heavy imports (matplotlib, seaborn, extra metrics) to cut import time and memory.
# - Implemented robust CSV parsing with a lightweight sanity check and fallback delimiter/decimal to avoid rework and failures.
# - Derived required column names from DATASET_HEADERS and df.columns (case/whitespace tolerant) to prevent schema assumptions.
# - Converted features/labels to NumPy arrays with copy=False to minimize data movement and memory overhead.
# - Used StandardScaler(copy=False) to reduce intermediate allocations during scaling.
# - Ensured reproducibility via a fixed random seed and passing random_state to train_test_split and LogisticRegression.
# - Avoided redundant computations/outputs by computing only what is needed for final accuracy printing.