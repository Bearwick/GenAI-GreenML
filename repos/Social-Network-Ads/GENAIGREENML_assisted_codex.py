# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import os
import random
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

DATASET_PATH = "Social_Network_Ads.csv"
DATASET_HEADERS = "User ID,Gender,Age,EstimatedSalary,Purchased"

def set_seed(seed=42):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)

def normalize_column(name):
    return "".join(ch for ch in str(name) if ch.isalnum()).lower()

def valid_parse(df, expected_headers):
    if df.shape[1] < 2:
        return False
    norm_cols = {normalize_column(c) for c in df.columns}
    norm_expected = {normalize_column(h) for h in expected_headers}
    return len(norm_cols & norm_expected) >= 2

def read_dataset(path, expected_headers):
    df = pd.read_csv(path)
    if not valid_parse(df, expected_headers):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df

def map_columns(df, expected_headers):
    norm_cols = {normalize_column(c): c for c in df.columns}
    mapping = {}
    for header in expected_headers:
        key = normalize_column(header)
        if key in norm_cols:
            mapping[header] = norm_cols[key]
        else:
            match = None
            for norm, col in norm_cols.items():
                if key == norm or key in norm or norm in key:
                    match = col
                    break
            if match is None:
                raise ValueError(f"Missing column: {header}")
            mapping[header] = match
    return mapping

def find_header(expected_headers, target):
    key = normalize_column(target)
    for header in expected_headers:
        if normalize_column(header) == key:
            return header
    raise ValueError(f"Header not found: {target}")

def compute_metrics(y_true, y_pred, labels):
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    total = cm.sum()
    accuracy = cm.trace() / total if total else 0.0
    if cm.shape == (2, 2):
        tn, fp, fn, tp = cm.ravel()
    else:
        tn = fp = fn = tp = 0
        if labels.size:
            pos_idx = np.where(labels == 1)[0]
            if pos_idx.size:
                idx = pos_idx[0]
                tp = cm[idx, idx]
                fp = cm[:, idx].sum() - tp
                fn = cm[idx, :].sum() - tp
                tn = total - tp - fp - fn
            else:
                tn = cm.trace()
    error_rate = 1.0 - accuracy
    precision = tp / (tp + fp) if (tp + fp) else 0.0
    recall = tp / (tp + fn) if (tp + fn) else 0.0
    metrics = {"TP": int(tp), "FP": int(fp), "TN": int(tn), "FN": int(fn)}
    return cm, metrics, accuracy, error_rate, precision, recall

def main():
    set_seed(42)
    expected_headers = [h.strip() for h in DATASET_HEADERS.split(",")]
    df = read_dataset(DATASET_PATH, expected_headers)
    column_map = map_columns(df, expected_headers)
    age_header = find_header(expected_headers, "Age")
    salary_header = find_header(expected_headers, "EstimatedSalary")
    target_header = find_header(expected_headers, "Purchased")
    age_col = column_map[age_header]
    salary_col = column_map[salary_header]
    target_col = column_map[target_header]
    X = df[[age_col, salary_col]].to_numpy()
    y = df[target_col].to_numpy()
    del df
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42
    )
    scaler = StandardScaler(copy=False)
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    model = LogisticRegression(random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    labels = np.sort(np.unique(y))
    if labels.size == 0:
        accuracy = 0.0
    else:
        _, _, accuracy, _, _, _ = compute_metrics(y_test, y_pred, labels)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed unused visualization and logging to reduce runtime and imports.
# - Implemented robust CSV parsing with delimiter/decimal fallback and normalized headers.
# - Reused arrays with in-place scaling (copy=False) to cut memory usage.
# - Derived all metrics from a single confusion matrix computation to avoid redundancy.
# - Set fixed seeds and explicit random_state for deterministic, reproducible results.