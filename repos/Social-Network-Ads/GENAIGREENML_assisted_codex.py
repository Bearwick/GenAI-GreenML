# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.pipeline import make_pipeline

RANDOM_STATE = 42
DATASET_HEADERS = ["User ID", "Gender", "Age", "EstimatedSalary", "Purchased"]


def clean_name(name):
    return "".join(ch.lower() for ch in str(name) if ch.isalnum())


def is_parsing_valid(df, headers):
    cleaned_cols = {clean_name(c) for c in df.columns}
    cleaned_headers = {clean_name(h) for h in headers}
    return len(cleaned_cols & cleaned_headers) >= max(2, len(headers) // 2)


def read_csv_robust(path, headers):
    df = pd.read_csv(path)
    if not is_parsing_valid(df, headers):
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def resolve_column(df, header, headers):
    cleaned_header = clean_name(header)
    cleaned_map = {clean_name(c): c for c in df.columns}
    if cleaned_header in cleaned_map:
        return cleaned_map[cleaned_header]
    for cleaned, original in cleaned_map.items():
        if cleaned_header in cleaned or cleaned in cleaned_header:
            return original
    if header in headers:
        idx = headers.index(header)
        if idx < len(df.columns):
            return df.columns[idx]
    raise KeyError(header)


def compute_metrics(cm):
    if cm.size == 4:
        tn, fp, fn, tp = cm.ravel()
    else:
        tn = cm[0, 0] if cm.size else 0
        fp = fn = tp = 0
    total = tn + fp + fn + tp
    accuracy = (tp + tn) / total if total else 0.0
    error_rate = 1.0 - accuracy
    precision = tp / (tp + fp) if (tp + fp) else 0.0
    recall = tp / (tp + fn) if (tp + fn) else 0.0
    return {
        "TP": tp,
        "FP": fp,
        "TN": tn,
        "FN": fn,
        "accuracy": accuracy,
        "error_rate": error_rate,
        "precision": precision,
        "recall": recall,
    }


def main():
    np.random.seed(RANDOM_STATE)
    df = read_csv_robust("Social_Network_Ads.csv", DATASET_HEADERS)
    age_header = DATASET_HEADERS[2] if len(DATASET_HEADERS) > 2 else "Age"
    salary_header = DATASET_HEADERS[3] if len(DATASET_HEADERS) > 3 else "EstimatedSalary"
    target_header = DATASET_HEADERS[-1]
    age_col = resolve_column(df, age_header, DATASET_HEADERS)
    salary_col = resolve_column(df, salary_header, DATASET_HEADERS)
    target_col = resolve_column(df, target_header, DATASET_HEADERS)

    X = df[[age_col, salary_col]].to_numpy(copy=False)
    y = df[target_col].to_numpy(copy=False)
    del df

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=RANDOM_STATE
    )
    model = make_pipeline(StandardScaler(), LogisticRegression(random_state=RANDOM_STATE))
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    cm = confusion_matrix(y_test, y_pred)
    metrics = compute_metrics(cm)
    accuracy = metrics["accuracy"]
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Combined preprocessing and model fitting in a pipeline to avoid storing scaled copies of data.
# - Derived all metrics from a single confusion matrix to eliminate redundant metric computations.
# - Selected only required columns and converted them directly to NumPy arrays with copy avoidance to reduce memory use.
# - Implemented a robust CSV loading strategy with a fallback delimiter/decimal to prevent unnecessary reprocessing.
# - Fixed random seeds for deterministic splits and model training.