# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH_CANDIDATES = [
    "Social_Network_Ads.csv",
    "social_network_ads.csv",
    "data.csv",
    "dataset.csv",
]


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed_columns(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _try_read_csv(path):
    try:
        df = pd.read_csv(path)
        return df
    except Exception:
        return None


def _try_read_csv_fallback(path):
    try:
        df = pd.read_csv(path, sep=";", decimal=",")
        return df
    except Exception:
        return None


def _looks_misparsed(df):
    if df is None or df.empty:
        return True
    if df.shape[1] == 1:
        first_col = df.columns[0]
        if isinstance(first_col, str) and ("," in first_col or ";" in first_col):
            return True
        sample = df.iloc[:5, 0].astype(str).tolist()
        if any((";" in s or "," in s) for s in sample):
            return True
    return False


def load_dataset():
    # Allow environment override, otherwise try common names.
    path = os.environ.get("DATASET_PATH", "").strip()
    candidates = [path] if path else []
    candidates += DATASET_PATH_CANDIDATES

    last_err = None
    for p in candidates:
        if not p:
            continue
        df = _try_read_csv(p)
        if _looks_misparsed(df):
            df2 = _try_read_csv_fallback(p)
            if df2 is not None and not _looks_misparsed(df2):
                df = df2
        if df is not None and not df.empty:
            df.columns = _normalize_columns(df.columns)
            df = _drop_unnamed_columns(df)
            return df

    if last_err:
        raise last_err
    raise FileNotFoundError("Could not load dataset. Set DATASET_PATH env var to the CSV file.")


def pick_target_and_features(df, preferred_target_names=None):
    if preferred_target_names is None:
        preferred_target_names = ["Purchased", "Target", "target", "label", "Label", "y", "Y"]

    cols = list(df.columns)

    # Prefer known target names if present.
    target_col = None
    for name in preferred_target_names:
        if name in cols:
            target_col = name
            break

    # Otherwise pick a reasonable target: non-constant column, prefer numeric/binary.
    if target_col is None:
        nunique = df.nunique(dropna=True)
        candidates = [c for c in cols if nunique.get(c, 0) >= 2]
        if not candidates:
            target_col = cols[-1]
        else:
            numeric_candidates = []
            for c in candidates:
                s = pd.to_numeric(df[c], errors="coerce")
                if s.notna().mean() > 0.8:
                    numeric_candidates.append(c)
            binary_like = []
            for c in numeric_candidates:
                s = pd.to_numeric(df[c], errors="coerce")
                uniq = pd.Series(s.dropna().unique())
                if len(uniq) <= 2:
                    binary_like.append(c)
            if binary_like:
                target_col = binary_like[0]
            elif numeric_candidates:
                target_col = numeric_candidates[0]
            else:
                target_col = candidates[0]

    feature_cols = [c for c in cols if c != target_col]
    if not feature_cols:
        feature_cols = [target_col]
    return target_col, feature_cols


def coerce_numeric_safely(df, numeric_cols):
    df2 = df.copy()
    for c in numeric_cols:
        df2[c] = pd.to_numeric(df2[c], errors="coerce")
    return df2


def bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    r2 = 0.0 if ss_tot <= 0 else (1.0 - ss_res / ss_tot)
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    df = load_dataset()
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed_columns(df)
    assert df is not None and not df.empty

    target_col, feature_cols = pick_target_and_features(df)
    X = df[feature_cols].copy()
    y_raw = df[target_col].copy()

    # Identify numeric vs categorical features using safe coercion heuristic.
    numeric_cols = []
    categorical_cols = []
    for c in X.columns:
        s_num = pd.to_numeric(X[c], errors="coerce")
        if s_num.notna().mean() >= 0.8:
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)

    # Coerce numeric columns (avoid numeric ops on object dtype).
    X = coerce_numeric_safely(X, numeric_cols)

    # Decide classification vs regression based on target.
    y_num = pd.to_numeric(y_raw, errors="coerce")
    y_num_valid_ratio = float(y_num.notna().mean()) if len(y_num) else 0.0
    is_numeric_target = y_num_valid_ratio >= 0.8

    task = "classification"
    y = y_raw

    if is_numeric_target:
        y = y_num
        uniq = pd.Series(y.dropna().unique())
        if len(uniq) >= 2 and len(uniq) <= 20 and np.all(np.isfinite(uniq)):
            # If numeric with few unique values, treat as classification (common for 0/1 labels).
            task = "classification"
        else:
            task = "regression"
    else:
        task = "classification"

    # Basic cleanup for target.
    if task == "classification":
        y = y.astype("string")
        y = y.fillna("MISSING")
        # Reduce to binary if clearly binary; else keep multiclass.
        classes = pd.Index(pd.unique(y))
        if len(classes) < 2:
            # Fallback: trivial baseline with accuracy 1.0 if only one class.
            accuracy = 1.0
            print(f"ACCURACY={accuracy:.6f}")
            return
    else:
        y = pd.to_numeric(y, errors="coerce")
        # Drop rows with missing numeric target.
        mask = y.notna()
        X = X.loc[mask].copy()
        y = y.loc[mask].copy()
        assert len(y) > 0

    # Preprocess with ColumnTransformer for reproducibility and efficiency.
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Train/test split with fixed seed.
    # Stratify only when feasible (classification and >1 class).
    stratify = None
    if task == "classification":
        try:
            if pd.Series(y).nunique() >= 2:
                stratify = y
        except Exception:
            stratify = None

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=stratify
    )
    assert len(X_train) > 0 and len(X_test) > 0

    if task == "classification":
        # Lightweight linear classifier; saga handles sparse one-hot efficiently.
        clf = LogisticRegression(
            solver="saga",
            max_iter=300,
            n_jobs=1,
            random_state=42,
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) for CPU efficiency and strong baselines on tabular data.
# - Employs ColumnTransformer + Pipeline for reproducible, single-pass preprocessing (impute + scale + one-hot) without manual feature handling.
# - Uses sparse one-hot encoding and saga solver to keep memory/CPU usage low on categorical expansion.
# - Implements robust CSV parsing fallback (default, then sep=';' and decimal=',') and column normalization/drop of 'Unnamed' columns to avoid schema brittleness.
# - Automatically selects target/feature columns if headers differ; safely coerces numeric data (errors='coerce') and handles NaN via imputers/dropping invalid targets.
# - Regression fallback uses a bounded accuracy proxy in [0,1]: (R^2 + 1)/2 clipped, maintaining stable reporting under the required ACCURACY format.