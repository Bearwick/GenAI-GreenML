# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier

warnings.filterwarnings("ignore")

dataset_path = "Social_Network_Ads.csv"
dataset_headers_str = "User ID,Gender,Age,EstimatedSalary,Purchased"

def read_csv_robust(path):
    try:
        df_local = pd.read_csv(path)
    except Exception:
        df_local = pd.read_csv(path, sep=';', decimal=',')
        return df_local
    if df_local.shape[1] <= 1:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > df_local.shape[1]:
                df_local = df_alt
        except Exception:
            pass
    return df_local

df = read_csv_robust(dataset_path)
if df is None:
    df = pd.DataFrame()

df = df.copy()
df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
df = df.loc[:, [c for c in df.columns if not str(c).lower().startswith('unnamed')]]
df = df.dropna(axis=1, how='all')

assert df.shape[0] > 0 and df.shape[1] > 0

numeric_cols = []
for col in df.columns:
    coerced = pd.to_numeric(df[col], errors='coerce')
    if coerced.notna().sum() >= max(1, int(0.5 * len(df))):
        df[col] = coerced
        numeric_cols.append(col)

df.replace([np.inf, -np.inf], np.nan, inplace=True)

headers_candidate = [h.strip() for h in dataset_headers_str.split(',') if h.strip()]

def match_col(name, cols):
    for c in cols:
        if str(c).lower() == str(name).lower():
            return c
    return None

target = None
preferred = []
if headers_candidate:
    preferred = [headers_candidate[-1]] + headers_candidate
for cand in preferred:
    col = match_col(cand, df.columns)
    if col is not None:
        target = col
        break
if target is None:
    for col in numeric_cols:
        if df[col].nunique(dropna=True) > 1:
            target = col
            break
if target is None:
    for col in df.columns:
        if df[col].nunique(dropna=True) > 1:
            target = col
            break
if target is None:
    target = df.columns[0]

if df[target].notna().sum() == 0:
    df[target] = 0
else:
    df = df[df[target].notna()].copy()

df = df.dropna(axis=1, how='all')
numeric_cols = [c for c in numeric_cols if c in df.columns]

assert df.shape[0] > 0

id_like_cols = []
if len(df) > 0:
    for col in df.columns:
        if col == target:
            continue
        if 'id' in str(col).lower():
            nunique = df[col].nunique(dropna=True)
            if nunique >= 0.9 * len(df):
                id_like_cols.append(col)
if id_like_cols:
    df = df.drop(columns=id_like_cols, errors='ignore')
    numeric_cols = [c for c in numeric_cols if c in df.columns]

features = [c for c in df.columns if c != target]
if len(features) == 0:
    df['constant_feature'] = 0
    features = ['constant_feature']
    if 'constant_feature' not in numeric_cols:
        numeric_cols.append('constant_feature')

if len(df) < 2:
    df = pd.concat([df, df], ignore_index=True)

X = df[features]
y = df[target]

def is_classification_target(y_series):
    if y_series.dtype == object or y_series.dtype.name == 'category' or y_series.dtype == bool:
        return True
    unique_vals = y_series.dropna().unique()
    if len(unique_vals) == 0:
        return False
    if len(unique_vals) <= 20:
        if np.all(np.isclose(unique_vals, np.round(unique_vals))):
            return True
    return False

classification = is_classification_target(y)

numeric_features = [c for c in features if c in numeric_cols and df[c].notna().any()]
categorical_features = [c for c in features if c not in numeric_features]

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler(with_mean=False))
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
])

transformers = []
if len(numeric_features) > 0:
    transformers.append(('num', numeric_transformer, numeric_features))
if len(categorical_features) > 0:
    transformers.append(('cat', categorical_transformer, categorical_features))

if len(transformers) > 0:
    preprocessor = ColumnTransformer(transformers=transformers, remainder='drop', sparse_threshold=0.3)
else:
    preprocessor = 'passthrough'

test_size = 0.2
if len(df) < 5:
    test_size = 0.5

stratify = None
if classification:
    if y.nunique(dropna=True) >= 2 and y.value_counts().min() >= 2:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=42, stratify=stratify
)

assert X_train.shape[0] > 0 and X_test.shape[0] > 0

if classification:
    if y_train.nunique(dropna=True) < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear', random_state=42)
else:
    model = LinearRegression()

clf = Pipeline(steps=[('preprocess', preprocessor),
                     ('model', model)])

clf.fit(X_train, y_train)

preds = clf.predict(X_test)

if classification:
    try:
        accuracy = accuracy_score(y_test, preds)
    except Exception:
        accuracy = float(np.mean(preds == y_test))
else:
    if len(np.unique(y_test)) < 2:
        accuracy = 1.0
    else:
        r2 = r2_score(y_test, preds)
        accuracy = (r2 + 1.0) / 2.0
        if np.isnan(accuracy) or np.isinf(accuracy):
            accuracy = 0.0
        accuracy = max(0.0, min(1.0, accuracy))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# Used lightweight linear/logistic models with minimal preprocessing for CPU efficiency.
# ColumnTransformer with imputation and one-hot encoding handles mixed types robustly without heavy feature engineering.
# Regression fallback reports a bounded accuracy proxy by mapping R^2 to [0,1] for stable output.