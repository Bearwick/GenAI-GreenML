# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier

warnings.filterwarnings("ignore")

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
        return df
    if df.shape[1] <= 1:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > 1:
                df = df_alt
        except Exception:
            pass
    return df

df = read_csv_robust("Social_Network_Ads.csv")
if df is None:
    raise ValueError("Dataset could not be loaded")
df.columns = [re.sub(r"\s+", " ", str(c).strip()) for c in df.columns]
df = df.loc[:, ~df.columns.str.contains("^Unnamed", case=False, regex=True)]
assert df.shape[0] > 0

expected_headers = ["User ID", "Gender", "Age", "EstimatedSalary", "Purchased"]
preferred_targets = ["purchased", "purchase", "target", "label", "y", "class", "outcome", "response"]
lower_cols = {c.lower(): c for c in df.columns}
target_col = None
for name in preferred_targets:
    if name in lower_cols:
        target_col = lower_cols[name]
        break
if target_col is None:
    for name in expected_headers[::-1]:
        if name.lower() in lower_cols:
            target_col = lower_cols[name.lower()]
            break
if target_col is None:
    numeric_candidates = []
    for col in df.columns:
        series = pd.to_numeric(df[col], errors="coerce")
        if series.notna().sum() > 0 and series.nunique(dropna=True) > 1:
            numeric_candidates.append(col)
    if numeric_candidates:
        target_col = numeric_candidates[-1]
    else:
        target_col = df.columns[-1]

X = df.drop(columns=[target_col]).copy()
y_raw = df[target_col].copy()

for col in list(X.columns):
    if "id" in col.lower():
        if X[col].nunique(dropna=False) == len(X):
            X = X.drop(columns=[col])

if X.shape[1] == 0:
    X = pd.DataFrame({"constant": np.ones(len(df))}, index=df.index)

y_numeric = pd.to_numeric(y_raw, errors="coerce")
numeric_ratio = y_numeric.notna().mean() if len(y_numeric) > 0 else 0

if numeric_ratio >= 0.5:
    y_num_clean = y_numeric.replace([np.inf, -np.inf], np.nan)
    mask = y_num_clean.notna()
    y_processed = y_num_clean[mask]
    if y_processed.size > 0:
        unique_vals = y_processed.unique()
        is_int_like = np.all(np.isclose(unique_vals, np.round(unique_vals)))
    else:
        is_int_like = False
    if is_int_like and y_processed.nunique(dropna=True) <= 20:
        classification = True
        y_processed = y_processed.round().astype(int)
    else:
        classification = False
else:
    classification = True
    mask = y_raw.notna()
    y_processed = y_raw[mask].astype(str)

X = X.loc[mask].reset_index(drop=True)
y_processed = y_processed.reset_index(drop=True)

X = X.loc[:, X.notna().any()]
if X.shape[1] == 0:
    X = pd.DataFrame({"constant": np.ones(len(X))})

if len(X) < 2:
    X = pd.concat([X] * 2, ignore_index=True)
    y_processed = pd.concat([y_processed] * 2, ignore_index=True)

assert len(X) > 0

def infer_types(X_in):
    X_out = X_in.copy()
    numeric_features = []
    categorical_features = []
    for col in X_out.columns:
        converted = pd.to_numeric(X_out[col], errors="coerce")
        if converted.notna().mean() > 0.8:
            X_out[col] = converted
            numeric_features.append(col)
        else:
            categorical_features.append(col)
    return X_out, numeric_features, categorical_features

X, numeric_features, categorical_features = infer_types(X)

for col in numeric_features:
    X[col] = X[col].replace([np.inf, -np.inf], np.nan)

all_nan_num = [col for col in numeric_features if X[col].isna().all()]
if all_nan_num:
    X = X.drop(columns=all_nan_num)
    numeric_features = [col for col in numeric_features if col not in all_nan_num]

all_nan_cat = [col for col in categorical_features if X[col].isna().all()]
if all_nan_cat:
    X = X.drop(columns=all_nan_cat)
    categorical_features = [col for col in categorical_features if col not in all_nan_cat]

if X.shape[1] == 0:
    X = pd.DataFrame({"constant": np.ones(len(X))})
    numeric_features = ["constant"]
    categorical_features = []

transformers = []
if numeric_features:
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False))
    ])
    transformers.append(("num", numeric_transformer, numeric_features))
if categorical_features:
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])
    transformers.append(("cat", categorical_transformer, categorical_features))
if not transformers:
    X = pd.DataFrame({"constant": np.ones(len(X))})
    numeric_features = ["constant"]
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False))
    ])
    transformers = [("num", numeric_transformer, numeric_features)]

preprocessor = ColumnTransformer(transformers=transformers, remainder="drop")

if classification:
    n_classes = pd.Series(y_processed).nunique(dropna=True)
else:
    n_classes = None

if classification and n_classes is not None and n_classes >= 2:
    class_counts = pd.Series(y_processed).value_counts()
    if class_counts.min() >= 2:
        stratify = y_processed
    else:
        stratify = None
else:
    stratify = None

X_train, X_test, y_train, y_test = train_test_split(
    X, y_processed, test_size=0.25, random_state=42, stratify=stratify
)
assert len(X_train) > 0 and len(X_test) > 0

if classification:
    if pd.Series(y_train).nunique(dropna=True) < 2:
        model = DummyClassifier(strategy="most_frequent")
    else:
        model = LogisticRegression(max_iter=200, solver="liblinear", random_state=42)
else:
    model = LinearRegression()

pipeline = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    if np.isnan(r2):
        r2 = 0.0
    accuracy = max(0.0, min(1.0, r2))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# - Lightweight Logistic/Linear models keep CPU and energy usage low while providing a strong baseline.
# - ColumnTransformer with simple imputation/scaling/one-hot encoding ensures reproducible preprocessing without heavy feature engineering.
# - Robust schema and target inference with safe fallbacks maintains end-to-end execution on unknown data layouts.
# - Regression fallback uses clipped R^2 as a stable accuracy proxy within [0,1].