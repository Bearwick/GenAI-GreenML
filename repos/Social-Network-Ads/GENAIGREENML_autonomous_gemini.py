# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def load_and_preprocess():
    # Robust CSV loading
    path = "Social_Network_Ads.csv"
    try:
        df = pd.read_csv(path)
    except Exception:
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            # Create a dummy dataframe if file is missing to ensure script doesn't hard-crash in unexpected envs
            df = pd.DataFrame(columns=['User ID', 'Gender', 'Age', 'EstimatedSalary', 'Purchased'])

    # Clean column names: strip whitespace and normalize internal spaces
    df.columns = [str(col).strip() for col in df.columns]
    df.columns = [" ".join(col.split()) for col in df.columns]
    
    # Drop completely empty columns or Unnamed ones
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

    if df.empty:
        return None, None, 0.0

    # Determine Target
    target_candidates = ['Purchased', 'purchased', 'Target', 'target']
    target_col = None
    for cand in target_candidates:
        if cand in df.columns:
            target_col = cand
            break
    if not target_col:
        target_col = df.columns[-1]

    # Pre-processing: Handle missing values and types
    # Target must be numeric/categorical with no NaNs
    df = df.dropna(subset=[target_col])
    
    # Identify feature types
    # Exclude IDs if they look like unique counters
    features = [c for c in df.columns if c != target_col and 'id' not in c.lower()]
    
    X = df[features].copy()
    y = df[target_col].copy()

    # Coerce features to appropriate types
    num_cols = []
    cat_cols = []
    for col in features:
        # Try converting to numeric
        converted = pd.to_numeric(X[col], errors='coerce')
        if converted.isna().mean() < 0.5: # If mostly numeric
            X[col] = converted
            num_cols.append(col)
        else:
            X[col] = X[col].astype(str)
            cat_cols.append(col)

    # Final cleaning: Drop rows with NaN in features for simplicity/efficiency
    valid_idx = X.notna().all(axis=1)
    X = X[valid_idx]
    y = y[valid_idx]

    if len(X) < 10: # Minimal data check
        return None, None, 0.0

    return X, y, (num_cols, cat_cols)

def run_pipeline():
    X, y, col_types = load_and_preprocess()
    
    if X is None or len(np.unique(y)) < 2:
        # Fallback for degenerate cases
        print(f"ACCURACY={0.000000:.6f}")
        return

    num_cols, cat_cols = col_types
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=y if len(np.unique(y)) > 1 else None
    )

    # Lightweight Preprocessing Pipeline
    # StandardScaler for numeric (essential for Logistic Regression)
    # OneHotEncoder for categoricals
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), num_cols),
            ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
        ],
        remainder='drop'
    )

    # Model choice: Logistic Regression (extremely CPU efficient and low energy)
    model = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42))
    ])

    # Training
    model.fit(X_train, y_train)

    # Evaluation
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary:
# 1. Model Selection: Used Logistic Regression which is mathematically simple, 
#    requiring minimal CPU cycles compared to ensembles or deep learning.
# 2. Data Loading: Implemented robust CSV parsing with fallbacks and column 
#    normalization to prevent crashes on varying environments.
# 3. Preprocessing: Used sklearn Pipeline and ColumnTransformer to avoid 
#    redundant data copies and ensure an efficient, single-pass transformation.
# 4. Feature Engineering: Limited to standard scaling and one-hot encoding; 
#    avoided expensive polynomial features or complex embeddings.
# 5. Resource Management: Avoided large libraries like matplotlib or seaborn 
#    during the inference/accuracy step to minimize memory footprint and power draw.
# 6. Fallback Logic: Included checks for empty datasets or single-class targets 
#    to ensure the script terminates gracefully without burning CPU on errors.