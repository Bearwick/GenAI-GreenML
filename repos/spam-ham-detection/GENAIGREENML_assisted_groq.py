# Generated by generate_llm_code.py
# LLM: groq
# Mode: assisted

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC

email = pd.read_csv('emails.csv')
x, y = email['text'], email['spam']
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=1000, stratify=y
)

pipeline = Pipeline([
    ('vect', CountVectorizer(max_features=5000)),
    ('clf', LinearSVC())
])
pipeline.fit(x_train, y_train)

accuracy = pipeline.score(x_test, y_test)
print(f"ACCURACY={accuracy:.6f}")

def predict_email(text):
    return pipeline.predict([text])[0]

# Optimizations applied:
# 1. Replaced heavy SVC with fast LinearSVC for linear kernel, reducing training time and memory.
# 2. Used sklearn Pipeline to encapsulate vectorization and classification, avoiding duplicate data handling.
# 3. Limited CountVectorizer to max_features=5000 to constrain feature space, lowering memory footprint and speeding up training.
# 4. Removed all unnecessary prints and interactive prompts to reduce I/O overhead.
# 5. Kept reproducibility via fixed random_state and stratified split.
# 6. Avoided intermediate variables for feature matrices, fitting directly within the pipeline.