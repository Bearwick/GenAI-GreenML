# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC

def load_data(filepath):
    try:
        df = pd.read_csv(filepath)
        if 'text' not in df.columns:
            raise ValueError
    except:
        df = pd.read_csv(filepath, sep=';', decimal=',')
    return df

df = load_data('emails.csv')
cols = df.columns.tolist()
text_col = 'text' if 'text' in cols else cols[0]
label_col = 'spam' if 'spam' in cols else cols[1]

X = df[text_col]
y = df[label_col].astype(np.int8)

x_train, x_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1000
)

cv = CountVectorizer()
features_train = cv.fit_transform(x_train)
features_test = cv.transform(x_test)

linear_model = SVC(random_state=1000, cache_size=1000)
linear_model.fit(features_train, y_train)

accuracy = linear_model.score(features_test, y_test)

def predict_email(text):
    text_feature = cv.transform([text])
    return linear_model.predict(text_feature)[0]

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Robust Data Loading: Implemented fallback logic for CSV parsing (sep/decimal) and dynamic column detection to ensure reliability.
# 2. Memory Reduction: Downcast the target labels ('spam') to int8 to minimize the memory footprint.
# 3. Computational Efficiency: Increased the SVC cache_size to 1000MB, reducing the overhead of kernel calculations during training.
# 4. Redundancy Elimination: Removed all intermediate prints, data previews, and logging statements to reduce I/O overhead and runtime.
# 5. Pipeline Streamlining: Combined feature transformation steps and removed unnecessary intermediate data structures.
# 6. Reproducibility: Applied a fixed random_state to both the train_test_split and the SVC model to ensure deterministic, stable results.
# 7. Preprocessing Optimization: Used direct column indexing and avoided redundant dataframe copies.