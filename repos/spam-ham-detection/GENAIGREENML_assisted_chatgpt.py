# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import random
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC


SEED = 1000


def _set_reproducible(seed: int) -> None:
    random.seed(seed)
    np.random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)


def _read_csv_robust(path: str, expected_headers: tuple[str, ...]) -> pd.DataFrame:
    df = pd.read_csv(path)
    if not set(expected_headers).issubset(df.columns):
        df = pd.read_csv(path, sep=";", decimal=",")
    if not set(expected_headers).issubset(df.columns):
        raise ValueError(
            f"CSV schema mismatch. Expected columns {expected_headers}, got {tuple(df.columns)}"
        )
    return df


def _prepare_xy(df: pd.DataFrame, text_col: str, label_col: str):
    x = df[text_col].astype(str)
    y = df[label_col]
    return x, y


def train_and_evaluate(dataset_path: str = "emails.csv") -> float:
    _set_reproducible(SEED)

    df = _read_csv_robust(dataset_path, expected_headers=("text", "spam"))

    text_col = "text" if "text" in df.columns else df.columns[0]
    label_col = "spam" if "spam" in df.columns else df.columns[1]

    x, y = _prepare_xy(df, text_col, label_col)

    x_train, x_test, y_train, y_test = train_test_split(
        x, y, test_size=0.2, random_state=SEED
    )

    vectorizer = CountVectorizer(dtype=np.int32)
    x_train_vec = vectorizer.fit_transform(x_train)

    model = SVC()
    model.fit(x_train_vec, y_train)

    x_test_vec = vectorizer.transform(x_test)
    accuracy = float(model.score(x_test_vec, y_test))

    print(f"ACCURACY={accuracy:.6f}")
    return accuracy


if __name__ == "__main__":
    train_and_evaluate()

# Optimization Summary
# - Removed all exploratory prints and interactive prediction to avoid unnecessary I/O and runtime overhead.
# - Added robust CSV parsing with a fallback delimiter/decimal strategy to prevent wasted computation on mis-parsed data.
# - Ensured reproducibility by setting fixed seeds (random, numpy, PYTHONHASHSEED) and using the original split seed.
# - Vectorized text with CountVectorizer using int32 counts to reduce memory footprint with identical feature values.
# - Avoided storing redundant intermediate objects and eliminated unused functions/outputs to reduce data movement and memory use.