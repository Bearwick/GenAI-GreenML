# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _read_csv_robust(path: str) -> pd.DataFrame:
    # Try default CSV parsing first, then fallback to common European formatting.
    try:
        df0 = pd.read_csv(path)
    except Exception:
        df0 = None

    def _looks_wrong(df: pd.DataFrame) -> bool:
        if df is None or not isinstance(df, pd.DataFrame) or df.shape[0] == 0:
            return True
        # If only 1 column but we expect multiple, parsing might be wrong.
        if df.shape[1] == 1:
            return True
        # If many columns are unnamed, may be misparsed.
        unnamed = sum(str(c).startswith("Unnamed:") for c in df.columns)
        if unnamed >= max(1, df.shape[1] // 2):
            return True
        return False

    if _looks_wrong(df0):
        try:
            df1 = pd.read_csv(path, sep=";", decimal=",")
            if not _looks_wrong(df1):
                return df1
        except Exception:
            pass
    if df0 is None:
        # Last resort: read raw lines and attempt basic CSV parsing
        df0 = pd.read_csv(path, sep=None, engine="python")
    return df0


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols = []
    for c in df.columns:
        s = str(c)
        s = s.strip()
        s = re.sub(r"\s+", " ", s)
        cols.append(s)
    df = df.copy()
    df.columns = cols
    # Drop unnamed columns
    drop_cols = [c for c in df.columns if str(c).startswith("Unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _pick_target(df: pd.DataFrame, preferred=("spam",)) -> str:
    lower_map = {str(c).strip().lower(): c for c in df.columns}
    for p in preferred:
        if p in lower_map:
            return lower_map[p]
    # Prefer a non-constant numeric/bool-like column
    best = None
    best_nuniq = -1
    for c in df.columns:
        s = df[c]
        if s.dtype == "O":
            # Try coercing; but don't mutate here
            coerced = pd.to_numeric(s, errors="coerce")
            if coerced.notna().sum() == 0:
                continue
            nun = coerced.nunique(dropna=True)
        else:
            nun = s.nunique(dropna=True)
        if nun >= 2 and nun > best_nuniq:
            best = c
            best_nuniq = nun
    # If none found, fall back to last column
    return best if best is not None else df.columns[-1]


def _find_text_feature(df: pd.DataFrame, exclude: str) -> str | None:
    # Prefer columns named like "text", "message", "body", etc.
    candidates_by_name = []
    for c in df.columns:
        if c == exclude:
            continue
        lc = str(c).lower()
        if any(k in lc for k in ["text", "message", "body", "email", "content", "subject"]):
            candidates_by_name.append(c)
    if candidates_by_name:
        return candidates_by_name[0]

    # Otherwise choose the object column with highest average string length
    best = None
    best_len = -1.0
    for c in df.columns:
        if c == exclude:
            continue
        if df[c].dtype == "O":
            s = df[c].astype(str)
            avg_len = s.str.len().replace([np.inf, -np.inf], np.nan).dropna().mean()
            if avg_len is not None and avg_len > best_len:
                best = c
                best_len = float(avg_len)
    return best


def _is_classification_target(y: pd.Series) -> bool:
    # Classification if small number of unique values relative to dataset size
    y_nonnull = y.dropna()
    if y_nonnull.shape[0] == 0:
        return False
    nun = y_nonnull.nunique(dropna=True)
    if nun < 2:
        return False
    # If values look like 0/1 or few categories, classify
    if nun <= 20:
        return True
    # Heuristic: if numeric and many unique, likely regression
    return False


def _coerce_target(y: pd.Series) -> pd.Series:
    # Attempt numeric coercion; keep original if it results in all NaNs.
    if y.dtype == "O":
        yn = pd.to_numeric(y, errors="coerce")
        if yn.notna().sum() > 0:
            return yn
    return y


def _bounded_regression_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    # Bounded [0,1] proxy: 1 / (1 + MAE / (MAD + eps))
    # Robust to scale; MAD uses median absolute deviation from median.
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    mae = np.mean(np.abs(yt - yp))
    med = np.median(yt)
    mad = np.median(np.abs(yt - med))
    eps = 1e-12
    score = 1.0 / (1.0 + (mae / (mad + eps)))
    if not np.isfinite(score):
        return 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    df = _read_csv_robust("emails.csv")
    df = _normalize_columns(df)

    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _pick_target(df, preferred=("spam",))
    y_raw = df[target_col]
    y_raw = _coerce_target(y_raw)

    text_col = _find_text_feature(df, exclude=target_col)

    feature_cols = [c for c in df.columns if c != target_col]
    X = df[feature_cols].copy()

    # Ensure numeric coercion is safe later by not operating on object columns directly.
    # Split features into text, categorical, numeric
    if text_col is not None and text_col in X.columns:
        text_features = [text_col]
    else:
        text_features = []

    other_cols = [c for c in X.columns if c not in text_features]
    numeric_features = []
    categorical_features = []
    for c in other_cols:
        if X[c].dtype == "O":
            # Try numeric coercion; if mostly numeric, treat as numeric
            coerced = pd.to_numeric(X[c], errors="coerce")
            if coerced.notna().sum() >= max(1, int(0.9 * len(coerced))):
                X[c] = coerced
                numeric_features.append(c)
            else:
                categorical_features.append(c)
        else:
            numeric_features.append(c)

    # Build preprocessors
    transformers = []

    if text_features:
        # Lightweight text features: TF-IDF with capped vocab; efficient on CPU and sparse.
        text_transformer = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="constant", fill_value="")),
                ("tfidf", TfidfVectorizer(lowercase=True, stop_words="english", max_features=20000, ngram_range=(1, 1))),
            ]
        )
        transformers.append(("text", text_transformer, text_features[0]))

    if categorical_features:
        cat_transformer = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
            ]
        )
        transformers.append(("cat", cat_transformer, categorical_features))

    if numeric_features:
        num_transformer = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="median")),
            ]
        )
        transformers.append(("num", num_transformer, numeric_features))

    if not transformers:
        # If no usable features, create a single dummy numeric feature.
        X = pd.DataFrame({"__bias__": np.ones(len(df), dtype=np.float32)})
        numeric_features = ["__bias__"]
        transformers = [("num", Pipeline(steps=[("imputer", SimpleImputer(strategy="median"))]), numeric_features)]

    preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    # Decide task type with defensive checks
    y = y_raw.copy()

    # If target is object and looks categorical, keep as-is; else coerce numeric.
    if y.dtype == "O":
        # Leave object target for classification if small number of classes
        pass
    else:
        # Already numeric
        pass

    # Drop rows where target is missing
    mask = pd.notna(y)
    X = X.loc[mask].reset_index(drop=True)
    y = y.loc[mask].reset_index(drop=True)

    assert X.shape[0] > 1

    is_clf = _is_classification_target(y)

    # Train/test split
    # If classification, stratify when possible
    stratify = None
    if is_clf:
        try:
            if y.nunique(dropna=True) >= 2:
                stratify = y
        except Exception:
            stratify = None

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=stratify,
    )

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    if is_clf:
        # If classes collapse in split, fallback to regression-style proxy on numeric encoding
        if pd.Series(y_train).nunique(dropna=True) < 2 or pd.Series(y_test).nunique(dropna=True) < 2:
            is_clf = False

    if is_clf:
        # Logistic regression is a strong, efficient baseline for sparse text features.
        clf = LogisticRegression(max_iter=200, solver="liblinear")
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        # Regression fallback with bounded proxy score in [0,1]
        y_train_num = pd.to_numeric(y_train, errors="coerce")
        y_test_num = pd.to_numeric(y_test, errors="coerce")

        # If conversion fails, use constant baseline and score=0.0
        if y_train_num.notna().sum() == 0 or y_test_num.notna().sum() == 0:
            accuracy = 0.0
        else:
            # Align rows where y is numeric
            tr_mask = y_train_num.notna()
            te_mask = y_test_num.notna()
            X_train2 = X_train.loc[tr_mask].reset_index(drop=True)
            y_train2 = y_train_num.loc[tr_mask].reset_index(drop=True)
            X_test2 = X_test.loc[te_mask].reset_index(drop=True)
            y_test2 = y_test_num.loc[te_mask].reset_index(drop=True)

            if X_train2.shape[0] == 0 or X_test2.shape[0] == 0:
                accuracy = 0.0
            else:
                reg = Ridge(alpha=1.0, random_state=42)
                model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
                model.fit(X_train2, y_train2)
                y_pred = model.predict(X_test2)
                accuracy = _bounded_regression_score(y_test2.to_numpy(), y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used TF-IDF with max_features cap to keep memory/CPU bounded while retaining strong baseline performance for spam text.
# - Preferred LogisticRegression (liblinear) as a lightweight, CPU-friendly linear classifier on sparse features; avoided heavy ensembles/deep models.
# - Built a single reproducible sklearn Pipeline + ColumnTransformer to prevent redundant preprocessing and ensure deterministic behavior.
# - Implemented robust CSV parsing fallback (default then sep=';' + decimal=',') and column normalization to reduce schema brittleness.
# - Defensive target/feature selection: uses provided headers if available, otherwise chooses a non-constant numeric target; selects a likely text column by name/length heuristics.
# - Safe numeric coercion and missing-value handling via SimpleImputer; drops missing targets.
# - Regression fallback uses Ridge and reports a bounded [0,1] proxy score: 1/(1+MAE/(MAD+eps)) to keep ACCURACY comparable/stable across scales.