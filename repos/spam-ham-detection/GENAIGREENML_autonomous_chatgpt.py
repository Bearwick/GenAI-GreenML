# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.linear_model import SGDClassifier, SGDRegressor
from sklearn.metrics import accuracy_score


RANDOM_STATE = 42
DATASET_PATH = "emails.csv"


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    # Try default CSV parsing first
    try:
        df = pd.read_csv(path)
        if df.shape[1] >= 2:
            return df
    except Exception:
        df = None

    # Fallback: semicolon separator and comma decimal
    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
        return df2
    except Exception:
        if df is not None:
            return df
        raise


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _pick_text_column(df, preferred=("text", "message", "email", "body", "content")):
    cols_lower = {c.lower(): c for c in df.columns}
    for p in preferred:
        if p in cols_lower:
            return cols_lower[p]

    # pick the "most text-like" column: object dtype with highest avg length
    obj_cols = [c for c in df.columns if df[c].dtype == "object"]
    best_col = None
    best_score = -1.0
    for c in obj_cols:
        s = df[c].astype(str)
        avg_len = s.str.len().replace([np.inf, -np.inf], np.nan).dropna()
        score = float(avg_len.mean()) if len(avg_len) else 0.0
        if score > best_score:
            best_score = score
            best_col = c
    return best_col


def _pick_target_column(df, preferred=("spam", "label", "target", "y", "class")):
    cols_lower = {c.lower(): c for c in df.columns}
    for p in preferred:
        if p in cols_lower:
            return cols_lower[p]

    # Prefer numeric non-constant column
    numeric_candidates = []
    for c in df.columns:
        if c == _pick_text_column(df):
            continue
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() == 0:
            continue
        nun = s.nunique(dropna=True)
        if nun >= 2:
            numeric_candidates.append((nun, c))
    if numeric_candidates:
        numeric_candidates.sort(reverse=True)
        return numeric_candidates[0][1]

    # Otherwise pick any non-constant column
    for c in df.columns:
        if c == _pick_text_column(df):
            continue
        nun = df[c].nunique(dropna=True)
        if nun >= 2:
            return c

    return None


def _coerce_binary_labels(y):
    # Convert typical spam/ham encodings into 0/1 when possible
    y0 = y.copy()

    if y0.dtype == "object":
        ys = y0.astype(str).str.strip().str.lower()
        mapping = {
            "spam": 1, "ham": 0,
            "true": 1, "false": 0,
            "yes": 1, "no": 0,
            "1": 1, "0": 0,
            "pos": 1, "neg": 0,
            "positive": 1, "negative": 0,
        }
        y_mapped = ys.map(mapping)
        if y_mapped.notna().mean() >= 0.9:
            y0 = y_mapped
        else:
            # try numeric coercion
            y_num = pd.to_numeric(y0, errors="coerce")
            if y_num.notna().sum() > 0:
                y0 = y_num

    else:
        y0 = pd.to_numeric(y0, errors="coerce")

    return y0


def _bounded_regression_score(y_true, y_pred):
    # Stable proxy in [0,1]: 1 / (1 + normalized RMSE)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    rmse = float(np.sqrt(np.mean((yt - yp) ** 2)))
    scale = float(np.std(yt)) + 1e-12
    nrmse = rmse / scale
    score = 1.0 / (1.0 + nrmse)
    return float(np.clip(score, 0.0, 1.0))


def main():
    if not os.path.exists(DATASET_PATH):
        raise FileNotFoundError(f"Dataset not found at path: {DATASET_PATH}")

    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Ensure non-empty
    assert df.shape[0] > 0 and df.shape[1] > 0

    text_col = _pick_text_column(df)
    target_col = _pick_target_column(df)

    # If text column missing, create a lightweight surrogate text from all columns
    if text_col is None:
        text_col = "__text__"
        df[text_col] = df.astype(str).agg(" ".join, axis=1)

    # If target missing, create a trivial target to keep pipeline runnable
    if target_col is None:
        target_col = "__target__"
        df[target_col] = 0

    X_text = df[text_col].astype(str).fillna("")
    y_raw = df[target_col]

    y = _coerce_binary_labels(y_raw)

    # Drop rows with missing target after coercion when possible
    valid = pd.Series(np.ones(len(df), dtype=bool), index=df.index)
    if pd.api.types.is_numeric_dtype(y):
        valid = valid & y.notna() & np.isfinite(y)
    else:
        valid = valid & y.notna()

    X_text = X_text.loc[valid]
    y = y.loc[valid]

    assert len(X_text) > 1

    # Determine classification viability
    is_classification = False
    y_for_model = y

    if pd.api.types.is_numeric_dtype(y_for_model):
        # If numeric and looks like binary/small-cardinality, treat as classification
        nun = int(pd.Series(y_for_model).nunique(dropna=True))
        if nun >= 2 and nun <= 20:
            is_classification = True
        else:
            is_classification = False
    else:
        # Non-numeric: treat as classification if at least 2 classes
        nun = int(pd.Series(y_for_model).nunique(dropna=True))
        is_classification = nun >= 2

    # Train/test split defensively
    if is_classification:
        # Ensure at least 2 classes in both splits if possible
        y_series = pd.Series(y_for_model)
        if y_series.nunique(dropna=True) < 2:
            is_classification = False
        else:
            strat = y_series if y_series.nunique() > 1 else None
            X_train, X_test, y_train, y_test = train_test_split(
                X_text,
                y_series,
                test_size=0.2,
                random_state=RANDOM_STATE,
                stratify=strat,
            )
            assert len(X_train) > 0 and len(X_test) > 0

            # Energy-efficient text pipeline: HashingVectorizer avoids vocabulary build and saves memory
            clf = Pipeline(
                steps=[
                    ("vec", HashingVectorizer(
                        n_features=2**18,
                        alternate_sign=False,
                        norm="l2",
                        lowercase=True,
                        stop_words=None,
                        ngram_range=(1, 1),
                    )),
                    ("clf", SGDClassifier(
                        loss="log_loss",
                        alpha=1e-5,
                        max_iter=1000,
                        tol=1e-3,
                        random_state=RANDOM_STATE,
                    )),
                ]
            )

            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

    # Regression fallback (also covers degenerate classification target)
    y_num = pd.to_numeric(y_for_model, errors="coerce")
    valid2 = y_num.notna() & np.isfinite(y_num)
    X_text2 = X_text.loc[valid2]
    y_num2 = y_num.loc[valid2]
    if len(X_text2) <= 1 or y_num2.nunique(dropna=True) < 2:
        # Trivial baseline: predict constant, score 0.0 if no variance
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    X_train, X_test, y_train, y_test = train_test_split(
        X_text2,
        y_num2,
        test_size=0.2,
        random_state=RANDOM_STATE,
    )
    assert len(X_train) > 0 and len(X_test) > 0

    reg = Pipeline(
        steps=[
            ("vec", HashingVectorizer(
                n_features=2**18,
                alternate_sign=False,
                norm="l2",
                lowercase=True,
                stop_words=None,
                ngram_range=(1, 1),
            )),
            ("reg", SGDRegressor(
                loss="squared_error",
                alpha=1e-5,
                max_iter=1000,
                tol=1e-3,
                random_state=RANDOM_STATE,
            )),
        ]
    )

    reg.fit(X_train, y_train)
    y_pred = reg.predict(X_test)
    accuracy = _bounded_regression_score(y_test.to_numpy(), y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses HashingVectorizer (fixed feature size, no vocabulary building) to minimize memory/CPU overhead and ensure fast, reproducible text featurization.
# - Uses SGDClassifier with logistic loss (linear model) as a lightweight baseline suitable for high-dimensional sparse text on CPU.
# - Robust CSV loading with delimiter/decimal fallback; normalizes column names and drops 'Unnamed:' columns to avoid schema-related failures.
# - Defensive target/text selection: prefers provided headers but adapts if missing by selecting best available columns; avoids hard failures on schema mismatch.
# - Coerces numeric labels safely and drops invalid rows; includes degeneracy handling (single-class/constant target) with regression or trivial baseline to keep end-to-end execution.
# - Regression fallback reports a bounded [0,1] proxy accuracy = 1/(1+NRMSE) for stability and comparability while still printing as ACCURACY.