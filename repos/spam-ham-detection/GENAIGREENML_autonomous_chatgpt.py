# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.linear_model import SGDClassifier, Ridge


def _read_csv_robust(path: str) -> pd.DataFrame:
    # Try default CSV parsing first
    try:
        df0 = pd.read_csv(path)
    except Exception:
        df0 = pd.DataFrame()

    def _looks_wrong(d: pd.DataFrame) -> bool:
        if d is None or d.empty:
            return True
        if d.shape[1] <= 1:
            return True
        # If typical "single column with separators inside" symptom
        sample = d.iloc[:50, 0].astype(str)
        if sample.str.contains(r";").mean() > 0.7:
            return True
        return False

    if _looks_wrong(df0):
        try:
            df1 = pd.read_csv(path, sep=";", decimal=",")
            if not df1.empty and df1.shape[1] > 1:
                return df1
        except Exception:
            pass
    return df0


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols = []
    for c in df.columns:
        s = str(c)
        s = s.strip()
        s = re.sub(r"\s+", " ", s)
        cols.append(s)
    df = df.copy()
    df.columns = cols
    # Drop unnamed columns (common CSV artifact)
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:\s*\d+$", c)]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _pick_text_column(df: pd.DataFrame, preferred=("text", "message", "email", "content", "body")) -> str:
    lower_map = {c.lower(): c for c in df.columns}
    for p in preferred:
        if p in lower_map:
            return lower_map[p]

    obj_cols = [c for c in df.columns if df[c].dtype == "object"]
    if not obj_cols:
        return df.columns[0]
    # Choose the object column with longest average length (likely the message body)
    best_col, best_score = obj_cols[0], -1.0
    for c in obj_cols:
        s = df[c].astype(str)
        score = s.str.len().replace([np.inf, -np.inf], np.nan).dropna().mean()
        if pd.notna(score) and score > best_score:
            best_col, best_score = c, float(score)
    return best_col


def _pick_target_column(df: pd.DataFrame, text_col: str, preferred=("spam", "label", "target", "y", "class")) -> str:
    lower_map = {c.lower(): c for c in df.columns}
    for p in preferred:
        if p in lower_map and lower_map[p] != text_col:
            return lower_map[p]

    # If a non-text binary-ish column exists, prefer it
    candidates = [c for c in df.columns if c != text_col]
    if not candidates:
        return text_col

    # Prefer low-cardinality columns (classification labels)
    best_col, best_score = candidates[0], -1.0
    for c in candidates:
        s = df[c]
        nun = s.nunique(dropna=True)
        if nun <= 1:
            continue
        # Score: prefer 2 classes; penalize high cardinality
        score = 0.0
        if nun == 2:
            score = 10.0
        elif nun <= 10:
            score = 5.0
        else:
            score = 1.0 / float(nun)
        # Slight preference for numeric/bool
        if pd.api.types.is_bool_dtype(s) or pd.api.types.is_numeric_dtype(s):
            score += 0.5
        if score > best_score:
            best_col, best_score = c, score

    return best_col


def _safe_accuracy_proxy_regression(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    # Bounded proxy in [0,1] derived from normalized MAE: 1 - MAE/(scale + eps)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    mae = float(np.mean(np.abs(y_true - y_pred)))
    scale = float(np.std(y_true))
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = float(np.mean(np.abs(y_true))) + 1e-12
    acc = 1.0 - (mae / (scale + 1e-12))
    return float(np.clip(acc, 0.0, 1.0))


df = _read_csv_robust("emails.csv")
df = _normalize_columns(df)

assert df is not None and not df.empty, "Dataset is empty or could not be loaded."

text_col = _pick_text_column(df)
target_col = _pick_target_column(df, text_col=text_col)

X_text = df[text_col].astype(str).fillna("")
y_raw = df[target_col]

# Clean target
if pd.api.types.is_numeric_dtype(y_raw) or pd.api.types.is_bool_dtype(y_raw):
    y = pd.to_numeric(y_raw, errors="coerce")
else:
    # Attempt to coerce to numeric first; if mostly NaN, keep as string labels
    y_num = pd.to_numeric(y_raw, errors="coerce")
    if y_num.notna().mean() >= 0.8:
        y = y_num
    else:
        y = y_raw.astype(str).fillna("")

# Drop rows with missing target
mask_keep = pd.Series(np.ones(len(df), dtype=bool), index=df.index)
if pd.api.types.is_numeric_dtype(y) or pd.api.types.is_bool_dtype(y):
    mask_keep &= np.isfinite(pd.to_numeric(y, errors="coerce"))
else:
    mask_keep &= y.astype(str).str.len().fillna(0).astype(int) > 0

X_text = X_text.loc[mask_keep]
y = y.loc[mask_keep]

assert len(X_text) > 1, "Not enough data after preprocessing."

# Determine task type
is_classification = True
if pd.api.types.is_numeric_dtype(y) or pd.api.types.is_bool_dtype(y):
    y_coerced = pd.to_numeric(y, errors="coerce")
    nun = int(pd.Series(y_coerced).nunique(dropna=True))
    # Heuristic: treat small cardinality as classification; else regression
    is_classification = (nun >= 2 and nun <= 20)
else:
    nun = int(pd.Series(y).nunique(dropna=True))
    is_classification = (nun >= 2)

# Prepare split with defensive behavior
if is_classification:
    y_for_split = y
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X_text, y_for_split, test_size=0.2, random_state=42, stratify=y_for_split
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X_text, y_for_split, test_size=0.2, random_state=42
        )
else:
    y_for_split = pd.to_numeric(y, errors="coerce").astype(float)
    X_train, X_test, y_train, y_test = train_test_split(
        X_text, y_for_split, test_size=0.2, random_state=42
    )

assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

# Model pipeline: CPU-friendly text hashing + linear model
# HashingVectorizer avoids storing vocabulary (memory/energy efficient) and is fast on CPU.
vectorizer = HashingVectorizer(
    n_features=2**18,
    alternate_sign=False,
    norm="l2",
    ngram_range=(1, 2),
    lowercase=True,
)

if is_classification:
    # If target has <2 classes in train, fallback to trivial predictor
    n_train_classes = int(pd.Series(y_train).nunique(dropna=True))
    if n_train_classes < 2:
        # Predict the only observed class
        y_pred = np.full(shape=len(y_test), fill_value=pd.Series(y_train).iloc[0], dtype=object)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        clf = SGDClassifier(
            loss="log_loss",
            alpha=1e-5,
            max_iter=1000,
            tol=1e-3,
            random_state=42,
            n_jobs=1,
        )
        pipeline = Pipeline([("vec", vectorizer), ("clf", clf)])
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
else:
    reg = Ridge(alpha=1.0, random_state=42)
    pipeline = Pipeline([("vec", vectorizer), ("reg", reg)])
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    accuracy = _safe_accuracy_proxy_regression(y_test.to_numpy(), np.asarray(y_pred, dtype=float))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used HashingVectorizer to avoid building/storing a vocabulary (lower memory + faster, CPU-friendly).
# - Chose linear models (SGDClassifier / Ridge) for efficient training/inference on sparse text features.
# - Minimal preprocessing: robust CSV parsing, column normalization, safe type coercion and missing-target filtering.
# - Reproducibility: fixed random_state, sklearn Pipeline to prevent redundant transformations.
# - Defensive fallbacks: schema inference for text/target, stratify when possible, regression proxy score bounded to [0,1].