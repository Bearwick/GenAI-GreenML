# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# --- Robust CSV loading ---
df = None
try:
    df = pd.read_csv("Data/Diabetes-Training.csv")
    if df.shape[1] < 2:
        raise ValueError("Too few columns, retry with sep=';'")
except Exception:
    try:
        df = pd.read_csv("Data/Diabetes-Training.csv", sep=';', decimal=',')
    except Exception as e:
        raise RuntimeError(f"Could not parse CSV: {e}")

# --- Column name normalization ---
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
df = df[[c for c in df.columns if not c.startswith('Unnamed')]]

# --- Schema detection ---
expected_features = ['preg', 'plas', 'pres', 'skin', 'insu', 'mass', 'pedi', 'age']
expected_target = 'class'

available_cols = list(df.columns)

if expected_target in available_cols:
    target_col = expected_target
else:
    # Fallback: pick last column as target
    target_col = available_cols[-1]

feature_cols = [c for c in expected_features if c in available_cols]
if len(feature_cols) == 0:
    feature_cols = [c for c in available_cols if c != target_col]

# --- Prepare target ---
y_raw = df[target_col].copy()

# Determine if classification
is_classification = True
if y_raw.dtype == object or y_raw.dtype.name == 'category':
    # Encode string labels
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y = le.fit_transform(y_raw.astype(str))
else:
    y_raw_numeric = pd.to_numeric(y_raw, errors='coerce')
    n_unique = y_raw_numeric.dropna().nunique()
    if n_unique <= 20:
        le = None
        y = y_raw_numeric.values
    else:
        is_classification = False
        y = y_raw_numeric.values

# --- Prepare features ---
X = df[list(feature_cols)].copy()

# Coerce all feature columns to numeric
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors='coerce')

# --- Combine and drop NaN/inf ---
mask = np.isfinite(X).all(axis=1) & np.isfinite(y.astype(float))
X = X.loc[mask].reset_index(drop=True)
y = y[mask.values]

# Drop remaining NaNs
valid_idx = X.dropna().index
X = X.loc[valid_idx].reset_index(drop=True)
y = y[valid_idx.values]

assert len(X) > 0, "Dataset is empty after preprocessing"

# --- Check target viability for classification ---
n_classes = len(np.unique(y))
if is_classification and n_classes < 2:
    # Fallback to trivial baseline
    accuracy = 1.0
    print(f"ACCURACY={accuracy:.6f}")
    import sys
    sys.exit(0)

# --- Train/test split ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y if is_classification else None
)

assert len(X_train) > 0 and len(X_test) > 0, "Train or test set is empty"

# --- Model pipeline ---
if is_classification:
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('clf', Logis