# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset - Pima Indians Diabetes dataset based on headers
df = pd.read_csv("dataset.csv")

# Separate features and target
X = df.drop(columns=["class"]).values
y = df["class"].values

# Handle zero values that are likely missing in biological measurements
# Columns: plas(1), pres(2), skin(3), insu(4), mass(5) often have 0 = missing
cols_with_missing = [1, 2, 3, 4, 5]
X_df = pd.DataFrame(X)
for col in cols_with_missing:
    median_val = np.median(X_df[col][X_df[col] != 0])
    X_df[col] = X_df[col].replace(0, median_val)
X = X_df.values

# Standardize features for logistic regression convergence
scaler = StandardScaler()

# Use stratified k-fold cross-validation for robust evaluation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

accuracies = []
for train_idx, test_idx in skf.split(X, y):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Logistic Regression: lightweight, interpretable, efficient on CPU
    model = LogisticRegression(
        C=1.0,
        solver="lbfgs",
        max_iter=500,
        random_state=42
    )
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    accuracies.append(accuracy_score(y_test, y_pred))

accuracy = np.mean(accuracies)
print(f"ACCURACY={accuracy:.6f}")

# OPTIMIZATION SUMMARY
# 1. Dataset: Pima Indians Diabetes (small tabular dataset, 768 samples, 8 features)
# 2. Preprocessing: Replaced biological zero-values with median (imputation for likely missing data)
# 3. Scaling: StandardScaler for better convergence of logistic regression
# 4. Model: Logistic Regression - chosen for energy efficiency, fast training on CPU,
#    interpretability, and strong baseline performance on small binary classification tasks
# 5. Evaluation: 5-fold stratified cross-validation for robust accuracy estimation
# 6. No deep learning or heavy ensembles needed - logistic regression is sufficient
#    for this scale and avoids unnecessary computational overhead
# 7. Solver lbfgs is efficient for small datasets with L2 regularization