# Generated by generate_llm_code_2.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np

def load_and_preprocess(train_path, test_path):
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    def get_norm_components(df):
        features = df.iloc[:, :-1]
        labels = df.iloc[:, -1].values
        norm_features = (features - features.mean()) / features.std()
        return norm_features.values.astype(np.float32), labels

    x_train, y_train = get_norm_components(train_df)
    x_test, y_test = get_norm_components(test_df)
    
    return x_train, y_train, x_test, y_test

def run_knn_optimized():
    k = 1
    x_train, y_train, x_test, y_test = load_and_preprocess('Data/Diabetes-Training.csv', 'Data/Diabetes-Clasification.csv')
    
    results = []
    correct_hits = 0
    
    for i in range(len(x_test)):
        current_vec = x_test[i]
        
        distances_sq = np.sum(np.square(x_train - current_vec), axis=1)
        
        if k == 1:
            idx = np.argmin(distances_sq)
            neighbor_labels = [y_train[idx]]
        else:
            indices = np.argpartition(distances_sq, k)[:k]
            neighbor_labels = y_train[indices]
            
        neg_count = np.count_nonzero(neighbor_labels == 'tested_negative')
        pos_count = np.count_nonzero(neighbor_labels == 'tested_positive')
        
        assigned_class = 'tested_negative' if neg_count > pos_count else 'tested_positive'
        
        results.append([i + 1, neg_count, pos_count, assigned_class])
        
        if assigned_class == y_test[i]:
            correct_hits += 1

    pd.DataFrame(results, columns=["Instance", "tested_negative", "tested_positive", "Assigned class"]).to_csv('result_count.csv', index=False)
    
    accuracy = correct_hits / len(y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_knn_optimized()

"""
OPTIMIZATIONS APPLIED:
1. Vectorization: Replaced the manual Euclidean distance loop with NumPy broadcasting and `np.sum` for significant speedup and lower energy consumption.
2. Computational Efficiency: Avoided the expensive `sqrt` operation in distance calculations, as relative ordering is preserved with squared distances.
3. Algorithmic Optimization: Used `np.argmin` (for k=1) or `np.argpartition` (for k>1) instead of a full $O(N \log N)$ sort, reducing complexity to $O(N)$.
4. Data Handling: Converted data to `float32` arrays to reduce memory footprint and leverage faster SIMD instructions.
5. Minimized Data Movement: Processed normalization using efficient Pandas/NumPy internal methods instead of row-by-row iteration.
6. Reduced Overhead: Removed unnecessary dictionary creations and intermediate Python list structures inside the prediction loop.
"""