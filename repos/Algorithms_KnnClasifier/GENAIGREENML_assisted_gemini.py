# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np

def load_data(path):
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            raise ValueError
    except:
        df = pd.read_csv(path, sep=';', decimal=',')
    return df

def execute_knn_classification():
    train_path = 'Data/Diabetes-Training.csv'
    test_path = 'Data/Diabetes-Clasification.csv'
    
    train_df = load_data(train_path)
    test_df = load_data(test_path)
    
    x_train_raw = train_df.iloc[:, :-1].values.astype(np.float64)
    y_train = train_df.iloc[:, -1].values
    x_test_raw = test_df.iloc[:, :-1].values.astype(np.float64)
    y_test = test_df.iloc[:, -1].values
    
    x_train = (x_train_raw - np.mean(x_train_raw, axis=0)) / np.std(x_train_raw, axis=0, ddof=1)
    x_test = (x_test_raw - np.mean(x_test_raw, axis=0)) / np.std(x_test_raw, axis=0, ddof=1)
    
    results = []
    correct_count = 0
    
    for i in range(len(x_test)):
        distances = np.linalg.norm(x_train - x_test[i], axis=1)
        nearest_index = np.argmin(distances)
        assigned_label = y_train[nearest_index]
        
        neg_count = 1 if assigned_label == 'tested_negative' else 0
        pos_count = 1 if assigned_label == 'tested_positive' else 0
        
        results.append([i + 1, neg_count, pos_count, assigned_label])
        if assigned_label == y_test[i]:
            correct_count += 1
            
    pd.DataFrame(results, columns=["Instance", "tested_negative", "tested_positive", "Assigned class"]).to_csv('result_count.csv', index=False)
    
    accuracy = correct_count / len(x_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    execute_knn_classification()

# Optimization Summary
# 1. Vectorized the distance calculation using NumPy broadcasting, eliminating the redundant nested loop over the training set for every test instance.
# 2. Replaced the O(N log N) full-list sorting with O(N) index minimization using np.argmin for the k=1 classification case.
# 3. Streamlined data preprocessing by using vectorized NumPy operations for normalization instead of higher-overhead pandas Series operations.
# 4. Improved memory efficiency by converting DataFrames to NumPy arrays early and avoiding the creation of intermediate Python dictionary structures for class counting.
# 5. Implemented a robust data loading utility to handle different CSV formats automatically, ensuring stable execution without manual intervention.
# 6. Removed unnecessary library imports and simplified the code structure to reduce computational overhead and energy consumption.