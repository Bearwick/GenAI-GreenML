# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np

def open_data(path):
    return pd.read_csv(path)

def normalize_data(df):
    features = df.iloc[:, :-1]
    return (features - features.mean()) / features.std()

def knn_predict(X_train, y_train, X_test, k):
    results = []
    for test_row in X_test:
        distances = np.sqrt(np.sum((X_train - test_row) ** 2, axis=1))
        
        if k == 1:
            nearest_idx = np.argmin(distances)
            k_indices = [nearest_idx]
        else:
            k_indices = np.argpartition(distances, k)[:k]
            
        k_labels = y_train[k_indices]
        pos_count = np.sum(k_labels == 'tested_positive')
        neg_count = np.sum(k_labels == 'tested_negative')
        
        prediction = 'tested_positive' if pos_count >= neg_count else 'tested_negative'
        results.append((neg_count, pos_count, prediction))
    return results

def algorithm():
    k_value = 1
    
    train_df = open_data('Data/Diabetes-Training.csv')
    test_df = open_data('Data/Diabetes-Clasification.csv')
    
    X_train = normalize_data(train_df).values
    y_train = train_df['class'].values
    X_test = normalize_data(test_df).values
    y_test = test_df['class'].values
    
    predictions = knn_predict(X_train, y_train, X_test, k_value)
    
    output_data = []
    correct_counts = 0
    
    for i, (neg, pos, pred) in enumerate(predictions):
        output_data.append([i + 1, neg, pos, pred])
        if pred == y_test[i]:
            correct_counts += 1
            
    pd.DataFrame(output_data, columns=["Instance", "tested_negative", "tested_positive", "Assigned class"]).to_csv('result_count.csv', index=False)
    
    accuracy = correct_counts / len(y_test)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    algorithm()

# OPTIMIZATIONS APPLIED:
# 1. Vectorization: Replaced the manual loop and custom function for Euclidean distance with NumPy's broadcasted operations (axis-wise summation), drastically reducing Python-level overhead.
# 2. Efficient Partitioning: Replaced the O(N log N) full sort of distances with O(N) selection using np.argpartition (or np.argmin for k=1), which is significantly faster for large training sets.
# 3. Memory & Data Movement: Minimized data movement by converting Pandas DataFrames to NumPy arrays once. Removed redundant class dictionary initializations inside the loop.
# 4. Redundant Computation: Streamlined the normalization process and removed unnecessary intermediate list structures and redundant function calls.
# 5. Optimized Boolean Counting: Used NumPy's vectorized comparison (summing boolean masks) to count class votes instead of iterating through a list of neighbors.
