# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: assisted

import numpy as np
import pandas as pd

def load_and_preprocess(train_path, test_path):
    df_train = pd.read_csv(train_path)
    df_test = pd.read_csv(test_path)

    y_train = df_train['class'].values
    y_test = df_test['class'].values

    x_train_raw = df_train.iloc[:, :-1].values
    x_test_raw = df_test.iloc[:, :-1].values

    x_train = (x_train_raw - x_train_raw.mean(axis=0)) / x_train_raw.std(axis=0)
    x_test = (x_test_raw - x_test_raw.mean(axis=0)) / x_test_raw.std(axis=0)

    return x_train, y_train, x_test, y_test

def run_knn_optimized():
    k = 1
    x_train, y_train, x_test, y_test = load_and_preprocess('Data/Diabetes-Training.csv', 'Data/Diabetes-Clasification.csv')
    
    num_test = x_test.shape[0]
    predictions = []
    results_data = []
    correct_count = 0

    for i in range(num_test):
        diff = x_train - x_test[i]
        dist_sq = np.sum(diff**2, axis=1)
        
        if k == 1:
            nn_idx = np.argmin(dist_sq)
            nearest_labels = [y_train[nn_idx]]
        else:
            nn_indices = np.argpartition(dist_sq, k)[:k]
            nearest_labels = y_train[nn_indices]

        pos_count = 0
        neg_count = 0
        for label in nearest_labels:
            if label == 'tested_positive':
                pos_count += 1
            else:
                neg_count += 1
        
        assigned_class = 'tested_positive' if pos_count >= neg_count else 'tested_negative'
        
        results_data.append([i + 1, neg_count, pos_count, assigned_class])
        
        if assigned_class == y_test[i]:
            correct_count += 1

    pd.DataFrame(results_data, columns=["Instance", "tested_negative", "tested_positive", "Assigned class"]).to_csv('result_count.csv', index=False)
    
    accuracy = correct_count / num_test
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_knn_optimized()

"""
OPTIMIZATIONS APPLIED:
1. Vectorized Preprocessing: Replaced pandas-based normalization with NumPy broadcasting, reducing overhead from high-level pandas calls.
2. Vectorized Distance Computation: Replaced the manual loop for Euclidean distance with NumPy subtraction and summation across the entire training matrix, significantly reducing runtime complexity from O(N*M) Python-level operations to optimized C-level operations.
3. Efficient Sorting: Replaced the full sort (O(N log N)) of distances with np.argmin (for k=1) or np.argpartition (O(N) for k>1) to find the nearest neighbors.
4. Memory Management: Avoided creating large intermediate lists and dictionary objects within the inner loop by utilizing NumPy arrays and local counters.
5. Reduced Redundancy: Removed the redundant square root calculation for distance comparisons, as squared distance maintains the same rank order (though kept standard logic for exact behavior compatibility).
6. Minimized Data Movement: Processed data directly in NumPy arrays to avoid the overhead of repeated pandas indexing and structure copying.
"""
