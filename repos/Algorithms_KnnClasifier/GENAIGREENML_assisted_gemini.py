# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np

def load_and_preprocess(train_path, test_path):
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    X_train_raw = train_df.iloc[:, :-1].values
    y_train = train_df.iloc[:, -1].values
    X_test_raw = test_df.iloc[:, :-1].values
    y_test = test_df.iloc[:, -1].values

    X_train = (X_train_raw - X_train_raw.mean(axis=0)) / X_train_raw.std(axis=0)
    X_test = (X_test_raw - X_test_raw.mean(axis=0)) / X_test_raw.std(axis=0)

    return X_train, y_train, X_test, y_test

def run_knn_classification():
    k = 1
    X_train, y_train, X_test, y_test = load_and_preprocess('Data/Diabetes-Training.csv', 'Data/Diabetes-Clasification.csv')
    
    num_test = len(X_test)
    results = []
    correct_hits = 0

    for i in range(num_test):
        distances = np.linalg.norm(X_train - X_test[i], axis=1)
        
        if k == 1:
            nearest_idx = np.argmin(distances)
            neighbor_classes = [y_train[nearest_idx]]
        else:
            nearest_indices = np.argpartition(distances, k)[:k]
            neighbor_classes = y_train[nearest_indices]

        pos_count = np.sum(neighbor_classes == 'tested_positive')
        neg_count = np.sum(neighbor_classes == 'tested_negative')
        
        assigned_class = 'tested_negative' if neg_count > pos_count else 'tested_positive'
        
        results.append([i + 1, neg_count, pos_count, assigned_class])
        if assigned_class == y_test[i]:
            correct_hits += 1

    accuracy = correct_hits / num_test
    
    pd.DataFrame(results, columns=["Instance", "tested_negative", "tested_positive", "Assigned class"]).to_csv('result_count.csv', index=False)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_knn_classification()

# OPTIMIZATIONS APPLIED:
# 1. Vectorized Distance Computation: Replaced the manual Euclidean distance loop with np.linalg.norm. 
#    This leverages BLAS/LAPACK optimizations, significantly reducing CPU cycles and runtime.
# 2. Efficient Neighbor Selection: Used np.argmin (for k=1) or np.argpartition (for k>1) instead of 
#    sorting the entire distance array. This improves complexity from O(N log N) to O(N).
# 3. Reduced Data Movement: Minimized conversions between Pandas objects and Python lists by 
#    performing core computations directly in NumPy arrays.
# 4. Streamlined Preprocessing: Vectorized the normalization step using NumPy broadcasting 
#    instead of redundant Pandas operations.
# 5. Memory Footprint Reduction: Eliminated intermediate structures like the dictionary-based 
#    class counter inside the loop, using boolean masking on arrays instead.
# 6. Eliminated Redundant Function Calls: Combined data loading and normalization steps to 
#    minimize file I/O overhead and object creation.
