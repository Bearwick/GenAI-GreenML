# Generated by generate_llm_code.py
# LLM: groq
# Mode: assisted

import numpy as np
import pandas as pd

def _read_csv(path):
    return pd.read_csv(path)

def _normalize(df):
    features = df.iloc[:, :-1]
    return (features - features.mean()) / features.std()

def _knn_predict(X_train, y_train, X_test, k):
    distances = np.linalg.norm(X_train[None, :, :] - X_test[:, None, :], axis=2)
    idx = np.argpartition(distances, k, axis=1)[:, :k]
    neighbor_labels = y_train[idx]
    predictions = []
    for labels in neighbor_labels:
        pos = np.sum(labels == 'tested_positive')
        neg = np.sum(labels == 'tested_negative')
        predictions.append('tested_positive' if pos >= neg else 'tested_negative')
    return np.array(predictions)

def _generate_csv(results):
    pd.DataFrame(
        results,
        columns=["Instance", "tested_negative", "tested_positive", "Assigned class"]
    ).to_csv('result_count.csv', index=False)

def main():
    k = 1
    train_df = _read_csv('Data/Diabetes-Training.csv')
    test_df = _read_csv('Data/Diabetes-Clasification.csv')
    X_train = _normalize(train_df).to_numpy()
    y_train = train_df['class'].to_numpy()
    X_test = _normalize(test_df).to_numpy()
    y_test = test_df['class'].to_numpy()
    preds = _knn_predict(X_train, y_train, X_test, k)
    correct = np.sum(preds == y_test)
    accuracy = correct / len(y_test)
    output = [
        [i + 1, np.sum(preds == 'tested_negative'), np.sum(preds == 'tested_positive'), pred]
        for i, pred in enumerate(preds)
    ]
    _generate_csv(output)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()
# Optimizations applied:
# 1. Vectorized distance computation eliminates explicit Python loops, reducing runtime.
# 2. Using NumPy broadcasting and argpartition for k‑nearest selection provides efficient k‑NN search.
# 3. Normalization is performed using pandas vectorized ops, avoiding manual loops.
# 4. Intermediate pandas objects are converted to NumPy arrays immediately for fast arithmetic.
# 5. Single-pass prediction and result aggregation reduce memory overhead.
# 6. Removed unnecessary imports, prints, interactive input, and visualizations to lower energy usage.