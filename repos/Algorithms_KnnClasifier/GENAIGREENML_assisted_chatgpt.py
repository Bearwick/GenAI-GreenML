# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import numpy as np
import pandas as pd


RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)


def read_csv_robust(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if df.shape[1] == 1:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def get_feature_target(df: pd.DataFrame, dataset_headers: list[str]) -> tuple[np.ndarray, np.ndarray]:
    cols = list(df.columns)
    expected_last = dataset_headers[-1] if dataset_headers else None
    if expected_last in cols:
        target_col = expected_last
    else:
        target_col = cols[-1]
    feature_cols = [c for c in cols if c != target_col]
    X = df[feature_cols].to_numpy(dtype=np.float64, copy=False)
    y = df[target_col].to_numpy(copy=False)
    return X, y


def fit_standardizer(X: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
    mean = X.mean(axis=0)
    std = X.std(axis=0, ddof=1)
    std = np.where(std == 0.0, 1.0, std)
    return mean, std


def transform_standardizer(X: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:
    return (X - mean) / std


def knn_predict_k1_counts(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
    n_test = X_test.shape[0]
    neg_counts = np.empty(n_test, dtype=np.int32)
    pos_counts = np.empty(n_test, dtype=np.int32)

    for i in range(n_test):
        diff = X_train - X_test[i]
        d2 = np.einsum("ij,ij->i", diff, diff, optimize=True)
        nn_idx = int(d2.argmin())
        pred = y_train[nn_idx]
        neg_counts[i] = 1 if pred == "tested_negative" else 0
        pos_counts[i] = 1 if pred == "tested_positive" else 0

    return neg_counts, pos_counts


def algorithm() -> None:
    dataset_headers = ["preg", "plas", "pres", "skin", "insu", "mass", "pedi", "age", "class"]

    train_df = read_csv_robust("Data/Diabetes-Training.csv")
    test_df = read_csv_robust("Data/Diabetes-Clasification.csv")

    X_train_raw, y_train = get_feature_target(train_df, dataset_headers)
    X_test_raw, y_test = get_feature_target(test_df, dataset_headers)

    mean, std = fit_standardizer(X_train_raw)
    X_train = transform_standardizer(X_train_raw, mean, std)
    X_test = transform_standardizer(X_test_raw, mean, std)

    neg_counts, pos_counts = knn_predict_k1_counts(X_train, y_train, X_test)

    assigned = np.where(neg_counts > pos_counts, "tested_negative", "tested_positive")
    accuracy = float(np.mean(assigned == y_test))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    algorithm()

# Optimization Summary
# - Replaced per-feature DataFrame normalization with NumPy arrays to reduce overhead and data movement.
# - Fit normalization (mean/std) once on training data and reused for test data to avoid redundant computation.
# - Removed full distance list construction and sorting; for k=1 used argmin on squared distances (no sqrt) to minimize math and memory.
# - Used einsum for efficient squared-distance reduction and avoided intermediate Python lists in the hot loop.
# - Avoided unnecessary copies by using to_numpy(..., copy=False) where possible to reduce memory footprint.
# - Implemented robust CSV parsing fallback to handle alternative separators/decimals without manual intervention.
# - Ensured reproducibility via a fixed NumPy random seed (no stochastic steps present, but keeps runs stable if extended).