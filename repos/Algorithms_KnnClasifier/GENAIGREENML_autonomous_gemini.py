# Generated by generate_llm_code_2.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# JUSTIFICATION FOR ENERGY-EFFICIENT DESIGN:
# 1. Model Selection: Logistic Regression is chosen over KNN or Deep Learning due to its O(1) inference complexity. 
#    Once trained, the model only requires a simple dot product, significantly reducing CPU cycles and energy consumption per prediction.
# 2. Accelerated Convergence: By implementing Z-score normalization via StandardScaler, we ensure the L-BFGS solver 
#    converges in fewer iterations, directly minimizing the computational time and power required during training.
# 3. Resource Management: The script utilizes optimized, C-backed libraries (pandas/scikit-learn) for vectorized 
#    data processing, which is more energy-efficient than manual Python loops.
# 4. Hardware Efficiency: The solution is designed for low-memory, CPU-only execution, avoiding the high energy 
#    overhead associated with initializing and transferring data to specialized hardware like GPUs.

def run_pipeline():
    try:
        train_df = pd.read_csv('Data/Diabetes-Training.csv')
        test_df = pd.read_csv('Data/Diabetes-Clasification.csv')

        X_train = train_df.drop('class', axis=1)
        y_train = train_df['class']
        X_test = test_df.drop('class', axis=1)
        y_test = test_df['class']

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        model = LogisticRegression(solver='lbfgs', max_iter=1000, penalty='l2')
        model.fit(X_train_scaled, y_train)

        predictions = model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, predictions)

        print(f"ACCURACY={accuracy:.6f}")
    except Exception:
        pass

if __name__ == "__main__":
    run_pipeline()