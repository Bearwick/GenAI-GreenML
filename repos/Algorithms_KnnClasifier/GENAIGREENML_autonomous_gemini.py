# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

def load_and_preprocess(train_path, test_path):
    train_data = pd.read_csv(train_path)
    test_data = pd.read_csv(test_path)

    X_train = train_data.drop(columns=['class'])
    y_train = train_data['class']
    X_test = test_data.drop(columns=['class'])
    y_test = test_data['class']

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled, X_test_scaled, y_train, y_test

def train_and_evaluate():
    try:
        X_train, X_test, y_train, y_test = load_and_preprocess('Data/Diabetes-Training.csv', 'Data/Diabetes-Clasification.csv')
        
        model = LogisticRegression(
            penalty='l2',
            C=1.0,
            solver='lbfgs',
            max_iter=500,
            tol=1e-4
        )
        
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
        
        accuracy = accuracy_score(y_test, predictions)
        print(f"ACCURACY={accuracy:.6f}")

        results_df = pd.DataFrame({
            'Instance': range(1, len(predictions) + 1),
            'Assigned class': predictions
        })
        results_df.to_csv('result_count.csv', index=False)

    except FileNotFoundError:
        pass

if __name__ == "__main__":
    train_and_evaluate()

"""
JUSTIFICATION FOR ENERGY EFFICIENCY:
1. Model Selection: Logistic Regression was chosen over KNN. KNN requires O(N) complexity for every prediction (calculating distances to all training points), which is computationally expensive and energy-intensive for inference. Logistic Regression uses O(1) inference after training a simple weight vector.
2. Library Optimization: Used Scikit-learn's vectorized implementations which leverage optimized C/Fortran backends (BLAS/LAPACK), significantly reducing CPU instruction cycles compared to manual Python loops.
3. Preprocessing: StandardScaler ensures features are on the same scale, allowing the optimization algorithm (L-BFGS) to converge in fewer iterations, thereby reducing the total power consumption of the training phase.
4. Minimal Overhead: The solution avoids deep learning frameworks and unnecessary dependencies, minimizing memory footprint and initialization energy.
"""
