# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def run_pipeline():
    url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
    headers = ['preg', 'plas', 'pres', 'skin', 'insu', 'mass', 'pedi', 'age', 'class']
    
    try:
        df = pd.read_csv(url, names=headers)
    except:
        return

    X = df.drop('class', axis=1)
    y = df['class']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = LogisticRegression(solver='lbfgs', max_iter=500)
    model.fit(X_train_scaled, y_train)

    predictions = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, predictions)

    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

"""
JUSTIFICATION FOR ENERGY-EFFICIENT DESIGN:
1. Algorithm Choice: Logistic Regression was selected over KNN or Deep Learning. It is a linear model with extremely low computational overhead (O(d) inference complexity), requiring minimal CPU cycles and memory.
2. Vectorized Preprocessing: Using Scikit-learn's StandardScaler leverages optimized C/Cython backends. This is significantly more energy-efficient than manual Python loops for distance calculations.
3. Optimization: The L-BFGS solver used in Logistic Regression converges rapidly on small tabular datasets, minimizing the total duration of active CPU usage.
4. Minimal Hardware Requirements: The solution is designed for single-core CPU execution, avoiding the high idle-power consumption and specialized cooling requirements of GPUs.
5. Resource Management: The script avoids large dependency chains and memory-intensive data structures, ensuring a small carbon footprint during both training and inference.
"""
