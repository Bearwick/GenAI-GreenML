# Generated by ./scripts/generate_llm_code.sh
# LLM: gemini
# Mode: autonomous

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

"""
JUSTIFICATION FOR ENERGY-EFFICIENT DESIGN:
1. Model Selection: Logistic Regression is used instead of KNN. While KNN requires O(N*D) 
   computations during inference (comparing every test point to all training points), 
   Logistic Regression is O(D), significantly reducing CPU cycles and energy per prediction.
2. Vectorization: The solution uses scikit-learn's optimized C/Cython backends rather than 
   manual Python loops, minimizing the time the CPU stays in a high-power state.
3. Preprocessing: StandardScaler is used to normalize the feature space. This ensures 
   the L-BFGS optimization solver converges in fewer iterations, saving computation time.
4. Minimal Hardware: The approach is designed for CPU execution, avoiding the high 
   energy overhead and specialized hardware requirements of deep learning models.
"""

def run_efficient_pipeline():
    try:
        train_df = pd.read_csv('Data/Diabetes-Training.csv')
        test_df = pd.read_csv('Data/Diabetes-Clasification.csv')
        
        target_col = 'class'
        X_train = train_df.drop(target_col, axis=1)
        y_train = train_df[target_col]
        X_test = test_df.drop(target_col, axis=1)
        y_test = test_df[target_col]

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        model = LogisticRegression(solver='lbfgs', max_iter=1000, tol=1e-4)
        model.fit(X_train_scaled, y_train)

        accuracy = model.score(X_test_scaled, y_test)
        print(f"ACCURACY={accuracy:.6f}")

    except Exception:
        pass

if __name__ == "__main__":
    run_efficient_pipeline()
