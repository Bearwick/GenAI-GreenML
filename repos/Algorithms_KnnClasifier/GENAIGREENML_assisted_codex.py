# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd

SEED = 42
np.random.seed(SEED)

DATASET_HEADERS = "preg,plas,pres,skin,insu,mass,pedi,age,class"
EXPECTED_HEADERS = [h.strip() for h in DATASET_HEADERS.split(",") if h.strip()]

def _parsing_wrong(df, expected_headers):
    if df.shape[1] == 1:
        return True
    if expected_headers:
        cols = [str(c).strip() for c in df.columns]
        if len(cols) != len(expected_headers):
            return True
        if cols != expected_headers and set(cols) != set(expected_headers):
            return True
    return False

def read_csv_robust(path, expected_headers):
    df = pd.read_csv(path)
    if _parsing_wrong(df, expected_headers):
        df = pd.read_csv(path, sep=";", decimal=",")
    if expected_headers and set(expected_headers) == set(df.columns):
        df = df[expected_headers]
    return df

def normalize_data(df):
    values = df.iloc[:, :-1].to_numpy(dtype=float, copy=False)
    mean = values.mean(axis=0)
    std = values.std(axis=0, ddof=1)
    return (values - mean) / std

def knn_predict(X_train, y_train, X_test, k, neg_label, pos_label):
    n_train = X_train.shape[0]
    n_test = X_test.shape[0]
    if n_train == 0 or n_test == 0:
        return np.array([], dtype=object), np.array([], dtype=int), np.array([], dtype=int)
    train_sq = np.einsum("ij,ij->i", X_train, X_train)
    test_sq = np.einsum("ij,ij->i", X_test, X_test)
    dist_sq = test_sq[:, None] + train_sq[None, :] - 2.0 * X_test @ X_train.T
    dist_sq = np.maximum(dist_sq, 0.0)
    if k == 1:
        nearest_idx = np.argmin(dist_sq, axis=1)
        pred = y_train[nearest_idx]
        neg_count = (pred == neg_label).astype(int)
        pos_count = (pred == pos_label).astype(int)
        assigned = np.where(neg_count > pos_count, neg_label, pos_label)
        return assigned, neg_count, pos_count
    k_eff = min(k, n_train)
    idx_sorted = np.argsort(dist_sq, axis=1)[:, :k_eff]
    neighbor_labels = y_train[idx_sorted]
    neg_count = np.sum(neighbor_labels == neg_label, axis=1).astype(int)
    pos_count = np.sum(neighbor_labels == pos_label, axis=1).astype(int)
    assigned = np.where(neg_count > pos_count, neg_label, pos_label)
    return assigned, neg_count, pos_count

def main():
    k_value = 1
    train_df = read_csv_robust("Data/Diabetes-Training.csv", EXPECTED_HEADERS)
    test_df = read_csv_robust("Data/Diabetes-Clasification.csv", EXPECTED_HEADERS)
    X_train = normalize_data(train_df)
    X_test = normalize_data(test_df)
    y_train = train_df.iloc[:, -1].to_numpy()
    y_test = test_df.iloc[:, -1].to_numpy()
    neg_label = "tested_negative"
    pos_label = "tested_positive"
    assigned, neg_count, pos_count = knn_predict(X_train, y_train, X_test, k_value, neg_label, pos_label)
    n_test = X_test.shape[0]
    instances = np.arange(1, n_test + 1)
    output_df = pd.DataFrame({
        "Instance": instances,
        "tested_negative": neg_count,
        "tested_positive": pos_count,
        "Assigned class": assigned
    })
    output_df.to_csv("result_count.csv", index=False)
    accuracy = float(np.mean(assigned == y_test)) if n_test else 0.0
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# Vectorized distance computation with precomputed norms reduced Python loops and avoided square roots.
# Argmin-based k=1 selection eliminated full sorting and redundant work.
# NumPy-based normalization and array handling reduced pandas overhead and copies.
# Robust CSV parsing with header-based validation minimized misreads and unnecessary data movement.
# Direct DataFrame construction for results avoided intermediate lists while preserving output.