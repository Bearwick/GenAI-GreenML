# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd

def load_and_normalize(path):
    df = pd.read_csv(path)
    features = df.iloc[:, :-1].to_numpy(dtype=float)
    labels = df.iloc[:, -1].to_numpy()
    mean = features.mean(axis=0)
    std = features.std(axis=0, ddof=1)
    normalized = (features - mean) / std
    return normalized, labels

def knn_predict_one(X_train, y_train, X_test):
    train_sq = np.einsum('ij,ij->i', X_train, X_train)
    test_sq = np.einsum('ij,ij->i', X_test, X_test)
    distances = test_sq[:, None] + train_sq[None, :] - 2 * (X_test @ X_train.T)
    nearest_idx = np.argmin(distances, axis=1)
    return y_train[nearest_idx]

def main():
    X_train, y_train = load_and_normalize('Data/Diabetes-Training.csv')
    X_test, y_test = load_and_normalize('Data/Diabetes-Clasification.csv')
    predictions = knn_predict_one(X_train, y_train, X_test)
    accuracy = (predictions == y_test).mean() * 100.0
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimizations applied:
# - Combined loading and normalization to reduce intermediate objects.
# - Vectorized 1-NN distance computation with precomputed norms to remove loops.
# - Used squared distances to avoid costly square roots while preserving ordering.
# - Eliminated file output and redundant processing to reduce I/O.