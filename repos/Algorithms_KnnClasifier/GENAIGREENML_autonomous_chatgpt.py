# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        normed.append(c2)
    return normed


def _read_csv_robust(path):
    # First attempt: default CSV parsing
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(dfx):
        if dfx is None or dfx.empty:
            return True
        # Heuristic: if only 1 column but file is supposed to be comma/semicolon separated
        if dfx.shape[1] == 1:
            col0 = str(dfx.columns[0])
            if ("," in col0) or (";" in col0):
                return True
        return False

    if looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            # Last resort: try python engine with automatic separator inference
            df = pd.read_csv(path, sep=None, engine="python")

    return df


def _drop_unnamed(df):
    cols = []
    for c in df.columns:
        if isinstance(c, str) and c.strip().lower().startswith("unnamed"):
            continue
        cols.append(c)
    return df.loc[:, cols]


def _coerce_numeric_columns_inplace(df, numeric_cols):
    for c in numeric_cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")


def _pick_target(df, headers_hint):
    cols = list(df.columns)

    # Prefer provided "class" if present
    if "class" in cols:
        return "class"

    # If any column matches hints (case-insensitive)
    lower_map = {str(c).strip().lower(): c for c in cols}
    for h in headers_hint:
        hl = str(h).strip().lower()
        if hl in lower_map and hl in ("class", "target", "label", "y"):
            return lower_map[hl]

    # Otherwise choose a non-constant numeric column if possible
    numeric_candidates = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() >= max(10, int(0.2 * len(df))):
            nunique = s.nunique(dropna=True)
            if nunique >= 2:
                numeric_candidates.append((nunique, c))
    if numeric_candidates:
        numeric_candidates.sort(reverse=True)
        return numeric_candidates[0][1]

    # Fallback: last column
    return cols[-1] if cols else None


def _is_classification_target(y):
    # Determine classification if:
    # - object/string categories
    # - integer-like with few unique values
    # - boolean
    if y.dtype == bool:
        return True
    if y.dtype == object or str(y.dtype).startswith("category"):
        return True
    y_num = pd.to_numeric(y, errors="coerce")
    if y_num.notna().sum() > 0:
        # If all non-NA values are integers and unique count small, treat as classification
        non_na = y_num.dropna()
        if len(non_na) == 0:
            return False
        is_integer_like = np.all(np.isclose(non_na.values, np.round(non_na.values)))
        nunique = non_na.nunique()
        if is_integer_like and nunique <= 20:
            return True
    return False


def _bounded_regression_score(y_true, y_pred):
    # Stable "accuracy" proxy in [0,1] based on normalized MAE
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    yt = y_true[mask]
    yp = y_pred[mask]
    mae = np.mean(np.abs(yt - yp))
    scale = np.std(yt)
    if not np.isfinite(scale) or scale <= 1e-12:
        scale = np.mean(np.abs(yt - np.mean(yt))) + 1e-12
    score = 1.0 / (1.0 + (mae / (scale + 1e-12)))
    if not np.isfinite(score):
        return 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    dataset_path = os.path.join("Data", "Diabetes-Training.csv")
    headers_hint = ["preg", "plas", "pres", "skin", "insu", "mass", "pedi", "age", "class"]

    df = _read_csv_robust(dataset_path)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Defensive: ensure non-empty
    assert df is not None and not df.empty, "Dataset is empty or could not be read."

    # If the file accidentally loaded with a single combined column, attempt to split it
    if df.shape[1] == 1:
        only_col = df.columns[0]
        series = df[only_col].astype(str)
        sep = "," if series.str.contains(",").mean() > series.str.contains(";").mean() else ";"
        expanded = series.str.split(sep, expand=True)
        if expanded.shape[1] > 1:
            # If header row got ingested as data, attempt to detect by matching hint
            if all(isinstance(x, str) for x in expanded.iloc[0].tolist()):
                first_row = [str(x).strip().lower() for x in expanded.iloc[0].tolist()]
                hint_lower = [h.lower() for h in headers_hint]
                if sum(1 for x in first_row if x in hint_lower) >= 2:
                    expanded.columns = [str(x).strip() for x in expanded.iloc[0].tolist()]
                    expanded = expanded.iloc[1:].reset_index(drop=True)
                else:
                    expanded.columns = [f"col_{i}" for i in range(expanded.shape[1])]
            else:
                expanded.columns = [f"col_{i}" for i in range(expanded.shape[1])]
            df = expanded
            df.columns = _normalize_columns(df.columns)
            df = _drop_unnamed(df)

    # Pick target robustly
    target_col = _pick_target(df, headers_hint)
    assert target_col in df.columns, "Could not select a valid target column."

    # Basic cleanup: replace inf with nan
    df = df.replace([np.inf, -np.inf], np.nan)

    # Separate X/y
    y_raw = df[target_col]
    X_raw = df.drop(columns=[target_col])

    # Ensure we have some feature columns; if not, create a constant feature
    if X_raw.shape[1] == 0:
        X_raw = pd.DataFrame({"const": np.ones(len(df), dtype=float)})

    # Identify numeric/categorical
    numeric_cols = []
    categorical_cols = []
    for c in X_raw.columns:
        s = X_raw[c]
        if s.dtype == object:
            # try numeric coercion; if mostly numeric, treat as numeric
            s_num = pd.to_numeric(s, errors="coerce")
            if s_num.notna().mean() >= 0.8:
                numeric_cols.append(c)
            else:
                categorical_cols.append(c)
        else:
            # non-object: attempt numeric
            numeric_cols.append(c)

    # Coerce numeric columns safely
    X = X_raw.copy()
    _coerce_numeric_columns_inplace(X, numeric_cols)

    # Decide task
    is_clf = _is_classification_target(y_raw)

    # Prepare y
    if is_clf:
        y = y_raw.astype(str).fillna("NA")
        classes = y.unique()
        if len(classes) < 2:
            is_clf = False
    if not is_clf:
        y = pd.to_numeric(y_raw, errors="coerce")
        # if too many NaNs, fallback to classification with string labels if possible
        if y.notna().sum() < max(5, int(0.2 * len(y))):
            y = y_raw.astype(str).fillna("NA")
            is_clf = True
            if y.nunique() < 2:
                # ultimate fallback: create pseudo numeric target of zeros
                y = pd.Series(np.zeros(len(df), dtype=float))
                is_clf = False

    # Drop rows with missing y for regression; for classification keep NA class as category
    if not is_clf:
        keep = y.notna()
        X = X.loc[keep].reset_index(drop=True)
        y = y.loc[keep].reset_index(drop=True)

    assert len(X) > 0 and len(y) > 0, "No data remaining after preprocessing."

    # Train/test split with defensive sizing
    test_size = 0.2
    if len(X) < 10:
        test_size = 0.3 if len(X) >= 4 else 0.5

    stratify = y if is_clf and getattr(y, "nunique", lambda: 0)() >= 2 and len(y) >= 10 else None

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )

    assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

    # Preprocess
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=False)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if is_clf:
        model = LogisticRegression(
            solver="liblinear",
            max_iter=200,
            random_state=42,
        )
        clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        model = Ridge(alpha=1.0, random_state=42)
        reg = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        accuracy = _bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression with liblinear; Ridge) for CPU efficiency and strong baselines.
# - ColumnTransformer+Pipeline ensures single-pass, reproducible preprocessing without redundant conversions.
# - Robust CSV loading retries with alternative separators/decimal conventions to avoid costly manual fixes.
# - Numeric coercion with errors='coerce' and simple imputers avoids expensive/fragile custom cleaning.
# - OneHotEncoder with sparse output keeps memory/CPU use low for categorical features.
# - Regression fallback reports a bounded [0,1] "accuracy" proxy: 1/(1+MAE/scale), stable under varying target scales.