# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import glob
import warnings
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")


DATASET_HEADERS = ["preg", "plas", "pres", "skin", "insu", "mass", "pedi", "age", "class"]


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = "" if c is None else str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(x):
        if x is None:
            return True
        if x.shape[0] == 0 or x.shape[1] == 0:
            return True
        if x.shape[1] == 1:
            # often indicates wrong delimiter
            return True
        return False

    if looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            # final fallback: python engine with sep inference
            df = pd.read_csv(path, sep=None, engine="python")

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)
    return df


def _find_dataset_paths():
    # Prefer the known paths from the provided source, but robustly search for CSVs.
    candidates = [
        os.path.join("Data", "Diabetes-Training.csv"),
        os.path.join("Data", "Diabetes-Training.CSV"),
        os.path.join("Data", "Diabetes-Clasification.csv"),
        os.path.join("Data", "Diabetes-Classification.csv"),
        os.path.join("Data", "Diabetes-Clasification.CSV"),
        os.path.join("Data", "Diabetes-Classification.CSV"),
    ]
    existing = [p for p in candidates if os.path.exists(p)]
    if len(existing) >= 2:
        # Try to pick train/test by name
        train = next((p for p in existing if "Training" in os.path.basename(p)), existing[0])
        test = next((p for p in existing if ("Clasification" in os.path.basename(p) or "Classification" in os.path.basename(p)) and p != train), None)
        if test is None:
            rest = [p for p in existing if p != train]
            test = rest[0] if rest else train
        return train, test

    # Fallback: search recursively
    all_csv = sorted(set(glob.glob("**/*.csv", recursive=True) + glob.glob("**/*.CSV", recursive=True)))
    # Prefer diabetes named files if present
    diabetes = [p for p in all_csv if "diabetes" in os.path.basename(p).lower()]
    pool = diabetes if diabetes else all_csv
    if len(pool) >= 2:
        return pool[0], pool[1]
    if len(pool) == 1:
        return pool[0], None
    return None, None


def _coerce_numeric_inplace(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _choose_target(df):
    # Prefer a column matching DATASET_HEADERS last element if present.
    cols_lower = {c.lower(): c for c in df.columns}
    if "class" in cols_lower:
        return cols_lower["class"]

    # Otherwise, choose a reasonable target:
    # - prefer non-constant numeric column
    numeric_cols = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        if np.isfinite(s.to_numpy(dtype=float, copy=False)).any():
            numeric_cols.append(c)

    best = None
    best_unique = 0
    for c in numeric_cols:
        s = pd.to_numeric(df[c], errors="coerce")
        s = s[np.isfinite(s)]
        if s.shape[0] == 0:
            continue
        uniq = s.nunique(dropna=True)
        if uniq > best_unique:
            best_unique = uniq
            best = c
    if best is not None and best_unique > 1:
        return best

    # Last fallback: last column
    return df.columns[-1]


def _detect_task_and_prepare_y(y_raw):
    # Returns (task, y_processed, classes)
    # task in {"classification","regression"}
    y = y_raw.copy()

    # If object/category with limited unique -> classification
    if y.dtype == "O" or str(y.dtype).startswith("category") or str(y.dtype).startswith("string"):
        y = y.astype(str).fillna("nan")
        uniq = pd.Series(y).nunique(dropna=False)
        if uniq >= 2:
            return "classification", y, None
        # constant -> regression fallback on codes
        codes = pd.factorize(y)[0].astype(float)
        return "regression", codes, None

    # Numeric path
    y_num = pd.to_numeric(y, errors="coerce")
    y_num = y_num.replace([np.inf, -np.inf], np.nan)
    finite = y_num.dropna()
    if finite.shape[0] == 0:
        # can't model; make constant regression target
        return "regression", pd.Series(np.zeros(len(y)), index=y.index, dtype=float), None

    uniq = finite.nunique(dropna=True)
    if uniq < 2:
        return "regression", y_num.fillna(finite.median()), None

    # If looks like binary or few-class integers -> classification
    # (e.g., 0/1 or 1/2)
    if uniq <= 20:
        # Check if values are near integers
        vals = finite.to_numpy()
        if np.all(np.isclose(vals, np.round(vals))):
            y_cls = y_num.round().astype("Int64").astype(str)
            if y_cls.nunique(dropna=True) >= 2:
                return "classification", y_cls.fillna("nan"), None

    return "regression", y_num, None


def _build_preprocessor(X):
    # Identify numeric/categorical columns robustly
    num_cols = []
    cat_cols = []
    for c in X.columns:
        s = pd.to_numeric(X[c], errors="coerce")
        valid = s.notna().mean()
        if valid >= 0.9:
            num_cols.append(c)
        else:
            cat_cols.append(c)

    numeric_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    pre = ColumnTransformer(
        transformers=[
            ("num", numeric_pipe, num_cols),
            ("cat", categorical_pipe, cat_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )
    return pre, num_cols, cat_cols


def _bounded_regression_accuracy(y_true, y_pred):
    # Stable proxy in [0,1]: 0.5*(clip(R2, -1, 1)+1)
    r2 = r2_score(y_true, y_pred)
    r2 = float(np.clip(r2, -1.0, 1.0))
    return 0.5 * (r2 + 1.0)


def main():
    train_path, test_path = _find_dataset_paths()
    if train_path is None:
        # No dataset found: ensure end-to-end run with trivial accuracy
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df_train = _read_csv_robust(train_path)
    assert df_train is not None and df_train.shape[0] > 0 and df_train.shape[1] > 0

    # If a separate test file exists, use it; else do internal split.
    df_test = _read_csv_robust(test_path) if test_path is not None and os.path.exists(test_path) else None

    # Normalize/strip headers already done; now choose target robustly.
    target_col = _choose_target(df_train)

    # Align train/test columns if separate test provided
    if df_test is not None and df_test.shape[0] > 0:
        # Ensure target exists in both; if missing in test, fallback to internal split.
        if target_col not in df_test.columns:
            df_test = None

    # Prepare X/y
    y_raw = df_train[target_col]
    X_raw = df_train.drop(columns=[target_col])

    # If no features left, fallback to using all columns except maybe last
    if X_raw.shape[1] == 0 and df_train.shape[1] > 1:
        X_raw = df_train.drop(columns=[df_train.columns[-1]])
        if X_raw.shape[1] == 0:
            X_raw = df_train.copy()
            y_raw = pd.Series(np.zeros(len(df_train)), index=df_train.index)

    # Defensive: drop fully empty columns
    X_raw = X_raw.loc[:, X_raw.columns.map(lambda c: X_raw[c].notna().any())]

    assert X_raw.shape[0] > 0

    task, y_processed, _ = _detect_task_and_prepare_y(y_raw)

    # If separate test exists, use it; else split
    if df_test is not None:
        y_test_raw = df_test[target_col]
        X_test_raw = df_test.drop(columns=[target_col])
        # Keep only columns seen in train; add missing as NaN
        for c in X_raw.columns:
            if c not in X_test_raw.columns:
                X_test_raw[c] = np.nan
        X_test_raw = X_test_raw[X_raw.columns.tolist()]
        X_train = X_raw
        X_test = X_test_raw
        # Prepare y_test similarly
        task2, y_test_processed, _ = _detect_task_and_prepare_y(y_test_raw)
        if task2 != task:
            # Force regression if mismatch
            task = "regression"
            y_processed = pd.to_numeric(y_raw, errors="coerce")
            y_test_processed = pd.to_numeric(y_test_raw, errors="coerce")
        y_train = y_processed
        y_test = y_test_processed
    else:
        # Split with fixed random_state for reproducibility
        stratify = None
        if task == "classification":
            y_tmp = y_processed
            if pd.Series(y_tmp).nunique(dropna=False) >= 2:
                stratify = y_tmp
        X_train, X_test, y_train, y_test = train_test_split(
            X_raw,
            y_processed,
            test_size=0.25,
            random_state=42,
            stratify=stratify,
        )

    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    preprocessor, _, _ = _build_preprocessor(X_train)

    if task == "classification":
        # If <2 classes after split, fallback to regression proxy
        if pd.Series(y_train).nunique(dropna=False) < 2 or pd.Series(y_test).nunique(dropna=False) < 2:
            task = "regression"
        else:
            model = LogisticRegression(
                max_iter=200,
                solver="liblinear",  # CPU-friendly for small/medium sparse data
            )
            clf = Pipeline(steps=[("pre", preprocessor), ("model", model)])
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
            print(f"ACCURACY={accuracy:.6f}")
            return

    # Regression fallback path
    # Coerce y to numeric and handle NaN safely
    y_train_num = pd.to_numeric(pd.Series(y_train), errors="coerce").replace([np.inf, -np.inf], np.nan)
    y_test_num = pd.to_numeric(pd.Series(y_test), errors="coerce").replace([np.inf, -np.inf], np.nan)
    # If too many NaNs, fill with median from train
    med = float(np.nanmedian(y_train_num.to_numpy())) if np.isfinite(y_train_num.to_numpy()).any() else 0.0
    y_train_num = y_train_num.fillna(med)
    y_test_num = y_test_num.fillna(med)

    reg = Ridge(alpha=1.0, random_state=42)
    regr = Pipeline(steps=[("pre", preprocessor), ("model", reg)])
    regr.fit(X_train, y_train_num)
    y_pred = regr.predict(X_test)
    accuracy = float(_bounded_regression_accuracy(y_test_num, y_pred))
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression / Ridge) for CPU-friendly training and inference.
# - Employs a single sklearn Pipeline + ColumnTransformer to avoid redundant preprocessing work and ensure reproducibility.
# - Robust CSV loading retries alternative delimiter/decimal formats to reduce parsing failures without manual edits.
# - Column name normalization + dropping 'Unnamed' columns prevents schema noise and wasted compute.
# - Minimal feature engineering: median/mode imputation + standardization + sparse one-hot encoding (memory/energy efficient).
# - Defensive fallbacks: auto-selects a viable target/feature set; switches to bounded R2-based accuracy proxy in [0,1] if regression is necessary.