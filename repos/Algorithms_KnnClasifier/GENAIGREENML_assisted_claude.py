# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import pandas as pd

def read_csv_robust(path):
    df = pd.read_csv(path)
    if df.shape[1] < 2:
        df = pd.read_csv(path, sep=';', decimal=',')
    return df

def algorithm():
    np.random.seed(42)
    k_value = 1

    data_train = read_csv_robust('Data/Diabetes-Training.csv')
    data_test = read_csv_robust('Data/Diabetes-Clasification.csv')

    feature_cols = data_train.columns[:-1]
    class_col = data_train.columns[-1]

    X_train_raw = data_train[feature_cols].values.astype(np.float64)
    y_train = data_train[class_col].values
    X_test_raw = data_test[feature_cols].values.astype(np.float64)
    y_test = data_test[class_col].values

    mean = X_train_raw.mean(axis=0)
    std = X_train_raw.std(axis=0)
    std[std == 0] = 1.0

    X_train = (X_train_raw - mean) / std

    mean_test = X_test_raw.mean(axis=0)
    std_test = X_test_raw.std(axis=0)
    std_test[std_test == 0] = 1.0
    X_test = (X_test_raw - mean_test) / std_test

    train_sq = np.sum(X_train ** 2, axis=1)
    test_sq = np.sum(X_test ** 2, axis=1)
    cross = X_test @ X_train.T
    dist_matrix = np.sqrt(np.maximum(test_sq[:, np.newaxis] + train_sq[np.newaxis, :] - 2.0 * cross, 0.0))

    if k_value == 1:
        nearest_indices = np.argmin(dist_matrix, axis=1)
        predictions = y_train[nearest_indices]
    else:
        nearest_indices = np.argpartition(dist_matrix, k_value, axis=1)[:, :k_value]
        predictions = np.empty(len(X_test), dtype=y_train.dtype)
        positive_label = 'tested_positive'
        negative_label = 'tested_negative'
        for i in range(len(X_test)):
            neighbors = y_train[nearest_indices[i]]
            pos_count = np.sum(neighbors == positive_label)
            neg_count = k_value - pos_count
            predictions[i] = negative_label if neg_count > pos_count else positive_label

    correct_counts = np.sum(predictions == y_test)
    accuracy = correct_counts / len(y_test)

    print(f"ACCURACY={accuracy:.6f}")

algorithm()

# Optimization Summary
# 1. Replaced per-instance Euclidean distance loop with vectorized distance matrix computation using broadcasting, drastically reducing runtime and CPU cycles.
# 2. For k=1, used np.argmin directly instead of sorting, reducing from O(n log n) to O(n) per test instance.
# 3. Removed CSV file generation (artifact saving) as per requirements.
# 4. Removed all print statements, plots, and interactive inputs as per requirements.
# 5. Used robust CSV reading with semicolon/decimal fallback for input reliability.
# 6. Pre-computed squared norms and used the identity ||a-b||^2 = ||a||^2 + ||b||^2 - 2*a.b to avoid redundant computation.
# 7. Normalized using the same approach as original (each dataset normalized by its own mean/std) to preserve original behavior.
# 8. Set random seed for reproducibility.
# 9. Reduced memory footprint by avoiding intermediate lists and using numpy arrays throughout.
# 10. Used np.maximum(..., 0.0) to handle floating point