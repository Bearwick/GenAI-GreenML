# Generated by generate_llm_code.py
# LLM: claude
# Mode: assisted

import numpy as np
import pandas as pd

def algorithm():
    k_value = 1

    data_train = pd.read_csv('Data/Diabetes-Training.csv')
    data_test = pd.read_csv('Data/Diabetes-Clasification.csv')

    X_train_raw = data_train.iloc[:, :-1].values.astype(np.float64)
    y_train = data_train['class'].values
    X_test_raw = data_test.iloc[:, :-1].values.astype(np.float64)
    y_test = data_test['class'].values

    mean = X_train_raw.mean(axis=0)
    std = X_train_raw.std(axis=0)
    std[std == 0] = 1.0

    X_train = (X_train_raw - mean) / std

    mean_test = X_test_raw.mean(axis=0)
    std_test = X_test_raw.std(axis=0)
    std_test[std_test == 0] = 1.0
    X_test = (X_test_raw - mean_test) / std_test

    train_sq = np.sum(X_train ** 2, axis=1)
    test_sq = np.sum(X_test ** 2, axis=1)
    cross = X_test @ X_train.T
    dist_sq = test_sq[:, np.newaxis] + train_sq[np.newaxis, :] - 2.0 * cross

    if k_value == 1:
        nearest_idx = np.argmin(dist_sq, axis=1)
        predictions = y_train[nearest_idx]
    else:
        nearest_indices = np.argpartition(dist_sq, k_value, axis=1)[:, :k_value]
        correct_counts = 0
        predictions = np.empty(len(X_test), dtype=y_train.dtype)
        for i in range(len(X_test)):
            neighbors = y_train[nearest_indices[i]]
            neg_count = np.sum(neighbors == 'tested_negative')
            pos_count = np.sum(neighbors == 'tested_positive')
            predictions[i] = 'tested_negative' if neg_count > pos_count else 'tested_positive'

    correct_counts = np.sum(predictions == y_test)
    accuracy = correct_counts / len(y_test)

    print(f"ACCURACY={accuracy:.6f}")

algorithm()

# OPTIMIZATION SUMMARY
# 1. Vectorized distance computation using matrix operations instead of per-instance loop, avoiding O(n*m) Python-level iterations.
# 2. Precomputed squared norms and used the identity ||a-b||^2 = ||a||^2 + ||b||^2 - 2*a.b to avoid explicit subtraction per pair.
# 3. For k=1, used np.argmin directly instead of sorting, reducing complexity from O(n log n) to O(n) per test instance.
# 4. Removed CSV generation, plots, prints, and interactive inputs to reduce I/O overhead.
# 5. Used numpy arrays throughout instead of Python lists and dicts for neighbor counting.
# 6. Normalized using numpy operations on raw arrays instead of pandas DataFrame operations for lower overhead.
# 7. Preserved original normalization behavior: each dataset normalized by its own mean/std (matching original code).