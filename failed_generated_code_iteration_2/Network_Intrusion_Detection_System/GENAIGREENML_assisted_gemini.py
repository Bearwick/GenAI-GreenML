# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier

def run_pipeline():
    seed = 42
    np.random.seed(seed)

    def load_dataset(path):
        try:
            df = pd.read_csv(path, header=None, engine='c', low_memory=True)
            if df.shape[1] <= 1:
                raise ValueError
        except Exception:
            df = pd.read_csv(path, header=None, sep=';', decimal=',', engine='c', low_memory=True)
        return df

    train_df = load_dataset("data/raw/Train.txt")
    test_df = load_dataset("data/raw/Test.txt")

    y_train = (train_df.iloc[:, 41] != 'normal').astype(np.int8)
    y_test = (test_df.iloc[:, 41] != 'normal').astype(np.int8)

    X_train = train_df.iloc[:, :41]
    X_test = test_df.iloc[:, :41]

    del train_df
    del test_df

    cat_indices = [1, 2, 3]
    X_train = pd.get_dummies(X_train, columns=cat_indices)
    X_test = pd.get_dummies(X_test, columns=cat_indices)
    
    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)

    X_train = X_train.astype(np.float32)
    X_test = X_test.astype(np.float32)

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    pca = PCA(n_components=0.95, random_state=seed)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)

    del X_train
    del X_test

    model = XGBClassifier(
        n_estimators=200,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        eval_metric="logloss",
        random_state=seed,
        tree_method='hist',
        n_jobs=-1
    )
    model.fit(X_train_pca, y_train)

    y_pred = model.predict(X_test_pca)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary
# 1. Improved CSV parsing with 'c' engine and low_memory=True for faster, more energy-efficient I/O.
# 2. Implemented memory-efficient data typing by downcasting labels to int8 and features to float32.
# 3. Utilized 'del' to remove large intermediate dataframes (train_df, test_df, X_train, X_test) from memory.
# 4. Replaced default XGBoost tree_method with 'hist', significantly reducing training time and computational overhead.
# 5. Parallelized XGBoost training using n_jobs=-1 to complete the task faster and minimize total energy runtime.
# 6. Eliminated redundant file I/O by removing joblib artifact saving and plot generation.
# 7. Simplified the preprocessing pipeline by using direct pandas dummies and reindexing instead of external custom modules.
# 8. Fixed random seeds across numpy, PCA, and XGBoost to ensure reproducibility and avoid wasted cycles on inconsistent runs.