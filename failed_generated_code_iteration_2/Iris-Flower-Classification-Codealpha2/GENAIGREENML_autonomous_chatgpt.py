# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "Iris.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c).strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _read_csv_robust(path):
    # Try default read
    try:
        df1 = pd.read_csv(path)
    except Exception:
        df1 = None

    # Fallback read
    try:
        df2 = pd.read_csv(path, sep=";", decimal=",")
    except Exception:
        df2 = None

    if df1 is None and df2 is None:
        raise FileNotFoundError(f"Could not read dataset at path: {path}")

    def score_df(dfx):
        if dfx is None:
            return -1
        score = 0
        score += int(dfx.shape[1])
        score += int(dfx.shape[0] > 0)
        # Penalize obvious single-column wrong parsing
        if dfx.shape[1] == 1:
            score -= 10
        # Reward presence of likely header tokens if available
        cols = [str(c).lower() for c in dfx.columns]
        tokens = ["sepal", "petal", "species", "id"]
        score += sum(any(t in c for c in cols) for t in tokens)
        return score

    return df1 if score_df(df1) >= score_df(df2) else df2


df = _read_csv_robust(DATASET_PATH)
df.columns = _normalize_columns(df.columns)

# Drop unnamed columns defensively
unnamed = [c for c in df.columns if re.match(r"^Unnamed:?", str(c), flags=re.IGNORECASE)]
if unnamed:
    df = df.drop(columns=unnamed, errors="ignore")

# Basic sanity cleanup: remove fully empty rows
df = df.dropna(how="all")
assert df.shape[0] > 0 and df.shape[1] > 0

# Identify candidate target
preferred_targets = ["Species"]
target_col = None
for t in preferred_targets:
    if t in df.columns:
        target_col = t
        break

# If preferred missing, pick a reasonable fallback:
# 1) object column with >1 unique values
# 2) else numeric with >1 unique values
if target_col is None:
    obj_cols = [c for c in df.columns if df[c].dtype == "object"]
    for c in obj_cols:
        nun = df[c].nunique(dropna=True)
        if nun >= 2:
            target_col = c
            break

if target_col is None:
    # Coerce numeric candidates and pick non-constant
    numeric_candidates = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        nun = s.nunique(dropna=True)
        if nun >= 2:
            numeric_candidates.append((c, nun))
    if numeric_candidates:
        numeric_candidates.sort(key=lambda x: x[1], reverse=True)
        target_col = numeric_candidates[0][0]

if target_col is None:
    # Last resort: choose last column
    target_col = df.columns[-1]

y_raw = df[target_col].copy()
X = df.drop(columns=[target_col], errors="ignore").copy()

# Drop obvious ID-like columns if they are unique per row (but don't fail if none)
id_like = []
for c in X.columns:
    if "id" in str(c).lower():
        id_like.append(c)
    else:
        # Unique per-row high-cardinality numeric/object often acts like an ID
        nun = X[c].nunique(dropna=True)
        if nun >= max(20, int(0.9 * len(X))):
            id_like.append(c)
if id_like and len(id_like) < X.shape[1]:
    X = X.drop(columns=list(set(id_like)), errors="ignore")

# Infer numeric/categorical feature columns robustly
num_cols = []
cat_cols = []
for c in X.columns:
    s_num = pd.to_numeric(X[c], errors="coerce")
    non_na_ratio = float(s_num.notna().mean()) if len(s_num) else 0.0
    # If most values can be coerced -> numeric
    if non_na_ratio >= 0.8:
        num_cols.append(c)
        X[c] = s_num
    else:
        cat_cols.append(c)

# Ensure we have some features; if not, create a constant feature
if X.shape[1] == 0:
    X = pd.DataFrame({"__bias__": np.ones(len(df), dtype=np.float32)})
    num_cols = ["__bias__"]
    cat_cols = []

# Clean inf values in numeric columns
for c in num_cols:
    if c in X.columns:
        arr = X[c].to_numpy(copy=False)
        if np.issubdtype(arr.dtype, np.number):
            X.loc[~np.isfinite(arr), c] = np.nan

# Determine task type
# If target is object or has few unique values -> classification, else regression fallback
y_is_object = (y_raw.dtype == "object")
if not y_is_object:
    # Sometimes species could be read as category-like numeric; treat low-cardinality as classification
    y_num_try = pd.to_numeric(y_raw, errors="coerce")
    if y_num_try.notna().mean() >= 0.8:
        nun = y_num_try.nunique(dropna=True)
        y_is_object = (nun <= max(20, int(0.05 * len(y_num_try)) + 1))

is_classification = y_raw.dtype == "object" or y_is_object

# Prepare y for modeling
if is_classification:
    y = y_raw.astype(str).fillna("MISSING")
    # If target has <2 classes, fallback to regression with numeric encoding
    if y.nunique(dropna=False) < 2:
        is_classification = False
else:
    y = pd.to_numeric(y_raw, errors="coerce")

# Build preprocessing
numeric_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ]
)

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, num_cols if len(num_cols) else []),
        ("cat", categorical_transformer, cat_cols if len(cat_cols) else []),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# Train/test split
if is_classification:
    # stratify only if feasible
    stratify = y if y.nunique() >= 2 and y.value_counts().min() >= 2 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
    )
else:
    # Drop rows with NaN target for regression
    valid = pd.to_numeric(y, errors="coerce").notna()
    X = X.loc[valid].reset_index(drop=True)
    y = pd.to_numeric(y.loc[valid], errors="coerce").reset_index(drop=True)
    assert len(X) > 1
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE
    )

assert len(X_train) > 0 and len(X_test) > 0

# Model selection: lightweight linear models
if is_classification:
    model = LogisticRegression(
        solver="lbfgs",
        max_iter=200,
        n_jobs=1,
        multi_class="auto",
    )
    clf = Pipeline(steps=[("preprocess", preprocess), ("model", model)])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
else:
    # Regression fallback with bounded score in [0,1] derived from R^2
    model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
    reg = Pipeline(steps=[("preprocess", preprocess), ("model", model)])
    reg.fit(X_train, y_train)
    r2 = float(reg.score(X_test, y_test))
    # Map R^2 to [0,1] (clip to keep stable): accuracy_proxy = clip((r2+1)/2, 0, 1)
    accuracy = float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used simple linear models (LogisticRegression/Ridge) for strong baseline accuracy with low CPU cost and memory use.
# - Preprocessing via ColumnTransformer+Pipeline ensures reproducibility and avoids redundant work; one-hot is sparse for efficiency.
# - Robust CSV reading fallback (separator/decimal) and defensive schema handling minimize failures without manual edits.
# - Numeric coercion + inf/NaN handling prevents expensive exceptions and keeps computations stable.
# - Avoided heavy ensembles/deep learning; limited iterations and single-threaded training to reduce energy usage and variability.
# - Regression fallback uses a bounded proxy score in [0,1] from R^2 to keep required ACCURACY output stable.