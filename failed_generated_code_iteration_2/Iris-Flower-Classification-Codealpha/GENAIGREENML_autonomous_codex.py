# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, r2_score

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
        return df
    if df.shape[1] == 1:
        try:
            df2 = pd.read_csv(path, sep=';', decimal=',')
            if df2.shape[1] > 1:
                df = df2
        except Exception:
            pass
    return df

path = "Iris.csv"
df = read_csv_robust(path)

df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
df = df.loc[:, ~df.columns.str.contains(r'^Unnamed', case=False, regex=True)]

assert df.shape[0] > 0, "Empty dataset after loading"

lower_map = {c.lower(): c for c in df.columns}
target = None
for key in ['target', 'label', 'class', 'species', 'y']:
    if key in lower_map:
        target = lower_map[key]
        break
if target is None:
    obj_cols = [c for c in df.columns if df[c].dtype == object]
    for c in obj_cols:
        if df[c].nunique(dropna=True) > 1:
            target = c
            break
if target is None:
    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    best = None
    max_unique = 0
    for c in num_cols:
        nu = df[c].nunique(dropna=True)
        if nu > 1 and nu > max_unique:
            best = c
            max_unique = nu
    if best is not None:
        target = best
if target is None:
    target = df.columns[-1]

feature_cols = [c for c in df.columns if c != target]
if len(feature_cols) == 0:
    df['constant_feature'] = 1.0
    feature_cols = ['constant_feature']

feature_df = df[feature_cols].copy()
feature_df.replace([np.inf, -np.inf], np.nan, inplace=True)

numeric_cols = []
categorical_cols = []
for col in feature_cols:
    col_series = feature_df[col]
    coerced = pd.to_numeric(col_series, errors='coerce')
    non_null = col_series.notna().sum()
    if non_null > 0 and coerced.notna().sum() / non_null >= 0.8:
        feature_df[col] = coerced
        numeric_cols.append(col)
    else:
        categorical_cols.append(col)

all_nan_cols = [c for c in feature_df.columns if feature_df[c].isna().all()]
if all_nan_cols:
    feature_df = feature_df.drop(columns=all_nan_cols)
    numeric_cols = [c for c in numeric_cols if c not in all_nan_cols]
    categorical_cols = [c for c in categorical_cols if c not in all_nan_cols]

if feature_df.shape[1] == 0:
    feature_df['constant_feature'] = 1.0
    numeric_cols = ['constant_feature']
    categorical_cols = []

y_raw = df[target]
nunique = y_raw.nunique(dropna=True)
task = 'regression'
if y_raw.dtype == object or y_raw.dtype == bool:
    task = 'classification'
elif pd.api.types.is_integer_dtype(y_raw) and nunique <= 20:
    task = 'classification'
elif nunique <= 20 and nunique / max(len(y_raw), 1) < 0.2:
    task = 'classification'

if task == 'classification':
    if not pd.api.types.is_numeric_dtype(y_raw):
        le = LabelEncoder()
        y_series = y_raw.copy()
        mask = y_series.notna()
        y_encoded = pd.Series(np.nan, index=y_series.index, dtype=float)
        if mask.any():
            y_encoded.loc[mask] = le.fit_transform(y_series.loc[mask].astype(str))
        y = y_encoded
    else:
        y = pd.to_numeric(y_raw, errors='coerce')
    y.replace([np.inf, -np.inf], np.nan, inplace=True)
    mask = y.notna()
    X = feature_df.loc[mask]
    y = y.loc[mask]
    if y.nunique(dropna=True) < 2:
        task = 'regression'
        y = pd.to_numeric(y_raw, errors='coerce')
        y.replace([np.inf, -np.inf], np.nan, inplace=True)
        mask = y.notna()
        X = feature_df.loc[mask]
        y = y.loc[mask]
else:
    y = pd.to_numeric(y_raw, errors='coerce')
    y.replace([np.inf, -np.inf], np.nan, inplace=True)
    mask = y.notna()
    X = feature_df.loc[mask]
    y = y.loc[mask]

assert X.shape[0] > 1, "Not enough data after preprocessing"

transformers = []
if len(numeric_cols) > 0:
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    transformers.append(('num', numeric_transformer, numeric_cols))
if len(categorical_cols) > 0:
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    transformers.append(('cat', categorical_transformer, categorical_cols))
if len(transformers) == 0:
    X = X.copy()
    X['constant_feature'] = 1.0
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    transformers = [('num', numeric_transformer, ['constant_feature'])]

preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')

if task == 'classification':
    model = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='auto')
else:
    model = LinearRegression()

clf = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])

stratify = None
if task == 'classification':
    counts = y.value_counts()
    if counts.min() >= 2 and counts.shape[0] > 1:
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=stratify
)

assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed"

clf.fit(X_train, y_train)
preds = clf.predict(X_test)

if task == 'classification':
    try:
        accuracy = float(accuracy_score(y_test, preds))
    except Exception:
        accuracy = 0.0
else:
    try:
        r2 = float(r2_score(y_test, preds))
    except Exception:
        r2 = 0.0
    if not np.isfinite(r2):
        r2 = 0.0
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Selected lightweight linear/logistic models and minimal preprocessing to reduce CPU load.
# - Robust column handling with numeric coercion, simple imputation, and one-hot encoding for schema flexibility.
# - Implemented safe task detection with fallback to regression and bounded R2-to-accuracy proxy.
# - Used fixed random_state and small train/test split for reproducibility without heavy computation.