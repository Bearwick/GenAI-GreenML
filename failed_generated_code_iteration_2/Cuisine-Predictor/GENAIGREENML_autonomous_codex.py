# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

def robust_load(path):
    df = None
    try:
        df = pd.read_json(path)
    except Exception:
        try:
            df = pd.read_json(path, lines=True)
        except Exception:
            df = None
    if df is None:
        try:
            df = pd.read_csv(path)
        except Exception:
            try:
                df = pd.read_csv(path, sep=';', decimal=',')
            except Exception:
                df = None
    if df is None:
        df = pd.DataFrame()
    return df

def normalize_columns(df):
    cols = []
    for c in df.columns:
        if isinstance(c, str):
            c_norm = " ".join(c.strip().split())
        else:
            c_norm = str(c)
        cols.append(c_norm)
    df.columns = cols
    df = df.loc[:, ~df.columns.str.match(r'^Unnamed', case=False)]
    return df

def choose_target(df):
    if df.shape[1] == 0:
        return None
    cols = list(df.columns)
    lower_cols = [c.lower() for c in cols]
    for key in ['target', 'label', 'class', 'cuisine', 'y', 'output', 'response']:
        if key in lower_cols:
            return cols[lower_cols.index(key)]
    candidate = None
    min_unique = None
    for c in cols:
        series = df[c]
        nunique = series.nunique(dropna=True)
        if nunique <= 1:
            continue
        if series.dtype == object or pd.api.types.is_categorical_dtype(series):
            if min_unique is None or nunique < min_unique:
                candidate = c
                min_unique = nunique
    if candidate is not None:
        return candidate
    for c in cols:
        s = pd.to_numeric(df[c], errors='coerce')
        if s.notna().sum() > 0 and s.nunique(dropna=True) > 1:
            return c
    return cols[-1]

def normalize_value(v):
    if isinstance(v, (list, tuple, set)):
        return " ".join(map(str, v))
    if isinstance(v, dict):
        return " ".join(map(str, v.values()))
    return v

data_path = "train.json"
df = robust_load(data_path)
df = normalize_columns(df)
df = df.dropna(how='all')
assert df.shape[0] > 0

target_col = choose_target(df)
if target_col is None:
    target_col = df.columns[-1]

y = df[target_col].apply(normalize_value)
y_num = pd.to_numeric(y, errors='coerce')
if y_num.notna().mean() > 0.9:
    y = y_num
else:
    if y.dtype == object:
        y = y.astype(str)

mask = y.notna()
X = df.drop(columns=[target_col]).loc[mask].copy()
y = y.loc[mask].copy()
assert len(y) > 0

X.replace([np.inf, -np.inf], np.nan, inplace=True)

text_cols = []
cat_cols = []
numeric_cols = []
n_rows = len(X)

for col in X.columns:
    s = X[col]
    sample = s.dropna().iloc[0] if not s.dropna().empty else None
    if isinstance(sample, (list, tuple, set)):
        X[col] = s.apply(lambda v: " ".join(map(str, v)) if isinstance(v, (list, tuple, set)) else ("" if pd.isna(v) else str(v)))
        text_cols.append(col)
        continue
    if isinstance(sample, dict):
        X[col] = s.apply(lambda v: " ".join(map(str, v.values())) if isinstance(v, dict) else ("" if pd.isna(v) else str(v)))
        text_cols.append(col)
        continue
    if pd.api.types.is_numeric_dtype(s):
        X[col] = pd.to_numeric(s, errors='coerce')
        numeric_cols.append(col)
        continue
    s_num = pd.to_numeric(s, errors='coerce')
    if s_num.notna().mean() > 0.9:
        X[col] = s_num
        numeric_cols.append(col)
    else:
        s_str = s.astype(str)
        avg_len = s_str.str.len().mean() if n_rows > 0 else 0
        n_unique = s_str.nunique(dropna=True)
        if avg_len > 30 or n_unique > min(50, max(1, int(0.2 * n_rows))):
            text_cols.append(col)
            X[col] = s_str
        else:
            cat_cols.append(col)
            X[col] = s_str

text_col = None
if len(text_cols) > 0:
    X['__combined_text__'] = X[text_cols].fillna('').agg(' '.join, axis=1)
    text_col = '__combined_text__'
    X = X.drop(columns=text_cols)

numeric_cols = [c for c in numeric_cols if c in X.columns]
cat_cols = [c for c in cat_cols if c in X.columns]

if text_col is not None:
    X[text_col] = X[text_col].fillna('')

if X.shape[1] == 0:
    X['__dummy__'] = 0
    numeric_cols = ['__dummy__']

n_unique = y.nunique(dropna=True)
if y.dtype == object or pd.api.types.is_categorical_dtype(y):
    is_classification = True
else:
    is_classification = n_unique <= max(20, int(0.2 * len(y)))

use_dummy_classifier = is_classification and n_unique < 2
use_dummy_regressor = (not is_classification) and n_unique < 2

n_samples = len(X)
test_size = max(1, int(0.2 * n_samples))
if test_size >= n_samples:
    test_size = max(1, n_samples // 2)

stratify = None
if is_classification and n_unique > 1:
    counts = y.value_counts(dropna=True)
    if counts.min() >= 2 and test_size >= n_unique and n_samples >= (2 * n_unique):
        stratify = y

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=stratify)
assert len(X_train) > 0 and len(X_test) > 0

if is_classification:
    if use_dummy_classifier:
        clf = Pipeline([('model', DummyClassifier(strategy='most_frequent'))])
    else:
        if text_col is not None:
            preprocessor = ColumnTransformer([('text', CountVectorizer(max_features=5000), text_col)], remainder='drop')
            model = MultinomialNB()
        else:
            transformers = []
            if numeric_cols:
                num_pipe = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler(with_mean=False))])
                transformers.append(('num', num_pipe, numeric_cols))
            if cat_cols:
                cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))])
                transformers.append(('cat', cat_pipe, cat_cols))
            if not transformers:
                X_train = X_train.copy()
                X_test = X_test.copy()
                X_train['__dummy__'] = 0
                X_test['__dummy__'] = 0
                numeric_cols = ['__dummy__']
                num_pipe = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler(with_mean=False))])
                transformers = [('num', num_pipe, numeric_cols)]
            preprocessor = ColumnTransformer(transformers, remainder='drop')
            model = LogisticRegression(max_iter=200, solver='liblinear')
        clf = Pipeline([('preprocessor', preprocessor), ('model', model)])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
else:
    if use_dummy_regressor:
        clf = Pipeline([('model', DummyRegressor(strategy='mean'))])
    else:
        transformers = []
        if text_col is not None:
            transformers.append(('text', CountVectorizer(max_features=5000), text_col))
        if numeric_cols:
            num_pipe = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler(with_mean=False))])
            transformers.append(('num', num_pipe, numeric_cols))
        if cat_cols:
            cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))])
            transformers.append(('cat', cat_pipe, cat_cols))
        if not transformers:
            X_train = X_train.copy()
            X_test = X_test.copy()
            X_train['__dummy__'] = 0
            X_test['__dummy__'] = 0
            numeric_cols = ['__dummy__']
            num_pipe = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler(with_mean=False))])
            transformers = [('num', num_pipe, numeric_cols)]
        preprocessor = ColumnTransformer(transformers, remainder='drop')
        model = Ridge(alpha=1.0)
        clf = Pipeline([('preprocessor', preprocessor), ('model', model)])
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    try:
        r2 = r2_score(y_test, y_pred)
    except Exception:
        r2 = 0.0
    if not np.isfinite(r2):
        r2 = 0.0
    accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Used simple CountVectorizer with MultinomialNB for text-heavy cases to minimize CPU cost.
# - Applied lightweight imputing and scaling/encoding only where needed to keep the pipeline efficient.
# - Added dummy-model fallbacks and scaled R2 for robustness on edge-case targets while ensuring reproducibility.