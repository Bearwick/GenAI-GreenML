# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import json
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score


def _normalize_columns(cols):
    out = []
    for c in cols:
        c = "" if c is None else str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        out.append(c)
    return out


def _drop_unnamed_columns(df):
    drop_cols = [c for c in df.columns if isinstance(c, str) and c.strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def load_dataset(path):
    # Primary path: JSON (dataset is .json per context). Fallback to CSV parsing rules if JSON fails.
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        df = pd.json_normalize(data)
        if df is None or df.shape[0] == 0:
            raise ValueError("Empty JSON after normalization")
        return df
    except Exception:
        # CSV fallback per requirements
        try:
            df = pd.read_csv(path)
        except Exception:
            df = pd.read_csv(path, sep=";", decimal=",")
        return df


def pick_target_and_text(df):
    cols_lower = {str(c).strip().lower(): c for c in df.columns}

    # Likely schema for "What's Cooking": target=cuisine, text from ingredients
    target_col = cols_lower.get("cuisine", None)
    if target_col is None:
        # pick an object column with >1 unique values as classification target
        obj_cols = [c for c in df.columns if df[c].dtype == "object"]
        best = None
        best_nuniq = 0
        for c in obj_cols:
            nun = df[c].nunique(dropna=True)
            if nun > best_nuniq:
                best_nuniq = nun
                best = c
        target_col = best

    # Find ingredients-like column(s)
    ing_col = cols_lower.get("ingredients", None)
    if ing_col is None:
        # choose a column containing list-like objects or long-ish text
        best = None
        best_score = -1
        for c in df.columns:
            s = df[c]
            if s.dtype == "object":
                # quick heuristic: sample non-null values and score list-ness / average length
                vals = s.dropna().head(200).tolist()
                if not vals:
                    continue
                list_count = sum(isinstance(v, (list, tuple)) for v in vals)
                strlens = [len(str(v)) for v in vals]
                score = list_count * 10 + (float(np.mean(strlens)) if strlens else 0.0)
                if score > best_score:
                    best_score = score
                    best = c
        ing_col = best

    return target_col, ing_col


def build_text_series(df, text_col):
    if text_col is None or text_col not in df.columns:
        # Fallback: concatenate all non-target object columns into text
        obj_cols = [c for c in df.columns if df[c].dtype == "object"]
        if not obj_cols:
            # Last resort: stringify all columns
            return df.astype(str).agg(" ".join, axis=1)

        def row_join(r):
            parts = []
            for c in obj_cols:
                v = r.get(c, "")
                if isinstance(v, (list, tuple)):
                    parts.append(" ".join([str(x) for x in v]))
                else:
                    parts.append(str(v))
            return " ".join(parts)

        return df[obj_cols].apply(lambda r: row_join(r), axis=1)

    s = df[text_col]

    # Convert list-of-ingredients to a single whitespace-separated string
    def to_text(v):
        if isinstance(v, (list, tuple)):
            return " ".join([str(x) for x in v if x is not None])
        if pd.isna(v):
            return ""
        return str(v)

    return s.apply(to_text)


def main():
    df = load_dataset("train.json")
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed_columns(df)

    # Defensive: ensure non-empty
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col, text_col = pick_target_and_text(df)

    # If we can't find a reasonable target, fallback to a stable trivial score
    if target_col is None or target_col not in df.columns:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    y = df[target_col]
    X_text = build_text_series(df, text_col)

    # Clean target
    y = y.astype(str).replace({"nan": np.nan, "None": np.nan, "": np.nan})
    mask = y.notna() & X_text.notna()
    y = y[mask]
    X_text = X_text[mask].astype(str)

    assert len(y) > 0

    # If only one class, use trivial baseline (predict the only class) and compute accuracy
    n_classes = y.nunique(dropna=True)
    if n_classes < 2:
        accuracy = 1.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Train/test split with fixed seed; stratify when feasible
    stratify = y if n_classes >= 2 and len(y) >= n_classes * 2 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X_text,
        y,
        test_size=0.2,
        random_state=42,
        stratify=stratify,
    )

    assert len(X_train) > 0 and len(X_test) > 0

    # CPU-friendly baseline: TF-IDF + MultinomialNB
    # Constrain vocabulary size for energy efficiency; unigrams/bigrams capture informative ingredient phrases.
    pipeline = Pipeline(
        steps=[
            (
                "tfidf",
                TfidfVectorizer(
                    lowercase=True,
                    strip_accents="unicode",
                    token_pattern=r"(?u)\b[a-zA-Z][a-zA-Z]+\b",
                    ngram_range=(1, 2),
                    max_features=30000,
                    min_df=2,
                    dtype=np.float32,
                ),
            ),
            ("clf", MultinomialNB(alpha=0.5)),
        ]
    )

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)

    accuracy = float(accuracy_score(y_test, y_pred))
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used TF-IDF + Multinomial Naive Bayes: strong baseline for text classification with low CPU/memory cost vs. deep models.
# - Capped vocabulary (max_features=30000) and pruned rare tokens (min_df=2) to reduce RAM/compute and improve generalization.
# - Used float32 in vectorizer to cut memory bandwidth/footprint on CPU.
# - Minimal preprocessing and a single sklearn Pipeline for reproducibility and to avoid redundant transformations.
# - Defensive schema handling: auto-detect target/text columns; robustly converts list-of-ingredients to text; falls back safely when schema differs.
# - Fixed random_state for deterministic train/test split; limited stdout to the required ACCURACY line.