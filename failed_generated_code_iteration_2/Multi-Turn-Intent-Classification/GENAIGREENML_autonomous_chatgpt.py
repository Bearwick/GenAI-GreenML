# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import json
import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.feature_extraction.text import TfidfVectorizer


RANDOM_STATE = 42


def _normalize_colname(c):
    c = "" if c is None else str(c)
    c = c.strip()
    c = re.sub(r"\s+", " ", c)
    return c


def _coerce_numeric_series(s: pd.Series) -> pd.Series:
    return pd.to_numeric(s, errors="coerce")


def _safe_json_load(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def _to_dataframe(obj):
    # Robustly convert JSON-like structures into a flat-ish DataFrame.
    if isinstance(obj, list):
        if len(obj) == 0:
            return pd.DataFrame()
        if all(isinstance(x, dict) for x in obj):
            return pd.json_normalize(obj)
        return pd.DataFrame({"value": obj})
    if isinstance(obj, dict):
        # Prefer common keys holding records
        for k in ["data", "records", "items", "conversations", "examples", "samples"]:
            if k in obj and isinstance(obj[k], list):
                return pd.json_normalize(obj[k])
        # If dict of lists (columnar)
        if all(isinstance(v, list) for v in obj.values()) and len(obj) > 0:
            try:
                return pd.DataFrame(obj)
            except Exception:
                pass
        # Fallback single record
        return pd.json_normalize(obj)
    # Fallback
    return pd.DataFrame({"value": [obj]})


def _drop_unnamed_cols(df: pd.DataFrame) -> pd.DataFrame:
    keep = []
    for c in df.columns:
        if c is None:
            continue
        cs = str(c)
        if cs.startswith("Unnamed:"):
            continue
        keep.append(c)
    return df[keep]


def _clean_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [_normalize_colname(c) for c in df.columns]
    df = _drop_unnamed_cols(df)
    # Deduplicate columns if any collisions after normalization
    if df.columns.duplicated().any():
        new_cols = []
        seen = {}
        for c in df.columns:
            if c not in seen:
                seen[c] = 0
                new_cols.append(c)
            else:
                seen[c] += 1
                new_cols.append(f"{c}__dup{seen[c]}")
        df.columns = new_cols
    return df


def _read_dataset(path: str) -> pd.DataFrame:
    # Support JSON primarily, but include CSV fallback with robust parsing.
    ext = os.path.splitext(path)[1].lower()
    if ext in [".json", ".jsonl"]:
        obj = _safe_json_load(path)
        df = _to_dataframe(obj)
        df = _clean_columns(df)
        return df

    # CSV/TSV fallback
    try:
        df = pd.read_csv(path)
        df = _clean_columns(df)
        if df.shape[1] <= 1:
            raise ValueError("Parsing looks wrong; retrying with ';' and decimal=','")
        return df
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
        df = _clean_columns(df)
        return df


def _choose_target(df: pd.DataFrame):
    # Prefer explicit target-like columns if present.
    cols_lower = {c: c.lower() for c in df.columns}
    preferred = ["target", "label", "intent", "y", "class", "category", "outcome"]
    for p in preferred:
        for c, cl in cols_lower.items():
            if cl == p or cl.endswith(f"_{p}") or cl.startswith(f"{p}_"):
                if df[c].notna().sum() > 0:
                    return c

    # Otherwise choose a reasonable column:
    # Prefer non-constant categorical-like with limited unique values, else a non-constant numeric.
    best_cat = None
    best_cat_uniques = None
    for c in df.columns:
        s = df[c]
        nunq = s.nunique(dropna=True)
        if nunq <= 1:
            continue
        # If object/string-like, likely classification label
        if s.dtype == "object" or pd.api.types.is_string_dtype(s) or pd.api.types.is_bool_dtype(s):
            # Prefer small-to-moderate cardinality
            if 2 <= nunq <= 50:
                if best_cat is None or nunq < best_cat_uniques:
                    best_cat = c
                    best_cat_uniques = nunq

    if best_cat is not None:
        return best_cat

    # Numeric fallback
    numeric_candidates = []
    for c in df.columns:
        s_num = _coerce_numeric_series(df[c])
        nunq = s_num.nunique(dropna=True)
        if nunq > 1 and s_num.notna().sum() > 0:
            numeric_candidates.append((c, nunq, s_num.notna().mean()))
    if numeric_candidates:
        # Prefer fewer uniques and more coverage to avoid ID-like targets
        numeric_candidates.sort(key=lambda x: (x[1], -x[2]))
        return numeric_candidates[0][0]

    return df.columns[-1] if df.shape[1] > 0 else None


def _detect_text_columns(df: pd.DataFrame, feature_cols):
    text_cols = []
    for c in feature_cols:
        if c not in df.columns:
            continue
        s = df[c]
        if pd.api.types.is_string_dtype(s) or s.dtype == "object":
            # Heuristic: treat as text if average length is relatively large
            non_null = s.dropna().astype(str)
            if len(non_null) == 0:
                continue
            avg_len = non_null.str.len().mean()
            uniq_ratio = non_null.nunique() / max(1, len(non_null))
            if avg_len >= 20 or (avg_len >= 10 and uniq_ratio > 0.3):
                text_cols.append(c)
    return text_cols


def _make_text_feature(df: pd.DataFrame, text_cols):
    if not text_cols:
        return None
    # Efficiently concatenate text columns into one text field to avoid multi-vectorizer overhead.
    parts = []
    for c in text_cols:
        parts.append(df[c].fillna("").astype(str))
    text = parts[0]
    for p in parts[1:]:
        text = text.str.cat(p, sep=" ")
    return text


def _bounded_regression_score(y_true, y_pred):
    # Convert R2 to a stable [0,1] proxy: clamp R2 to [-1,1], then map to [0,1]
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    denom = np.sum((y_true - np.mean(y_true)) ** 2)
    if denom <= 0:
        return 0.0
    r2 = 1.0 - (np.sum((y_true - y_pred) ** 2) / denom)
    r2 = float(np.clip(r2, -1.0, 1.0))
    return 0.5 * (r2 + 1.0)


def main():
    path = "data/input.json"
    df = _read_dataset(path)
    df = _clean_columns(df)

    # Defensive: ensure non-empty
    if df is None or df.shape[0] == 0:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    target_col = _choose_target(df)
    if target_col is None or target_col not in df.columns:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    y_raw = df[target_col]
    X = df.drop(columns=[target_col])

    # If no features remain, use dummy baselines end-to-end
    if X.shape[1] == 0:
        # Choose classification vs regression based on target type
        y_as_num = _coerce_numeric_series(y_raw)
        is_reg = y_as_num.notna().mean() > 0.9 and y_as_num.nunique(dropna=True) > 10
        if is_reg:
            y = y_as_num.fillna(y_as_num.median() if y_as_num.notna().any() else 0.0).values
            X_idx = np.arange(len(y)).reshape(-1, 1)
            X_train, X_test, y_train, y_test = train_test_split(
                X_idx, y, test_size=0.2, random_state=RANDOM_STATE
            )
            assert len(y_train) > 0 and len(y_test) > 0
            model = DummyRegressor(strategy="mean")
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = _bounded_regression_score(y_test, y_pred)
        else:
            y = y_raw.astype(str).fillna("NA").values
            X_idx = np.arange(len(y)).reshape(-1, 1)
            X_train, X_test, y_train, y_test = train_test_split(
                X_idx, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y if len(np.unique(y)) > 1 else None
            )
            assert len(y_train) > 0 and len(y_test) > 0
            model = DummyClassifier(strategy="most_frequent")
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
        print(f"ACCURACY={accuracy:.6f}")
        return

    # Determine task type
    y_num = _coerce_numeric_series(y_raw)
    unique_non_null = y_raw.dropna().nunique()
    # Heuristic: if numeric and many unique values -> regression
    is_regression = (y_num.notna().mean() > 0.9) and (y_num.nunique(dropna=True) > 20)
    # If target is object but looks numeric and high-cardinality -> regression
    if (not is_regression) and (y_raw.dtype == "object" or pd.api.types.is_string_dtype(y_raw)):
        if y_num.notna().mean() > 0.95 and y_num.nunique(dropna=True) > 20:
            is_regression = True

    feature_cols = list(X.columns)

    # Identify text columns and build single text feature
    text_cols = _detect_text_columns(df, feature_cols)
    text_series = _make_text_feature(X, text_cols) if text_cols else None

    # Remaining structured columns
    structured_cols = [c for c in feature_cols if c not in set(text_cols)]
    num_cols = []
    cat_cols = []
    for c in structured_cols:
        s = X[c]
        # Identify numeric-like columns by coercion coverage
        s_num = _coerce_numeric_series(s)
        if s_num.notna().mean() > 0.85 and s_num.nunique(dropna=True) > 0:
            num_cols.append(c)
        else:
            cat_cols.append(c)

    # Build preprocessors
    transformers = []

    if len(num_cols) > 0:
        num_pipe = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
        ])
        transformers.append(("num", num_pipe, num_cols))

    if len(cat_cols) > 0:
        cat_pipe = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ])
        transformers.append(("cat", cat_pipe, cat_cols))

    preprocessor = ColumnTransformer(
        transformers=transformers,
        remainder="drop",
        sparse_threshold=0.3
    )

    # If we have text, we will use FeatureUnion-like behavior via a ColumnTransformer on a combined DataFrame
    # by adding a temporary '___text___' column (cheap) and including TF-IDF vectorizer.
    X_work = X.copy()
    if text_series is not None:
        X_work["___text___"] = text_series
        tfidf = TfidfVectorizer(
            lowercase=True,
            strip_accents="unicode",
            max_features=20000,  # cap for CPU/energy efficiency
            ngram_range=(1, 2),
            min_df=2
        )
        transformers2 = list(transformers)
        transformers2.append(("text", tfidf, "___text___"))
        preprocessor = ColumnTransformer(
            transformers=transformers2,
            remainder="drop",
            sparse_threshold=0.3
        )

    # Prepare y
    if is_regression:
        y = y_num
        if y.notna().sum() == 0:
            # fallback: treat as classification
            is_regression = False
        else:
            y = y.fillna(y.median() if y.notna().any() else 0.0).values.astype(float)
    if not is_regression:
        y = y_raw.astype(str).fillna("NA").values
        # If <2 classes, fallback to regression on numeric coercion if possible, else dummy
        if len(np.unique(y)) < 2:
            y_num2 = _coerce_numeric_series(y_raw)
            if y_num2.notna().sum() > 0 and y_num2.nunique(dropna=True) > 1:
                is_regression = True
                y = y_num2.fillna(y_num2.median() if y_num2.notna().any() else 0.0).values.astype(float)

    # Train/test split
    if is_regression:
        X_train, X_test, y_train, y_test = train_test_split(
            X_work, y, test_size=0.2, random_state=RANDOM_STATE
        )
        assert len(y_train) > 0 and len(y_test) > 0

        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        pipe = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ])
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = _bounded_regression_score(y_test, y_pred)
    else:
        strat = y if len(np.unique(y)) >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X_work, y, test_size=0.2, random_state=RANDOM_STATE, stratify=strat
        )
        assert len(y_train) > 0 and len(y_test) > 0

        # Use a small, CPU-friendly linear classifier; saga handles sparse (TF-IDF/OHE) efficiently.
        # If extremely small data, liblinear can be slightly cheaper.
        solver = "liblinear" if len(y_train) < 2000 else "saga"
        clf = LogisticRegression(
            max_iter=200,
            solver=solver,
            n_jobs=1,
            random_state=RANDOM_STATE
        )
        pipe = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", clf),
        ])
        try:
            pipe.fit(X_train, y_train)
            y_pred = pipe.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))
        except Exception:
            # Safe fallback: dummy classifier
            dummy = DummyClassifier(strategy="most_frequent")
            # Need numeric array for dummy; use index feature
            X_idx = np.arange(len(y)).reshape(-1, 1)
            X_train2, X_test2, y_train2, y_test2 = train_test_split(
                X_idx, y, test_size=0.2, random_state=RANDOM_STATE, stratify=strat
            )
            dummy.fit(X_train2, y_train2)
            y_pred2 = dummy.predict(X_test2)
            accuracy = float(accuracy_score(y_test2, y_pred2))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression / Ridge) for CPU-friendly training and inference; avoids deep learning and large ensembles.
# - Caps TF-IDF dimensionality (max_features=20000) and uses sparse representations to reduce memory/compute; concatenates multiple text fields into one to avoid multiple vectorizers.
# - ColumnTransformer+Pipeline ensures single-pass preprocessing, reproducibility, and prevents redundant transformations.
# - Robust schema handling: target chosen heuristically; columns normalized; numeric coercion with errors='coerce'; safe fallbacks to Dummy* models.
# - For regression fallback, prints ACCURACY as a bounded proxy score: clamp R2 to [-1,1] and map to [0,1] for stable reporting across datasets.