# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, r2_score

def run_pipeline():
    # 1. Robust CSV Parsing
    file_path = 'Iris.csv'
    try:
        df = pd.read_csv(file_path)
        if df.shape[1] <= 1:
            raise ValueError
    except:
        df = pd.read_csv(file_path, sep=';', decimal=',')

    # 2. Schema Normalization
    df.columns = [str(c).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

    # 3. Feature and Target Identification
    # Exclude ID-like columns
    potential_features = [c for c in df.columns if 'id' not in c.lower()]
    
    if not potential_features:
        # Fallback if all columns filtered
        potential_features = list(df.columns)

    # Assume last column is target if 'Species' is not found
    target_col = 'Species' if 'Species' in potential_features else potential_features[-1]
    feature_cols = [c for c in potential_features if c != target_col]

    # 4. Data Cleaning and Coercion
    # Coerce features to numeric, drop non-numeric columns from features
    X_raw = df[feature_cols].apply(pd.to_numeric, errors='coerce')
    valid_feature_mask = X_raw.columns[X_raw.notna().any()]
    X_raw = X_raw[list(valid_feature_mask)]
    
    y_raw = df[target_col]

    # Handle NaN/Inf
    clean_mask = X_raw.notna().all(axis=1) & y_raw.notna()
    X = X_raw[clean_mask]
    y = y_raw[clean_mask]

    assert not X.empty, "Dataset is empty after preprocessing"

    # 5. Model Selection (Classification vs Regression)
    # Check if target is categorical or numeric with few levels
    is_classification = True
    if pd.api.types.is_numeric_dtype(y):
        if len(np.unique(y)) > 10: # Threshold for regression
            is_classification = False

    # 6. Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    assert len(X_train) > 0, "Training set is empty"

    # 7. Energy-Efficient Scaling and Modeling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    if is_classification:
        if len(np.unique(y_train)) < 2:
            # Trivial case: only one class present in training
            accuracy = 1.0 if (y_test == y_train.iloc[0]).all() else 0.0
        else:
            # Logistic Regression is highly efficient for CPU
            model = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='auto')
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)
    else:
        # Regression fallback using Ridge (lightweight linear model)
        model = Ridge(alpha=1.0)
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        # Convert R^2 to accuracy proxy in [0, 1]
        r2 = r2_score(y_test, y_pred)
        accuracy = max(0, min(1, r2))

    # 8. Final Output
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary:
# 1. Used Logistic Regression (classification) and Ridge (regression) as they have low computational complexity O(n*d).
# 2. Implemented StandardScaler to ensure faster convergence, reducing total CPU cycles/energy consumption.
# 3. Minimized data movement by using column indexing and boolean masking for cleaning.
# 4. Avoided high-variance/high-compute models like Gradient Boosted Trees or Neural Networks.
# 5. Robust parsing handles various CSV formats without requiring manual intervention or multiple passes.
# 6. Accuracy proxy for regression (R^2 clamped to [0,1]) ensures the pipeline remains generic but stable.
# 7. Fixed random_state ensures reproducibility without stochastic overhead in evaluation.
# 8. Filtered 'Id' and 'Unnamed' columns to reduce memory footprint and prevent model overfitting on noise.