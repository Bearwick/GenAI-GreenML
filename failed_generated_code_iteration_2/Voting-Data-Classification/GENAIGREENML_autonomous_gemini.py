# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
import sys
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

def load_data_robustly(filepath):
    try:
        df = pd.read_csv(filepath)
        if df.shape[1] <= 1:
            raise ValueError("Possible wrong separator")
    except:
        try:
            df = pd.read_csv(filepath, sep=';', decimal=',')
        except:
            return pd.DataFrame()
    
    # Normalize column names: strip, collapse whitespace, drop Unnamed
    df.columns = [re.sub(r'\s+', ' ', str(col).strip()) for col in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    return df

def solve():
    # Load dataset
    df = load_data_robustly('voting_data.csv')
    
    if df.empty:
        print("ACCURACY=0.000000")
        return

    # Identify target and feature columns
    target_col = None
    text_col = None
    
    # Priority 1: Use known schema
    cols = list(df.columns)
    if 'label' in cols:
        target_col = 'label'
    if 'text' in cols:
        text_col = 'text'
        
    # Priority 2: Heuristics if schema names differ
    if not target_col:
        # Prefer the last column if it has few unique values relative to size
        potential_target = cols[-1]
        if df[potential_target].nunique() < 100:
            target_col = potential_target
            
    if not text_col:
        # Prefer the first column with highest average string length
        object_cols = df.select_dtypes(include=['object']).columns
        if len(object_cols) > 0:
            text_col = object_cols[0]
            
    # Fallback/Safety
    if not target_col or not text_col or target_col == text_col:
        # Use simple numeric slice if possible
        available = [c for c in cols if c != target_col]
        if not target_col and len(cols) > 1:
            target_col = cols[-1]
            text_col = cols[0]
        elif not target_col:
            print("ACCURACY=0.000000")
            return

    # Data Cleaning
    df = df.dropna(subset=[target_col, text_col])
    
    # Ensure text_col is string and target_col is coerced
    df[text_col] = df[text_col].astype(str)
    
    # Check if we have enough classes for classification
    unique_classes = df[target_col].nunique()
    if unique_classes < 2:
        # Cannot classify with 1 class
        print("ACCURACY=1.000000" if unique_classes == 1 else "ACCURACY=0.000000")
        return

    # Train/Test Split
    X = df[text_col]
    y = df[target_col]
    
    try:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)
    except ValueError:
        # Fallback if stratify fails due to class counts
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

    # Pipeline Design: TF-IDF + Logistic Regression
    # Energy efficiency: 
    # 1. TfidfVectorizer with max_features limits the dimensionality of the weight matrix.
    # 2. LogisticRegression with liblinear is fast for small-medium text datasets.
    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(
            max_features=3000, 
            stop_words='english', 
            ngram_range=(1, 1),
            sublinear_tf=True
        )),
        ('clf', LogisticRegression(
            solver='liblinear', 
            max_iter=500, 
            multi_class='auto',
            C=1.0
        ))
    ])

    # Fit and Evaluate
    pipeline.fit(X_train, y_train)
    predictions = pipeline.predict(X_test)
    
    # Determine metric (Accuracy for classification)
    accuracy = accuracy_score(y_test, predictions)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    solve()

# Optimization Summary:
# 1. Used LogisticRegression with 'liblinear' solver: Highly energy-efficient for CPU-based training on text data, offering faster convergence than large-scale SVMs or Deep Learning for small datasets.
# 2. Optimized TfidfVectorizer: Restricted 'max_features' to 3000 to reduce memory footprint and computational cost during feature extraction and matrix multiplication.
# 3. Robust Schema Handling: Implemented flexible CSV parsing and column identification (regex normalization and dtype inspection) to prevent crashes on varying file formats.
# 4. CPU-Friendly: Avoided dense embeddings (Word2Vec/BERT) which require significant RAM and compute power, favoring sparse vectorization which is native to scikit-learn's efficient C-based implementations.
# 5. Pipeline Architecture: Used sklearn.pipeline.Pipeline to minimize data transformations and ensure no leakage, keeping the execution flow streamlined.
# 6. Error Resilience: Included checks for empty datasets and single-class targets to ensure the script returns a valid accuracy proxy even under adversarial data conditions.