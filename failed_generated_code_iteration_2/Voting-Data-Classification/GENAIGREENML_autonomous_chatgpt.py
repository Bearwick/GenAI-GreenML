# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier


DATASET_PATH = "voting_data.csv"
RANDOM_STATE = 42


def _read_csv_robust(path: str) -> pd.DataFrame:
    # Try default CSV parsing
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    # Fallback to ; separator and , decimal (common in EU exports)
    if df is None or df.shape[1] <= 1:
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df is None or df2.shape[1] > df.shape[1]:
                df = df2
        except Exception:
            pass

    if df is None:
        # Last-resort: read as text lines and build minimal frame
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            lines = [ln.rstrip("\n") for ln in f.readlines()]
        df = pd.DataFrame({"text": lines})

    return df


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols = []
    for c in df.columns:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        cols.append(c2)
    df = df.copy()
    df.columns = cols
    drop_cols = [c for c in df.columns if str(c).startswith("Unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _pick_target(df: pd.DataFrame, headers_hint=None) -> str:
    headers_hint = headers_hint or []
    # Prefer explicitly hinted target names if present
    for cand in headers_hint:
        if cand in df.columns:
            return cand

    # Common target names
    for cand in ["label", "target", "y", "class", "Category", "LABEL", "Label"]:
        if cand in df.columns:
            return cand

    # Prefer an object/categorical column with low cardinality (good for classification)
    obj_cols = [c for c in df.columns if df[c].dtype == "object"]
    best = None
    best_score = -1
    for c in obj_cols:
        s = df[c].astype(str)
        nun = s.nunique(dropna=True)
        if 2 <= nun <= max(2, min(50, int(0.2 * max(1, len(df))))):
            score = -nun  # fewer classes generally simpler
            if score > best_score:
                best_score = score
                best = c
    if best is not None:
        return best

    # Otherwise pick a numeric non-constant column
    for c in df.columns:
        x = pd.to_numeric(df[c], errors="coerce")
        if x.notna().sum() >= 2 and x.nunique(dropna=True) > 1:
            return c

    # If everything fails, pick last column
    return df.columns[-1]


def _pick_text_feature(df: pd.DataFrame, target_col: str, headers_hint=None) -> str:
    headers_hint = headers_hint or []
    for cand in headers_hint:
        if cand in df.columns and cand != target_col:
            return cand

    # Common text column names
    for cand in ["text", "sentence", "comment", "body", "review", "content", "Text"]:
        if cand in df.columns and cand != target_col:
            return cand

    # Otherwise choose the object column with highest average string length (likely text)
    obj_cols = [c for c in df.columns if df[c].dtype == "object" and c != target_col]
    if obj_cols:
        best = None
        best_len = -1.0
        for c in obj_cols:
            s = df[c].astype(str)
            avg_len = s.str.len().replace([np.inf, -np.inf], np.nan).dropna()
            if len(avg_len) == 0:
                continue
            v = float(avg_len.mean())
            if v > best_len:
                best_len = v
                best = c
        if best is not None:
            return best

    # If no object cols exist, just pick any non-target column
    for c in df.columns:
        if c != target_col:
            return c

    # Degenerate case: only target exists
    return target_col


def _build_pipeline(text_col: str, categorical_cols, numeric_cols):
    # Keep TF-IDF small for CPU efficiency; word unigrams + bigrams capture signal without heavy models
    text_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="constant", fill_value="")),
            ("tfidf", TfidfVectorizer(lowercase=True, ngram_range=(1, 2), max_features=20000, min_df=2)),
        ]
    )

    cat_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    num_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
        ]
    )

    transformers = []
    if text_col is not None:
        transformers.append(("text", text_pipe, [text_col]))
    if categorical_cols:
        transformers.append(("cat", cat_pipe, list(categorical_cols)))
    if numeric_cols:
        transformers.append(("num", num_pipe, list(numeric_cols)))

    pre = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    # LogisticRegression with saga supports sparse input efficiently
    clf = LogisticRegression(
        solver="saga",
        max_iter=300,
        C=1.0,
        n_jobs=1,
        random_state=RANDOM_STATE,
    )

    return Pipeline(steps=[("pre", pre), ("model", clf)])


def main():
    df = _read_csv_robust(DATASET_PATH)
    df = _normalize_columns(df)

    # Basic safety: drop fully empty rows
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.dropna(how="all")
    assert len(df) > 0, "Dataset is empty after initial cleanup."

    # Use provided headers hints if available
    headers_hint = ["text", "label"]
    target_col = _pick_target(df, headers_hint=["label"])
    text_col = _pick_text_feature(df, target_col, headers_hint=["text"])

    # Prepare y
    y_raw = df[target_col]
    if y_raw.dtype != "object":
        # Coerce numeric target safely if it isn't object
        y_num = pd.to_numeric(y_raw, errors="coerce")
        y = y_num
    else:
        y = y_raw.astype(str).replace({"nan": np.nan})

    # Features: all columns except target; keep original text column separate
    feature_cols = [c for c in df.columns if c != target_col]
    if len(feature_cols) == 0:
        # Create a trivial feature so pipeline can run
        df = df.copy()
        df["_constant_feature"] = 1
        feature_cols = ["_constant_feature"]
        if text_col == target_col:
            text_col = None

    X = df[feature_cols].copy()

    # Identify column types for preprocessing
    # Force numeric coercion only for genuinely non-text numeric-like columns
    # Keep text_col as text; other object cols treated as categoricals
    if text_col in X.columns:
        X[text_col] = X[text_col].astype(str).replace({"nan": ""})

    categorical_cols = []
    numeric_cols = []
    for c in X.columns:
        if c == text_col:
            continue
        if X[c].dtype == "object":
            categorical_cols.append(c)
        else:
            numeric_cols.append(c)

    # Coerce numeric columns safely
    for c in numeric_cols:
        X[c] = pd.to_numeric(X[c], errors="coerce")

    # If y is numeric but has many unique values, treat as regression-like via binning fallback later
    # For classification: require >=2 classes after dropping missing labels
    valid_mask = pd.Series(True, index=df.index)
    if isinstance(y, pd.Series):
        valid_mask &= y.notna()
    X = X.loc[valid_mask].copy()
    y = y.loc[valid_mask].copy() if isinstance(y, pd.Series) else y

    assert len(X) > 1, "Not enough samples after removing missing targets."

    # Determine if classification is viable
    is_classification = True
    y_for_split = y

    if not isinstance(y_for_split, pd.Series):
        is_classification = False
    else:
        if y_for_split.dtype != "object":
            # numeric target: if few unique values, allow classification; else treat as regression-like
            nun = pd.to_numeric(y_for_split, errors="coerce").nunique(dropna=True)
            is_classification = bool(2 <= nun <= 20)
            if is_classification:
                y_for_split = pd.to_numeric(y_for_split, errors="coerce").astype("Int64").astype(str)
        else:
            nun = y_for_split.astype(str).nunique(dropna=True)
            is_classification = nun >= 2

    if not is_classification:
        # Convert regression-like target into 5 quantile bins for a stable accuracy proxy
        y_num = pd.to_numeric(y, errors="coerce")
        valid = y_num.notna()
        X = X.loc[valid].copy()
        y_num = y_num.loc[valid].copy()
        assert len(X) > 1, "Not enough samples after numeric target coercion."
        # qcut can fail when many duplicates; use rank-based jitter-free approach
        try:
            y_binned = pd.qcut(y_num, q=min(5, max(2, int(y_num.nunique(dropna=True)))), duplicates="drop")
            y_for_split = y_binned.astype(str)
        except Exception:
            # Fallback to median split
            med = float(y_num.median())
            y_for_split = (y_num > med).astype(int).astype(str)

    # Train/test split with stratify when possible (saves variance, no extra compute)
    stratify = None
    try:
        if isinstance(y_for_split, pd.Series) and y_for_split.nunique(dropna=True) >= 2:
            stratify = y_for_split
    except Exception:
        stratify = None

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y_for_split,
        test_size=0.25,
        random_state=RANDOM_STATE,
        stratify=stratify,
    )

    assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

    # If only one class in train, use dummy baseline
    model = None
    try:
        if isinstance(pd.Series(y_train), pd.Series) and pd.Series(y_train).nunique(dropna=True) < 2:
            model = Pipeline(
                steps=[
                    ("pre", ColumnTransformer(transformers=[("passthrough", "passthrough", list(X_train.columns))])),
                    ("model", DummyClassifier(strategy="most_frequent", random_state=RANDOM_STATE)),
                ]
            )
        else:
            model = _build_pipeline(text_col if text_col in X_train.columns else None, categorical_cols, numeric_cols)
    except Exception:
        model = Pipeline(steps=[("model", DummyClassifier(strategy="most_frequent", random_state=RANDOM_STATE))])

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Accuracy is well-defined for classification; for regression-fallback we binned target above
    accuracy = float(accuracy_score(y_test, y_pred))
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used a lightweight, CPU-friendly baseline: TF-IDF + LogisticRegression(saga) on sparse features (efficient for text).
# - Capped TF-IDF feature space (max_features=20000, min_df=2) to reduce memory/CPU while keeping useful signal.
# - Implemented robust CSV parsing fallbacks (default then sep=';' & decimal=',') to avoid manual edits.
# - Normalized headers and dropped 'Unnamed:' columns to prevent schema noise from breaking the pipeline.
# - Defensive target/feature selection: derives target from available columns; prefers provided hints; avoids hard-fail.
# - Safe dtype handling: numeric coercion with errors='coerce', inf->NaN replacement, and imputers to keep pipeline stable.
# - When true classification is not viable (e.g., numeric continuous target), bins target into quantiles to produce an
#   accuracy proxy in [0,1] while still running end-to-end with the same lightweight classifier.
# - Avoided heavy models/ensembles/deep learning to minimize energy use and ensure fast CPU execution.