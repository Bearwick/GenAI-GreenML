# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
import sys

# Robust CSV parsing logic
data_path = 'Iris.csv'
try:
    # Try default comma-separated
    df = pd.read_csv(data_path)
    if len(df.columns) < 2:
        raise ValueError
except Exception:
    # Fallback for European style CSVs or semicolon-delimited files
    try:
        df = pd.read_csv(data_path, sep=';', decimal=',')
    except Exception:
        # If file is missing or unreadable, create a minimal dummy to allow script to exit gracefully
        df = pd.DataFrame()

# Cleanup column names: remove surrounding whitespace, collapse internal spaces, drop Unnamed
df.columns = [" ".join(str(c).strip().split()) for c in df.columns]
df = df.loc[:, ~df.columns.str.contains('^Unnamed', case=False)]

if df.empty:
    print(f"ACCURACY={0.000000:.6f}")
    sys.exit(0)

# Identify target column: Look for common names, fallback to the last column
target_candidates = ['species', 'target', 'class', 'variety', 'type', 'label']
target_col = None
for cand in target_candidates:
    for col in df.columns:
        if cand in col.lower():
            target_col = col
            break
    if target_col:
        break

if not target_col:
    target_col = df.columns[-1]

# Identify feature columns: Numeric columns, excluding target and ID-like strings
features = []
for col in df.columns:
    if col == target_col:
        continue
    # Heuristic to skip ID columns (numeric but unique per row or contains 'id')
    if 'id' in col.lower():
        continue
    
    # Coerce to numeric for modeling
    df[col] = pd.to_numeric(df[col], errors='coerce')
    if not df[col].isna().all():
        features.append(col)

# Ensure we have data left after basic cleaning
df = df.dropna(subset=[target_col])
if df.empty or len(features) == 0:
    print(f"ACCURACY={0.000000:.6f}")
    sys.exit(0)

# Prepare X and y
X = df[list(features)]
y = df[target_col]

# Encode target labels (works for both numeric and string targets)
le = LabelEncoder()
y_encoded = le.fit_transform(y.astype(str))

# Train-Test Split (Reproducible)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42
)

# Robust Preprocessing Pipeline
# SimpleImputer is energy efficient compared to iterative/KNN imputation
imputer = SimpleImputer(strategy='mean')
scaler = StandardScaler()

try:
    # Process Training Data
    X_train_imputed = imputer.fit_transform(X_train)
    X_train_scaled = scaler.fit_transform(X_train_imputed)

    # Process Test Data
    X_test_imputed = imputer.transform(X_test)
    X_test_scaled = scaler.transform(X_test_imputed)

    # Model Selection: Logistic Regression
    # Extremely low energy footprint, small memory usage, fast convergence on small datasets.
    unique_classes = np.unique(y_train)
    if len(unique_classes) > 1:
        model = LogisticRegression(
            max_iter=1000, 
            random_state=42, 
            solver='lbfgs', 
            multi_class='auto'
        )
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        # Trivial baseline if only one class is present in training subset
        accuracy = accuracy_score(y_test, np.full(y_test.shape, unique_classes[0]))

except Exception:
    # General fallback to avoid hard failure
    accuracy = 0.000000

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Model Choice: Logistic Regression was selected as a green, CPU-efficient alternative to deep learning or large ensembles.
# 2. Memory Management: Used standard scikit-learn transformers that operate in-place or via efficient NumPy arrays.
# 3. Robustness: Implemented multi-stage CSV parsing and dynamic column normalization to handle schema variations without manual intervention.
# 4. Feature Engineering: Minimalist approach using StandardScaler and Mean Imputation to reduce computational overhead per prediction.
# 5. Energy Efficiency: Focused on O(n) complexity operations, avoiding iterative or distance-based algorithms like KNN/SVM which scale poorly with data.
# 6. Target Reliability: Included LabelEncoding for species names to handle categorical targets efficiently.
# 7. Error Handling: Included defensive checks for empty dataframes or single-class targets to ensure the pipeline always outputs a valid score.
# 8. Regression Fallback: Accuracy is used as a standard classification metric; for data with single classes or failures, a safe 0.0-1.0 proxy is maintained.
[Instruction]
The task is to classify species of the Iris flower. The solution uses Logistic Regression, which is highly efficient for small, low-dimensional datasets like Iris. It ensures end-to-end execution with robust column detection.
[/Instruction]