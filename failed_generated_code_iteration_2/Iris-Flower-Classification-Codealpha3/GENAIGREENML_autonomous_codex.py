# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")

data_path = "Iris.csv"

def read_csv_with_fallback(path):
    try:
        df_local = pd.read_csv(path)
    except Exception:
        df_local = pd.read_csv(path, sep=';', decimal=',')
        return df_local
    if df_local.shape[1] == 1 or any(';' in str(c) for c in df_local.columns):
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > df_local.shape[1]:
                df_local = df_alt
        except Exception:
            pass
    return df_local

df = read_csv_with_fallback(data_path)
df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
df = df.loc[:, ~df.columns.str.match(r'^Unnamed')]
df = df.dropna(axis=1, how='all')
df.replace([np.inf, -np.inf], np.nan, inplace=True)

cols = list(df.columns)
target_col = None
lower_map = {c.lower(): c for c in cols}
for name in ['species', 'target', 'label', 'class']:
    if name in lower_map:
        target_col = lower_map[name]
        break
if target_col is None:
    obj_cols = [c for c in cols if df[c].dtype == object or str(df[c].dtype).startswith('category')]
    for c in obj_cols[::-1]:
        if df[c].nunique(dropna=True) > 1:
            target_col = c
            break
if target_col is None:
    for c in cols[::-1]:
        ser = pd.to_numeric(df[c], errors='coerce')
        if ser.notna().sum() > 0 and ser.nunique(dropna=True) > 1:
            target_col = c
            break
if target_col is None and cols:
    target_col = cols[-1]
if target_col is None:
    df['_target'] = 0
    target_col = '_target'

y_raw = df[target_col]
valid_mask = ~y_raw.isna()
df = df.loc[valid_mask].copy()
y_raw = y_raw.loc[valid_mask]

assert len(df) > 0

features = [c for c in df.columns if c != target_col]
features_cleaned = []
for c in features:
    cname = c.lower()
    if cname in ('id', 'index'):
        continue
    if cname.endswith('id') and df[c].nunique(dropna=True) == len(df):
        continue
    features_cleaned.append(c)
features = features_cleaned
if len(features) == 0:
    df['_dummy_feature'] = 0
    features = ['_dummy_feature']

n_rows = len(df)
for c in features:
    ser = pd.to_numeric(df[c], errors='coerce')
    non_na = ser.notna().sum()
    if non_na > 0 and non_na >= max(1, int(0.5 * n_rows)):
        df[c] = ser

y_numeric = pd.to_numeric(y_raw, errors='coerce')
numeric_ratio = y_numeric.notna().mean() if len(y_numeric) > 0 else 0.0
unique_y = y_raw.nunique(dropna=True)

if y_raw.dtype == object or str(y_raw.dtype).startswith('category'):
    if numeric_ratio >= 0.9 and unique_y > max(20, 0.1 * n_rows):
        is_classification = False
        y = y_numeric
    else:
        is_classification = True
        y = y_raw
else:
    if unique_y <= max(20, 0.1 * n_rows):
        is_classification = True
        y = y_raw
    else:
        is_classification = False
        y = y_numeric

if not is_classification:
    valid_mask = ~pd.isna(y)
    df = df.loc[valid_mask].copy()
    y = y.loc[valid_mask]

assert len(df) > 0

X = df[features]

numeric_features = [c for c in features if pd.api.types.is_numeric_dtype(df[c])]
categorical_features = [c for c in features if c not in numeric_features]

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])
transformers = []
if numeric_features:
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    transformers.append(('cat', categorical_transformer, categorical_features))
if not transformers:
    transformers.append(('num', numeric_transformer, numeric_features))
preprocess = ColumnTransformer(transformers=transformers, remainder='drop')

n_samples = len(df)
if n_samples >= 2:
    test_size = 0.2 if n_samples > 4 else 0.5
    stratify = None
    if is_classification and y.nunique(dropna=True) > 1:
        counts = y.value_counts()
        if counts.min() >= 2 and n_samples >= counts.size * 2:
            stratify = y
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=stratify)
else:
    X_train = X.copy()
    X_test = X.copy()
    y_train = y.copy()
    y_test = y.copy()

assert len(X_train) > 0 and len(X_test) > 0

if is_classification:
    if y.nunique(dropna=True) < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    if y.nunique(dropna=True) < 2:
        model = DummyRegressor(strategy='mean')
    else:
        model = LinearRegression()

pipeline = Pipeline(steps=[('preprocess', preprocess), ('model', model)])
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if is_classification:
    try:
        accuracy = accuracy_score(y_test, y_pred)
    except Exception:
        accuracy = 0.0
else:
    if len(y_test) < 2:
        accuracy = 1.0 if np.allclose(np.array(y_test), np.array(y_pred)) else 0.0
    else:
        r2 = r2_score(y_test, y_pred)
        if np.isnan(r2):
            r2 = 0.0
        accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# - Lightweight linear/logistic or dummy models chosen to minimize CPU and energy use while providing a solid baseline.
# - Reproducible preprocessing via ColumnTransformer with simple imputation and one-hot encoding for categorical data.
# - Numeric coercion is applied defensively; regression accuracy is a bounded (R2+1)/2 proxy for stability.