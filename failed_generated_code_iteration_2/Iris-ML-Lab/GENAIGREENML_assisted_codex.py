# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd

DATASET_PATH = "iris.csv"
DATASET_HEADERS = "sepal_length,sepal_width,petal_length,petal_width,species"

np.random.seed(0)

def _parsing_wrong(df, expected_len):
    return df.shape[1] != expected_len or df.shape[1] == 1

def _drop_index_col(df, expected_len):
    if df.shape[1] == expected_len + 1:
        first_col = df.columns[0]
        if isinstance(first_col, str) and first_col.lower().startswith("unnamed"):
            df = df.iloc[:, 1:]
    return df

def read_csv_robust(path, expected_headers):
    expected_len = len(expected_headers)
    df = pd.read_csv(path)
    df = _drop_index_col(df, expected_len)
    if _parsing_wrong(df, expected_len):
        df = pd.read_csv(path, sep=";", decimal=",")
        df = _drop_index_col(df, expected_len)
    if _parsing_wrong(df, expected_len):
        df = pd.read_csv(path, header=None, sep=";", decimal=",")
        df = _drop_index_col(df, expected_len)
    if _parsing_wrong(df, expected_len):
        df = pd.read_csv(path, header=None)
        df = _drop_index_col(df, expected_len)
    return df

def compute_centroids(features, labels, n_classes):
    sums = np.zeros((n_classes, features.shape[1]), dtype=np.float64)
    np.add.at(sums, labels, features)
    counts = np.bincount(labels, minlength=n_classes).astype(np.float64)
    return sums / counts[:, None]

def predict_nearest_centroid(features, centroids):
    x2 = np.einsum("ij,ij->i", features, features)
    c2 = np.einsum("ij,ij->i", centroids, centroids)
    distances_sq = x2[:, None] + c2[None, :] - 2.0 * features.dot(centroids.T)
    return np.argmin(distances_sq, axis=1)

def main():
    expected_headers = [h.strip() for h in DATASET_HEADERS.split(",")]
    df = read_csv_robust(DATASET_PATH, expected_headers)
    df.columns = [str(c).strip() for c in df.columns]
    if all(h in df.columns for h in expected_headers):
        df = df[expected_headers]
    elif df.shape[1] == len(expected_headers):
        df.columns = expected_headers
    else:
        raise ValueError("Unexpected dataset schema")
    petal_cols = expected_headers[2:4]
    label_col = expected_headers[-1]
    petal_features = df[petal_cols].to_numpy(dtype=np.float64)
    labels_raw = df[label_col].astype(str).str.strip().to_numpy()
    labels = pd.Categorical(labels_raw, categories=["setosa", "versicolor", "virginica"], ordered=True).codes
    if (labels < 0).any():
        raise ValueError("Unknown label found")
    labels = labels.astype(int)
    n_classes = labels.max() + 1
    centroids = compute_centroids(petal_features, labels, n_classes)
    y_pred = predict_nearest_centroid(petal_features, centroids)
    accuracy = np.mean(y_pred == labels)
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# - Removed plotting and file outputs to avoid unnecessary computation and disk I/O.
# - Replaced SciPy distance calculations with vectorized NumPy math to reduce dependencies.
# - Computed centroids using np.add.at and vectorized distance formula to minimize loops and memory.
# - Selected only required feature columns to reduce data movement and memory footprint.
# - Implemented robust CSV loading with minimal retries and consistent column alignment for reliability.