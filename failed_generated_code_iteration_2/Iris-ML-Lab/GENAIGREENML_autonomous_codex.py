# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, r2_score

dataset_path = "iris.csv"

def read_csv_robust(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=';', decimal=',')
    if df.shape[1] == 1:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > 1:
                df = df_alt
        except Exception:
            pass
    return df

def normalize_columns(df):
    df = df.copy()
    df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
    df = df.loc[:, ~df.columns.str.match(r'^Unnamed')]
    df = df.dropna(axis=1, how='all')
    return df

def choose_target(df):
    cols = list(df.columns)
    if not cols:
        return None
    lower_map = {c.lower(): c for c in cols}
    for key in ['target', 'label', 'class', 'species', 'y']:
        if key in lower_map:
            return lower_map[key]
    n_rows = len(df)
    candidate = None
    threshold = max(2, min(20, int(n_rows * 0.2) if n_rows > 0 else 2))
    for c in cols:
        series = df[c]
        n_unique = series.dropna().nunique()
        if n_unique <= 1:
            continue
        if series.dtype == 'object':
            return c
        if n_unique <= threshold:
            candidate = c
    if candidate is not None:
        return candidate
    best_col = None
    best_var = -np.inf
    for c in cols:
        ser = pd.to_numeric(df[c], errors='coerce')
        if ser.notna().sum() == 0:
            continue
        var = ser.var()
        if pd.notna(var) and var > best_var:
            best_var = var
            best_col = c
    if best_col is not None:
        return best_col
    return cols[-1]

df = read_csv_robust(dataset_path)
df = normalize_columns(df)
assert df.shape[0] > 0 and df.shape[1] > 0

target_col = choose_target(df)
if target_col is None:
    raise ValueError("No target column found")

df.replace([np.inf, -np.inf], np.nan, inplace=True)

features = [c for c in df.columns if c != target_col]
if len(features) == 0:
    df['__constant__'] = 1
    features = ['__constant__']

y_raw = df[target_col]
y_non_na = y_raw.dropna()
if y_non_na.empty:
    task = 'regression'
else:
    n_unique = y_non_na.nunique()
    threshold = max(2, min(20, int(len(df) * 0.2) if len(df) > 0 else 2))
    if y_raw.dtype == 'object' or n_unique <= threshold:
        task = 'classification'
    else:
        task = 'regression'

if task == 'classification':
    df = df.loc[~df[target_col].isna()].copy()
    y = df[target_col].astype(str)
    le = LabelEncoder()
    y = le.fit_transform(y)
else:
    y_numeric = pd.to_numeric(df[target_col], errors='coerce')
    mask = y_numeric.notna()
    df = df.loc[mask].copy()
    y = y_numeric.loc[mask]

assert df.shape[0] > 0

numeric_cols = []
categorical_cols = []
for c in features:
    if c not in df.columns:
        continue
    series = df[c]
    if series.dtype == 'object':
        converted = pd.to_numeric(series, errors='coerce')
        if converted.notna().sum() >= max(1, int(0.8 * len(series))):
            df[c] = converted
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)
    else:
        df[c] = pd.to_numeric(series, errors='coerce')
        numeric_cols.append(c)

numeric_cols = [c for c in numeric_cols if c in df.columns and df[c].notna().any()]
categorical_cols = [c for c in categorical_cols if c in df.columns and df[c].notna().any()]

if len(numeric_cols) + len(categorical_cols) == 0:
    df['__constant__'] = 1
    numeric_cols = ['__constant__']
    categorical_cols = []

features = numeric_cols + categorical_cols

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))
])

transformers = []
if numeric_cols:
    transformers.append(('num', numeric_transformer, numeric_cols))
if categorical_cols:
    transformers.append(('cat', categorical_transformer, categorical_cols))

preprocessor = ColumnTransformer(transformers=transformers, remainder='drop', sparse_threshold=0.0)

if task == 'classification':
    n_classes = len(np.unique(y))
    if n_classes < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='auto')
else:
    if len(y) < 2:
        model = DummyRegressor(strategy='mean')
    else:
        model = LinearRegression()

clf = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])

X = df[features]
n_samples = len(df)

if n_samples < 2:
    X_train, X_test, y_train, y_test = X, X, y, y
else:
    test_size = 0.2 if n_samples >= 5 else 0.5
    stratify = None
    if task == 'classification':
        unique, counts = np.unique(y, return_counts=True)
        if len(unique) > 1 and np.all(counts >= 2) and n_samples >= 5:
            stratify = y
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=stratify
    )

assert len(X_train) > 0 and len(X_test) > 0

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if task == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
else:
    y_test_arr = np.asarray(y_test)
    y_pred_arr = np.asarray(y_pred)
    if y_test_arr.size < 2:
        accuracy = 1.0
    elif np.nanvar(y_test_arr) == 0:
        accuracy = 1.0 if np.allclose(y_pred_arr, y_test_arr[0]) else 0.0
    else:
        r2 = r2_score(y_test_arr, y_pred_arr)
        accuracy = max(0.0, min(1.0, (r2 + 1.0) / 2.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Robust CSV loading with delimiter fallback and column normalization improves reproducibility without heavy processing.
# - Lightweight preprocessing (imputation, scaling, one-hot) is consolidated in a ColumnTransformer to avoid redundant passes.
# - CPU-friendly models (logistic/linear regression or dummy baselines) are used for energy efficiency.
# - Regression accuracy proxy maps R2 to [0,1] to maintain a stable metric when classification is not applicable.