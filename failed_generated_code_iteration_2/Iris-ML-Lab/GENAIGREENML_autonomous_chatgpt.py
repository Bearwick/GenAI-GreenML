# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "iris.csv"


def _normalize_columns(cols):
    norm = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        norm.append(c2)
    return norm


def _drop_unnamed_columns(df):
    to_drop = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if to_drop:
        df = df.drop(columns=to_drop, errors="ignore")
    return df


def _try_read_csv(path):
    # Robust CSV parsing fallback: default first, then retry with European settings
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d):
        if d is None or d.empty:
            return True
        # If everything collapsed into one column, delimiter likely wrong
        if d.shape[1] == 1:
            return True
        return False

    if _looks_wrong(df):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df2 is not None and not df2.empty and df2.shape[1] > 1:
                df = df2
        except Exception:
            pass

    if df is None:
        raise RuntimeError("Failed to read dataset.")
    return df


def _choose_target(df, dataset_headers=None):
    cols = list(df.columns)

    # Prefer a known target from provided headers if present
    preferred_candidates = []
    if dataset_headers:
        preferred_candidates = [h.strip() for h in str(dataset_headers).split(",") if h.strip()]
        # Common target-like names last in header lists; try those first for robustness
        preferred_candidates = preferred_candidates[::-1]

    # Otherwise guess common names
    preferred_candidates += ["target", "label", "class", "species", "y"]

    for cand in preferred_candidates:
        if cand in cols:
            return cand

    # If no known target, pick a non-constant column that looks categorical/object first
    nunique = df.nunique(dropna=True)
    obj_cols = [c for c in cols if df[c].dtype == "object"]
    obj_cols = [c for c in obj_cols if nunique.get(c, 0) >= 2]
    if obj_cols:
        return obj_cols[0]

    # Else choose a numeric non-constant column
    numeric_cols = []
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() > 0:
            numeric_cols.append(c)
    numeric_cols = [c for c in numeric_cols if nunique.get(c, 0) >= 2]
    if numeric_cols:
        return numeric_cols[0]

    # Fallback: last column
    return cols[-1] if cols else None


def _split_features(df, target_col):
    feature_cols = [c for c in df.columns if c != target_col]
    if not feature_cols:
        # Create a dummy feature to allow pipeline to run end-to-end
        df = df.copy()
        df["_dummy_feature"] = 0.0
        feature_cols = ["_dummy_feature"]
    return df, feature_cols


def _build_preprocessor(X, numeric_features, categorical_features):
    numeric_transformer = Pipeline(
        steps=[
            ("to_numeric", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )
    return preprocessor


def _safe_bounded_regression_score(y_true, y_pred):
    # Convert regression error into a stable [0,1] "accuracy proxy":
    # score = 1 / (1 + normalized_mae), normalized by (std + eps) to be scale-robust.
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = np.mean(np.abs(y_true - y_pred))
    scale = np.std(y_true)
    denom = float(scale) + 1e-12
    nmae = float(mae) / denom
    score = 1.0 / (1.0 + nmae)
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    df = _try_read_csv(DATASET_PATH)

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed_columns(df)

    # Normalize any provided dataset headers similarly (if used)
    dataset_headers = "sepal_length,sepal_width,petal_length,petal_width,species"
    dataset_headers = ",".join(_normalize_columns(dataset_headers.split(",")))

    # Basic sanity
    assert df is not None and df.shape[0] > 0 and df.shape[1] > 0

    target_col = _choose_target(df, dataset_headers=dataset_headers)
    if target_col is None or target_col not in df.columns:
        # Fallback: choose last available column
        target_col = df.columns[-1]

    # Coerce numeric columns defensively before splitting features
    # (does not touch the target yet; handled separately)
    df2 = df.copy()
    for c in df2.columns:
        if c == target_col:
            continue
        # Try numeric conversion; keep original if mostly non-numeric
        s_num = pd.to_numeric(df2[c], errors="coerce")
        if s_num.notna().sum() >= max(1, int(0.5 * len(df2))):
            df2[c] = s_num

    df2, feature_cols = _split_features(df2, target_col)

    X = df2[feature_cols].copy()
    y_raw = df2[target_col].copy()

    # Determine task type (classification preferred if target is non-numeric or few unique values)
    y_num = pd.to_numeric(y_raw, errors="coerce")
    y_is_numeric = y_num.notna().sum() >= max(1, int(0.9 * len(y_raw)))

    if y_is_numeric:
        y_candidate = y_num
        nunique_y = y_candidate.nunique(dropna=True)
        # If few unique values, treat as classification; else regression
        treat_as_classification = (nunique_y >= 2) and (nunique_y <= 20)
    else:
        treat_as_classification = True

    # Prepare features lists
    numeric_features = []
    categorical_features = []
    for c in feature_cols:
        if pd.api.types.is_numeric_dtype(X[c]):
            numeric_features.append(c)
        else:
            categorical_features.append(c)

    # Train/test split
    test_size = 0.2 if len(df2) >= 10 else 0.5
    random_state = 42

    if treat_as_classification:
        # Encode labels
        if y_is_numeric:
            y = y_num
            # If numeric but treated as class, convert to string labels to avoid float class issues
            y = y.astype("Int64").astype(str)
        else:
            y = y_raw.astype(str)

        # Drop rows with missing target
        mask = y.notna()
        Xc = X.loc[mask].copy()
        y = y.loc[mask].copy()

        assert len(Xc) > 1

        # If only one class remains, fallback to regression proxy
        if y.nunique(dropna=True) < 2:
            treat_as_classification = False
        else:
            # Stratify for stability when possible
            stratify = y if y.nunique() >= 2 and len(y) >= 10 else None
            X_train, X_test, y_train, y_test = train_test_split(
                Xc, y, test_size=test_size, random_state=random_state, stratify=stratify
            )
            assert len(X_train) > 0 and len(X_test) > 0

            preprocessor = _build_preprocessor(X_train, numeric_features, categorical_features)

            # Lightweight, CPU-friendly baseline
            clf = LogisticRegression(
                solver="lbfgs",
                max_iter=200,
                n_jobs=1,
                multi_class="auto",
            )

            model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = float(accuracy_score(y_test, y_pred))

            print(f"ACCURACY={accuracy:.6f}")
            return

    # Regression path (either by choice or classification fallback)
    y = pd.to_numeric(y_raw, errors="coerce")
    mask = y.notna()
    Xr = X.loc[mask].copy()
    y = y.loc[mask].copy()

    if len(Xr) < 2:
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    X_train, X_test, y_train, y_test = train_test_split(
        Xr, y, test_size=test_size, random_state=random_state
    )
    assert len(X_train) > 0 and len(X_test) > 0

    preprocessor = _build_preprocessor(X_train, numeric_features, categorical_features)

    reg = Ridge(alpha=1.0, random_state=random_state)

    model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = _safe_bounded_regression_score(y_test, y_pred)
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Used lightweight, CPU-friendly baselines: LogisticRegression for classification; Ridge for regression fallback.
# - Kept preprocessing minimal and reproducible via sklearn Pipeline/ColumnTransformer to avoid repeated work.
# - Robust CSV loading with delimiter/decimal fallback to prevent costly manual debugging and reruns.
# - Defensive schema handling: normalized column names, dropped "Unnamed" columns, inferred target safely.
# - Coerced numeric features with errors='coerce' and used SimpleImputer+StandardScaler (cheap and stable).
# - OneHotEncoder(handle_unknown='ignore') for categoricals to prevent failures on unseen categories.
# - Fixed random_state for reproducibility; single-threaded training (n_jobs=1) to reduce CPU spikes/energy use.
# - Regression "accuracy" proxy is bounded in [0,1] using 1/(1+normalized MAE) for stable reporting when classification isn't viable.