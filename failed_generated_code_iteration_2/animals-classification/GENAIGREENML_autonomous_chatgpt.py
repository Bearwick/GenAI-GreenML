# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "zoo.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = "" if c is None else str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _read_csv_robust(path):
    df = None
    # Try default parsing first
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def _looks_wrong(d):
        if d is None or d.empty:
            return True
        # Heuristic: if only one column and it contains separator chars, likely wrong sep
        if d.shape[1] == 1:
            s = d.iloc[:, 0].astype(str).head(10).tolist()
            joined = " ".join(s)
            if ";" in joined or "," in joined:
                return True
        return False

    if _looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            pass

    if df is None:
        raise FileNotFoundError(f"Could not read dataset at path: {path}")
    return df


def _drop_unnamed(df):
    cols = []
    for c in df.columns:
        if isinstance(c, str) and c.strip().startswith("Unnamed:"):
            continue
        cols.append(c)
    return df[cols]


def _select_target_and_features(df, dataset_headers_hint=None):
    cols = list(df.columns)

    # Prefer known target if present
    preferred_targets = []
    if dataset_headers_hint:
        preferred_targets = [h.strip() for h in dataset_headers_hint if isinstance(h, str)]
    for t in ["class_type", "target", "label", "y", "Class", "class"]:
        if t not in preferred_targets:
            preferred_targets.append(t)

    target = None
    for t in preferred_targets:
        if t in cols:
            target = t
            break

    # If not found, choose a non-constant numeric column as target
    if target is None:
        numeric_candidates = []
        for c in cols:
            s = pd.to_numeric(df[c], errors="coerce")
            nun = s.nunique(dropna=True)
            if nun >= 2:
                numeric_candidates.append((c, nun))
        numeric_candidates.sort(key=lambda x: x[1], reverse=True)
        if numeric_candidates:
            target = numeric_candidates[0][0]

    # If still none, choose any non-constant column
    if target is None:
        nonconst = []
        for c in cols:
            nun = df[c].nunique(dropna=True)
            if nun >= 2:
                nonconst.append((c, nun))
        nonconst.sort(key=lambda x: x[1], reverse=True)
        if nonconst:
            target = nonconst[0][0]

    if target is None:
        # Degenerate: no usable target
        target = cols[-1] if cols else None

    features = [c for c in cols if c != target]
    return target, features


def _prepare_xy(df, target, features):
    X = df[features].copy()
    y = df[target].copy()

    # Coerce numeric-like columns safely; do not force object columns into numeric if clearly non-numeric
    for c in X.columns:
        if X[c].dtype == "object":
            # Try numeric coercion; keep original if mostly non-numeric
            coerced = pd.to_numeric(X[c], errors="coerce")
            non_nan_ratio = float(coerced.notna().mean()) if len(coerced) else 0.0
            if non_nan_ratio >= 0.85:
                X[c] = coerced
        else:
            X[c] = pd.to_numeric(X[c], errors="coerce")

    # Clean inf
    X = X.replace([np.inf, -np.inf], np.nan)

    return X, y


def _is_classification_target(y):
    # If y is numeric with small number of unique values -> classification
    y_non_na = y.dropna()
    if y_non_na.empty:
        return False
    if y_non_na.dtype == "object":
        return True
    y_num = pd.to_numeric(y_non_na, errors="coerce")
    if y_num.notna().mean() >= 0.95:
        nun = y_num.nunique(dropna=True)
        if 2 <= nun <= 50:
            return True
    # Also treat integer-like with few unique values as classification
    if pd.api.types.is_integer_dtype(y_non_na) or pd.api.types.is_bool_dtype(y_non_na):
        nun = y_non_na.nunique(dropna=True)
        if 2 <= nun <= 50:
            return True
    return False


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    y_mean = float(np.mean(y_true)) if y_true.size else 0.0
    ss_tot = float(np.sum((y_true - y_mean) ** 2))
    if ss_tot <= 1e-12:
        r2 = 0.0
    else:
        r2 = 1.0 - (ss_res / ss_tot)
    # Bound to [0,1] for stable "accuracy" proxy
    acc = (r2 + 1.0) / 2.0
    if not np.isfinite(acc):
        acc = 0.0
    return float(np.clip(acc, 0.0, 1.0))


def main():
    if not os.path.exists(DATASET_PATH):
        raise FileNotFoundError(f"Dataset not found: {DATASET_PATH}")

    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # If file accidentally loaded with a single column containing comma-separated values, split it
    if df.shape[1] == 1:
        col0 = df.columns[0]
        sample = df[col0].astype(str).head(5).tolist()
        if any("," in s for s in sample):
            split_df = df[col0].astype(str).str.split(",", expand=True)
            split_df.columns = [f"col_{i}" for i in range(split_df.shape[1])]
            df = split_df

    assert df is not None and not df.empty, "Empty dataset after loading."

    dataset_headers = [h.strip() for h in "animal_name,hair,feathers,eggs,milk,airborne,aquatic,predator,toothed,backbone,breathes,venomous,fins,legs,tail,domestic,catsize,class_type".split(",")]
    target, features = _select_target_and_features(df, dataset_headers_hint=dataset_headers)

    # If target column missing due to parsing oddities, pick last column
    if target not in df.columns:
        target = df.columns[-1]
        features = [c for c in df.columns if c != target]

    X, y = _prepare_xy(df, target, features)

    # Drop rows with missing target
    y_is_obj = (y.dtype == "object")
    if not y_is_obj:
        y_num = pd.to_numeric(y, errors="coerce")
        keep = y_num.notna()
        y = y_num[keep]
        X = X.loc[keep]
    else:
        keep = y.notna()
        y = y[keep]
        X = X.loc[keep]

    assert not X.empty and len(y) > 0, "No samples after dropping missing target."

    is_clf = _is_classification_target(y)

    # If classification but only one class, fallback to regression proxy or trivial accuracy
    if is_clf:
        y_clf = y.astype(str) if y.dtype == "object" else y
        nun = pd.Series(y_clf).nunique(dropna=True)
        if nun < 2:
            is_clf = False

    # Identify column types for preprocessing
    num_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    cat_features = [c for c in X.columns if c not in num_features]

    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False)),
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, num_features),
            ("cat", categorical_transformer, cat_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    if is_clf:
        # Logistic regression is a lightweight, strong baseline for tabular data
        model = LogisticRegression(max_iter=300, solver="lbfgs", n_jobs=1)
        clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y if pd.Series(y).nunique() > 1 else None
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Regression fallback: Ridge is CPU-friendly and stable with one-hot features
        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        reg = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

        y_reg = pd.to_numeric(y, errors="coerce")
        keep2 = y_reg.notna()
        X2 = X.loc[keep2]
        y2 = y_reg[keep2]
        if len(y2) < 3:
            accuracy = 0.0
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                X2, y2, test_size=0.2, random_state=RANDOM_STATE
            )
            assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."
            reg.fit(X_train, y_train)
            y_pred = reg.predict(X_test)
            accuracy = _bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression / Ridge) to stay CPU-friendly and energy-efficient versus heavy ensembles/deep nets.
# - ColumnTransformer+Pipeline ensures single-pass, reproducible preprocessing and avoids duplicated transformations.
# - Robust CSV loading with delimiter fallback reduces manual retries and wasted compute.
# - Sparse one-hot encoding keeps memory/compute low for categorical features; StandardScaler(with_mean=False) supports sparse matrices efficiently.
# - Defensive schema handling selects a valid target/features even under header mismatches, preventing costly failures and reruns.
# - Regression fallback uses a bounded (R2+1)/2 score in [0,1] as an "accuracy" proxy when classification is not feasible.