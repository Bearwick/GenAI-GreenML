# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


RANDOM_STATE = 42
DATASET_PATH = "diabetes.csv"


def _normalize_columns(cols):
    out = []
    for c in cols:
        c = "" if c is None else str(c)
        c = c.strip()
        c = re.sub(r"\s+", " ", c)
        out.append(c)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if isinstance(c, str) and c.strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    return df


def _read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    def looks_wrong(d):
        if d is None or d.empty:
            return True
        if d.shape[1] <= 1:
            return True
        # Heuristic: if only one column and it contains commas/semicolons, parsing likely failed
        if d.shape[1] == 1:
            col0 = d.columns[0]
            sample = d[col0].astype(str).head(10).tolist()
            joined = " ".join(sample)
            if ("," in joined) or (";" in joined):
                return True
        return False

    if looks_wrong(df):
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
        except Exception:
            # Last resort: try python engine with separator inference
            df = pd.read_csv(path, sep=None, engine="python")
    return df


def _choose_target(df, expected_headers=None):
    cols_lower = {str(c).strip().lower(): c for c in df.columns}

    # Prefer expected target names if present
    for cand in ["outcome", "target", "y", "label", "class"]:
        if cand in cols_lower:
            return cols_lower[cand]

    # If dataset headers provided, try last header as target when present
    if expected_headers:
        exp = [h.strip() for h in expected_headers if h and str(h).strip()]
        if exp:
            last = exp[-1].strip().lower()
            if last in cols_lower:
                return cols_lower[last]

    # Prefer a non-constant numeric-ish column
    best = None
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        nun = int(s.nunique(dropna=True))
        if nun >= 2:
            # Prefer binary-like targets if any
            vals = sorted([v for v in s.dropna().unique().tolist() if np.isfinite(v)])
            if len(vals) <= 10:
                best = c
                if nun == 2:
                    return c
    # Fallback: last column
    return best if best is not None else df.columns[-1]


def _classification_or_regression_target(y_raw):
    # Return (task, y_processed)
    y_num = pd.to_numeric(y_raw, errors="coerce")

    # If many NaNs, try categorical mapping
    nan_rate = float(y_num.isna().mean())
    if nan_rate > 0.5:
        y_cat = y_raw.astype(str).fillna("nan")
        nun = int(y_cat.nunique(dropna=True))
        if nun >= 2:
            return "classification", y_cat
        return "regression", y_num

    # If numeric and has 2-20 unique values, treat as classification; especially binary {0,1}
    y_clean = y_num.replace([np.inf, -np.inf], np.nan).dropna()
    nun = int(y_clean.nunique(dropna=True))
    if nun >= 2 and nun <= 20:
        # If values are nearly integers, classification is plausible
        vals = y_clean.unique()
        if np.all(np.isfinite(vals)):
            close_to_int = np.mean(np.isclose(vals, np.round(vals))) if len(vals) else 0.0
            if close_to_int >= 0.8:
                return "classification", y_num.round().astype("Int64")
        return "classification", y_num
    if nun < 2:
        return "regression", y_num
    return "regression", y_num


def _bounded_regression_score(y_true, y_pred):
    # Stable "accuracy" proxy in [0,1]: 1 / (1 + normalized MAE)
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    mae = float(np.mean(np.abs(y_true - y_pred)))
    scale = float(np.std(y_true)) + 1e-9
    nmae = mae / scale
    score = 1.0 / (1.0 + nmae)
    if not np.isfinite(score):
        return 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    df = _read_csv_robust(DATASET_PATH)

    # Normalize and clean columns
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)
    df = df.copy()

    # Defensive: ensure not empty
    assert df is not None and not df.empty and df.shape[1] >= 2

    expected_headers = ["Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin",
                        "BMI", "DiabetesPedigreeFunction", "Age", "Outcome"]

    target_col = _choose_target(df, expected_headers=expected_headers)

    # Features = all except target; if that yields none, take all and set target to last
    feature_cols = [c for c in df.columns if c != target_col]
    if len(feature_cols) == 0:
        target_col = df.columns[-1]
        feature_cols = [c for c in df.columns if c != target_col]

    X = df[feature_cols].copy()
    y_raw = df[target_col].copy()

    task, y_processed = _classification_or_regression_target(y_raw)

    # Identify column types
    # Coerce numerics defensively for type detection
    numeric_cols = []
    categorical_cols = []
    for c in X.columns:
        s_num = pd.to_numeric(X[c], errors="coerce")
        # Consider numeric if it has a reasonable fraction of non-NaN after coercion
        non_nan_rate = float(s_num.notna().mean())
        if non_nan_rate >= 0.7:
            numeric_cols.append(c)
            X[c] = s_num
        else:
            categorical_cols.append(c)
            X[c] = X[c].astype(str)

    # Build preprocessors
    num_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])

    cat_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    pre = ColumnTransformer(
        transformers=[
            ("num", num_pipe, numeric_cols),
            ("cat", cat_pipe, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Prepare y and drop rows with missing target for training/eval stability
    if task == "classification":
        y = y_processed
        if pd.api.types.is_numeric_dtype(y):
            y = pd.to_numeric(y, errors="coerce")
        # Drop rows where y is missing
        mask = pd.Series(y).notna()
        X = X.loc[mask].copy()
        y = pd.Series(y).loc[mask].copy()

        # If still too few samples, fallback to trivial baseline
        assert len(X) > 1

        # If class count <2, fallback to regression-like proxy with constant prediction
        classes = pd.Series(y).unique()
        classes = [c for c in classes if pd.notna(c)]
        if len(classes) < 2:
            # Trivial constant prediction accuracy = 1.0
            accuracy = 1.0
            print(f"ACCURACY={accuracy:.6f}")
            return

        # Stratify only if safe
        stratify = y if pd.Series(y).nunique(dropna=True) >= 2 else None

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify
        )
        assert len(X_train) > 0 and len(X_test) > 0

        # Lightweight linear classifier
        clf = LogisticRegression(
            solver="lbfgs",
            max_iter=200,
            n_jobs=1,
        )

        pipe = Pipeline(steps=[("pre", pre), ("model", clf)])
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))

    else:
        # Regression fallback
        y = pd.to_numeric(y_processed, errors="coerce").replace([np.inf, -np.inf], np.nan)
        mask = y.notna()
        X = X.loc[mask].copy()
        y = y.loc[mask].copy()
        assert len(X) > 1

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE
        )
        assert len(X_train) > 0 and len(X_test) > 0

        reg = Ridge(alpha=1.0, random_state=RANDOM_STATE)

        pipe = Pipeline(steps=[("pre", pre), ("model", reg)])
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = _bounded_regression_score(y_test.values, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses a small linear model (LogisticRegression / Ridge) to stay CPU-friendly and energy-efficient vs. ensembles/deep nets.
# - ColumnTransformer + Pipeline ensures single-pass, reproducible preprocessing and avoids duplicated work.
# - Robust CSV parsing fallback (default, then ';' + ',' decimal, then python inference) reduces manual intervention.
# - Defensive schema handling: normalizes headers, drops Unnamed columns, selects target heuristically if expected not found.
# - Numeric coercion with errors='coerce' and median imputation prevents failures from mixed dtypes with minimal compute.
# - OneHotEncoder(handle_unknown='ignore') keeps categorical handling lightweight and safe for unseen categories.
# - Regression fallback reports a bounded [0,1] proxy score: 1/(1+normalized MAE) to print as ACCURACY when classification is ill-posed.