# Generated by generate_llm_code.py
# LLM: codex
# Mode: assisted

import numpy as np
import pandas as pd
import warnings
from sklearn.feature_selection import chi2
from sklearn.linear_model import LogisticRegression

SEED = 12345
DATASET_PATH = "diabetes.csv"
DATASET_HEADERS = [
    "Pregnancies",
    "Glucose",
    "BloodPressure",
    "SkinThickness",
    "Insulin",
    "BMI",
    "DiabetesPedigreeFunction",
    "Age",
    "Outcome",
]


def read_csv_robust(path, headers):
    def _read(**kwargs):
        return pd.read_csv(path, **kwargs)

    df = _read()

    def parsing_wrong(df_local):
        if df_local.shape[1] == 1 and len(headers) > 1:
            return True
        if len(headers) and df_local.shape[1] != len(headers):
            return True
        if df_local.shape[1] == 1:
            col = str(df_local.columns[0])
            if "," in col or ";" in col:
                return True
        return False

    if parsing_wrong(df):
        df_alt = _read(sep=";", decimal=",")
        if not parsing_wrong(df_alt):
            df = df_alt
    return df


def harmonize_columns(df, headers):
    if len(df.columns) == len(headers):
        df_cols = [str(c).strip() for c in df.columns]
        header_cols = [str(h).strip() for h in headers]
        if [c.lower() for c in df_cols] != [h.lower() for h in header_cols]:
            df.columns = headers
    return df


def align_to_headers(df, headers):
    lower_map = {str(c).strip().lower(): c for c in df.columns}
    if all(str(h).strip().lower() in lower_map for h in headers):
        ordered = [lower_map[str(h).strip().lower()] for h in headers]
        return df[ordered]
    return df


def impute_zero_as_mean(df, cols):
    if not cols:
        return df
    data = df[cols].to_numpy(dtype=np.float64, copy=True)
    data[data == 0] = np.nan
    means = np.nanmean(data, axis=0)
    means = np.where(np.isnan(means), 0.0, means)
    nan_rows, nan_cols = np.where(np.isnan(data))
    if nan_rows.size:
        data[nan_rows, nan_cols] = means[nan_cols]
    df[cols] = data
    return df


def random_split(X, y, train_frac, seed):
    n = X.shape[0]
    if n < 2:
        return X, X, y, y
    rng = np.random.default_rng(seed)
    idx = rng.permutation(n)
    split = int(train_frac * n)
    split = min(max(split, 1), n - 1)
    train_idx = idx[:split]
    test_idx = idx[split:]
    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]


def main():
    warnings.filterwarnings("ignore")

    df = read_csv_robust(DATASET_PATH, DATASET_HEADERS)
    df = harmonize_columns(df, DATASET_HEADERS)
    if len(df.columns) != len(DATASET_HEADERS) and df.shape[1] == len(DATASET_HEADERS):
        df.columns = DATASET_HEADERS
    df = align_to_headers(df, DATASET_HEADERS)
    df = df.apply(pd.to_numeric, errors="coerce")

    col_map = {str(c).lower(): c for c in df.columns}
    label_col = col_map.get("outcome", df.columns[-1])
    feature_cols = [c for c in df.columns if c != label_col]

    zero_names = ["Glucose", "BloodPressure", "SkinThickness", "BMI", "Insulin"]
    zero_cols = [col_map.get(name.lower()) for name in zero_names if col_map.get(name.lower()) is not None]
    df = impute_zero_as_mean(df, zero_cols)

    X = df[feature_cols].to_numpy(dtype=np.float64)
    y_series = pd.to_numeric(df[label_col], errors="coerce")
    if y_series.isna().any():
        mode_val = y_series.mode(dropna=True)
        fill_val = int(mode_val.iloc[0]) if not mode_val.empty else 0
        y_series = y_series.fillna(fill_val)
    y = y_series.to_numpy(dtype=np.int64)

    if np.isnan(X).any():
        col_means = np.nanmean(X, axis=0)
        col_means = np.where(np.isnan(col_means), 0.0, col_means)
        nan_rows, nan_cols = np.where(np.isnan(X))
        if nan_rows.size:
            X[nan_rows, nan_cols] = col_means[nan_cols]

    std = X.std(axis=0, ddof=0)
    std[std == 0] = 1.0
    X_scaled = X / std

    X_train, X_test, y_train, y_test = random_split(X_scaled, y, 0.8, SEED)

    dataset_size = y_train.shape[0]
    num_pos = np.sum(y_train == 1)
    num_neg = dataset_size - num_pos
    balancing_ratio = num_neg / dataset_size if dataset_size else 0.0
    weights = np.where(y_train == 1, balancing_ratio, 1 - balancing_ratio)

    _, p_values = chi2(X_train, y_train)
    selected_mask = p_values <= 0.05
    if not np.any(selected_mask):
        selected_mask = np.ones_like(p_values, dtype=bool)
    X_train_sel = X_train[:, selected_mask]
    X_test_sel = X_test[:, selected_mask]

    lr = LogisticRegression(max_iter=10, solver="liblinear", random_state=SEED)
    lr.fit(X_train_sel, y_train, sample_weight=weights)
    preds = lr.predict(X_test_sel) if X_test_sel.shape[0] else np.array([])
    accuracy = float(np.mean(preds == y_test)) if y_test.size else 0.0
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()
# Optimization Summary
# - Replaced Spark-based processing with in-memory pandas/numpy operations to remove distributed overhead on a small dataset.
# - Used vectorized imputation and scaling to reduce redundant transformations and data movement.
# - Performed chi-square feature selection once and reused the mask for train/test to avoid repeated computation.
# - Implemented a lightweight deterministic split and direct accuracy calculation to minimize extra library overhead.