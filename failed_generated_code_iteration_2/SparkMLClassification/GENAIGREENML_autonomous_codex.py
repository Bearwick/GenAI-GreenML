# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, r2_score

warnings.filterwarnings("ignore")

DATASET_PATH = "diabetes.csv"

def robust_read_csv(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            df = pd.DataFrame()
            return df
    if df.shape[1] == 1:
        try:
            df_alt = pd.read_csv(path, sep=';', decimal=',')
            if df_alt.shape[1] > 1:
                df = df_alt
        except Exception:
            pass
    return df

df = robust_read_csv(DATASET_PATH)
if df is None:
    df = pd.DataFrame()

if not df.empty:
    df.columns = [re.sub(r'\s+', ' ', str(c).strip()) for c in df.columns]
    df = df.loc[:, [not re.match(r'^Unnamed', str(c), flags=re.I) for c in df.columns]]
    df = df.loc[:, [str(c).strip() != '' for c in df.columns]]

if not df.empty:
    for col in df.columns:
        if df[col].dtype == object:
            series = df[col].astype(str).str.strip()
            series = series.replace({'': np.nan})
            series_num = series.str.replace(',', '.', regex=False)
            converted = pd.to_numeric(series_num, errors='coerce')
            if converted.notna().sum() >= max(1, int(0.6 * len(df))):
                df[col] = converted
            else:
                df[col] = series
    df.replace([np.inf, -np.inf], np.nan, inplace=True)

def choose_target(dataframe):
    if dataframe.empty:
        return None
    candidates = ['Outcome', 'Target', 'Label', 'Class', 'Y', 'outcome', 'target', 'label', 'class', 'y']
    lower_map = {col.lower(): col for col in dataframe.columns}
    for cand in candidates:
        if cand.lower() in lower_map:
            return lower_map[cand.lower()]
    numeric_cols = [col for col in dataframe.columns if pd.api.types.is_numeric_dtype(dataframe[col])]
    for col in reversed(numeric_cols):
        if dataframe[col].nunique(dropna=True) > 1:
            return col
    for col in reversed(dataframe.columns):
        if dataframe[col].nunique(dropna=True) > 1:
            return col
    return dataframe.columns[-1] if len(dataframe.columns) > 0 else None

target_col = choose_target(df)
if target_col is None:
    df['target'] = 0
    target_col = 'target'

if target_col in df.columns:
    y = df[target_col]
    X = df.drop(columns=[target_col])
else:
    y = pd.Series([0] * len(df))
    X = df.copy()

valid_idx = y.notna()
X = X.loc[valid_idx].reset_index(drop=True)
y = y.loc[valid_idx].reset_index(drop=True)

assert len(X) > 0 and len(y) > 0

if X.shape[1] == 0:
    X = pd.DataFrame({'dummy': np.zeros(len(y))})

y_nonan = y
n_unique = y_nonan.nunique(dropna=True)
is_classification = False
if y_nonan.dtype == object or str(y_nonan.dtype).startswith('category'):
    is_classification = True
else:
    if pd.api.types.is_numeric_dtype(y_nonan):
        values = y_nonan.dropna().values
        integer_like = np.all(np.isclose(values, np.round(values))) if len(values) > 0 else False
        if integer_like and n_unique <= max(20, int(0.1 * len(y_nonan)) + 1):
            is_classification = True
        else:
            is_classification = False
    else:
        is_classification = True

if is_classification and n_unique < 2:
    is_classification = False

if is_classification:
    if not pd.api.types.is_numeric_dtype(y):
        le = LabelEncoder()
        y = le.fit_transform(y.astype(str))
    else:
        y = y.values
else:
    if not pd.api.types.is_numeric_dtype(y):
        y_num = pd.to_numeric(y.astype(str).str.replace(',', '.', regex=False), errors='coerce')
        if y_num.notna().sum() == 0:
            le = LabelEncoder()
            y_num = le.fit_transform(y.astype(str))
        y = y_num
    y_series = pd.Series(y)
    valid_idx = y_series.notna()
    X = X.loc[valid_idx].reset_index(drop=True)
    y = y_series.loc[valid_idx].values

assert len(X) > 0 and len(np.asarray(y)) > 0

numeric_features = [col for col in X.columns if pd.api.types.is_numeric_dtype(X[col])]
categorical_features = [col for col in X.columns if col not in numeric_features]

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='drop'
)

if is_classification:
    model = LogisticRegression(max_iter=200, solver='liblinear', random_state=42)
else:
    model = LinearRegression()

clf = Pipeline(steps=[
    ('preprocess', preprocessor),
    ('model', model)
])

n_samples = len(np.asarray(y))
if n_samples < 2:
    X_train, X_test, y_train, y_test = X, X, y, y
else:
    test_size = 0.2
    if n_samples < 10:
        test_size = 0.4
    elif n_samples < 30:
        test_size = 0.3
    stratify = None
    if is_classification:
        y_series = pd.Series(y)
        class_counts = y_series.value_counts()
        if class_counts.min() >= 2:
            stratify = y
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=stratify
        )
    except Exception:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )

assert len(X_train) > 0 and len(X_test) > 0

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

if is_classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    if r2 != r2:
        mae = np.mean(np.abs(y_test - y_pred))
        accuracy = 1.0 / (1.0 + mae)
    else:
        accuracy = (r2 + 1.0) / 2.0
        accuracy = max(0.0, min(1.0, accuracy))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Lightweight LogisticRegression/LinearRegression selected for CPU-efficient baseline training.
# - Simple preprocessing with imputation, scaling, and one-hot encoding in a Pipeline ensures reproducibility.
# - Robust CSV parsing and schema handling minimize failures with unknown headers while keeping overhead low.
# - Regression fallback maps R2 to [0,1] (clipped) or uses 1/(1+MAE) when R2 is undefined to provide a stable accuracy proxy.