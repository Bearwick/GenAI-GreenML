# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier, DummyRegressor

DATASET_PATH = "heart.csv"
DATASET_HEADERS_STR = "70.0 1.0 4.0 130.0 322.0 0.0 2.0 109.0 0.0 2.4 2.0 3.0 3.0 2"

provided_headers = [h for h in DATASET_HEADERS_STR.split() if h.strip()]

def is_number(x):
    try:
        float(str(x))
        return True
    except Exception:
        return False

def read_csv_flexible(path, expected_n_cols=None, header="infer"):
    df = None
    try:
        df = pd.read_csv(path, header=header)
    except Exception:
        df = None
    if df is None:
        try:
            df = pd.read_csv(path, sep=';', decimal=',', header=header)
        except Exception:
            df = None
    else:
        if df.shape[1] == 1 and expected_n_cols and expected_n_cols > 1:
            try:
                df_alt = pd.read_csv(path, sep=';', decimal=',', header=header)
                if df_alt.shape[1] > df.shape[1]:
                    df = df_alt
            except Exception:
                pass
    if df is None or (df.shape[1] == 1 and expected_n_cols and expected_n_cols > 1):
        try:
            df_alt = pd.read_csv(path, delim_whitespace=True, header=header)
            if df is None or df_alt.shape[1] > df.shape[1]:
                df = df_alt
        except Exception:
            pass
    if df is None:
        df = pd.DataFrame()
    return df

def make_unique_columns(cols):
    counts = {}
    new_cols = []
    for c in cols:
        if c in counts:
            counts[c] += 1
            new_cols.append(f"{c}_{counts[c]}")
        else:
            counts[c] = 0
            new_cols.append(c)
    return new_cols

def clean_columns(df):
    df = df.copy()
    df.columns = [str(c) for c in df.columns]
    cleaned = []
    for c in df.columns:
        c_str = str(c).strip()
        c_str = " ".join(c_str.split())
        cleaned.append(c_str)
    df.columns = cleaned
    drop_cols = [c for c in df.columns if c.lower().startswith("unnamed")]
    if drop_cols:
        df = df.drop(columns=drop_cols)
    df.columns = make_unique_columns(df.columns)
    return df

expected_n_cols = len(provided_headers) if provided_headers else None
df = read_csv_flexible(DATASET_PATH, expected_n_cols=expected_n_cols, header="infer")
df = clean_columns(df)

if df.shape[1] > 0:
    numeric_name_ratio = sum(is_number(c) for c in df.columns) / len(df.columns)
    if numeric_name_ratio > 0.6:
        df_alt = read_csv_flexible(DATASET_PATH, expected_n_cols=expected_n_cols, header=None)
        if df_alt.shape[0] >= df.shape[0]:
            df = clean_columns(df_alt)

if provided_headers and all(not is_number(h) for h in provided_headers) and len(provided_headers) == df.shape[1]:
    df.columns = make_unique_columns([h.strip() for h in provided_headers])

assert df.shape[0] > 0 and df.shape[1] > 0

def column_numeric_ratio(series):
    return pd.to_numeric(series, errors='coerce').notna().mean()

def is_column_numeric(series):
    return column_numeric_ratio(series) > 0.5

def choose_target(df):
    cols = list(df.columns)
    keywords = ['target', 'label', 'class', 'outcome', 'disease', 'diagnosis']
    for key in keywords:
        for c in cols:
            if key in c.lower():
                return c, True
    return (cols[-1] if cols else None), False

def find_alternative_target(df, current_target):
    best_col = None
    best_unique = 1
    for col in df.columns:
        if col == current_target:
            continue
        ser_num = pd.to_numeric(df[col], errors='coerce')
        if ser_num.notna().mean() > 0.5:
            uniq = ser_num.nunique(dropna=True)
            if uniq > best_unique:
                best_unique = uniq
                best_col = col
    return best_col

target_col, matched_keyword = choose_target(df)
if target_col is None:
    target_col = df.columns[-1]

if not matched_keyword:
    if not is_column_numeric(df[target_col]):
        alt = find_alternative_target(df, target_col)
        if alt is not None:
            target_col = alt

temp_series = pd.to_numeric(df[target_col], errors='coerce')
if temp_series.notna().mean() <= 0.5:
    temp_series = df[target_col].astype(str)
if temp_series.nunique(dropna=True) < 2:
    alt = find_alternative_target(df, target_col)
    if alt is not None:
        target_col = alt

feature_cols = [c for c in df.columns if c != target_col]
if len(feature_cols) == 0:
    df["_constant_feature"] = 1.0
    feature_cols = ["_constant_feature"]

numeric_features = []
categorical_features = []
for col in feature_cols:
    converted = pd.to_numeric(df[col], errors='coerce')
    if converted.notna().mean() > 0.5:
        df[col] = converted
        numeric_features.append(col)
    else:
        categorical_features.append(col)

target_raw = df[target_col]
target_num = pd.to_numeric(target_raw, errors='coerce')
target_num_ratio = target_num.notna().mean()
if target_num_ratio > 0.5:
    df[target_col] = target_num
    target_is_numeric = True
else:
    df[target_col] = target_raw.astype(str)
    target_is_numeric = False

df = df.replace([np.inf, -np.inf], np.nan)
df_model = df.dropna(subset=[target_col])
assert df_model.shape[0] > 0

X = df_model[feature_cols]
y = df_model[target_col]

if not target_is_numeric:
    problem_type = "classification"
else:
    unique_count = y.nunique(dropna=True)
    try:
        is_integer = np.all(np.isclose((y.dropna() % 1), 0))
    except Exception:
        is_integer = False
    if unique_count <= 20 and (unique_count <= max(10, int(0.1 * len(y))) or is_integer):
        problem_type = "classification"
    else:
        problem_type = "regression"

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ],
    remainder="drop"
)

n_samples = len(y)
if n_samples >= 2:
    if problem_type == "classification" and y.nunique() >= 2:
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
        except Exception:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
else:
    X_train = X_test = X
    y_train = y_test = y

assert len(X_train) > 0 and len(X_test) > 0

if problem_type == "classification":
    if y_train.nunique() < 2:
        model = DummyClassifier(strategy="most_frequent")
    else:
        model = LogisticRegression(max_iter=200, solver="liblinear")
    pipeline = Pipeline(steps=[("preprocessor", preprocessor),
                              ("model", model)])
    try:
        pipeline.fit(X_train, y_train)
        preds = pipeline.predict(X_test)
    except Exception:
        model = DummyClassifier(strategy="most_frequent")
        pipeline = Pipeline(steps=[("preprocessor", preprocessor),
                                  ("model", model)])
        pipeline.fit(X_train, y_train)
        preds = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, preds)
else:
    model = LinearRegression()
    pipeline = Pipeline(steps=[("preprocessor", preprocessor),
                              ("model", model)])
    try:
        pipeline.fit(X_train, y_train)
        preds = pipeline.predict(X_test)
    except Exception:
        model = DummyRegressor(strategy="mean")
        pipeline = Pipeline(steps=[("preprocessor", preprocessor),
                                  ("model", model)])
        pipeline.fit(X_train, y_train)
        preds = pipeline.predict(X_test)
    try:
        r2 = r2_score(y_test, preds)
    except Exception:
        r2 = np.nan
    if (not np.isfinite(r2)) or (r2 < 0):
        try:
            mse = np.mean((y_test - preds) ** 2)
            accuracy = 1.0 / (1.0 + mse) if np.isfinite(mse) else 0.0
        except Exception:
            accuracy = 0.0
    else:
        accuracy = float(max(0.0, min(1.0, r2)))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Lightweight linear/logistic models with simple preprocessing keep CPU usage low and enable reproducible pipelines.
# - Minimal imputation, scaling, and one-hot encoding provide robustness for unknown schemas without heavy computation.
# - Regression fallback uses bounded R^2 when valid, otherwise 1/(1+MSE) to keep ACCURACY in [0,1].