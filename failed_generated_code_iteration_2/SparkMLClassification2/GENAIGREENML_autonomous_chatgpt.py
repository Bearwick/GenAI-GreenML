# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "heart.csv"
DATASET_HEADERS = "70.0 1.0 4.0 130.0 322.0 0.0 2.0 109.0 0.0 2.4 2.0 3.0 3.0 2"


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        normed.append(c2)
    return normed


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if re.match(r"^Unnamed:?", str(c))]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _try_read_csv(path):
    # try default; if parsing likely wrong, retry with EU-style separators/decimals
    df1 = None
    try:
        df1 = pd.read_csv(path)
    except Exception:
        df1 = None

    def looks_wrong(d):
        if d is None:
            return True
        if d.shape[1] <= 1:
            return True
        # if most rows have NaN across many columns, parsing might be off
        if d.shape[0] > 5:
            frac_all_nan_rows = (d.isna().all(axis=1).mean())
            if frac_all_nan_rows > 0.2:
                return True
        return False

    if looks_wrong(df1):
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            return df2
        except Exception:
            return df1
    return df1


def _coerce_numeric_series(s):
    return pd.to_numeric(s, errors="coerce")


def _choose_target(df, dataset_headers_str):
    # If headers look numeric (common with missing header row), dataset_headers_str likely are first-row values.
    # We use robust target selection:
    # 1) prefer a column named like 'target', 'label', 'class', 'output', 'y'
    # 2) else choose a non-constant numeric column with low cardinality (classification-friendly)
    # 3) else choose last column
    lowered = {c.lower(): c for c in df.columns}
    for key in ["target", "label", "class", "output", "y", "diagnosis", "outcome"]:
        if key in lowered:
            return lowered[key]

    # numeric candidates
    num_candidates = []
    for c in df.columns:
        s = _coerce_numeric_series(df[c])
        non_na = s.dropna()
        if non_na.shape[0] == 0:
            continue
        nunique = non_na.nunique(dropna=True)
        if nunique >= 2:
            num_candidates.append((c, nunique, non_na.shape[0]))

    if not num_candidates:
        return df.columns[-1]

    # prefer low unique counts first (likely classification label), but avoid too many unique values
    # also prefer columns with fewer unique values but enough samples
    num_candidates.sort(key=lambda x: (x[1], -x[2]))
    for c, nunique, _ in num_candidates:
        if nunique <= 20:
            return c

    # fallback: last numeric with variance
    return num_candidates[-1][0]


def _is_classification_target(y):
    y_num = _coerce_numeric_series(y)
    y_non_na = y_num.dropna()
    if y_non_na.shape[0] == 0:
        return False, None
    # if values are close to integers and unique count small, treat as classification
    uniq = np.sort(y_non_na.unique())
    nunique = uniq.size
    if nunique < 2:
        return False, y_non_na
    if nunique <= 20:
        # check integer-likeness
        if np.all(np.isclose(uniq, np.round(uniq))):
            return True, y_non_na
    return False, y_non_na


def _safe_accuracy_proxy_r2(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    if y_true.size == 0:
        return 0.0
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    if ss_tot <= 1e-12:
        # if constant target, perfect prediction if also constant
        return 1.0 if ss_res <= 1e-12 else 0.0
    r2 = 1.0 - (ss_res / ss_tot)
    # map to [0,1] bounded stability proxy
    acc = float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))
    return acc


def main():
    df = _try_read_csv(DATASET_PATH)
    if df is None:
        raise RuntimeError("Failed to read dataset")

    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # If file was read with wrong header (numeric-like colnames), retry with header=None and generate names
    def header_looks_like_data(cols, dataset_headers_str):
        # many column names parse as floats/ints -> likely first data row became header
        numeric_like = 0
        for c in cols:
            try:
                float(str(c))
                numeric_like += 1
            except Exception:
                pass
        if len(cols) > 0 and numeric_like / len(cols) >= 0.7:
            return True
        # dataset_headers_str provided as numeric tokens, indicates likely missing header
        toks = str(dataset_headers_str).strip().split()
        if len(toks) >= 5:
            # if most tokens numeric, treat as hint
            numt = 0
            for t in toks:
                try:
                    float(t)
                    numt += 1
                except Exception:
                    pass
            if numt / len(toks) >= 0.8:
                return True
        return False

    if header_looks_like_data(df.columns, DATASET_HEADERS):
        df2 = None
        try:
            df2 = pd.read_csv(DATASET_PATH, header=None)
        except Exception:
            try:
                df2 = pd.read_csv(DATASET_PATH, header=None, sep=";", decimal=",")
            except Exception:
                df2 = None
        if df2 is not None and df2.shape[1] >= 2:
            df = df2
            df.columns = [f"col_{i}" for i in range(df.shape[1])]
            df.columns = _normalize_columns(df.columns)
            df = _drop_unnamed(df)

    # Basic cleanup
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.dropna(axis=0, how="all")
    df = df.dropna(axis=1, how="all")

    assert df.shape[0] > 0 and df.shape[1] >= 2

    target_col = _choose_target(df, DATASET_HEADERS)
    if target_col not in df.columns:
        target_col = df.columns[-1]

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # Identify column types robustly
    numeric_cols = []
    categorical_cols = []
    for c in X.columns:
        s = X[c]
        # if coercion yields mostly numeric, treat numeric
        s_num = _coerce_numeric_series(s)
        non_na = s_num.notna().mean()
        if non_na >= 0.8:
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)

    # Ensure we have at least one feature column; if not, create a constant feature
    if X.shape[1] == 0:
        X = pd.DataFrame({"const": np.ones(len(df), dtype=float)})
        numeric_cols = ["const"]
        categorical_cols = []

    # Decide task
    is_clf, y_num_non_na = _is_classification_target(y_raw)

    # Prepare y and drop rows where y is missing after coercion for modeling stability
    y_num = _coerce_numeric_series(y_raw)
    valid_mask = y_num.notna()
    X = X.loc[valid_mask].copy()
    y_num = y_num.loc[valid_mask].copy()

    assert X.shape[0] > 1

    # If classification but only 1 class after filtering, fallback to regression proxy
    if is_clf:
        y_int = np.round(y_num).astype(int)
        if y_int.nunique(dropna=True) < 2:
            is_clf = False
        y_for_model = y_int if is_clf else y_num
    else:
        y_for_model = y_num

    # Preprocessing pipeline
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Split
    stratify = y_for_model if is_clf and getattr(y_for_model, "nunique", lambda: 0)() >= 2 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_for_model, test_size=0.2, random_state=42, stratify=stratify
    )
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0

    if is_clf:
        # lightweight linear classifier, CPU-friendly
        clf = LogisticRegression(
            solver="lbfgs",
            max_iter=200,
            n_jobs=1,
        )
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", clf)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        reg = Ridge(alpha=1.0, random_state=42)
        model = Pipeline(steps=[("preprocess", preprocessor), ("model", reg)])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = _safe_accuracy_proxy_r2(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression / Ridge) for CPU efficiency and low energy use.
# - ColumnTransformer+Pipeline ensures single-pass preprocessing and reproducibility with fixed random_state.
# - Robust schema handling: normalizes column names, drops Unnamed cols, detects missing headers, and auto-selects a plausible target.
# - Numeric coercion with errors='coerce' plus NaN/inf handling avoids expensive/fragile type inference and prevents crashes.
# - OneHotEncoder with sparse output limits memory/compute for categorical features; no deep learning or large ensembles used.
# - Regression fallback uses a bounded R2-derived proxy mapped to [0,1] to keep "ACCURACY" stable when classification is not viable.