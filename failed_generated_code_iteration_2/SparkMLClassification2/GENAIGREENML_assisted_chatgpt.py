# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: assisted

import os
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator


SEED = 12345


def _read_heart_csv(path: str, headers_example: str) -> pd.DataFrame:
    n_cols = len(headers_example.strip().split())
    df = None

    try:
        df = pd.read_csv(path)
        if df.shape[1] < n_cols:
            df = None
    except Exception:
        df = None

    if df is None:
        try:
            df = pd.read_csv(path, sep=";", decimal=",")
            if df.shape[1] < n_cols:
                df = None
        except Exception:
            df = None

    if df is None:
        df = pd.read_csv(path, sep=r"\s+", header=None, engine="python")
    elif df.shape[1] > n_cols:
        df = df.iloc[:, :n_cols]

    if df.shape[1] != n_cols:
        df = df.iloc[:, :n_cols]

    return df


def _build_spark_session() -> SparkSession:
    return (
        SparkSession.builder.appName("SparkMLClassification_Heart_Green")
        .config("spark.sql.shuffle.partitions", "8")
        .config("spark.default.parallelism", "8")
        .getOrCreate()
    )


def _prepare_dataframe(spark: SparkSession, csv_path: str, headers_example: str):
    pdf = _read_heart_csv(csv_path, headers_example)

    cols = [
        "age",
        "sex",
        "chest_pain",
        "resting_blood_pressure",
        "serum_cholesterol",
        "fasting_blood_sugar",
        "resting_ecg_results",
        "maximum_heart_rate_achieved",
        "exercise_induced_angina",
        "st_depression",
        "slope",
        "number_of_major_vessels",
        "thal",
        "last",
    ]

    if pdf.shape[1] >= len(cols):
        pdf = pdf.iloc[:, : len(cols)]
    else:
        cols = cols[: pdf.shape[1]]

    pdf.columns = cols

    if "thal" not in pdf.columns:
        raise ValueError("Required column for label derivation ('thal') is missing after parsing.")

    for c in pdf.columns:
        pdf[c] = pd.to_numeric(pdf[c], errors="coerce")

    pdf = pdf.iloc[:, :13].copy()
    pdf["label"] = pdf["thal"].isin([3, 7]).astype("int32").map({True: 0, False: 1})
    pdf["label"] = pdf["label"].astype("double")

    return spark.createDataFrame(pdf)


def classify() -> float:
    spark = _build_spark_session()
    spark.sparkContext.setLogLevel("ERROR")

    df = _prepare_dataframe(spark, "heart.csv", "70.0 1.0 4.0 130.0 322.0 0.0 2.0 109.0 0.0 2.4 2.0 3.0 3.0 2")

    feature_cols = [c for c in df.columns if c not in ("thal", "last", "label")]
    feature_cols = sorted(feature_cols, key=lambda x: df.columns.index(x))

    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    scaler = StandardScaler(inputCol="features", outputCol="Scaled_features", withMean=False, withStd=True)
    rf = RandomForestClassifier(
        labelCol="label",
        featuresCol="Scaled_features",
        numTrees=200,
        seed=SEED,
    )

    pipeline = Pipeline(stages=[assembler, scaler, rf])

    base = df.select(*(feature_cols + ["label"]))
    base = base.persist()

    training, test = base.randomSplit([0.5, 0.5], seed=SEED)

    model = pipeline.fit(training)
    predict_test = model.transform(test)

    evaluator = BinaryClassificationEvaluator(labelCol="label")
    accuracy = float(evaluator.evaluate(predict_test, {evaluator.metricName: "areaUnderROC"}))

    base.unpersist()
    spark.stop()
    return accuracy


def main():
    accuracy = classify()
    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    os.environ.setdefault("PYTHONHASHSEED", str(SEED))
    main()

# Optimization Summary
# - Removed all dataframe .show(), plotting, and non-essential prints to avoid expensive Spark actions and driver-side overhead.
# - Replaced multiple manual transformations with a single Spark ML Pipeline (VectorAssembler + StandardScaler + RandomForest) to reduce repeated passes over data.
# - Selected and persisted only the necessary columns (features + label) to cut memory usage and data movement.
# - Reduced shuffle/parallelism defaults via Spark configs to lower overhead for small datasets while preserving results intent.
# - Added deterministic behavior via fixed seeds (randomSplit and RandomForest) and stable feature column ordering derived from df.columns.
# - Implemented robust CSV parsing fallback (default -> sep=';' with decimal=',' -> whitespace) to ensure end-to-end reliability across formatting variants.
# - Avoided creating redundant intermediate Spark DataFrames by chaining pipeline stages and minimizing materializations.