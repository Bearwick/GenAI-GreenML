# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, mean_squared_error

warnings.filterwarnings("ignore")

DATASET_PATH = "Fake.csv"
expected_headers = [h.strip() for h in "title,text,subject,date".split(",")]

def load_dataset(path):
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
        return df
    if df.shape[1] == 1:
        try:
            df_alt = pd.read_csv(path, sep=";", decimal=",")
            if df_alt.shape[1] > 1:
                df = df_alt
        except Exception:
            pass
    return df

df = load_dataset(DATASET_PATH)

if df is None or df.shape[0] == 0:
    df = pd.DataFrame({"__dummy__": [0]})

df.columns = [re.sub(r"\s+", " ", str(c).strip()) for c in df.columns]
df = df.loc[:, ~df.columns.str.contains("^Unnamed", case=False, regex=True)]

if all(re.fullmatch(r"\d+", str(c)) for c in df.columns) and len(df.columns) == len(expected_headers):
    df.columns = expected_headers

if df.shape[1] == 0:
    df = pd.DataFrame({"__dummy__": [0]})

assert df.shape[0] > 0

def choose_target(df):
    cols = list(df.columns)
    if not cols:
        return None
    lower_map = {col: str(col).lower() for col in cols}
    preferred = ["label", "target", "class", "y", "is_fake", "fake", "category", "subject"]
    for p in preferred:
        for col in cols:
            if lower_map[col] == p:
                return col
    for col in cols:
        series = pd.to_numeric(df[col], errors="coerce")
        if series.notna().sum() > 0 and series.nunique(dropna=True) > 1:
            return col
    n_rows = len(df)
    max_unique = min(20, max(2, n_rows // 2))
    best_col = None
    best_unique = None
    for col in cols:
        series = df[col]
        if series.dropna().empty:
            continue
        n_unique = series.nunique(dropna=True)
        if 1 < n_unique <= max_unique:
            if best_unique is None or n_unique < best_unique:
                best_unique = n_unique
                best_col = col
    if best_col is not None:
        return best_col
    return cols[0]

target_col = choose_target(df)
if target_col is None or target_col not in df.columns:
    target_col = df.columns[0]

y_raw = df[target_col]

is_numeric_target = pd.api.types.is_numeric_dtype(y_raw)
if is_numeric_target:
    y_numeric = pd.to_numeric(y_raw, errors="coerce")
    unique_target = y_numeric.nunique(dropna=True)
    task = "classification" if unique_target <= 20 else "regression"
else:
    task = "classification"

if task == "regression":
    y = pd.to_numeric(y_raw, errors="coerce")
else:
    y = y_raw

mask = y.notna()
X = df.drop(columns=[target_col])
if X.shape[1] == 0:
    X = pd.DataFrame({"__constant__": np.ones(len(df))})
X = X.loc[mask].copy()
y = y.loc[mask]

if len(X) == 0:
    X = pd.DataFrame({"__constant__": [1.0]})
    y = pd.Series([0])
    task = "regression"

X.replace([np.inf, -np.inf], np.nan, inplace=True)
X = X.loc[:, X.notna().any()]

if X.shape[1] == 0:
    X = pd.DataFrame({"__constant__": np.ones(len(y))})

assert len(X) > 0

obj_cols = list(X.select_dtypes(include=["object"]).columns)
text_cols = []
text_name_hints = {"text", "title", "content", "body"}
text_name_hints.update({h.lower() for h in expected_headers if h.lower() in {"text", "title", "content", "body"}})
for col in obj_cols:
    col_lower = str(col).lower()
    series = X[col].dropna().astype(str)
    if col_lower in text_name_hints:
        text_cols.append(col)
        continue
    if series.empty:
        continue
    if len(series) > 1000:
        series = series.sample(1000, random_state=42)
    if series.str.len().median() > 30:
        text_cols.append(col)

text_col = None
if text_cols:
    X = X.copy()
    X["combined_text"] = X[text_cols].fillna("").astype(str).agg(" ".join, axis=1)
    X.drop(columns=text_cols, inplace=True)
    text_col = "combined_text"

num_cols = []
cat_cols = []
for col in X.columns:
    if text_col is not None and col == text_col:
        continue
    series = X[col]
    if pd.api.types.is_numeric_dtype(series):
        num_cols.append(col)
    else:
        coerced = pd.to_numeric(series, errors="coerce")
        if coerced.notna().mean() > 0.8:
            X[col] = coerced
            num_cols.append(col)
        else:
            cat_cols.append(col)

if text_col is None and not num_cols and not cat_cols:
    X["__constant__"] = 1.0
    num_cols = ["__constant__"]

transformers = []
if text_col is not None:
    transformers.append(("text", TfidfVectorizer(max_features=3000, stop_words="english", dtype=np.float32), text_col))
if num_cols:
    transformers.append(("num", Pipeline(steps=[("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler(with_mean=False))]), num_cols))
if cat_cols:
    transformers.append(("cat", Pipeline(steps=[("imputer", SimpleImputer(strategy="most_frequent")), ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=True))]), cat_cols))

preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

use_dummy = False
if task == "classification":
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    n_classes = len(le.classes_)
    if n_classes < 2 or n_classes > 50:
        use_dummy = True
else:
    y_encoded = pd.to_numeric(y, errors="coerce").values

if task == "classification":
    if use_dummy:
        model = DummyClassifier(strategy="most_frequent")
    else:
        model = LogisticRegression(max_iter=200, solver="liblinear")
else:
    if pd.Series(y_encoded).nunique(dropna=True) < 2:
        model = DummyRegressor(strategy="mean")
    else:
        model = Ridge(alpha=1.0)

n_samples = len(X)
if n_samples < 2:
    X_train = X
    X_test = X
    y_train = y_encoded
    y_test = y_encoded
else:
    test_size = max(1, int(round(n_samples * 0.2)))
    if n_samples - test_size < 1:
        test_size = n_samples - 1
    stratify = None
    if task == "classification" and not use_dummy:
        counts = np.bincount(y_encoded)
        if counts.min() >= 2:
            stratify = y_encoded
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=test_size, random_state=42, stratify=stratify
    )

assert len(X_train) > 0 and len(X_test) > 0

model_pipeline = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
model_pipeline.fit(X_train, y_train)
y_pred = model_pipeline.predict(X_test)

if task == "classification":
    accuracy = accuracy_score(y_test, y_pred)
else:
    mse = mean_squared_error(y_test, y_pred)
    accuracy = 1.0 / (1.0 + mse) if np.isfinite(mse) else 0.0

accuracy = float(accuracy)
print(f"ACCURACY={accuracy:.6f}")
# Optimization Summary
# - Limited TF-IDF vocabulary size and used linear models to keep computation CPU-friendly.
# - Employed a simple ColumnTransformer with imputation/one-hot encoding for reproducible, minimal preprocessing.
# - Regression fallback reports a bounded proxy accuracy of 1/(1+MSE) for stability without extra overhead.