# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline

DATASET_PATH = "music.csv"

# Robust CSV loading
try:
    df = pd.read_csv(DATASET_PATH)
    if df.shape[1] < 2:
        df = pd.read_csv(DATASET_PATH, sep=';', decimal=',')
except Exception:
    df = pd.read_csv(DATASET_PATH, sep=';', decimal=',')

# Strip/normalize column names
df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
df = df[[c for c in df.columns if not c.startswith('Unnamed')]]

# Identify columns based on expected schema: age, gender, genre
expected_features = ['age', 'gender']
expected_target = 'genre'

# Lowercase column matching for robustness
col_map = {c.lower(): c for c in df.columns}

feature_cols = []
for ef in expected_features:
    if ef.lower() in col_map:
        feature_cols.append(col_map[ef.lower()])

target_col = None
if expected_target.lower() in col_map:
    target_col = col_map[expected_target.lower()]

# Fallback: if expected columns not found, pick available columns
if not feature_cols or target_col is None:
    # Try to pick last column as target, rest as features
    all_cols = list(df.columns)
    if target_col is None:
        target_col = all_cols[-1]
    if not feature_cols:
        feature_cols = [c for c in all_cols if c != target_col]

assert len(feature_cols) > 0, "No feature columns found"
assert target_col is not None, "No target column found"

# Coerce numeric features where possible
for c in feature_cols:
    df[c] = pd.to_numeric(df[c], errors='coerce')

# Handle target: encode if categorical
le = LabelEncoder()
if df[target_col].dtype == object or df[target_col].dtype.name == 'category':
    df[target_col] = le.fit_transform(df[target_col].astype(str))
    is_classification = True
else:
    df[target_col] = pd.to_numeric(df[target_col], errors='coerce')
    n_unique = df[target_col].nunique()
    is_classification = n_unique <= 20  # heuristic for small dataset

# Drop rows with NaN in features or target
df = df.dropna(subset=feature_cols + [target_col])
df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=feature_cols + [target_col])

assert len(df) > 0, "Dataset is empty after preprocessing"

X = df[list(feature_cols)].values
y = df[target_col].values

# Check classification feasibility
n_classes = len(np.unique(y))
if n_classes < 2:
    # Trivial baseline: predict the single class, accuracy = 1.0
    accuracy = 1.0
    print(f"ACCURACY={accuracy:.6f}")
else:
    # Train/test split with fixed random_state for reproducibility
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    assert len(X_train) > 0, "Training set is empty"
    assert len(X_test) > 0, "Test set is empty"

    # Use a small DecisionTreeClassifier as per project context
    # max_depth limited for energy efficiency and to avoid overfitting on small data
    model = DecisionTreeClassifier(
        max_depth=5,
        random_state=42
    )