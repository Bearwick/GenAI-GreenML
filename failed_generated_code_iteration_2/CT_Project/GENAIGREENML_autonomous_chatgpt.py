# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


def _read_csv_robust(path: str) -> pd.DataFrame:
    try:
        df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
        return df

    # If parsing likely wrong (single column), retry with common European formatting.
    if df.shape[1] <= 1:
        try:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df2.shape[1] > df.shape[1]:
                df = df2
        except Exception:
            pass
    return df


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols = []
    for c in df.columns:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        cols.append(c2)
    df = df.copy()
    df.columns = cols
    # Drop "Unnamed: ..." columns (common index artifacts)
    drop_cols = [c for c in df.columns if c.lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _pick_target_and_features(df: pd.DataFrame, headers_hint=None):
    headers_hint = headers_hint or []
    lower_cols = {c.lower(): c for c in df.columns}

    # Prefer header hints if present
    target_col = None
    for h in headers_hint:
        if isinstance(h, str) and h.strip():
            key = h.strip().lower()
            if key in lower_cols:
                target_col = lower_cols[key]
                break

    # If hint didn't exist, try common target names
    if target_col is None:
        for cand in ["category", "label", "target", "class", "y"]:
            if cand in lower_cols:
                target_col = lower_cols[cand]
                break

    # If still none: choose a non-constant column (prefer low cardinality object/categorical for classification)
    if target_col is None:
        nunique = df.nunique(dropna=True)
        candidates = [c for c in df.columns if nunique.get(c, 0) > 1]
        if candidates:
            obj_cands = [c for c in candidates if df[c].dtype == "object"]
            if obj_cands:
                # pick smallest unique count >1 to resemble a class label
                obj_cands_sorted = sorted(obj_cands, key=lambda c: (nunique.get(c, 1e9), c))
                target_col = obj_cands_sorted[0]
            else:
                # fall back: pick a numeric column with variability
                num_cands = [c for c in candidates if pd.api.types.is_numeric_dtype(df[c])]
                target_col = num_cands[0] if num_cands else candidates[0]

    feature_cols = [c for c in df.columns if c != target_col]

    # If feature cols empty but target exists, keep running with a dummy constant feature
    if not feature_cols:
        df = df.copy()
        df["_constant_feature_"] = 1
        feature_cols = ["_constant_feature_"]

    return df, target_col, feature_cols


def _coerce_numeric_safely(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    for c in df.columns:
        if df[c].dtype == "object":
            # Do not coerce objects globally; only if it clearly looks numeric later via selector.
            continue
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _make_classification_pipeline(text_cols, cat_cols, num_cols):
    transformers = []

    if text_cols:
        # Lightweight sparse text features; cap size for CPU/energy efficiency
        transformers.append(
            ("text", TfidfVectorizer(lowercase=True, stop_words="english", max_features=2000), text_cols[0])
        )

    if cat_cols:
        transformers.append(
            (
                "cat",
                Pipeline(steps=[
                    ("imputer", SimpleImputer(strategy="most_frequent")),
                    ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
                ]),
                cat_cols,
            )
        )

    if num_cols:
        transformers.append(
            (
                "num",
                Pipeline(steps=[
                    ("imputer", SimpleImputer(strategy="median")),
                    ("scaler", StandardScaler(with_mean=False)),
                ]),
                num_cols,
            )
        )

    pre = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    clf = LogisticRegression(
        solver="liblinear",
        max_iter=300,
        C=1.0,
        class_weight=None,
        random_state=42,
    )

    return Pipeline(steps=[("preprocess", pre), ("model", clf)])


def _make_regression_pipeline(text_cols, cat_cols, num_cols):
    transformers = []

    if text_cols:
        transformers.append(
            ("text", TfidfVectorizer(lowercase=True, stop_words="english", max_features=2000), text_cols[0])
        )

    if cat_cols:
        transformers.append(
            (
                "cat",
                Pipeline(steps=[
                    ("imputer", SimpleImputer(strategy="most_frequent")),
                    ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
                ]),
                cat_cols,
            )
        )

    if num_cols:
        transformers.append(
            (
                "num",
                Pipeline(steps=[
                    ("imputer", SimpleImputer(strategy="median")),
                    ("scaler", StandardScaler(with_mean=False)),
                ]),
                num_cols,
            )
        )

    pre = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    reg = Ridge(alpha=1.0, random_state=42)

    return Pipeline(steps=[("preprocess", pre), ("model", reg)])


def main():
    path = "mail_data.csv"
    df = _read_csv_robust(path)
    df = _normalize_columns(df)
    df = _coerce_numeric_safely(df)

    assert df is not None and len(df) > 0, "Dataset is empty after loading."

    headers_hint = ["Category", "Message"]
    df, target_col, feature_cols = _pick_target_and_features(df, headers_hint=headers_hint)

    # Basic cleanup: remove completely empty rows
    df = df.dropna(how="all").reset_index(drop=True)
    assert len(df) > 0, "Dataset is empty after dropping empty rows."

    y_raw = df[target_col]
    X = df[feature_cols].copy()

    # Identify a primary text column (prefer 'Message' if present)
    text_col = None
    for cand in ["Message", "message", "text", "body", "content"]:
        if cand in df.columns and cand != target_col:
            text_col = cand
            break

    if text_col is None:
        # If any object column exists, choose the one with highest avg string length as text proxy
        obj_cols = [c for c in feature_cols if X[c].dtype == "object"]
        if obj_cols:
            lengths = {}
            for c in obj_cols:
                s = X[c].astype(str)
                lengths[c] = float(s.str.len().replace([np.inf, -np.inf], np.nan).fillna(0).mean())
            text_col = max(lengths, key=lengths.get) if lengths else obj_cols[0]

    text_cols = [text_col] if text_col is not None else []

    # Other categorical and numeric columns
    cat_cols = [c for c in feature_cols if c not in text_cols and X[c].dtype == "object"]
    num_cols = [c for c in feature_cols if c not in text_cols and pd.api.types.is_numeric_dtype(X[c])]

    # Prepare target: classification if it looks categorical; else regression
    # Coerce to string for classification; keep numeric for regression
    y_is_numeric = pd.api.types.is_numeric_dtype(y_raw)
    y_nunique = y_raw.nunique(dropna=True)

    do_classification = (not y_is_numeric) and (y_nunique >= 2)
    # If numeric but low number of unique values, treat as classification
    if y_is_numeric and y_nunique >= 2 and y_nunique <= max(20, int(0.05 * len(df))):
        do_classification = True

    # Drop rows with missing target
    mask = ~pd.isna(y_raw)
    X = X.loc[mask].reset_index(drop=True)
    y_raw = y_raw.loc[mask].reset_index(drop=True)

    assert len(X) > 2, "Not enough samples after removing missing target."

    # Split
    if do_classification:
        y = y_raw.astype(str)
        # If too many classes (likely regression-like IDs), fallback to regression
        if y.nunique(dropna=True) < 2:
            do_classification = False
        elif y.nunique(dropna=True) > max(50, int(0.2 * len(y))):
            do_classification = False

    if do_classification:
        stratify = y if y.nunique() >= 2 and y.nunique() <= len(y) else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

        pipe = _make_classification_pipeline(text_cols, cat_cols, num_cols)
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Regression fallback
        y = pd.to_numeric(y_raw, errors="coerce")
        mask2 = ~pd.isna(y)
        X = X.loc[mask2].reset_index(drop=True)
        y = y.loc[mask2].reset_index(drop=True)

        if len(X) < 3 or y.nunique(dropna=True) < 2:
            # Trivial baseline: constant predictor -> accuracy proxy 0.0 (no signal)
            accuracy = 0.0
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

            pipe = _make_regression_pipeline(text_cols, cat_cols, num_cols)
            pipe.fit(X_train, y_train)
            y_pred = pipe.predict(X_test)

            # Stable bounded score in [0,1]: 1/(1+NRMSE), normalized by target std (or 1 if tiny)
            err = np.asarray(y_test) - np.asarray(y_pred)
            rmse = float(np.sqrt(np.mean(err * err)))
            scale = float(np.std(y_test))
            if not np.isfinite(scale) or scale < 1e-12:
                scale = 1.0
            nrmse = rmse / scale
            accuracy = float(1.0 / (1.0 + max(0.0, nrmse)))

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses a CPU-friendly linear model (LogisticRegression with liblinear) for text classification; avoids heavy ensembles/deep learning.
# - Uses TF-IDF with a capped vocabulary (max_features=2000) to limit memory/compute while retaining strong baseline performance.
# - ColumnTransformer + Pipeline ensures single-pass, reproducible preprocessing and avoids redundant transformations.
# - Robust CSV parsing fallback (default then ';' with decimal=',') plus column-name normalization to handle schema quirks.
# - Defensive target/feature selection avoids hard failures if headers differ; picks a usable target and continues end-to-end.
# - Regression fallback uses lightweight Ridge and reports a bounded [0,1] "accuracy" proxy: 1/(1+NRMSE) for stability.