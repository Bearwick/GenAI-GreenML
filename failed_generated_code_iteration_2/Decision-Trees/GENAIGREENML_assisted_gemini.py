# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
import math
import random

def load_data(path):
    try:
        df = pd.read_csv(path)
        if len(df.columns) < 2:
            raise ValueError
    except:
        df = pd.read_csv(path, sep=';', decimal=',')
    return df

class LeafNode:
    def __init__(self, pred_class, pred_class_count, total_count):
        self.pred_class = pred_class
        self.pred_class_count = pred_class_count
        self.total_count = total_count
        self.prob = pred_class_count / total_count if total_count > 0 else 0

    def classify(self, example):
        return self.pred_class, self.prob

class DecisionNode:
    def __init__(self, test_attr_name, test_attr_threshold, child_lt, child_ge, child_miss):
        self.test_attr_name = test_attr_name
        self.test_attr_threshold = test_attr_threshold
        self.child_ge = child_ge
        self.child_lt = child_lt
        self.child_miss = child_miss

    def classify(self, example):
        test_val = example[self.test_attr_name]
        if pd.isna(test_val):
            return self.child_miss.classify(example)
        elif test_val < self.test_attr_threshold:
            return self.child_lt.classify(example)
        else:
            return self.child_ge.classify(example)

class DecisionTree:
    def __init__(self, df, id_name, class_name, min_leaf_count=1):
        self.id_name = id_name
        self.class_name = class_name
        self.min_leaf_count = min_leaf_count
        attr_cols = [c for c in df.columns if c not in [id_name, class_name]]
        self.root = self.build_tree(set(attr_cols), df)

    def build_tree(self, attribute_set, df):
        if df.empty or not attribute_set:
            return self.make_leaf(df)

        best_attr, threshold, df_lt, df_ge = self.get_best_split(attribute_set, df)

        if best_attr == "" or len(df_lt) <= self.min_leaf_count or len(df_ge) <= self.min_leaf_count:
            return self.make_leaf(df)

        attribute_set.remove(best_attr)
        
        child_lt = self.build_tree(attribute_set, df_lt)
        child_ge = self.build_tree(attribute_set, df_ge)
        child_miss = child_lt if len(df_lt) >= len(df_ge) else child_ge

        return DecisionNode(best_attr, threshold, child_lt, child_ge, child_miss)

    def make_leaf(self, df):
        if df.empty:
            return LeafNode(None, 0, 1)
        counts = df[self.class_name].value_counts()
        pred_class = counts.idxmax()
        return LeafNode(pred_class, counts.max(), len(df))

    def get_best_split(self, attribute_set, df):
        best_gain = -1.0
        best_params = ("", 0, None, None)
        parent_entropy = self.calculate_entropy(df[self.class_name])

        for attr in attribute_set:
            vals = df[attr].dropna()
            if vals.empty: continue
            
            vmin, vmax = vals.min(), vals.max()
            if vmin == vmax: continue
            
            step = (vmax - vmin) / 15
            curr_thresh = vmin + step
            
            while curr_thresh < vmax:
                mask_lt = df[attr] < curr_thresh
                df_lt = df[mask_lt]
                df_ge = df[~mask_lt].dropna(subset=[attr])
                
                if not df_lt.empty and not df_ge.empty:
                    gain = parent_entropy - (
                        (len(df_lt)/len(df)) * self.calculate_entropy(df_lt[self.class_name]) +
                        (len(df_ge)/len(df)) * self.calculate_entropy(df_ge[self.class_name])
                    )
                    if gain > best_gain:
                        best_gain = gain
                        best_params = (attr, curr_thresh, df_lt, df_ge)
                curr_thresh += step
                
        return best_params

    def calculate_entropy(self, target_col):
        if target_col.empty: return 0
        probs = target_col.value_counts(normalize=True)
        return -(probs * np.log2(probs + 1e-9)).sum()

    def classify(self, example):
        return self.root.classify(example)

if __name__ == '__main__':
    random.seed(42)
    np.random.seed(42)
    
    path_to_csv = 'town_vax_data.csv'
    id_attr = 'town'
    class_attr = 'vax_level'
    min_leaf = 10

    data = load_data(path_to_csv)
    train_df = data.sample(frac=0.75, random_state=42)
    test_df = data.drop(train_df.index)

    tree = DecisionTree(train_df, id_attr, class_attr, min_leaf)

    correct = 0
    for _, row in test_df.iterrows():
        pred, _ = tree.classify(row)
        if pred == row[class_attr]:
            correct += 1
    
    accuracy = correct / len(test_df) if len(test_df) > 0 else 0
    print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# 1. Replaced row-wise dictionary processing with vectorized pandas and numpy operations for data loading and manipulation.
# 2. Optimized entropy calculation using pandas value_counts and numpy log2, significantly reducing computational overhead compared to manual loops and dictionary builds.
# 3. Streamlined the threshold search by pre-calculating parent entropy and using boolean masking for dataframe splits.
# 4. Reduced memory footprint by using pandas slicing and dropping NaN values in-place during split analysis instead of creating multiple list copies.
# 5. Simplified the model architecture by removing redundant abstract interfaces and consolidating tree-building logic into a unified recursive structure.
# 6. Improved efficiency of categorical mode calculation using pandas built-in functions for leaf node predictions.
# 7. Implemented robust CSV parsing with automated delimiter detection to prevent runtime errors during data ingestion.