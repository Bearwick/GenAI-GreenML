# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor


DATASET_PATH = "MNIST_train.csv"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = re.sub(r"\s+", " ", str(c).strip())
        out.append(c2)
    return out


def _read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
        # If only one column, parsing likely wrong delimiter
        if df.shape[1] <= 1:
            raise ValueError("Parsing likely wrong; retry with alternative separator.")
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _drop_unnamed(df):
    cols_to_drop = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if cols_to_drop:
        df = df.drop(columns=cols_to_drop, errors="ignore")
    return df


def _pick_target(df, expected_headers=None):
    cols_lower = {c.lower(): c for c in df.columns}
    # Prefer an explicitly named label/target if present
    for cand in ["label", "target", "y", "class"]:
        if cand in cols_lower:
            return cols_lower[cand]

    # Otherwise choose a non-constant numeric-like column (after coercion)
    best_col = None
    best_var = -np.inf
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().sum() < max(10, int(0.05 * len(df))):
            continue
        var = np.nanvar(s.to_numpy(dtype=float))
        if var > best_var:
            best_var = var
            best_col = c

    # Fallback to last column if nothing else
    return best_col if best_col is not None else df.columns[-1]


def _is_classification_target(y_series):
    y_nonnull = y_series.dropna()
    if y_nonnull.empty:
        return False
    # If integer-like and not too many unique values, treat as classification
    y_num = pd.to_numeric(y_nonnull, errors="coerce")
    if y_num.notna().mean() >= 0.95:
        y_intlike = np.all(np.isclose(y_num.dropna().to_numpy(), np.round(y_num.dropna().to_numpy())))
        n_unique = int(pd.Series(np.round(y_num.dropna())).nunique())
        if y_intlike and 2 <= n_unique <= 50:
            return True
    # Object/categorical with limited uniques
    n_unique_obj = int(y_nonnull.astype(str).nunique())
    if 2 <= n_unique_obj <= 50:
        return True
    return False


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    ss_res = float(np.sum((y_true - y_pred) ** 2))
    ss_tot = float(np.sum((y_true - float(np.mean(y_true))) ** 2))
    r2 = 0.0 if ss_tot <= 1e-12 else (1.0 - ss_res / ss_tot)
    # Bound to [0,1] as an "accuracy-like" proxy
    return float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))


def main():
    if not os.path.exists(DATASET_PATH):
        # Minimal fallback that still runs end-to-end
        accuracy = 0.0
        print(f"ACCURACY={accuracy:.6f}")
        return

    df = _read_csv_robust(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Ensure non-empty
    assert df.shape[0] > 0 and df.shape[1] > 0, "Dataset is empty after loading/cleanup."

    target_col = _pick_target(df)
    if target_col is None or target_col not in df.columns:
        target_col = df.columns[0]

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # If no feature columns, create a constant feature so pipeline can run
    if X.shape[1] == 0:
        X = pd.DataFrame({"__const__": np.ones(len(df), dtype=float)})

    # Identify numeric/categorical features robustly
    numeric_features = []
    categorical_features = []

    for c in X.columns:
        s_num = pd.to_numeric(X[c], errors="coerce")
        # Consider numeric if enough values parse as numbers
        if s_num.notna().mean() >= 0.8:
            numeric_features.append(c)
        else:
            categorical_features.append(c)

    # Build preprocessing (lightweight; scales only numeric, OHE only if categoricals exist)
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
        ]
    )

    transformers = []
    if numeric_features:
        transformers.append(("num", numeric_transformer, numeric_features))
    if categorical_features:
        transformers.append(("cat", categorical_transformer, categorical_features))

    if not transformers:
        # All features missing? create constant
        X = pd.DataFrame({"__const__": np.ones(len(df), dtype=float)})
        numeric_features = ["__const__"]
        transformers = [("num", numeric_transformer, numeric_features)]

    preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", sparse_threshold=0.3)

    # Decide task type
    is_clf = _is_classification_target(y_raw)

    # Train/test split with defensive behavior
    if is_clf:
        y_for_split = y_raw.astype(str)
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_for_split, test_size=0.2, random_state=RANDOM_STATE, stratify=y_for_split
            )
        except Exception:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_for_split, test_size=0.2, random_state=RANDOM_STATE
            )

        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

        n_classes = int(pd.Series(y_train).nunique())
        if n_classes < 2:
            model = DummyClassifier(strategy="most_frequent")
        else:
            # Use multinomial logistic regression (small, strong baseline for MNIST) with CPU-friendly solver
            model = LogisticRegression(
                max_iter=200,
                solver="saga",
                multi_class="auto",
                n_jobs=1,
                tol=1e-3,
                C=1.0,
            )

        clf = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        y_num = pd.to_numeric(y_raw, errors="coerce")
        # Drop rows with missing target to avoid undefined objective
        valid = y_num.notna()
        X2 = X.loc[valid].copy()
        y2 = y_num.loc[valid].astype(float)
        if len(y2) < 5:
            accuracy = 0.0
            print(f"ACCURACY={accuracy:.6f}")
            return

        X_train, X_test, y_train, y_test = train_test_split(
            X2, y2, test_size=0.2, random_state=RANDOM_STATE
        )
        assert len(X_train) > 0 and len(X_test) > 0, "Train/test split failed."

        # Lightweight linear regression with L2 regularization
        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        reg = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        try:
            reg.fit(X_train, y_train)
            y_pred = reg.predict(X_test)
            accuracy = _bounded_regression_score(y_test, y_pred)
        except Exception:
            dummy = Pipeline(steps=[("preprocess", preprocessor), ("model", DummyRegressor(strategy="mean"))])
            dummy.fit(X_train, y_train)
            y_pred = dummy.predict(X_test)
            accuracy = _bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Chosen model: multinomial LogisticRegression (saga) as a strong yet lightweight linear baseline for MNIST-like pixels; CPU-friendly and avoids deep learning/ensembles.
# - Preprocessing uses ColumnTransformer+Pipeline for reproducibility and single-pass transforms; numeric pixels are imputed+standardized, categoricals (if any) one-hot encoded.
# - Robust schema handling: target inferred from common names or non-constant numeric column; features inferred from actual df.columns with numeric coercion.
# - Robust CSV loading: retries with ';' separator and ',' decimal if default parsing yields a single-column dataframe.
# - Defensive checks ensure end-to-end execution: handles empty/degenerate datasets, missing features, single-class targets (DummyClassifier), and regression fallback with bounded R2 proxy mapped to [0,1].