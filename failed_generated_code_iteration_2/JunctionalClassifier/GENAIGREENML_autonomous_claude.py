# Generated by generate_llm_code.py
# LLM: claude
# Mode: autonomous

import pickle
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings("ignore")

# Load the pickle file
DATASET_PATH = "dict.pickle"

with open(DATASET_PATH, "rb") as f:
    data = pickle.load(f, encoding="latin1")

# Inspect what we got
if isinstance(data, dict):
    # Try to convert dict to DataFrame
    # Common patterns: {col_name: array, ...} or nested structures
    # Check if values are arrays/lists of same length
    if all(isinstance(v, (list, np.ndarray)) for v in data.values()):
        lengths = {k: len(v) if hasattr(v, '__len__') else 1 for k, v in data.items()}
        # If all same length, straightforward DataFrame
        unique_lengths = set(lengths.values())
        if len(unique_lengths) == 1:
            df = pd.DataFrame(data)
        else:
            # Try filtering to most common length
            from collections import Counter
            most_common_len = Counter(lengths.values()).most_common(1)[0][0]
            filtered = {k: v for k, v in data.items() if len(v) == most_common_len}
            df = pd.DataFrame(filtered)
    elif all(isinstance(v, dict) for v in data.values()):
        df = pd.DataFrame(data).T
    else:
        # Try direct conversion
        df = pd.DataFrame(data)
elif isinstance(data, (list, np.ndarray)):
    df = pd.DataFrame(data)
elif isinstance(data, pd.DataFrame):
    df = data
else:
    # Last resort
    df = pd.DataFrame([data]) if not hasattr(data, '__len__') else pd.DataFrame(data)

# Clean column names
df.columns = [str(c).strip() for c in df.columns]
df.columns = [' '.join(c.split()) for c in df.columns]
# Drop unnamed columns
df = df[[c for c in df.columns if not c.lower().startswith('unnamed')]]

assert df.shape[0] > 0, "Dataset is empty after loading"
assert df.shape[1] > 1, "Dataset has fewer than 2 columns"

# Based on README context: classification of retinal blood vessel junctions
# Labels are -1 (remodelling), 0 (mixed/uncertainty), 1 (inactive)
# Try to identify target column

# Look for a column that could be the target/label
target_col = None
feature_cols = []

# Heuristic: find columns with few unique values that look like labels
# Also check for common target column names
target_candidates = ['label', 'class', 'target', 'y', 'category', 'output', 'prediction']

for col in df.columns:
    if col.lower() in target_candidates:
        target_col = col
        break

if target_col is None:
    # Look for a column with values in {-1, 0, 1} or very few unique values
    for col in df.columns:
        try:
            vals = pd.to_numeric(df[col], errors='coerce').dropna()
            unique_vals = set(vals.unique())
            if unique_vals.issubset({-1, 0, 1, -1.0, 0.0, 1.0}) and len(unique_vals) >= 2:
                target_col = col
                break
        except:
            pass

if target_col is None:
    # Pick the column with fewest unique values (likely categorical target)
    # Exclude columns that are all unique (likely IDs)
    min_unique = float('inf')
    for col in df.columns:
        n_unique = df[col].nunique()
        if 2 <=