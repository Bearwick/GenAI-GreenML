# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge, LogisticRegression
from sklearn.metrics import accuracy_score, r2_score
from sklearn.dummy import DummyClassifier

warnings.filterwarnings("ignore")

path = "data/AirQualityUCI.csv"
try:
    df = pd.read_csv(path)
    if df.shape[1] <= 1 or any(';' in str(c) for c in df.columns):
        raise ValueError("bad parse")
except Exception:
    df = pd.read_csv(path, sep=';', decimal=',')

def clean_and_make_unique(cols):
    cleaned = [re.sub(r'\s+', ' ', str(c).strip()) for c in cols]
    seen = {}
    unique = []
    for c in cleaned:
        if c not in seen:
            seen[c] = 0
            unique.append(c)
        else:
            seen[c] += 1
            unique.append(f"{c}_{seen[c]}")
    return unique

df.columns = clean_and_make_unique(df.columns)
df = df.loc[:, [c for c in df.columns if c and str(c).lower() != 'nan' and not str(c).startswith('Unnamed')]]
df.replace(r'^\s*$', np.nan, regex=True, inplace=True)
df = df.dropna(axis=1, how='all')

numeric_cols = []
for col in df.columns:
    col_num = pd.to_numeric(df[col], errors='coerce')
    if col_num.notna().sum() > 0:
        numeric_cols.append(col)
        df[col] = col_num

if numeric_cols:
    df[numeric_cols] = df[numeric_cols].replace(-200, np.nan)

df.replace([np.inf, -np.inf], np.nan, inplace=True)
df = df.dropna(axis=1, how='all')

numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]

def select_target(dataframe, num_cols):
    candidates = [c for c in num_cols if dataframe[c].nunique(dropna=True) > 1]
    if candidates:
        preferred = [c for c in candidates if re.search(r'(target|label|risk|visit)', c, re.I)]
        if preferred:
            return preferred[0]
        return max(candidates, key=lambda c: dataframe[c].notna().sum())
    for c in dataframe.columns:
        if dataframe[c].nunique(dropna=True) > 1:
            return c
    return dataframe.columns[0]

target = select_target(df, numeric_cols)

y = df[target]
if target in numeric_cols:
    y = pd.to_numeric(y, errors='coerce')
mask = y.notna()
df_model = df.loc[mask].copy()
y = y.loc[mask]

df_model = df_model.dropna(axis=1, how='all')
numeric_cols = [c for c in numeric_cols if c in df_model.columns]
if target not in df_model.columns:
    df_model[target] = y
    if pd.api.types.is_numeric_dtype(y) and target not in numeric_cols:
        numeric_cols.append(target)

assert df_model.shape[0] > 0

feature_cols = [c for c in df_model.columns if c != target]
if len(feature_cols) == 0:
    df_model["constant"] = 1.0
    feature_cols = ["constant"]
    if "constant" not in numeric_cols:
        numeric_cols.append("constant")

X = df_model[feature_cols]

is_classification = False
if target not in numeric_cols:
    is_classification = True
else:
    unique_vals = y.dropna().unique()
    if len(unique_vals) <= 10 and np.all(np.equal(unique_vals, np.round(unique_vals))):
        is_classification = True

use_dummy = False
if is_classification:
    y_factorized, uniques = pd.factorize(y.astype(str))
    y = y_factorized
    if len(uniques) < 2:
        use_dummy = True
else:
    y = pd.to_numeric(y, errors='coerce')

numeric_features = [c for c in feature_cols if c in numeric_cols]
categorical_features = [c for c in feature_cols if c not in numeric_cols]
max_categories = 50
categorical_features = [c for c in categorical_features if df_model[c].nunique(dropna=True) <= max_categories]

transformers = []
if numeric_features:
    num_pipeline = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False))
    ])
    transformers.append(("num", num_pipeline, numeric_features))
if categorical_features:
    cat_pipeline = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse=True))
    ])
    transformers.append(("cat", cat_pipeline, categorical_features))

preprocess = ColumnTransformer(transformers=transformers, remainder='drop') if transformers else "passthrough"

if is_classification:
    model = DummyClassifier(strategy="most_frequent") if use_dummy else LogisticRegression(max_iter=200, solver="liblinear")
else:
    model = Ridge(alpha=1.0)

pipeline = Pipeline(steps=[("preprocess", preprocess), ("model", model)])

stratify = y if (is_classification and not use_dummy and len(np.unique(y)) > 1) else None
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=stratify)

assert X_train.shape[0] > 0 and X_test.shape[0] > 0

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

if is_classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    r2 = r2_score(y_test, y_pred)
    accuracy = float(np.clip((r2 + 1.0) / 2.0, 0.0, 1.0))

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Linear Ridge/Logistic models provide energy-efficient CPU training and inference.
# - Preprocessing uses simple imputation and limited one-hot encoding while dropping high-cardinality categories.
# - Regression accuracy is reported as a clipped (R2+1)/2 proxy to keep the score in [0,1].