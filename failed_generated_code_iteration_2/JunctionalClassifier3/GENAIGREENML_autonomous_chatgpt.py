# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import math
import pickle
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


RANDOM_STATE = 42


def _normalize_columns(cols):
    normed = []
    for c in cols:
        c2 = "" if c is None else str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        normed.append(c2)
    return normed


def _drop_unnamed_columns(df):
    drop_cols = [c for c in df.columns if isinstance(c, str) and (c.strip() == "" or c.startswith("Unnamed:"))]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _load_data_any(path):
    ext = os.path.splitext(path)[1].lower()
    if ext in [".csv", ".txt"]:
        try:
            df = pd.read_csv(path)
        except Exception:
            df = None
        if df is None or df.shape[1] <= 1:
            df = pd.read_csv(path, sep=";", decimal=",")
        return df
    elif ext in [".pickle", ".pkl"]:
        with open(path, "rb") as f:
            obj = pickle.load(f)
        if isinstance(obj, pd.DataFrame):
            return obj
        if isinstance(obj, dict):
            # Try common patterns: {"X":..., "y":...}, {"data":...}, etc.
            for k in ["df", "dataframe", "data", "train", "dataset"]:
                if k in obj and isinstance(obj[k], pd.DataFrame):
                    return obj[k]

            if "X" in obj and "y" in obj:
                X = obj["X"]
                y = obj["y"]
                if isinstance(X, pd.DataFrame):
                    df = X.copy()
                else:
                    X_arr = np.asarray(X)
                    df = pd.DataFrame(X_arr, columns=[f"f{i}" for i in range(X_arr.shape[1] if X_arr.ndim > 1 else 1)])
                df["target"] = np.asarray(y).ravel()
                return df

            if "features" in obj and "labels" in obj:
                X = obj["features"]
                y = obj["labels"]
                X_arr = np.asarray(X)
                df = pd.DataFrame(X_arr, columns=[f"f{i}" for i in range(X_arr.shape[1] if X_arr.ndim > 1 else 1)])
                df["target"] = np.asarray(y).ravel()
                return df

            # If it's a dict of arrays/lists with equal length, build dataframe
            try:
                lens = []
                keys = []
                for k, v in obj.items():
                    if isinstance(k, (str, int, float)):
                        vv = np.asarray(v)
                        if vv.ndim == 0:
                            continue
                        lens.append(len(vv))
                        keys.append(k)
                if lens and len(set(lens)) == 1:
                    data = {}
                    for k in keys:
                        data[str(k)] = np.asarray(obj[k])
                    return pd.DataFrame(data)
            except Exception:
                pass

            # Otherwise, last resort: try to interpret as list-like
            try:
                arr = np.asarray(obj)
                if arr.ndim == 2 and arr.shape[0] > 0 and arr.shape[1] > 0:
                    return pd.DataFrame(arr, columns=[f"f{i}" for i in range(arr.shape[1])])
            except Exception:
                pass

        # Try array-like
        arr = np.asarray(obj)
        if arr.ndim == 2 and arr.shape[0] > 0 and arr.shape[1] > 0:
            return pd.DataFrame(arr, columns=[f"f{i}" for i in range(arr.shape[1])])

        raise ValueError("Unsupported pickle content for dataset.")
    else:
        raise ValueError("Unsupported dataset extension.")


def _coerce_numeric_safely(df, cols):
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _choose_target_column(df):
    cols = list(df.columns)
    cols_l = [str(c).lower() for c in cols]
    preferred = ["target", "label", "labels", "y", "class", "classes", "output"]
    for p in preferred:
        if p in cols_l:
            return cols[cols_l.index(p)]

    # Next: object/categorical with low cardinality
    best = None
    best_unique = None
    for c in cols:
        s = df[c]
        nun = s.nunique(dropna=True)
        if nun is None or nun <= 0:
            continue
        if s.dtype == "object" or str(s.dtype).startswith("category") or str(s.dtype) == "bool":
            if nun >= 2 and (best_unique is None or nun < best_unique):
                best = c
                best_unique = nun

    if best is not None:
        return best

    # Next: numeric with low-ish cardinality (classification-style)
    num_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]
    for c in num_cols:
        s = df[c]
        nun = s.nunique(dropna=True)
        if nun is None:
            continue
        if 2 <= nun <= 50:
            return c

    # Else: any non-constant numeric (regression fallback)
    for c in num_cols:
        s = df[c]
        nun = s.nunique(dropna=True)
        if nun is not None and nun >= 2:
            return c

    # Else: last resort: first column
    return cols[0] if cols else None


def _bounded_regression_score(y_true, y_pred):
    y_true = np.asarray(y_true).astype(float)
    y_pred = np.asarray(y_pred).astype(float)
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return 0.0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    denom = np.var(y_true)
    if not np.isfinite(denom) or denom <= 1e-12:
        mae = np.mean(np.abs(y_true - y_pred))
        return float(1.0 / (1.0 + mae)) if np.isfinite(mae) else 0.0
    sse = np.mean((y_true - y_pred) ** 2)
    score = 1.0 - (sse / (denom + 1e-12))
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 1.0))


def main():
    dataset_path = "dict.pickle"
    df = _load_data_any(dataset_path)

    # Normalize columns and drop obvious index columns
    df = df.copy()
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed_columns(df)

    # Ensure 2D tabular
    assert isinstance(df, pd.DataFrame)
    assert df.shape[0] > 0 and df.shape[1] > 0

    # Coerce obvious numeric columns cautiously (only those that look numeric)
    # (This is lightweight and avoids scanning every cell expensively.)
    sample = df.head(min(200, len(df)))
    likely_num = []
    for c in df.columns:
        if pd.api.types.is_numeric_dtype(df[c]):
            likely_num.append(c)
            continue
        # Try parse on a small sample to see if mostly numeric
        s = sample[c]
        if s.dtype == "object":
            parsed = pd.to_numeric(s, errors="coerce")
            frac = float(np.isfinite(parsed).mean()) if len(parsed) else 0.0
            if frac >= 0.8:
                likely_num.append(c)
    df = _coerce_numeric_safely(df, likely_num)

    target_col = _choose_target_column(df)
    if target_col is None or target_col not in df.columns:
        # Construct a dummy target to keep pipeline running (trivial baseline)
        df["target"] = 0
        target_col = "target"

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # If no features remain, create a constant feature
    if X.shape[1] == 0:
        X = pd.DataFrame({"const": np.ones(len(df), dtype=float)})

    # Identify feature types for ColumnTransformer
    num_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    cat_features = [c for c in X.columns if c not in num_features]

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler(with_mean=False)),
            ]), num_features),
            ("cat", Pipeline(steps=[
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
            ]), cat_features),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )

    # Decide classification vs regression robustly
    task = "classification"
    y = y_raw.copy()

    # Normalize target: try numeric conversion; keep original for class detection
    y_num = pd.to_numeric(y, errors="coerce")
    unique_nonnull = y.dropna().unique()
    nunique = len(unique_nonnull)

    # Classification if target is non-numeric object OR numeric with few unique values
    if (y.dtype == "object" or str(y.dtype).startswith("category") or str(y.dtype) == "bool"):
        task = "classification"
    else:
        # numeric-like
        y_valid = y_num[np.isfinite(y_num)]
        nun = int(pd.Series(y_valid).nunique(dropna=True)) if len(y_valid) else 0
        if 2 <= nun <= 50:
            task = "classification"
        else:
            task = "regression"

    # Prepare y for modeling
    if task == "classification":
        # If numeric but sparse values, keep as int-like categories when possible
        if y.dtype != "object" and np.isfinite(y_num).any():
            # Fill NaN with mode of finite values
            finite = y_num[np.isfinite(y_num)]
            if len(finite) == 0:
                y_clean = pd.Series(np.zeros(len(y), dtype=int))
            else:
                mode_val = float(pd.Series(finite).mode(dropna=True).iloc[0])
                y_filled = y_num.fillna(mode_val)
                # Reduce float noise for labels
                y_clean = y_filled.round(6)
        else:
            y_clean = y.astype(str).fillna("nan")

        # If fewer than 2 classes, fallback to regression on numeric target if possible; else trivial baseline
        n_classes = int(pd.Series(y_clean).nunique(dropna=True))
        if n_classes < 2:
            if np.isfinite(y_num).sum() >= 3 and pd.Series(y_num[np.isfinite(y_num)]).nunique() >= 2:
                task = "regression"
                y_model = y_num
            else:
                # trivial baseline accuracy = 1.0 if only one class and we predict it
                accuracy = 1.0
                print(f"ACCURACY={accuracy:.6f}")
                return
        else:
            y_model = y_clean
            model = LogisticRegression(
                max_iter=200,
                solver="lbfgs",
                n_jobs=1,
            )
            pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
    if task == "regression":
        # Fill numeric target; if still invalid, trivial baseline
        if not np.isfinite(y_num).any():
            accuracy = 0.0
            print(f"ACCURACY={accuracy:.6f}")
            return
        finite = y_num[np.isfinite(y_num)]
        fill_val = float(np.median(finite)) if len(finite) else 0.0
        y_model = y_num.fillna(fill_val)
        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_model, test_size=0.2, random_state=RANDOM_STATE, shuffle=True
    )
    assert len(X_train) > 0 and len(X_test) > 0

    # Fit and evaluate
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)

    if task == "classification":
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        accuracy = _bounded_regression_score(y_test, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) for CPU efficiency and strong baselines on tabular data.
# - ColumnTransformer + Pipeline ensures reproducible preprocessing with minimal redundant computation.
# - OneHotEncoder(handle_unknown="ignore") avoids costly schema-specific handling and prevents failures on unseen categories.
# - Median/most_frequent imputers are fast, robust, and avoid iterative or model-based imputation.
# - StandardScaler(with_mean=False) is sparse-friendly and avoids dense conversions, reducing memory/CPU.
# - Robust target selection avoids hard-coded column names and falls back safely to regression or trivial baseline.
# - Regression "accuracy" is a bounded [0,1] proxy derived from variance-normalized MSE (clipped), stable across scales.