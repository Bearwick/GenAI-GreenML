# Generated by generate_llm_code.py
# LLM: gemini
# Mode: autonomous

import pandas as pd
import numpy as np
import pickle
import os
import sys
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

def load_data(path):
    # Robust loading for pickle or fallback to CSV
    df = None
    if os.path.exists(path):
        try:
            with open(path, 'rb') as f:
                data = pickle.load(f)
            if isinstance(data, pd.DataFrame):
                df = data
            elif isinstance(data, dict):
                # Try to find a dataframe in the dictionary
                for key in data:
                    if isinstance(data[key], pd.DataFrame):
                        df = data[key]
                        break
        except Exception:
            pass
    
    if df is None:
        # Fallback to searching for CSV files if pickle fails or is missing
        csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]
        # Prioritize 14k.csv as mentioned in context
        target_csv = '14k.csv' if '14k.csv' in csv_files else (csv_files[0] if csv_files else None)
        
        if target_csv:
            try:
                df = pd.read_csv(target_csv)
            except Exception:
                try:
                    df = pd.read_csv(target_csv, sep=';', decimal=',')
                except Exception:
                    return pd.DataFrame()
        else:
            return pd.DataFrame()
    return df

def clean_df(df):
    if df.empty:
        return df
    # Normalize column names
    df.columns = [str(c).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    
    # Coerce numeric columns
    for col in df.columns:
        try:
            # Try converting to numeric, if mostly numeric
            converted = pd.to_numeric(df[col], errors='coerce')
            if converted.notnull().sum() > (len(df) * 0.5):
                df[col] = converted
        except Exception:
            continue
    return df

def select_target_and_features(df):
    cols = list(df.columns)
    if not cols:
        return None, None
    
    target_col = None
    # 1. Look for context-specific names
    context_hints = ['class', 'target', 'label', 'junction', 'remodelling', 'output']
    for c in cols:
        if any(hint in c.lower() for hint in context_hints):
            target_col = c
            break
            
    # 2. Look for the specific values mentioned in README (-1, 0, 1)
    if not target_col:
        for c in cols:
            unique_vals = set(df[c].dropna().unique())
            if unique_vals.issubset({-1, 0, 1, -1.0, 0.0, 1.0}) and len(unique_vals) >= 2:
                target_col = c
                break
                
    # 3. Default to last column
    if not target_col:
        target_col = cols[-1]
        
    features = [c for c in cols if c != target_col]
    # Filter only numeric or low-cardinality features for the baseline
    numeric_features = df[features].select_dtypes(include=[np.number]).columns.tolist()
    
    return target_col, numeric_features

def run_pipeline():
    df = load_data('dict.pickle')
    df = clean_df(df)
    
    if df.empty or len(df) < 5:
        # Trivial fallback for empty/tiny data
        print(f"ACCURACY={0.000000:.6f}")
        return

    target, features = select_target_and_features(df)
    
    if not target or not features:
        print(f"ACCURACY={0.000000:.6f}")
        return

    # Drop rows with missing target
    df = df.dropna(subset=[target])
    
    X = df[features]
    y = df[target]

    # Handle categorical target if necessary
    if y.dtype == 'object' or len(np.unique(y)) > 20:
        # If it looks continuous, treat it as regression proxy
        le = LabelEncoder()
        try:
            y = le.fit_transform(y.astype(str))
        except:
            pass

    # Ensure we have at least 2 classes for classification
    if len(np.unique(y)) < 2:
        print(f"ACCURACY={1.000000:.6f}")
        return

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Pipeline: Simple Imputer -> Scaler -> Logistic Regression (Energy Efficient)
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(max_iter=500, solver='lbfgs', multi_class='auto'))
    ])

    try:
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
    except Exception:
        acc = 0.0

    print(f"ACCURACY={acc:.6f}")

if __name__ == "__main__":
    run_pipeline()

# Optimization Summary:
# 1. Model Choice: Logistic Regression was selected over SVM or ANN as it is computationally 
#    lightweight (O(n_samples * n_features)), runs efficiently on CPU, and is highly energy-efficient.
# 2. Data Loading: Implemented a robust fallback system that attempts to load from pickle, 
#    then searches for CSVs, ensuring the pipeline doesn't crash regardless of environment state.
# 3. Preprocessing: Used a Scikit-Learn Pipeline with SimpleImputer and StandardScaler. 
#    Standardization ensures faster convergence for the solver, reducing CPU cycles.
# 4. Feature Selection: Automatically identifies numeric columns to avoid heavy categorical 
#    encoding or text processing, minimizing memory footprint.
# 5. Schema Robustness: Used string normalization and substring matching to find labels 
#    (-1, 0, 1) dynamically, making the solution resilient to header variations.
# 6. Resource Usage: Avoided heavy ensembles (RandomForest/XGBoost) to keep memory 
#    usage low and prevent excessive heat generation during training.