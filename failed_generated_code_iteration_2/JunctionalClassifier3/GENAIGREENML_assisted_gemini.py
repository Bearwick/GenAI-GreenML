# Generated by generate_llm_code.py
# LLM: gemini
# Mode: assisted

import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import os
import pickle

def load_data(path):
    if not os.path.exists(path):
        return None
    try:
        df = pd.read_csv(path, engine='c', low_memory=True)
        if df.shape[1] <= 1:
            df = pd.read_csv(path, sep=';', decimal=',', engine='c')
        return df
    except:
        try:
            with open(path, 'rb') as f:
                data = pickle.load(f)
                if isinstance(data, pd.DataFrame):
                    return data
                if isinstance(data, dict):
                    return pd.DataFrame(data)
                if isinstance(data, np.ndarray):
                    return pd.DataFrame(data)
        except:
            return None
    return None

def main():
    target_path = '14k.csv'
    alt_path = 'dict.pickle'
    
    dataset_path = target_path if os.path.exists(target_path) else alt_path
    df = load_data(dataset_path)
    
    if df is None:
        return

    df = df.dropna(axis=1, how='all').dropna(axis=0, how='any')
    
    numeric_data = df.apply(pd.to_numeric, errors='coerce').dropna()
    
    if numeric_data.empty:
        return

    X = numeric_data.iloc[:, :-1].values
    y_raw = numeric_data.iloc[:, -1].values
    
    y = np.where(y_raw > 0, 1, np.where(y_raw < 0, -1, 0))

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    clf = SVC(kernel='linear', random_state=42, cache_size=1000)
    clf.fit(X_train, y_train)
    
    predictions = clf.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    
    print(f"ACCURACY={accuracy:.6f}")

if __name__ == "__main__":
    main()

# Optimization Summary
# 1. Replaced manual row-by-row CSV parsing and list appending with pandas.read_csv for faster I/O and reduced memory overhead.
# 2. Utilized vectorized numpy.where operations for label transformation instead of Python loops and conditional logic.
# 3. Eliminated global variables to improve memory management and garbage collection efficiency.
# 4. Implemented a robust data loading mechanism with automatic delimiter detection and pickle fallback.
# 5. Increased SVC cache_size to 1000MB to reduce computation time during the optimization phase of the linear kernel.
# 6. Set a fixed random_state (42) in both train_test_split and SVC to ensure reproducibility.
# 7. Removed redundant file saving (pickle.dump) and repeated disk access to minimize energy consumption and I/O wait times.
# 8. Optimized data cleaning using dropna and vectorized pd.to_numeric to handle inconsistent CSV formatting efficiently.