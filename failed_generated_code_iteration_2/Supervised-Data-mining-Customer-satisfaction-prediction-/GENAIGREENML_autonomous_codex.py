# Generated by generate_llm_code.py
# LLM: codex
# Mode: autonomous

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, r2_score
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.dummy import DummyClassifier, DummyRegressor

warnings.filterwarnings("ignore")

DATASET_PATH = "EireJet (1).csv"
DATASET_HEADERS = [
    "Gender", "Frequent Flyer", "Age", "Type of Travel", "Class",
    "Flight Distance", "Inflight wifi service", "Departure/Arrival time convenient",
    "Ease of Online booking", "Gate location", "Food and drink", "Online boarding",
    "Seat comfort", "Inflight entertainment", "On-board service", "Leg room service",
    "Baggage handling", "Checkin service", "Inflight service", "Cleanliness",
    "Departure Delay in Minutes", "Arrival Delay in Minutes", "satisfaction"
]

def read_csv_robust(path):
    df = None
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None
    if df is None or df.shape[1] <= 1:
        try:
            df = pd.read_csv(path, sep=';', decimal=',')
        except Exception:
            pass
    if df is None:
        raise FileNotFoundError(f"Unable to read dataset at {path}")
    return df

df = read_csv_robust(DATASET_PATH)

if isinstance(df.columns, pd.RangeIndex) or all(str(c).isdigit() for c in df.columns):
    if len(df.columns) == len(DATASET_HEADERS):
        df.columns = DATASET_HEADERS

def normalize_col(c):
    return re.sub(r'\s+', ' ', str(c).strip())

df.columns = [normalize_col(c) for c in df.columns]
df = df.loc[:, [c for c in df.columns if not re.match(r'^Unnamed', c, flags=re.IGNORECASE)]]

counts = {}
new_cols = []
for c in df.columns:
    if c in counts:
        counts[c] += 1
        new_cols.append(f"{c}_{counts[c]}")
    else:
        counts[c] = 0
        new_cols.append(c)
df.columns = new_cols

lower_cols = [c.lower() for c in df.columns]
target_col = None
target_from_numeric = False
for cand in ["satisfaction", "target", "label", "y"]:
    if cand in lower_cols:
        target_col = df.columns[lower_cols.index(cand)]
        break

if target_col is None:
    for col in df.columns:
        series = pd.to_numeric(df[col], errors='coerce')
        if series.notna().sum() > 0 and series.nunique(dropna=True) > 1:
            target_col = col
            target_from_numeric = True
            break
if target_col is None:
    for col in df.columns:
        if df[col].nunique(dropna=True) > 1:
            target_col = col
            break
if target_col is None:
    target_col = df.columns[0]

y_raw = df[target_col]

is_classification = False
if not target_from_numeric:
    if pd.api.types.is_object_dtype(y_raw) or pd.api.types.is_bool_dtype(y_raw) or str(y_raw.dtype).startswith("category"):
        is_classification = True

if is_classification:
    y_clean = y_raw.copy()
    mask = y_clean.notna()
    df = df.loc[mask].copy()
    y_clean = y_clean.loc[mask].astype(str)
    le = LabelEncoder()
    y = le.fit_transform(y_clean)
else:
    y_num = pd.to_numeric(y_raw, errors='coerce')
    y_num = y_num.replace([np.inf, -np.inf], np.nan)
    mask = y_num.notna()
    df = df.loc[mask].copy()
    y = y_num.loc[mask].astype(float).values

df = df.replace([np.inf, -np.inf], np.nan)

feature_cols = [c for c in df.columns if c != target_col]
if len(feature_cols) == 0:
    df["_constant_feature"] = 1
    feature_cols = ["_constant_feature"]

numeric_features = []
categorical_features = []
for col in feature_cols:
    series = pd.to_numeric(df[col], errors='coerce')
    non_na_ratio = series.notna().mean()
    if non_na_ratio >= 0.6:
        numeric_features.append(col)
        df[col] = series
    else:
        categorical_features.append(col)

transformers = []
if numeric_features:
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler(with_mean=False))
    ])
    transformers.append(('num', numeric_transformer, numeric_features))
if categorical_features:
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    transformers.append(('cat', categorical_transformer, categorical_features))
if not transformers:
    transformers = [('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median'))]), feature_cols)]

preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')

assert df.shape[0] > 0
X = df[feature_cols]

y_arr = np.asarray(y)
n_samples = len(y_arr)
test_size = 0.2 if n_samples >= 5 else 0.5

if is_classification:
    n_classes = len(np.unique(y_arr))
    stratify = None
    if n_classes > 1:
        counts = np.bincount(y_arr)
        if counts.min() >= 2 and n_samples >= 4:
            stratify = y_arr
    if n_samples < 2:
        X_train, X_test, y_train, y_test = X, X, y_arr, y_arr
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_arr, test_size=test_size, random_state=42, stratify=stratify
        )
    assert len(y_train) > 0 and len(y_test) > 0
    if n_classes < 2:
        model = DummyClassifier(strategy='most_frequent')
    else:
        model = LogisticRegression(max_iter=200, solver='liblinear')
else:
    if n_samples < 2:
        X_train, X_test, y_train, y_test = X, X, y_arr.astype(float), y_arr.astype(float)
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_arr.astype(float), test_size=test_size, random_state=42
        )
    assert len(y_train) > 0 and len(y_test) > 0
    if np.unique(y_arr).shape[0] < 2:
        model = DummyRegressor(strategy='mean')
    else:
        model = Ridge(alpha=1.0)

clf = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

if is_classification:
    accuracy = accuracy_score(y_test, y_pred)
else:
    try:
        r2 = r2_score(y_test, y_pred)
        if np.isnan(r2):
            r2 = 0.0
    except Exception:
        r2 = 0.0
    accuracy = (r2 + 1.0) / 2.0
    if accuracy < 0.0:
        accuracy = 0.0
    if accuracy > 1.0:
        accuracy = 1.0

print(f"ACCURACY={accuracy:.6f}")

# Optimization Summary
# - Robust CSV loading with fallback delimiter/decimal ensures reliable parsing without manual edits.
# - Lightweight preprocessing (imputation, scaling, one-hot) via a single ColumnTransformer keeps the pipeline efficient and reproducible.
# - Automatic target selection and safe fallbacks prevent schema-related failures and allow end-to-end execution.
# - LogisticRegression/Ridge/Dummy models provide CPU-friendly baselines with low energy use.
# - Regression accuracy proxy maps r2 to [0,1] using (r2+1)/2 for stable reporting.