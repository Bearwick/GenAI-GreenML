# Generated by generate_llm_code.py
# LLM: chatgpt
# Mode: autonomous

import os
import re
import pickle
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score


DATASET_PATH = "dict.pickle"
RANDOM_STATE = 42


def _normalize_columns(cols):
    out = []
    for c in cols:
        c2 = str(c)
        c2 = c2.strip()
        c2 = re.sub(r"\s+", " ", c2)
        out.append(c2)
    return out


def _drop_unnamed(df):
    drop_cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed:")]
    if drop_cols:
        df = df.drop(columns=drop_cols, errors="ignore")
    return df


def _try_read_csv(path):
    # Robust fallback: try default, then ; separator with decimal comma
    try:
        df = pd.read_csv(path)
        if df.shape[1] <= 1:
            df2 = pd.read_csv(path, sep=";", decimal=",")
            if df2.shape[1] > df.shape[1]:
                df = df2
    except Exception:
        df = pd.read_csv(path, sep=";", decimal=",")
    return df


def _load_dataset_any(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"Dataset not found at: {path}")

    ext = os.path.splitext(path)[1].lower()

    if ext in [".csv", ".tsv", ".txt"]:
        df = _try_read_csv(path)
        df.columns = _normalize_columns(df.columns)
        df = _drop_unnamed(df)
        return df

    if ext in [".pkl", ".pickle"]:
        with open(path, "rb") as f:
            obj = pickle.load(f)

        # Handle multiple plausible pickle payload types defensively
        if isinstance(obj, pd.DataFrame):
            df = obj.copy()
            df.columns = _normalize_columns(df.columns)
            df = _drop_unnamed(df)
            return df

        if isinstance(obj, dict):
            # Common cases:
            # 1) {"X":..., "y":...} or similar
            # 2) dict of columns -> arrays/lists
            # 3) dict of row_id -> dict/sequence
            keys_norm = {str(k).strip().lower(): k for k in obj.keys()}
            x_key = None
            y_key = None
            for cand in ["y", "target", "label", "labels", "class"]:
                if cand in keys_norm:
                    y_key = keys_norm[cand]
                    break
            for cand in ["x", "data", "features", "inputs"]:
                if cand in keys_norm:
                    x_key = keys_norm[cand]
                    break

            if x_key is not None:
                X = obj.get(x_key)
                if isinstance(X, pd.DataFrame):
                    dfX = X.copy()
                else:
                    dfX = pd.DataFrame(X)
                dfX.columns = _normalize_columns(dfX.columns)
                dfX = _drop_unnamed(dfX)

                if y_key is not None:
                    y = obj.get(y_key)
                    y_ser = pd.Series(y, name="target")
                    df = dfX.copy()
                    # Align lengths safely
                    n = min(len(df), len(y_ser))
                    df = df.iloc[:n].copy()
                    df["target"] = y_ser.iloc[:n].values
                    return df
                return dfX

            # Try as dict of columns
            try:
                df = pd.DataFrame(obj)
                df.columns = _normalize_columns(df.columns)
                df = _drop_unnamed(df)
                return df
            except Exception:
                pass

            # Try dict of rows
            try:
                df = pd.DataFrame.from_dict(obj, orient="index")
                df.columns = _normalize_columns(df.columns)
                df = _drop_unnamed(df)
                return df
            except Exception:
                pass

        if isinstance(obj, (list, tuple, np.ndarray)):
            df = pd.DataFrame(obj)
            df.columns = _normalize_columns(df.columns)
            df = _drop_unnamed(df)
            return df

        # Last resort: try to coerce to DataFrame
        df = pd.DataFrame(obj)
        df.columns = _normalize_columns(df.columns)
        df = _drop_unnamed(df)
        return df

    raise ValueError(f"Unsupported dataset format: {ext}")


def _is_probably_target_col(series, name):
    n = len(series)
    if n < 4:
        return False
    nunique = series.nunique(dropna=True)
    if nunique <= 1:
        return False
    lname = str(name).strip().lower()
    if lname in ("target", "label", "class", "y", "output"):
        return True
    if lname.startswith("target") or lname.startswith("label"):
        return True
    return False


def _choose_target(df):
    # Prefer explicit-ish target columns
    for c in df.columns:
        if _is_probably_target_col(df[c], c):
            return c

    # Otherwise: pick a non-constant numeric column with small-ish unique count for classification
    numeric_candidates = []
    for c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        nunique = s.nunique(dropna=True)
        if nunique > 1:
            numeric_candidates.append((c, nunique, s.notna().mean()))
    numeric_candidates.sort(key=lambda t: (-t[2], t[1]))  # prioritize completeness, then lower cardinality

    if numeric_candidates:
        # If any have very low unique count, treat as classification target
        for c, nunique, _ in numeric_candidates:
            if nunique <= 20:
                return c
        # Else pick the most complete numeric as regression target fallback
        return numeric_candidates[0][0]

    # Else: pick a non-constant object column (classification)
    obj_candidates = []
    for c in df.columns:
        s = df[c].astype("string")
        nunique = s.nunique(dropna=True)
        if nunique > 1:
            obj_candidates.append((c, nunique, s.notna().mean()))
    obj_candidates.sort(key=lambda t: (-t[2], t[1]))
    if obj_candidates:
        return obj_candidates[0][0]

    return None


def _safe_coerce_numeric_df(df, cols):
    out = df.copy()
    for c in cols:
        out[c] = pd.to_numeric(out[c], errors="coerce")
    return out


def _build_preprocessor(X):
    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = [c for c in X.columns if c not in numeric_cols]

    numeric_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False)),
    ])

    categorical_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_pipe, numeric_cols),
            ("cat", categorical_pipe, categorical_cols),
        ],
        remainder="drop",
        sparse_threshold=0.3,
    )
    return preprocessor, numeric_cols, categorical_cols


def _bounded_regression_score(y_true, y_pred):
    # Stable accuracy proxy in [0,1]: 1 / (1 + MAE / (IQR + eps))
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = np.mean(np.abs(y_true - y_pred))
    q75, q25 = np.nanpercentile(y_true, [75, 25])
    iqr = float(q75 - q25)
    eps = 1e-12
    return float(1.0 / (1.0 + (mae / (iqr + eps))))


def main():
    df = _load_dataset_any(DATASET_PATH)
    df.columns = _normalize_columns(df.columns)
    df = _drop_unnamed(df)

    # Remove fully empty columns/rows
    df = df.dropna(axis=1, how="all")
    df = df.dropna(axis=0, how="all")

    assert df is not None and isinstance(df, pd.DataFrame)
    assert df.shape[0] > 0 and df.shape[1] > 0

    target_col = _choose_target(df)
    if target_col is None:
        # Trivial: no usable target; define proxy target as row index parity to allow end-to-end run
        df = df.copy()
        df["target"] = (np.arange(len(df)) % 2).astype(int)
        target_col = "target"

    y_raw = df[target_col]
    X = df.drop(columns=[target_col], errors="ignore")

    # If no features, create a constant feature so pipeline runs
    if X.shape[1] == 0:
        X = pd.DataFrame({"const": np.ones(len(df), dtype=float)})

    # Attempt to coerce object columns that look numeric
    obj_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()
    X_tmp = X.copy()
    for c in obj_cols:
        coerced = pd.to_numeric(X_tmp[c], errors="coerce")
        # If most values become numeric, keep numeric
        if coerced.notna().mean() >= 0.8:
            X_tmp[c] = coerced
    X = X_tmp

    # Clean infinities
    X = X.replace([np.inf, -np.inf], np.nan)

    # Determine task
    y_name = str(target_col).strip().lower()
    y_num = pd.to_numeric(y_raw, errors="coerce")
    y_is_numeric = y_num.notna().mean() >= 0.9

    is_classification = False
    if not y_is_numeric:
        is_classification = True
        y = y_raw.astype("string").fillna("NA")
    else:
        nunique = y_num.nunique(dropna=True)
        # Heuristic: small unique count suggests classification labels
        if nunique >= 2 and nunique <= 20:
            is_classification = True
            y = y_num.astype(int)
        else:
            is_classification = False
            y = y_num.astype(float)

    # If classification but only one class -> regression fallback
    if is_classification:
        y_nonan = pd.Series(y).dropna()
        if y_nonan.nunique() < 2:
            is_classification = False
            y = y_num.astype(float)

    # Train/test split
    if is_classification:
        strat = y if pd.Series(y).nunique(dropna=True) >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=strat
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE
        )

    assert len(X_train) > 0 and len(X_test) > 0

    preprocessor, _, _ = _build_preprocessor(X_train)

    if is_classification:
        # Small, CPU-friendly linear classifier; saga handles sparse one-hot efficiently
        model = LogisticRegression(
            solver="saga",
            penalty="l2",
            C=1.0,
            max_iter=200,
            n_jobs=1,
            random_state=RANDOM_STATE,
        )
        pipe = Pipeline(steps=[("prep", preprocessor), ("model", model)])
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        accuracy = float(accuracy_score(y_test, y_pred))
    else:
        # Ridge is lightweight, stable with mixed/collinear features
        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
        pipe = Pipeline(steps=[("prep", preprocessor), ("model", model)])
        # Drop rows with missing y for regression
        train_mask = pd.Series(y_train).notna().values
        test_mask = pd.Series(y_test).notna().values
        X_train2 = X_train.iloc[np.where(train_mask)[0]].copy()
        y_train2 = np.asarray(pd.Series(y_train).iloc[np.where(train_mask)[0]], dtype=float)
        X_test2 = X_test.iloc[np.where(test_mask)[0]].copy()
        y_test2 = np.asarray(pd.Series(y_test).iloc[np.where(test_mask)[0]], dtype=float)

        if len(X_train2) == 0 or len(X_test2) == 0:
            # Trivial baseline if target missing
            accuracy = 0.0
        else:
            pipe.fit(X_train2, y_train2)
            y_pred = pipe.predict(X_test2)
            accuracy = _bounded_regression_score(y_test2, y_pred)

    print(f"ACCURACY={accuracy:.6f}")


if __name__ == "__main__":
    main()

# Optimization Summary
# - Uses lightweight linear models (LogisticRegression/Ridge) for CPU efficiency and strong baselines on tabular data.
# - Reproducible split via fixed random_state; minimal stdout (only ACCURACY=...).
# - Robust schema handling: loads pickle/CSV, normalizes headers, drops Unnamed columns, selects target defensively.
# - ColumnTransformer + simple imputers + sparse one-hot keeps preprocessing efficient; StandardScaler(with_mean=False) avoids densifying sparse matrices.
# - Falls back safely if classification target is degenerate; regression uses bounded [0,1] proxy score: 1/(1+MAE/(IQR+eps)).